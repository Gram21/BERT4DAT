{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-djxLyuc-Opk"
   },
   "source": [
    "# Prepare\n",
    "Install required libraries and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Epk5taxa99eI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr6bTWdl-XzF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rdgM39FGZpM"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QyiWRCDw8HW"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertAdam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L57NdoEnLQa2"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azCEB4CuQk9A"
   },
   "source": [
    "Check, if and what kind of GPU is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wtzha3q7QjjU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(cuda_available)\n",
    "if cuda_available:\n",
    "    curr_device = torch.cuda.current_device()\n",
    "    print(torch.cuda.get_device_name(curr_device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvckhVpveeA5"
   },
   "source": [
    "Set up where the data comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYcCTWgMediC"
   },
   "outputs": [],
   "source": [
    "data_folder = './data/'\n",
    "data_filenames = ['1_classCorpus_Shrunk.tsv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVU_viFX-ezy"
   },
   "source": [
    "To import the dataset, first we have to connect to our Google drive (if data should be loaded from gdrive). For this, we have to authenticating the access and mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmGISBrhW-VJ"
   },
   "outputs": [],
   "source": [
    "load_from_gdrive = False\n",
    "\n",
    "if load_from_gdrive:\n",
    "    from google.colab import drive\n",
    "    # Connect to drive to load the corpus from there\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    data_folder = data_folder.replace('.', '/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOKXVgJgGtYV"
   },
   "source": [
    "Create a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0lgLyC6Gsnf"
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "config = Config(\n",
    "    model_path = './model/',\n",
    "    model_name = 'BERT4DAT_e20_1_classCorpus_BegOnly_512.pkl', # TODO adapt to your needs!\n",
    "    log_file = './log/classifierResults.txt',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "er1yzLHFQq1U"
   },
   "outputs": [],
   "source": [
    "def logLine(line):\n",
    "    with open(config.log_file, 'a') as log:\n",
    "        log.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ptp6NhIC_FQb"
   },
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeaTvNRTypP0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    fpath = data_folder + filename\n",
    "    df = pd.read_csv(fpath, sep='\\t', usecols=['file', 'label', 'text'])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def load_all_data(filenames):\n",
    "    df = load_data(filenames[0])\n",
    "    for i in range(1, len(filenames)):\n",
    "        df = df.append(load_data(filenames[i]))\n",
    "    return df\n",
    "\n",
    "# load the dataset(s)\n",
    "df = load_all_data(data_filenames)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bp4M3Jvwdmv"
   },
   "source": [
    "# BERT-related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uyb0kvJwitY"
   },
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
    "\n",
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]\n",
    "\n",
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SClv488eQC8B"
   },
   "source": [
    "# Predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prtJ_TGhC2PO"
   },
   "source": [
    "Create a predictor class. Just uses the prediction of the classifier/learner, but labels with confidentiality below a threshold get labeled as 'unrelated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qubb_Ka-C78O"
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, classifier, threshold=0.85, default_value =  'unrelated'):\n",
    "        self.classifier = classifier\n",
    "        self.threshold = threshold\n",
    "        self.classes = self.classifier.data.classes\n",
    "        self.default_value = default_value\n",
    "\n",
    "    def predict(self, text):\n",
    "        prediction = self.classifier.predict(text)\n",
    "        prediction_class = prediction[1]\n",
    "        prob = prediction[2][prediction_class].item()\n",
    "        if prob > self.threshold:\n",
    "            return self.classes[prediction_class]\n",
    "        else: return self.default_value   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaSiIy2jNHiU"
   },
   "source": [
    "# Load and use\n",
    "Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7erioqzNHLi"
   },
   "outputs": [],
   "source": [
    "classifier = load_learner(config.model_path, config.model_name)\n",
    "predictor = Predictor(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5Lr_rK_RwWI"
   },
   "source": [
    "Predict/classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWj98hQKNXsw"
   },
   "outputs": [],
   "source": [
    "logLine(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "for index, row in df.iterrows():\n",
    "    filename = row['file']\n",
    "    class_text = row['text']\n",
    "    label = row['label']\n",
    "    prediction = predictor.predict(class_text)\n",
    "    log_text = '{} -> {} ({})'.format(filename, prediction, label)\n",
    "    logLine(log_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT4DAT_useClassifier.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
