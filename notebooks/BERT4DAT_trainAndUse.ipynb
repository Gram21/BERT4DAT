{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT4DAT_trainAndUse.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-djxLyuc-Opk"
      },
      "source": [
        "# Prepare\n",
        "Install required libraries and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Epk5taxa99eI",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fr6bTWdl-XzF",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rdgM39FGZpM",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpZkAwZl0DaA",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertAdam\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L57NdoEnLQa2",
        "colab": {}
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azCEB4CuQk9A"
      },
      "source": [
        "Check, if and what kind of GPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wtzha3q7QjjU",
        "colab": {}
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "print(cuda_available)\n",
        "if cuda_available:\n",
        "    curr_device = torch.cuda.current_device()\n",
        "    print(torch.cuda.get_device_name(curr_device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOKXVgJgGtYV"
      },
      "source": [
        "Create a config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0lgLyC6Gsnf",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "config = Config(\n",
        "    bert_model_name=\"bert-base-uncased\", # default: \"bert-base-uncased\", alt: \"bert-large-uncased\"\n",
        "    max_lr=2e-5, # default: 3e-5\n",
        "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
        "    epochs=10,\n",
        "    use_fp16=False, # default: False\n",
        "    bs=8, # default: 2 or 4\n",
        "    max_seq_len=512,\n",
        "    train_size=0.9,\n",
        "    use_bertAdam=True,\n",
        "    loss_func=nn.CrossEntropyLoss(), #default: None or nn.CrossEntropyLoss()\n",
        "    seed=904727489, #default: 904727489, 424242 (reproducibility) or None\n",
        ")\n",
        "\n",
        "config_use = Config(\n",
        "    threshold = 0.75,\n",
        "    data_folder = './data/',\n",
        "    data_filenames = ['Hadoop_BegOnly_512.tsv'],\n",
        "    model_path = './model/',\n",
        "    model_name = 'BERT4DAT.pkl',\n",
        "    log_file = './log/classifierResults' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        ")\n",
        "\n",
        "load_from_gdrive = True\n",
        "save_model = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rvckhVpveeA5"
      },
      "source": [
        "Set up where the data comes from"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYcCTWgMediC",
        "colab": {}
      },
      "source": [
        "data_folder = './data/'\n",
        "data_filenames = ['1_classCorpus_BegOnly_512.tsv']\n",
        "# 1_classCorpus_BegOnly_512\n",
        "# 1_classCorpus_Shrunk\n",
        "# 2_classCorpus_BegOnly_512\n",
        "# 2_classCorpus_Shrunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVU_viFX-ezy"
      },
      "source": [
        "To import the dataset, first we have to connect to our Google drive (if data should be loaded from gdrive). For this, we have to authenticating the access and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmGISBrhW-VJ",
        "colab": {}
      },
      "source": [
        "if load_from_gdrive:\n",
        "    from google.colab import drive\n",
        "    # Connect to drive to load the corpus from there\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    data_folder = data_folder.replace('.', '/content/drive/My Drive')\n",
        "    config_use.model_path = data_folder\n",
        "    config_use.data_folder = config_use.data_folder.replace('.', '/content/drive/My Drive')\n",
        "    config_use.log_file = config_use.log_file.replace('.', config_use.data_folder, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "er1yzLHFQq1U",
        "colab": {}
      },
      "source": [
        "def logLine(line):\n",
        "    with open(config_use.log_file, 'a') as log:\n",
        "        log.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE5NcUY6_Hja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_info():\n",
        "    model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, msl: {}, train_size: {}, BERT-Adam: {}, FP16: {}, Loss: {}, Threshold: {}, Seed: {}, Data: {}'.format(config.bert_model_name, config.max_lr, config.epochs, config.bs, config.max_seq_len, config.train_size, config.use_bertAdam, config.use_fp16, config.loss_func, config_use.threshold, config.seed, data_filenames)\n",
        "    return model_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ptp6NhIC_FQb"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JmN-0wJEBAEH"
      },
      "source": [
        "Create proper tokenizer for our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6anB63ppBAtB",
        "colab": {}
      },
      "source": [
        "\n",
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1G8rFbEEJWyu"
      },
      "source": [
        "Now, we can create our own databunch using the tokenizer above. Notice we're passing the include_bos=False and include_eos=False options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens.\n",
        "\n",
        "We can pass our own list of Preprocessors to the databunch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNRRj6jIJrp2",
        "colab": {}
      },
      "source": [
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]\n",
        "\n",
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IwxQFKykzpQq"
      },
      "source": [
        "Load the different data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeaTvNRTypP0",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "    fpath = data_folder + filename\n",
        "    df = pd.read_csv(fpath, sep='\\t', usecols=['file', 'label', 'text'])\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "def load_all_data(filenames):\n",
        "    df = load_data(filenames[0])\n",
        "    for i in range(1, len(filenames)):\n",
        "        df = df.append(load_data(filenames[i]))\n",
        "    return df\n",
        "\n",
        "# load the datasets from files\n",
        "df = load_all_data(data_filenames)\n",
        "\n",
        "# shuffle the dataset a bit and get the amount of labels\n",
        "df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
        "num_labels = df['label'].nunique()\n",
        "\n",
        "print(df.shape)\n",
        "print(df['label'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kynIImgf2cQd",
        "colab": {}
      },
      "source": [
        "# load the eval dataset(s)\n",
        "df_use = load_all_data(config_use.data_filenames)\n",
        "\n",
        "print(df_use.shape)\n",
        "print(df_use['label'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M--YMkbq6MY7"
      },
      "source": [
        "Create the dictionary that contains the labels along with their indices. This is useful for evaluation and similar.\n",
        "\n",
        "Usual dict: {'audit': 0, 'authenticate': 1, 'heartbeat': 2, 'pooling': 3, 'scheduler': 4, 'unrelated': 5}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TWP1X17N5tJx",
        "colab": {}
      },
      "source": [
        "def create_label_indices(df):\n",
        "    #prepare labels\n",
        "    labels = df['label'].unique()\n",
        "    labels = np.delete(labels, np.where(labels == 'unrelated'))\n",
        "    labels.sort() \n",
        "  \n",
        "    #create dict\n",
        "    labelDict = dict()\n",
        "    for i in range (0, len(labels)):\n",
        "        labelDict[labels[i]] = i\n",
        "    labelDict['unrelated'] = len(labels)\n",
        "    return labelDict\n",
        "\n",
        "label_indices = create_label_indices(df)\n",
        "print(label_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyVQS13d5Sft"
      },
      "source": [
        "# Create and train the learner/classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GeTaowHZzrSJ"
      },
      "source": [
        "Create the needed functions to create and train a classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T83UogVz5XJJ",
        "colab": {}
      },
      "source": [
        "def split_dataframe(df, train_size = 0.9, random_state = None):\n",
        "    # split data into training and validation set\n",
        "    df_trn, df_valid = train_test_split(df, stratify = df['label'], train_size = train_size, random_state = random_state)\n",
        "    return df_trn, df_valid\n",
        "  \n",
        "def create_databunch(config, df_trn, df_valid):\n",
        "    bert_tok = BertTokenizer.from_pretrained(config.bert_model_name,)\n",
        "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
        "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
        "    return BertDataBunch.from_df(\".\", \n",
        "                   train_df=df_trn,\n",
        "                   valid_df=df_valid,\n",
        "                   tokenizer=fastai_tokenizer,\n",
        "                   vocab=fastai_bert_vocab,\n",
        "                   bs=config.bs,\n",
        "                   text_cols='text',\n",
        "                   label_cols='label',\n",
        "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "              )\n",
        "\n",
        "\n",
        "def create_learner(config, databunch):\n",
        "    bert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=num_labels)\n",
        "\n",
        "    optimizer = AdamW # AdamW is the default optimizer of fastai.Learner\n",
        "    if config.use_bertAdam:\n",
        "      # BertAdam optimizer\n",
        "      optimizer = partial(BertAdam)\n",
        "\n",
        "    learner = Learner(\n",
        "        databunch, bert_model,\n",
        "        optimizer,\n",
        "        metrics=accuracy,\n",
        "        loss_func=config.loss_func\n",
        "    )\n",
        "    if config.use_fp16:\n",
        "        learner.to_fp16()\n",
        "    return learner\n",
        "\n",
        "\n",
        "def train(config, df):\n",
        "    df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
        "    databunch = create_databunch(config, df_trn, df_valid)\n",
        "\n",
        "    learner = create_learner(config, databunch)\n",
        "    learner.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms)\n",
        "\n",
        "    return learner\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1KUoNpCq8fh"
      },
      "source": [
        "Actually create the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIOWiuX4y85P",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "    if seed is None:\n",
        "        seed = random.randint(0, 2**31)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "da-sBmbOCLYc",
        "colab": {}
      },
      "source": [
        "set_seed(config.seed)\n",
        "classifier = train(config, df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZxIqAcL9rjN"
      },
      "source": [
        "Save the model along with its config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXTWGILJ4kJx",
        "colab": {}
      },
      "source": [
        "def create_model_name():\n",
        "    name = 'BERT4DAT_e{epochs}_{data_filename}'.format(epochs=str(config.epochs),data_filename=data_filenames[0][:-4])\n",
        "    return name\n",
        "\n",
        "def save_config(model_save_path, model_name):\n",
        "    settings = ''\n",
        "    for item in config.__dict__:\n",
        "        value = config[item]\n",
        "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
        "        settings += setting\n",
        "    save_path = model_save_path + model_name + '.config'\n",
        "    with open(save_path, 'w', encoding='utf-8') as out:\n",
        "        out.write(settings)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmbFM_7C9jp3",
        "colab": {}
      },
      "source": [
        "if save_model:\n",
        "    model_name = create_model_name()\n",
        "    save_config(config_use.model_path, model_name)\n",
        "    model_save_path = config_use.model_path + model_name + '.pkl'\n",
        "    classifier.export(file = model_save_path)\n",
        "    #classifier.save('model_name', return_path=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SClv488eQC8B"
      },
      "source": [
        "# Predictor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prtJ_TGhC2PO"
      },
      "source": [
        "Create a predictor class. Just uses the prediction of the classifier/learner, but labels with confidentiality below a threshold get labeled as 'unrelated'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qubb_Ka-C78O",
        "colab": {}
      },
      "source": [
        "class Predictor:\n",
        "    def __init__(self, classifier, threshold=0.90, default_value =  'unrelated'):\n",
        "        self.classifier = classifier\n",
        "        self.threshold = threshold\n",
        "        self.classes = self.classifier.data.classes\n",
        "        self.default_value = default_value\n",
        "\n",
        "    def predict(self, text):\n",
        "        prediction = self.classifier.predict(text)\n",
        "        prediction_class = prediction[1]\n",
        "        prob = prediction[2][prediction_class].item()\n",
        "        if prob > self.threshold:\n",
        "            return self.classes[prediction_class]\n",
        "        else: return self.default_value   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqS-wqZk1mJy",
        "colab_type": "text"
      },
      "source": [
        "# Use trained classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaSiIy2jNHiU"
      },
      "source": [
        "Load the saved model and create the predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7erioqzNHLi",
        "colab": {}
      },
      "source": [
        "#classifier = load_learner(config_use.model_path, config_use.model_name)\n",
        "predictor = Predictor(classifier, threshold=config_use.threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5Lr_rK_RwWI"
      },
      "source": [
        "Predict/classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lWj98hQKNXsw",
        "colab": {}
      },
      "source": [
        "log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
        "logLine(log_txt)\n",
        "for row in df_use.itertuples():\n",
        "    filename = row.file\n",
        "    class_text = row.text\n",
        "    prediction = predictor.predict(class_text)\n",
        "    log_text = '{} -> {}'.format(filename, prediction)\n",
        "    logLine(log_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}