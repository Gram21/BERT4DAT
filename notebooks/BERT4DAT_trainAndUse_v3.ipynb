{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT4DAT_trainAndUse v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_FXneEfpdMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers fastprogress pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnVIV6Vt8f4d",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule, ConstantLRSchedule\n",
        "\n",
        "from fastprogress import master_bar, progress_bar\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZLtX-CGxG7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the lr_finder-library only if it does not exist yet\n",
        "!(if [ ! -f \"lr_finder.py\" ]; then wget https://raw.githubusercontent.com/davidtvs/pytorch-lr-finder/master/lr_finder.py; fi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_03qqs3xH_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lr_finder import LRFinder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoUBPFpRnK_6",
        "colab_type": "text"
      },
      "source": [
        "Configuration and basic settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7B-obGjNKp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "if cuda_available:\n",
        "    curr_device = torch.cuda.current_device()\n",
        "    print(torch.cuda.get_device_name(curr_device))\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0lgLyC6Gsnf",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "config = Config(\n",
        "    model_name = 'bert-base-uncased',\n",
        "    bs = 2,\n",
        "    epochs = 10,\n",
        "    lr = 2e-3, #2e-5,\n",
        "    adam_betas = (0.8, 0.7), # (0.8, 0.7), (0.9, 0.999)\n",
        "    adam_epsilon = 1e-8,\n",
        "    #training_test_share = 0.1,\n",
        "    num_labels = 6,\n",
        "    threshold = 0.9,\n",
        "    seed = 31337,\n",
        ")\n",
        "\n",
        "config_data = Config(\n",
        "    root_folder = '.',\n",
        "    data_folder = '/data/',\n",
        "    train_data = ['1_classCorpus_BegOnly_512.tsv'],\n",
        "    eval_data = ['Hadoop_BegOnly_512.tsv'],\n",
        "    log_file = '/log/classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        "    answer_set = '/AnswerSetHadoop.md',\n",
        "    eval_script = '/scripts/eval.py',\n",
        "    result_file = '/log/classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        ")\n",
        "\n",
        "load_from_gdrive = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUYrv06z8gaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_seed(seed=31337):\n",
        "    rn.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seed(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVU_viFX-ezy"
      },
      "source": [
        "To import the dataset, first we have to connect to our Google drive (if data should be loaded from gdrive). For this, we have to authenticating the access and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmGISBrhW-VJ",
        "colab": {}
      },
      "source": [
        "if load_from_gdrive:\n",
        "    from google.colab import drive\n",
        "    # Connect to drive to load the corpus from there\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    config_data.root_folder = '/content/drive/My Drive/BERT4DAT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVsM8jl6h7f8",
        "colab_type": "text"
      },
      "source": [
        "## Util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9elisqVbiAU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device)/1000000\n",
        "\n",
        "def get_memory_usage_str():\n",
        "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())\n",
        "\n",
        "def get_info():\n",
        "    model_config = 'model: {}, lr: {}, epochs: {}, bs: {}, Threshold: {}, Seed: {}, Data: {}'.format(config.model_name, config.lr, config.epochs, config.bs, config.threshold, config.seed, config_data.train_data)\n",
        "    return model_config\n",
        "\n",
        "def initLog():\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
        "    with open(logfile, 'w') as log:\n",
        "        log.write(log_txt + '\\n')\n",
        "\n",
        "def logLine(line):\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(line + '\\n')\n",
        "\n",
        "def logResult(result):\n",
        "    logfile = config_data.root_folder + config_data.result_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(get_info() + '\\n')\n",
        "        for line in result:\n",
        "            log.write(line + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgkbhHcB17GY",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg4c6GzoSdiI",
        "colab_type": "text"
      },
      "source": [
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeaTvNRTypP0",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "    fpath = config_data.root_folder+ config_data.data_folder + filename\n",
        "    df = pd.read_csv(fpath, sep='\\t', usecols=['file', 'label', 'text'])\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "def load_all_data(filenames):\n",
        "    df = load_data(filenames[0])\n",
        "    for i in range(1, len(filenames)):\n",
        "        df = df.append(load_data(filenames[i]))\n",
        "    return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzNAS24_ysIM",
        "colab_type": "text"
      },
      "source": [
        "Load the train datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxX3iWkuyo-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = load_all_data(config_data.train_data)\n",
        "\n",
        "# shuffle the dataset a bit and get the amount of labels\n",
        "df_train = df_train.sample(frac=1, axis=0, random_state = config.seed)\n",
        "config.num_labels = df_train['label'].nunique()\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_train['label'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR2xLLDayxxU",
        "colab_type": "text"
      },
      "source": [
        "Load the eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kynIImgf2cQd",
        "colab": {}
      },
      "source": [
        "df_eval = load_all_data(config_data.eval_data)\n",
        "\n",
        "print(df_eval.shape)\n",
        "print(df_eval['label'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhvrSyJCSfqX",
        "colab_type": "text"
      },
      "source": [
        "Divide texts and labels properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VENo6tqG7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d[2], d[1]), np.array(df_train))))\n",
        "\n",
        "len(train_texts), len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f90QNmvdvY-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = sorted(list(set(train_labels)))\n",
        "config.num_labels = len(labels)\n",
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsqxOe-ZSpR-",
        "colab_type": "text"
      },
      "source": [
        "Initialize tokenizer and tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty24UrRjqIsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(config.model_name, do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k9rcOzQr5Zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "\n",
        "len(train_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NFkcA2dS1iO",
        "colab_type": "text"
      },
      "source": [
        "Pad the tokens and convert them to their IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ca7KKnhuT5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7POtHuIOV-6",
        "colab": {}
      },
      "source": [
        "train_y = np.array([labels.index(x) for x in train_labels])\n",
        "print(train_y.shape, np.unique(train_y, return_counts=True)[1] / len(train_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFqF7DlETC-2",
        "colab_type": "text"
      },
      "source": [
        "Masks for tokens, so we can tell the model where the unimportant (padded) part is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-xXMEqXOWTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_hEhebQ3YqI",
        "colab_type": "text"
      },
      "source": [
        "# Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvY-5svjyy96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WrappedCrossEntropyLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"Wrapper around nn.CrossEntropyLoss to deal with the special behavior of our inputs that need to be transformed before calculating the loss\"\"\"\n",
        "    def __init__(self, num_labels):\n",
        "        super(WrappedCrossEntropyLoss, self).__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.loss_fct(input.view(-1, self.num_labels), target.view(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E234ByBa3Qtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: is it possible to do a multi-level classification? Like n binary classifications and the one with the highest confidentiality is taken\n",
        "class BertTextClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, model_name, num_labels, avg_pool = False):\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        super(BertTextClassifier, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        self.avg_pool = avg_pool\n",
        "        self.simple = False\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        #self.apply(self.init_weights)\n",
        "    \n",
        "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
        "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        # According to documentation of pytorch-transformers, pooled output might not be the best \n",
        "        # and youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\n",
        "        if self.avg_pool:\n",
        "            hidden_states = outputs[0]\n",
        "            pooled_output = torch.mean(hidden_states, 1)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(dropout_output)\n",
        "\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        if self.simple:\n",
        "            return probs\n",
        "\n",
        "        outputs = (probs,) + outputs[2:]\n",
        "        \n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(probs.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = WrappedCrossEntropyLoss(self.num_labels)\n",
        "                loss = loss_fct(probs, labels)\n",
        "            outputs = (loss,) + outputs          \n",
        "\n",
        "        return outputs\n",
        "    \n",
        "    def predict(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
        "        predictions = self(tokens, labels=labels, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "        \n",
        "        if labels is not None:\n",
        "            loss, logits = predictions\n",
        "        else:\n",
        "            loss, logits = None, predictions[0]\n",
        "    \n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        return loss, probs \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1iEankiB2QR",
        "colab_type": "text"
      },
      "source": [
        "# Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67dqMkAdB1mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, model):\n",
        "        self.device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "        self.model = model.to(device)\n",
        "\n",
        "        self.max_grad_norm = 1.0\n",
        "    \n",
        "    def train(self, epochs, train_dataloader, optimizer, scheduler = None, test_dataloader = None):\n",
        "        \"\"\"Trains the model for the given amount of epochs using the given dataloader and optimizer\n",
        "\n",
        "        Parameters:\n",
        "            epochs(int): Number of Epochs\n",
        "            train_dataloader(torch.utils.data.DataLoader): Dataloader consisting of a TensorDataset with tokens, masks, and labels\n",
        "            optimizer(nn.Optimizer): Optimizer to use, e.g., AdamW\n",
        "            scheduler(Scheduler, optional): Scheduler to use. If none is provided, uses the default ConstantLRSchedule\n",
        "            test_dataloader(torch.utils.data.DataLoader, optional): Dataloader for the test-data. If provided, does a evaluation step after each epoch. Needs the same shape like the train_dataloader. \n",
        "        \"\"\"\n",
        "        master_bar_iterator = master_bar(range(epochs))\n",
        "        master_bar_iterator.first_bar.comment = get_memory_usage_str()\n",
        "        master_bar_iterator.update(0)\n",
        "\n",
        "        if scheduler is None:\n",
        "            scheduler = ConstantLRSchedule(optimizer)\n",
        "\n",
        "        for epoch_num in master_bar_iterator:\n",
        "            # Train for the epoch\n",
        "            train_loss = 0\n",
        "            self.model.zero_grad()\n",
        "            epoch_bar_iterator = progress_bar(train_dataloader, parent=master_bar_iterator)\n",
        "            for step_num, batch_data in enumerate(epoch_bar_iterator):\n",
        "                if step_num < 2:\n",
        "                    # update the memory usage; in the first steps only\n",
        "                    master_bar_iterator.first_bar.comment = get_memory_usage_str()\n",
        "                    master_bar_iterator.update(epoch_num)\n",
        "\n",
        "                self.model.train()\n",
        "                token_ids, masks, labels = tuple(t.to(self.device) for t in batch_data)\n",
        "                outputs = self.model(token_ids, labels=labels, attention_mask = masks)\n",
        "                \n",
        "                loss = outputs[0]\n",
        "                loss.backward()\n",
        "                clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.max_grad_norm)\n",
        "\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                self.model.zero_grad()\n",
        "                \n",
        "                train_loss += loss.item()\n",
        "                curr_loss = train_loss / (step_num + 1)\n",
        "                master_bar_iterator.child.comment = f'loss: {curr_loss}'\n",
        "                master_bar_iterator.first_bar.comment = get_memory_usage_str()\n",
        "            # Calculate train stats\n",
        "            epoch_train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "            if test_dataloader is None:\n",
        "                master_bar_iterator.write(f'Finished epoch {epoch_num + 1}: train loss = {epoch_train_loss:.5}')\n",
        "                continue\n",
        "\n",
        "            # Evaluate the current classifier\n",
        "            self.model.eval()\n",
        "            test_loss = 0\n",
        "            predictions = None\n",
        "            corr_labels = None\n",
        "            with torch.no_grad():\n",
        "                for step_num, batch_data in enumerate(progress_bar(test_dataloader, parent=master_bar_iterator)):\n",
        "                    token_ids, masks, labels = tuple(batch.to(self.device) for batch in batch_data)\n",
        "\n",
        "                    outputs = self.model.predict(token_ids, labels=labels, attention_mask = masks)\n",
        "                    batch_loss, probs = outputs[:2]\n",
        "\n",
        "                    test_loss += batch_loss.item()\n",
        "\n",
        "                    numpy_probs = probs.cpu().detach().numpy()\n",
        "                    numpy_labels = labels.cpu().detach().numpy()\n",
        "                    if predictions is None:\n",
        "                        predictions = np.argmax(numpy_probs, axis = 1)\n",
        "                        corr_labels = numpy_labels.T[0]\n",
        "                    else:\n",
        "                        predictions = np.append(predictions, np.argmax(numpy_probs, axis = 1), axis=0)\n",
        "                        corr_labels = np.append(corr_labels, numpy_labels.T[0], axis=0)\n",
        "            \n",
        "            # Calculate test stats\n",
        "            epoch_test_loss = test_loss / len(test_dataloader)\n",
        "            accuracy = (predictions == corr_labels).mean()\n",
        "            f1 = f1_score(y_true=corr_labels, y_pred=predictions, average='macro')\n",
        "            master_bar_iterator.write(f'Finished epoch {epoch_num + 1}: train loss = {epoch_train_loss:.5}, test loss = {epoch_test_loss:.5}, accuracy = {accuracy:.2%}, f1 = {f1:.2%}')\n",
        "\n",
        "        return train_loss / len(train_dataloader)\n",
        "    \n",
        "\n",
        "    def evaluate(self, test_dataloader):\n",
        "        \"\"\" Evaluates the model using the given dataloader for the test-data\n",
        "        Parameters:\n",
        "            test_dataloader(torch.utils.data.DataLoader): Dataloader for the test-data consisting of a TensorDataset with tokens, masks, and labels\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        predicted = []\n",
        "        gold_answers = []\n",
        "        with torch.no_grad():\n",
        "            for batch_data in master_bar(test_dataloader):\n",
        "                token_ids, masks, labels = tuple(batch.to(self.device) for batch in batch_data)\n",
        "\n",
        "                logits = self.model(token_ids)[0]\n",
        "                numpy_logits = logits.cpu().detach().numpy()\n",
        "                numpy_labels = labels.cpu().detach().numpy()\n",
        "\n",
        "                predictions = np.argmax(numpy_logits, axis = 1)\n",
        "                predicted += list(predictions)\n",
        "                gold_answers += numpy_labels.T[0].tolist()\n",
        "\n",
        "        report = classification_report(gold_answers, predicted)\n",
        "        return report, predicted, gold_answers\n",
        "    \n",
        "\n",
        "    def find_lr(self, dataloader, optimizer, step_mode=\"exp\"):\n",
        "        \"\"\" Tries different learning rates and plots a curve describing the loss for the learning rates\n",
        "        Parameters:\n",
        "            dataloader: dataloader with a 2-dim-dataset consisting of train- and label-tensors\n",
        "            optimizer: the optimizer that should be used\n",
        "            step_mode(str): \"linear\" for within one exponent or \"exp\" for between exp.\n",
        "        \"\"\"\n",
        "        self.model.simple = True\n",
        "        loss_fct = WrappedCrossEntropyLoss(self.model.num_labels)\n",
        "        lr_finder = LRFinder(self.model, optimizer, loss_fct, device=\"cuda\")\n",
        "        \n",
        "        lr_finder.range_test(dataloader, end_lr=1, num_iter=100, step_mode=step_mode)\n",
        "\n",
        "        self.model.simple = False\n",
        "        lr_finder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9LPIYcn99r8",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tune BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEIg0WQLTe3V",
        "colab_type": "text"
      },
      "source": [
        "Create tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGwV0yqg_o2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1))\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "\n",
        "get_memory_usage_str()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSTx4KEoTtXu",
        "colab_type": "text"
      },
      "source": [
        "Create the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yl2JpCe9YAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=config.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWy77xCnTnEW",
        "colab_type": "text"
      },
      "source": [
        "Initialize the model and create the optimizer and scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sf9n8zouENRi",
        "colab": {}
      },
      "source": [
        "model = BertTextClassifier(config.model_name, config.num_labels)\n",
        "optimizer = AdamW(model.parameters(), lr=config.lr, betas=config.adam_betas, eps=config.adam_epsilon, correct_bias=False)\n",
        "scheduler = ConstantLRSchedule(optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6yIChYvBP5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjtIn6NDKaoG",
        "colab_type": "text"
      },
      "source": [
        "Create the learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFOw0QpGKdn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner = Learner(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3OvdAmCvb4M",
        "colab_type": "text"
      },
      "source": [
        "Optional: Find LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvSGcgy4veWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_find_lr = False\n",
        "\n",
        "if do_find_lr:\n",
        "    start_lr = 1e-10\n",
        "    lr_optimizer = AdamW(model.parameters(), lr=start_lr, betas=config.adam_betas, eps=config.adam_epsilon, correct_bias=False)\n",
        "    lr_train_dataset = TensorDataset(train_tokens_tensor, train_y_tensor)\n",
        "    lr_train_dataloader = DataLoader(lr_train_dataset, shuffle=True, batch_size=config.bs)\n",
        "    learner.find_lr(dataloader=lr_train_dataloader, optimizer=lr_optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNZ_3auqDbjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "get_memory_usage_str()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbeWgXufT8T_",
        "colab_type": "text"
      },
      "source": [
        "Start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVlvuB5rKZp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.train(epochs = config.epochs, train_dataloader=train_dataloader, optimizer=optimizer, scheduler = scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcq0SEF9TcMi",
        "colab_type": "text"
      },
      "source": [
        "# Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v09veBeuTduU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Predictor:\n",
        "    def __init__(self, classifier, tokenizer, class_labels, threshold=0.90, default_value =  'unrelated'):\n",
        "        self.classifier = classifier\n",
        "        self.tokenizer = tokenizer\n",
        "        self.threshold = threshold\n",
        "        self.class_labels = class_labels\n",
        "        self.default_value = default_value\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "\n",
        "    def predict(self, text):\n",
        "        tokens = [['[CLS]'] + tokenizer.tokenize(text)[:510] + ['[SEP]']]\n",
        "        tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "        masks = [[float(i > 0) for i in ii] for ii in tokens_ids]\n",
        "        token_ids = torch.tensor(tokens_ids).to(self.device)\n",
        "        masks = torch.tensor(masks).to(self.device)       \n",
        "        \n",
        "        probs = self.classifier.predict(token_ids, attention_mask=masks)[1]\n",
        "        numpy_probs = probs.cpu().detach().numpy()\n",
        "        prediction = np.argmax(numpy_probs)\n",
        "        prob = numpy_probs[0][prediction].item()\n",
        "        if prob > self.threshold:\n",
        "            return self.class_labels[prediction], prob\n",
        "        else: return self.default_value, prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9P6Y56XTgo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = Predictor(learner.model, tokenizer, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5Lr_rK_RwWI"
      },
      "source": [
        "Predict/classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lWj98hQKNXsw",
        "colab": {}
      },
      "source": [
        "initLog()\n",
        "for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
        "    filename = row.file\n",
        "    class_text = row.text\n",
        "    prediction, prob = predictor.predict(class_text)\n",
        "    log_text = '{} -> {}'.format(filename, prediction)\n",
        "    logLine(log_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJR0-iUFrjv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EVAL_SCRIPT = config_data.root_folder + config_data.eval_script\n",
        "ANSWERS = config_data.root_folder + config_data.answer_set\n",
        "LOG_FILE = config_data.root_folder + config_data.log_file\n",
        "\n",
        "eval_command = 'python \"{}\" \"{}\" \"{}\"'.format(EVAL_SCRIPT, ANSWERS, LOG_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j8iBtmrj6SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = !{eval_command}\n",
        "logResult(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCW9WL6YuplF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in result:\n",
        "    print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}