{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT4DAT_trainAndUse_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-djxLyuc-Opk"
      },
      "source": [
        "# Prepare\n",
        "Install required libraries and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Epk5taxa99eI",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "!pip install pytorch-transformers fastprogress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fr6bTWdl-XzF",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rdgM39FGZpM",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpZkAwZl0DaA",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
        "from pytorch_transformers import AdamW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L57NdoEnLQa2",
        "colab": {}
      },
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azCEB4CuQk9A"
      },
      "source": [
        "Check, if and what kind of GPU is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wtzha3q7QjjU",
        "colab": {}
      },
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "if cuda_available:\n",
        "    curr_device = torch.cuda.current_device()\n",
        "    print(torch.cuda.get_device_name(curr_device))\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl2KDTqN6zLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device)/1000000\n",
        "\n",
        "def get_memory_usage_str():\n",
        "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOKXVgJgGtYV"
      },
      "source": [
        "Create a config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0lgLyC6Gsnf",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "config = Config(\n",
        "    num_labels = 6, # will be set automatically\n",
        "    model_name=\"bert-base-uncased\", \n",
        "    max_lr=2e-5, # default: 2e-5\n",
        "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
        "    epochs=10,\n",
        "    bs=2, # default: 2 or 4\n",
        "    weight_decay = 0.01,\n",
        "    max_seq_len=512,\n",
        "    train_size=0.9375,\n",
        "    loss_func=nn.CrossEntropyLoss(), #default: None or nn.CrossEntropyLoss()\n",
        "    seed=904727489, #default: 904727489, 424242 (reproducibility) or None\n",
        "    threshold = 0.75,\n",
        "    undersample = False,\n",
        "    oversample = False,\n",
        ")\n",
        "\n",
        "config_data = Config(\n",
        "    root_folder = '.',\n",
        "    data_folder = '/data/',\n",
        "    train_data = ['1_classCorpus_Shrunk.tsv'], # 1_classCorpus_BegOnly_512, 1_classCorpus_Shrunk\n",
        "    eval_data = ['Hadoop_BegOnly_512.tsv'],\n",
        "    log_file = '/log/classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        "    answer_set = '/AnswerSetHadoop.md',\n",
        "    eval_script = '/scripts/eval.py',\n",
        "    result_file = '/log/classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt',\n",
        "    model_path = '/model/',\n",
        "    model_name = 'BERT4DAT.pkl',\n",
        ")\n",
        "\n",
        "load_from_gdrive = True\n",
        "save_model = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SVU_viFX-ezy"
      },
      "source": [
        "To import the dataset, first we have to connect to our Google drive (if data should be loaded from gdrive). For this, we have to authenticating the access and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmGISBrhW-VJ",
        "colab": {}
      },
      "source": [
        "if load_from_gdrive:\n",
        "    from google.colab import drive\n",
        "    # Connect to drive to load the corpus from there\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    config_data.root_folder = '/content/drive/My Drive/BERT4DAT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "er1yzLHFQq1U",
        "colab": {}
      },
      "source": [
        "def initLog():\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
        "    with open(logfile, 'w') as log:\n",
        "        log.write(log_txt + '\\n')\n",
        "\n",
        "def logLine(line):\n",
        "    logfile = config_data.root_folder + config_data.log_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(line + '\\n')\n",
        "\n",
        "def logResult(result):\n",
        "    logfile = config_data.root_folder + config_data.result_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(get_info() + '\\n')\n",
        "        for line in result:\n",
        "            log.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE5NcUY6_Hja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_info():\n",
        "    model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, train_size: {}, weight decay: {}, Threshold: {}, Seed: {}, Data: {}, Undersampling: {}'.format(config.model_name, config.max_lr, config.epochs, config.bs, config.train_size, config.weight_decay, config.threshold, config.seed, config_data.train_data, config.undersample)\n",
        "    return model_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIOWiuX4y85P",
        "colab": {}
      },
      "source": [
        "def set_seed(seed):\n",
        "    if seed is None:\n",
        "        seed = random.randint(0, 2**31)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    return seed\n",
        "\n",
        "set_seed(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ptp6NhIC_FQb"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JmN-0wJEBAEH"
      },
      "source": [
        "Create proper tokenizer for our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6anB63ppBAtB",
        "colab": {}
      },
      "source": [
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str):\n",
        "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1G8rFbEEJWyu"
      },
      "source": [
        "Now, we can create our own databunch using the tokenizer above. Notice we're passing the include_bos=False and include_eos=False options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens.\n",
        "\n",
        "We can pass our own list of Preprocessors to the databunch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNRRj6jIJrp2",
        "colab": {}
      },
      "source": [
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]\n",
        "\n",
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZESEaXBOuNfn",
        "colab_type": "text"
      },
      "source": [
        "Create the BertTextClassifier-Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_PRt9Q3eUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertTextClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        super(BertTextClassifier, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
        "        \n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        #self.apply(self.init_weights)\n",
        "    \n",
        "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
        "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "        \n",
        "        pooled_output = outputs[1]\n",
        "        # According to documentation of pytorch-transformers, pooled output might not be the best \n",
        "        # and youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence \n",
        "        #hidden_states = outputs[0]\n",
        "        #pooled_output = torch.mean(hidden_states, 1)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(dropout_output)\n",
        "\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IwxQFKykzpQq"
      },
      "source": [
        "Load the different data sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeaTvNRTypP0",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "    fpath = config_data.root_folder + config_data.data_folder + filename\n",
        "    df = pd.read_csv(fpath, sep='\\t', usecols=['file', 'label', 'text'])\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "def load_all_data(filenames, limit=-1):\n",
        "    df = load_data(filenames[0])\n",
        "    for i in range(1, len(filenames)):\n",
        "        df = df.append(load_data(filenames[i]))\n",
        "    df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
        "    return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5y3TNpCTRoh",
        "colab_type": "text"
      },
      "source": [
        "Add function for undersampling, which takes randomly chosen elements out of the bigger (label) groups to equalize the size of the groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD_TgECzKSnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def undersample(df):\n",
        "  sample_size = len(df)\n",
        "  df_undersampled = None\n",
        "\n",
        "  # find smallest group size\n",
        "  for label in df['label'].unique():\n",
        "      label_size = len(df[df['label'] == label])\n",
        "      if label_size < sample_size:\n",
        "          sample_size = label_size\n",
        "\n",
        "  # pick elements out of groups\n",
        "  for label in df['label'].unique():\n",
        "      indices = df[df['label'] == label].index\n",
        "      len(indices)\n",
        "      random_indices = np.random.choice(indices, sample_size, replace=False)\n",
        "      sample = df.loc[random_indices]\n",
        "      if df_undersampled is None:\n",
        "          df_undersampled = sample\n",
        "      else:\n",
        "          df_undersampled = df_undersampled.append(sample)\n",
        "  df_undersampled = df_undersampled.sample(frac=1, axis=0, random_state = config.seed)\n",
        "  return df_undersampled\n",
        "\n",
        "def oversample(df):\n",
        "  sample_size = 0\n",
        "  df_sampled = df\n",
        "\n",
        "  # find smallest group size\n",
        "  for label in df['label'].unique():\n",
        "      label_size = len(df[df['label'] == label])\n",
        "      if label_size > sample_size:\n",
        "          sample_size = label_size\n",
        "  \n",
        "  for label in df['label'].unique():\n",
        "      label_size = len(df[df['label'] == label])\n",
        "      multiplier = sample_size//label_size\n",
        "      indices = df[df['label'] == label].index\n",
        "      diff = sample_size - (multiplier * label_size)\n",
        "      random_indices = np.random.choice(indices, diff, replace=False)\n",
        "      sample = df.loc[random_indices]\n",
        "      for i in range(multiplier - 1):\n",
        "        sample = pd.concat([sample, df[df['label'] == label]], ignore_index=True)\n",
        "      df_sampled = df_sampled.append(sample)\n",
        "  df_sampled = df_sampled.sample(frac=1, axis=0, random_state = config.seed)\n",
        "  return df_sampled\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6o6UU0tUYck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the train datasets\n",
        "df = load_all_data(config_data.train_data, 50)\n",
        "\n",
        "# shuffle the dataset a bit and get the amount of labels\n",
        "df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
        "config.num_labels = df['label'].nunique()\n",
        "\n",
        "print(df.shape)\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# \"undersampling\"\n",
        "if config.undersample:\n",
        "    df = undersample(df)\n",
        "\n",
        "    print('\\nUsing Undersampling. Updated data:')\n",
        "    print(df.shape)\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "if config.oversample:\n",
        "    df = oversample(df)\n",
        "\n",
        "    print('\\nUsing Oversampling. Updated data:')\n",
        "    print(df.shape)\n",
        "    print(df['label'].value_counts())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kynIImgf2cQd",
        "colab": {}
      },
      "source": [
        "# load the eval dataset(s)\n",
        "df_eval = load_all_data(config_data.eval_data)\n",
        "\n",
        "print(df_eval.shape)\n",
        "print(df_eval['label'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M--YMkbq6MY7"
      },
      "source": [
        "Create the dictionary that contains the labels along with their indices. This is useful for evaluation and similar.\n",
        "\n",
        "Usual dict: {'audit': 0, 'authenticate': 1, 'heartbeat': 2, 'pooling': 3, 'scheduler': 4, 'unrelated': 5}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TWP1X17N5tJx",
        "colab": {}
      },
      "source": [
        "def create_label_indices(df):\n",
        "    #prepare labels\n",
        "    labels = df['label'].unique()\n",
        "    labels = np.delete(labels, np.where(labels == 'unrelated'))\n",
        "    labels.sort() \n",
        "  \n",
        "    #create dict\n",
        "    labelDict = dict()\n",
        "    for i in range (0, len(labels)):\n",
        "        labelDict[labels[i]] = i\n",
        "    labelDict['unrelated'] = len(labels)\n",
        "    return labelDict\n",
        "\n",
        "label_indices = create_label_indices(df)\n",
        "print(label_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyVQS13d5Sft"
      },
      "source": [
        "# Create and train the learner/classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GeTaowHZzrSJ"
      },
      "source": [
        "Create the needed functions to create and train a classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T83UogVz5XJJ",
        "colab": {}
      },
      "source": [
        "def split_dataframe(df, train_size = 0.9, random_state = None):\n",
        "    # split data into training and validation set\n",
        "    df_trn, df_valid = train_test_split(df, stratify = df['label'], train_size = train_size, random_state = random_state)\n",
        "    return df_trn, df_valid\n",
        "  \n",
        "def create_databunch(config, df_trn, df_valid):\n",
        "    bert_tok = BertTokenizer.from_pretrained(config.model_name,)\n",
        "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
        "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
        "    return BertDataBunch.from_df(\".\", \n",
        "                   train_df=df_trn,\n",
        "                   valid_df=df_valid,\n",
        "                   tokenizer=fastai_tokenizer,\n",
        "                   vocab=fastai_bert_vocab,\n",
        "                   bs=config.bs,\n",
        "                   text_cols='text',\n",
        "                   label_cols='label',\n",
        "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "              )\n",
        "\n",
        "\n",
        "def create_learner(config, databunch):\n",
        "    model = BertTextClassifier(config.model_name, config.num_labels)\n",
        "\n",
        "    optimizer = partial(AdamW)\n",
        "\n",
        "    learner = Learner(\n",
        "        databunch, model,\n",
        "        optimizer,\n",
        "        wd = config.weight_decay,\n",
        "        metrics=FBeta(beta=1, average='macro'),\n",
        "        loss_func=config.loss_func\n",
        "    )\n",
        "    return learner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLAYg1B8AcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To find LR:\n",
        "do_find_lr = False\n",
        "if do_find_lr:\n",
        "    df_trn, df_valid = split_dataframe(df, train_size = 0.7, random_state = config.seed)\n",
        "    databunch = create_databunch(config, df_trn, df_valid)\n",
        "    learner = create_learner(config, databunch)\n",
        "\n",
        "    learner.lr_find()\n",
        "    learner.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1KUoNpCq8fh"
      },
      "source": [
        "Actually create the trained classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "da-sBmbOCLYc",
        "colab": {}
      },
      "source": [
        "# Create the classifier\n",
        "df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
        "databunch = create_databunch(config, df_trn, df_valid)\n",
        "\n",
        "classifier = create_learner(config, databunch)\n",
        "get_memory_usage_str()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqGyLEubBw60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the classifier\n",
        "classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1aprgwB-Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_memory_usage_str()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZxIqAcL9rjN"
      },
      "source": [
        "Save the model along with its config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXTWGILJ4kJx",
        "colab": {}
      },
      "source": [
        "def create_model_name():\n",
        "    name = 'BERT4DAT_e{epochs}_{data_filename}'.format(epochs=str(config.epochs),data_filename=data_filenames[0][:-4])\n",
        "    return name\n",
        "\n",
        "def save_config(model_save_path, model_name):\n",
        "    settings = ''\n",
        "    for item in config.__dict__:\n",
        "        value = config[item]\n",
        "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
        "        settings += setting\n",
        "    save_path = model_save_path + model_name + '.config'\n",
        "    with open(save_path, 'w', encoding='utf-8') as out:\n",
        "        out.write(settings)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmbFM_7C9jp3",
        "colab": {}
      },
      "source": [
        "if save_model:\n",
        "    model_name = create_model_name()\n",
        "    model_save_path = config_data.root_folder + config_data.model_path\n",
        "    save_config(model_save_path, model_name)\n",
        "    model_save_file = model_save_path + model_name + '.pkl'\n",
        "    classifier.export(file = model_save_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SClv488eQC8B"
      },
      "source": [
        "# Predictor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prtJ_TGhC2PO"
      },
      "source": [
        "Create a predictor class. Just uses the prediction of the classifier/learner, but labels with confidentiality below a threshold get labeled as 'unrelated'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qubb_Ka-C78O",
        "colab": {}
      },
      "source": [
        "class Predictor:\n",
        "    def __init__(self, classifier, threshold=0.75, default_value =  'unrelated'):\n",
        "        self.classifier = classifier\n",
        "        self.threshold = threshold\n",
        "        self.classes = self.classifier.data.classes\n",
        "        self.default_value = default_value\n",
        "\n",
        "    def predict(self, text):\n",
        "        prediction = self.classifier.predict(text)\n",
        "        prediction_class = prediction[1]\n",
        "        prob = prediction[2][prediction_class].item()\n",
        "        if prob > self.threshold:\n",
        "            return self.classes[prediction_class]\n",
        "        else: return self.default_value   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqS-wqZk1mJy",
        "colab_type": "text"
      },
      "source": [
        "# Use trained classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YaSiIy2jNHiU"
      },
      "source": [
        "Load the saved model and create the predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7erioqzNHLi",
        "colab": {}
      },
      "source": [
        "#classifier = load_learner(config_data.root_folder + config_data.model_path, config_data.model_name)\n",
        "predictor = Predictor(classifier, threshold=config.threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_5Lr_rK_RwWI"
      },
      "source": [
        "Predict/classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lWj98hQKNXsw",
        "colab": {}
      },
      "source": [
        "initLog()\n",
        "for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
        "    filename = row.file\n",
        "    class_text = row.text\n",
        "    prediction = predictor.predict(class_text)\n",
        "    log_text = '{} -> {}'.format(filename, prediction)\n",
        "    logLine(log_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDdKrilfRsPJ",
        "colab_type": "text"
      },
      "source": [
        "Analyse outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVNyvchvRqzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EVAL_SCRIPT = config_data.root_folder + config_data.eval_script\n",
        "ANSWERS = config_data.root_folder + config_data.answer_set\n",
        "LOG_FILE = config_data.root_folder + config_data.log_file\n",
        "\n",
        "eval_command = 'python \"{}\" \"{}\" \"{}\"'.format(EVAL_SCRIPT, ANSWERS, LOG_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isqjn1nwRvTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = !{eval_command}\n",
        "logResult(result)\n",
        "\n",
        "for line in result:\n",
        "    print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}