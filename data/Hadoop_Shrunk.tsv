	file	label	text
0	common\src\java\org\apache\hadoop\HadoopIllegalArgumentException.java	unrelated	package org apache hadoop indicates method passed illegal invalid argument this exception thrown instead illegal argument exception differentiate exception thrown hadoop implementation one thrown jdk hadoop illegal argument exception extends illegal argument exception serial version uid l constructs exception specified detail message hadoop illegal argument exception string message
1	common\src\java\org\apache\hadoop\HadoopVersionAnnotation.java	unrelated	package org apache hadoop a package attribute captures version hadoop compiled hadoop version annotation get hadoop version string version get username compiled hadoop string user get date hadoop compiled string date get url subversion repository string url get subversion revision string revision get branch compiled string branch get checksum source files hadoop compiled string src checksum
2	common\src\java\org\apache\hadoop\classification\InterfaceAudience.java	unrelated	package org apache hadoop classification annotation inform users package method intended audience interface audience interface audience audience exist
3	common\src\java\org\apache\hadoop\classification\InterfaceStability.java	unrelated	package org apache hadoop classification annotation inform users much rely particular package method changing time interface stability stable evolving unstable
4	common\src\java\org\apache\hadoop\classification\tools\ExcludePrivateAnnotationsJDiffDoclet.java	unrelated	package org apache hadoop classification tools a href http java sun com javase docs jdk api javadoc doclet doclet excluding elements annotated link org apache hadoop classification interface audience private link org apache hadoop classification interface audience limited private it delegates j diff doclet takes options exclude private annotations j diff doclet language version language version boolean start root doc root exclude private annotations j diff doclet get simple name option length string option boolean valid options string options
5	common\src\java\org\apache\hadoop\classification\tools\ExcludePrivateAnnotationsStandardDoclet.java	unrelated	package org apache hadoop classification tools a href http java sun com javase docs jdk api javadoc doclet doclet excluding elements annotated link org apache hadoop classification interface audience private link org apache hadoop classification interface audience limited private it delegates standard doclet takes options exclude private annotations standard doclet language version language version boolean start root doc root exclude private annotations standard doclet get simple name option length string option boolean valid options string options
6	common\src\java\org\apache\hadoop\classification\tools\package-info.java	unrelated	package org apache hadoop classification tools
7	common\src\java\org\apache\hadoop\classification\tools\RootDocProcessor.java	unrelated	package org apache hadoop classification tools process link root doc substituting nested proxy objects exclude elements private limited private annotations p based code http www sixlegs com blog java exclude javadoc tag html root doc processor string stability stability options unstable option root doc process root doc root return root doc process root root doc object process object obj class type obj null class cls obj get class cls get name starts with com sun else obj instanceof object cls get component type array length return obj map object object proxies new weak hash map object object object get proxy object obj object proxy proxies get obj proxy null return proxy exclude handler implements invocation handler object target exclude handler object target object invoke object proxy method method object args throws throwable method name equals included doc doc doc target return exclude doc doc included target instanceof root doc method name equals else method name equals specified classes else method name equals specified packages else target instanceof class doc filtered args else target instanceof package doc method name equals classes class doc else method name equals annotation types annotation type doc else method name equals enums class doc else method name equals errors class doc else method name equals exceptions class doc else method name equals interfaces class doc else method name equals ordinary classes class doc method name equals compare to method name equals equals args unwrap args return process method invoke target args method get return type throw e get target exception boolean exclude doc doc annotations program element doc doc annotations annotations package doc doc annotations annotation desc annotation annotations string qualified type name annotation annotation type qualified type name qualified type name equals stability equals stability options evolving option interface stability unstable get canonical name stability equals stability options stable option interface stability unstable get canonical name object filter doc array class component type return array exclude entry list add process entry component type size object unwrap object proxy return exclude handler proxy get invocation handler proxy target boolean filtered object args
8	common\src\java\org\apache\hadoop\classification\tools\StabilityOptions.java	unrelated	package org apache hadoop classification tools stability options string stable option stable string evolving option evolving string unstable option unstable integer option length string option valid options string options root doc processor stability unstable option root doc processor stability evolving option root doc processor stability stable option string filter options string options options list add options
9	common\src\java\org\apache\hadoop\conf\Configurable.java	unrelated	package org apache hadoop conf something may configured link configuration configurable set configuration used object set conf configuration conf return configuration used object configuration get conf
10	common\src\java\org\apache\hadoop\conf\Configuration.java	unrelated	package org apache hadoop conf provides access configuration parameters id resources resources p configurations specified resources a resource contains set name value pairs xml data each resource named either code string code link path if named code string code classpath examined file name if named code path code local filesystem examined directly without referring classpath p unless explicitly turned hadoop default specifies two resources loaded order classpath ol li tt href doc root core default html core default xml tt read defaults hadoop li li tt core site xml tt site specific configuration given hadoop installation li ol applications may add additional resources loaded subsequent resources order added id final params final parameters p configuration parameters may declared once resource declares value subsequently loaded resource alter value for example one might define parameter tt pre lt property gt lt name gt dfs client buffer dir lt name gt lt value gt tmp hadoop dfs client lt value gt b lt gt true lt gt b lt property gt pre tt administrators typically define parameters tt core site xml tt values user applications may alter id variable expansion variable expansion p value strings first processed variable expansion the available properties ol li other properties defined configuration name undefined li li properties link system get properties li ol p for example configuration resource contains following property definitions tt pre lt property gt lt name gt basedir lt name gt lt value gt user user name lt value gt lt property gt lt property gt lt name gt tempdir lt name gt lt value gt basedir tmp lt value gt lt property gt pre tt when tt conf get tempdir tt called tt basedir tt resolved another property configuration tt user name tt would ordinarily resolved value system property name configuration implements iterable map entry string string log log log factory get log configuration boolean quietmode true list configuration resources array list object resources new array list object the value reported setting resource key set code rather file resource string unknown resource unknown list configuration parameters marked b b set string parameters new hash set string boolean load defaults true configuration objects weak hash map configuration object registry new weak hash map configuration object list default resources resources loaded order list entries copy on write array list string default resources new copy on write array list string map class loader map string class cache classes new weak hash map class loader map string class stores mapping key resource modifies loads key recently hash map string string updating resource class keep information keys replace deprecated ones this stores new keys replace deprecated keys also gives provision custom message deprecated key replaced it also provides method get appropriate warning message logged whenever deprecated key used deprecated key info string new keys string custom message boolean accessed deprecated key info string new keys string custom message string get warning message string key stores deprecated keys new keys replace deprecated keys custom message provided map string deprecated key info deprecated key map stores mapping superseding keys keys deprecate map
11	common\src\java\org\apache\hadoop\conf\Configured.java	unrelated	package org apache hadoop conf base things may configured link configuration configured implements configurable configuration conf construct configured configured construct configured configured configuration conf inherit javadoc set conf configuration conf inherit javadoc configuration get conf
12	common\src\java\org\apache\hadoop\conf\ConfServlet.java	unrelated	package org apache hadoop conf a servlet print running configuration data conf servlet extends http servlet serial version uid l string format json json string format xml xml string format param format return configuration daemon hosting servlet this populated http server starts configuration get conf from context configuration conf configuration get servlet context get attribute assert conf null return conf get http servlet request request http servlet response response do authorization http server administrator access get servlet context request string format request get parameter format param null format format xml equals format else format json equals format writer response get writer try catch bad format exception bfe close guts servlet extracted easy testing write response configuration conf writer string format throws io exception bad format exception format json equals format else format xml equals format else bad format exception extends exception serial version uid l bad format exception string msg
13	common\src\java\org\apache\hadoop\conf\Reconfigurable.java	unrelated	package org apache hadoop conf something whose link configuration changed run time reconfigurable extends configurable change configuration property object value specified change configuration property object value specified return previous value configuration property set null previously set if new val null set property default value if property cannot changed throw link reconfiguration exception string reconfigure property string property string new val return whether given property changeable run time if property reconfigurable returns true property change conf throw exception changing property boolean property reconfigurable string property return properties changed run time collection string get reconfigurable properties
14	common\src\java\org\apache\hadoop\conf\ReconfigurableBase.java	unrelated	package org apache hadoop conf utility base implementing reconfigurable subclasses reconfigure property impl change individual properties get reconfigurable properties get properties changed run time reconfigurable base extends configured implements reconfigurable log log reconfigurable base reconfigurable base configuration conf string reconfigure property string property string new val collection string get reconfigurable properties boolean property reconfigurable string property protected reconfigure property impl string property string new val
15	common\src\java\org\apache\hadoop\conf\ReconfigurationException.java	unrelated	package org apache hadoop conf exception indicating configuration property cannot changed run time reconfiguration exception extends exception serial version uid l string property string new val string old val construct exception message string construct message string property create new instance link reconfiguration exception reconfiguration exception create new instance link reconfiguration exception reconfiguration exception string property create new instance link reconfiguration exception reconfiguration exception string property get property cannot changed string get property get value property supposed changed string get new value get old value property cannot changed string get old value
16	common\src\java\org\apache\hadoop\conf\ReconfigurationServlet.java	unrelated	package org apache hadoop conf a servlet changing node configuration reloads configuration file verifies whether changes possible asks admin approve change reconfiguration servlet extends http servlet serial version uid l log log prefix used fing attribute holding reconfigurable given request get attribute prefix servlet path string conf servlet reconfigurable prefix inherit doc init throws servlet exception reconfigurable get reconfigurable http servlet request req print header print writer string node name print footer print writer print configuration options changed print conf print writer reconfigurable reconf enumeration string get params http servlet request req apply configuratio changes admin approved apply changes print writer reconfigurable reconf inherit doc protected get http servlet request req http servlet response resp inherit doc protected post http servlet request req http servlet response resp
17	common\src\java\org\apache\hadoop\conf\ReconfigurationUtil.java	unrelated	package org apache hadoop conf reconfiguration util property change string prop string old val string new val property change string prop string new val string old val collection property change get changed properties configuration new conf configuration old conf map string property change changes new hash map string property change iterate old configuration map entry string string old entry old conf iterate new configuration look properties present old conf map entry string string new entry new conf return changes values
18	common\src\java\org\apache\hadoop\fs\AbstractFileSystem.java	unrelated	package org apache hadoop fs this provides implementors hadoop file system analogous vfs unix applications access instead access files across file systems using link file context pathnames passed abstract file system fully qualified uri matches file system ie scheme authority slash relative name assumed relative root file system abstract file system log log log factory get log abstract file system recording statistics per file system map uri statistics cache constructors file system map class constructor constructor cache class uri config args the statistics file system protected statistics statistics uri uri statistics get statistics boolean valid name string src t t new instance class t class abstract file system create file system uri uri configuration conf protected synchronized statistics get statistics uri uri uri get base uri uri uri synchronized clear statistics synchronized print statistics protected synchronized map uri statistics get all statistics abstract file system get uri uri configuration conf abstract file system uri uri string supported scheme check scheme uri uri string supported scheme uri get uri uri uri string supported scheme get uri default port uri get uri check path path path string get uri path path p path make qualified path path path get initial working directory path get home directory fs server defaults get server defaults throws io exception fs data output stream create path f fs data output stream create internal path f mkdir path dir fs permission permission boolean delete path f boolean recursive fs data input stream open path f throws access control exception fs data input stream open path f buffer size boolean set replication path f rename path src path dst rename internal path src path dst rename internal path src path dst boolean supports symlinks create symlink path target path link path get link target path f throws io exception set permission path f set owner path f string username set times path f mtime file checksum get file checksum path f file status get file status path f file status get file link status path f block location get file block locations path f fs status get fs status path f throws access control exception fs status get fs status throws access control exception remote iterator file status list status iterator path f remote iterator located file status list located status path f file status list status path f remote iterator path list corrupt file blocks path path set verify checksum boolean verify checksum string get canonical service name list token get delegation tokens string renewer throws io exception hash code boolean equals object
19	common\src\java\org\apache\hadoop\fs\AvroFSInput.java	unrelated	package org apache hadoop fs adapts link fs data input stream avro seekable input avro fs input implements closeable seekable input fs data input stream stream len construct given link fs data input stream length avro fs input fs data input stream len construct given link file context link path avro fs input file context fc path p throws io exception length read byte b len throws io exception seek p throws io exception tell throws io exception close throws io exception
20	common\src\java\org\apache\hadoop\fs\BlockLocation.java	unrelated	package org apache hadoop fs a block location lists hosts offset length block block location implements writable register ctor string hosts hostnames datanodes string names hostname port number datanodes string topology paths full path name network topology offset offset block file length boolean corrupt default constructor block location constructor host name offset length block location string names string hosts offset constructor host name offset length corrupt flag block location string names string hosts offset constructor host name network topology offset length block location string names string hosts string topology paths constructor host name network topology offset length corrupt flag block location string names string hosts string topology paths get list hosts hostname hosting block string get hosts throws io exception get list names hostname port hosting block string get names throws io exception get list network topology paths hosts the last component path host string get topology paths throws io exception get start offset file associated block get offset get length block get length get corrupt flag boolean corrupt set start offset file associated block set offset offset set length block set length length set corrupt flag set corrupt boolean corrupt set hosts hosting block set hosts string hosts throws io exception set names host port hosting block set names string names throws io exception set network topology paths hosts set topology paths string topology paths throws io exception implement write writable write data output throws io exception implement read fields writable read fields data input throws io exception string string
21	common\src\java\org\apache\hadoop\fs\BufferedFSInputStream.java	unrelated	package org apache hadoop fs a optimizes reading fs input stream bufferring buffered fs input stream extends buffered input stream implements seekable positioned readable buffered fs input stream fs input stream size get pos throws io exception skip n throws io exception seek pos throws io exception boolean seek to new source target pos throws io exception read position byte buffer offset length throws io exception read fully position byte buffer offset length throws io exception read fully position byte buffer throws io exception
22	common\src\java\org\apache\hadoop\fs\ChecksumException.java	unrelated	package org apache hadoop fs thrown checksum errors checksum exception extends io exception serial version uid l pos checksum exception string description pos get pos
23	common\src\java\org\apache\hadoop\fs\ChecksumFileSystem.java	unrelated	package org apache hadoop fs abstract checksumed file system it provide basice implementation checksumed file system creates checksum file raw file it generates verifies checksums client side checksum file system extends filter file system byte checksum version new byte c r c bytes per checksum boolean verify checksum true get approx chk sum length size return checksum fs output summer chksum as fraction size checksum file system file system fs super fs set conf configuration conf super set conf conf conf null set whether verify checksum set verify checksum boolean verify checksum verify checksum verify checksum get raw file system file system get raw file system return fs return name checksum file associated file path get checksum file path file return new path file get parent file get name crc return true iff file checksum file name boolean checksum file path file string name file get name return name starts with name ends with crc return length checksum file given size actual file get checksum file length path file file size return get checksum length file size get bytes per sum return bytes per checksum get bytes per sum return bytes per checksum get sum buffer size bytes per sum buffer size default buffer size get conf get int proportional buffer size buffer size bytes per sum return math max bytes per sum for open fs input stream it verifies data matches checksums checksum fs input checker extends fs input checker log log checksum file system fs fs data input stream datas fs data input stream sums header length bytes per sum file len l checksum fs input checker checksum file system fs path file checksum fs input checker checksum file system fs path file buffer size get checksum file pos data pos protected get chunk position data pos available throws io exception read position byte b len close throws io exception boolean seek to new source target pos throws io exception protected read chunk pos byte buf offset len return file length get file length throws io exception skips discards code n code bytes data input stream the code skip code method skips smaller number bytes reaching end file code n code bytes skipped the actual number bytes skipped returned if code n code negative bytes skipped checksum exception chunk skip corrupted synchronized skip n throws io exception seek given position stream the next read position p this method allow seek past end file this produces io exception checksum exception chunk seek corrupted synchronized seek pos throws io exception opens fs data input stream indicated path fs data input stream open path f buffer size throws io exception return new fs data input stream inherit doc fs data output stream append path f buffer size throw new io exception not supported calculated length checksum file bytes get checksum length size bytes per sum checksum length equal size passed divided bytes per sum bytes written beginning checksum file return size bytes per sum bytes per sum this provides output stream checksummed file it generates checksums data checksum fs output summer extends fs
24	common\src\java\org\apache\hadoop\fs\ChecksumFs.java	unrelated	package org apache hadoop fs abstract checksumed fs it provide basic implementation checksumed fs creates checksum file raw file it generates verifies checksums client side checksum fs extends filter fs byte checksum version new byte c r c default bytes per checksum boolean verify checksum true get approx chk sum length size return checksum fs output summer chksum as fraction size checksum fs abstract file system fs throws io exception uri syntax exception super fs default bytes per checksum set whether verify checksum set verify checksum boolean verify checksum verify checksum verify checksum get raw file system abstract file system get raw fs return get my fs return name checksum file associated file path get checksum file path file return new path file get parent file get name crc return true iff file checksum file name boolean checksum file path file string name file get name return name starts with name ends with crc return length checksum file given size actual file get checksum file length path file file size return get checksum length file size get bytes per sum return bytes per checksum get bytes per sum return default bytes per checksum get sum buffer size bytes per sum buffer size throws io exception default buffer size get my fs get server defaults get file buffer size proportional buffer size buffer size bytes per sum return math max bytes per sum for open fs input stream it verifies data matches checksums checksum fs input checker extends fs input checker log log header length checksum fs fs fs data input stream datas fs data input stream sums bytes per sum file len l checksum fs input checker checksum fs fs path file checksum fs input checker checksum fs fs path file buffer size get checksum file pos data pos protected get chunk position data pos available throws io exception read position byte b len close throws io exception boolean seek to new source target pos throws io exception protected read chunk pos byte buf offset len return file length get file length throws io exception unresolved link exception skips discards code n code bytes data input stream the code skip code method skips smaller number bytes reaching end file code n code bytes skipped the actual number bytes skipped returned if code n code negative bytes skipped checksum exception chunk skip corrupted synchronized skip n throws io exception seek given position stream the next read position p this method allow seek past end file this produces io exception checksum exception chunk seek corrupted synchronized seek pos throws io exception opens fs data input stream indicated path fs data input stream open path f buffer size throws io exception unresolved link exception return new fs data input stream calculated length checksum file bytes get checksum length size bytes per sum checksum length equal size passed divided bytes per sum bytes written beginning checksum file return size bytes per sum bytes per sum this provides output stream checksummed file it generates checksums data checksum fs output summer extends fs output summer fs data output
25	common\src\java\org\apache\hadoop\fs\CommonConfigurationKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used common code it inherits publicly documented configuration keys adds unsupported keys common configuration keys extends common configuration keys public default location user home directories string fs home dir key fs home dir default value fs home dir key string fs home dir default user default umask files created hdfs string fs permissions umask key default value fs permissions umask key fs permissions umask default string fs client buffer dir key how often rpc client send pings rpc server string ipc ping interval key ipc ping interval default value ipc ping interval key ipc ping interval default enables pings rpc client server string ipc client ping key ipc client ping default value ipc client ping key boolean ipc client ping default true responses larger logged string ipc server rpc max response size key default value ipc server rpc max response size key ipc server rpc max response size default number threads rpc server reading socket string ipc server rpc read threads key default value ipc server rpc read threads key ipc server rpc read threads default how many calls per handler allowed queue string ipc server handler queue size key default value ipc server handler queue size key ipc server handler queue size default internal buffer size lzo compressor decompressors string io compression codec lzo buffersize key default value io compression codec lzo buffersize key io compression codec lzo buffersize default this specifying implementation mappings string net topology configured node mapping key internal buffer size snappy compressor decompressors string io compression codec snappy buffersize key default value io compression codec snappy buffersize key io compression codec snappy buffersize default
26	common\src\java\org\apache\hadoop\fs\CommonConfigurationKeysPublic.java	unrelated	package org apache hadoop fs this contains constants configuration keys used common code it includes publicly documented configuration keys in general used directly use common configuration keys instead common configuration keys public the keys see href doc root core default html core default xml string io native lib available key default value io native lib available key boolean io native lib available default true see href doc root core default html core default xml string net topology script number args key default value net topology script number args key net topology script number args default fs keys see href doc root core default html core default xml string fs default name key fs default fs default value fs default name key string fs default name default file see href doc root core default html core default xml string fs df interval key fs df interval default value fs df interval key fs df interval default defaults specified following keys see href doc root core default html core default xml string net topology script file name key see href doc root core default html core default xml string net topology node switch mapping impl key see href doc root core default html core default xml string fs trash checkpoint interval key default value fs trash checkpoint interval key fs trash checkpoint interval default tbd code still using hardcoded values e g fs automatic close instead constant e g fs automatic close key not used anywhere looks like default value fs local block size fs local block size default see href doc root core default html core default xml string fs automatic close key fs automatic close default value fs automatic close key boolean fs automatic close default true see href doc root core default html core default xml string fs file impl key fs file impl see href doc root core default html core default xml string fs ftp host key fs ftp host see href doc root core default html core default xml string fs ftp host port key fs ftp host port see href doc root core default html core default xml string fs trash interval key fs trash interval default value fs trash interval key fs trash interval default see href doc root core default html core default xml string io mapfile bloom size key default value io mapfile bloom size key io mapfile bloom size default see href doc root core default html core default xml string io mapfile bloom error rate key default value io mapfile bloom error rate key io mapfile bloom error rate default f codec implements lzo compression algorithm string io compression codec lzo class key see href doc root core default html core default xml string io map index interval key default value io map index interval default io map index interval default see href doc root core default html core default xml string io map index skip key io map index skip default value io map index skip key io map index skip default see href doc root core default html core default
27	common\src\java\org\apache\hadoop\fs\ContentSummary.java	unrelated	package org apache hadoop fs store summary content directory file content summary implements writable length file count directory count quota space consumed space quota constructor content summary constructor content summary length file count directory count constructor content summary get length return length get directory count return directory count get file count return file count return directory quota get quota return quota retuns disk space consumed get space consumed return space consumed returns disk space quota get space quota return space quota inherit doc write data output throws io exception inherit doc read fields data input throws io exception output format dir count file count content size file name string string format output format quota remaining quata space quota space quota rem dir count file count content size file name string quota string format string space quota string format the header string header string format string quota header string format return header output q option false output directory count file count content size q option true output quota remaining quota well string get header boolean q option inherit doc string string return representation object output format q option false output directory count file count content size q option true output quota remaining quota well string string boolean q option
28	common\src\java\org\apache\hadoop\fs\CreateFlag.java	unrelated	package org apache hadoop fs create flag specifies file create semantic users combine flags like br code enum set create flag create create flag append code p use create flag follows ol li create create file exist else throw file already exists li li append append file exists else throw file not found exception li li overwrite truncate file exists else throw file not found exception li li create append create file exist else append existing file li li create overwrite create file exist else overwrite existing file li ol following combination valid result link hadoop illegal argument exception ol li append overwrite li li create append overwrite li ol enum create flag create file see javadoc description already exists create short x truncate overwrite file same posix o trunc see javadoc description overwrite short x append file see javadoc description append short x short mode create flag short mode short get mode validate create flag throw exception invalid validate enum set create flag flag validate create flag create operation validate object path boolean path exists
29	common\src\java\org\apache\hadoop\fs\DelegateToFileSystem.java	unrelated	package org apache hadoop fs implementation abstract file system based existing implementation link file system delegate to file system extends abstract file system protected file system fs impl protected delegate to file system uri uri file system fs impl path get initial working directory fs data output stream create internal path f boolean delete path f boolean recursive throws io exception block location get file block locations path f start len file checksum get file checksum path f throws io exception file status get file status path f throws io exception file status get file link status path f throws io exception fs status get fs status throws io exception fs server defaults get server defaults throws io exception get uri default port file status list status path f throws io exception mkdir path dir fs permission permission boolean create parent fs data input stream open path f buffer size throws io exception rename internal path src path dst throws io exception set owner path f string username string groupname set permission path f fs permission permission boolean set replication path f short replication set times path f mtime atime throws io exception set verify checksum boolean verify checksum throws io exception boolean supports symlinks create symlink path target path link boolean create parent path get link target path f throws io exception string get canonical service name list token get delegation tokens string renewer throws io exception
30	common\src\java\org\apache\hadoop\fs\DF.java	unrelated	package org apache hadoop fs filesystem disk space usage statistics uses unix df program get mount points java io file space utilization tested linux free bsd cygwin df extends shell default df refresh interval df interval default string dir path file dir file string filesystem string mount enum os type string os name system get property os name os type os type get os type os name protected os type get os type string os name df file path configuration conf throws io exception df file path df interval throws io exception protected os type get os type accessors string get dir path string get filesystem throws io exception get capacity get used get available get percent used string get mount throws io exception string string protected string get exec string protected parse exec result buffered reader lines throws io exception main string args throws exception
31	common\src\java\org\apache\hadoop\fs\DU.java	unrelated	package org apache hadoop fs filesystem disk space usage statistics uses unix du program du extends shell string dir path atomic long used new atomic long volatile boolean run true thread refresh used io exception du exception null refresh interval keeps track disk usage du file path interval throws io exception super set shell interval always run command use one set thread sleep interval refresh interval interval dir path path get canonical path populate used variable run keeps track disk usage du file path configuration conf throws io exception path l minutes default refresh interval this thread refreshes used variable future improvements could permanently run thread instead run get used called du refresh thread implements runnable run decrease much disk space use dec dfs used value used add and get value increase much disk space use inc dfs used value used add and get value get used throws io exception updating thread started update demand refresh used null else return used value string get dir path return dir path start disk usage checking thread start start thread interval sane refresh interval shut refreshing thread shutdown run false refresh used null string string return protected string get exec string return new string du sk dir path protected parse exec result buffered reader lines throws io exception string line lines read line line null string tokens line split tokens length used set long parse long tokens main string args throws exception string path args length system println new du new file path new configuration string
32	common\src\java\org\apache\hadoop\fs\FileAlreadyExistsException.java	unrelated	package org apache hadoop fs used target file already exists operation configured overwritten file already exists exception file already exists exception file already exists exception string msg
33	common\src\java\org\apache\hadoop\fs\FileChecksum.java	unrelated	package org apache hadoop fs an representing file checksums files file checksum implements writable the checksum algorithm name string get algorithm name the length checksum bytes get length the value checksum bytes byte get bytes return true algorithms values boolean equals object inherit doc hash code
34	common\src\java\org\apache\hadoop\fs\FileContext.java	unrelated	package org apache hadoop fs the file context provides application writer using hadoop file system it provides set methods usual operation create open list etc p b path names b p the hadoop file system supports uri name space uri names it offers forest file systems referenced using fully qualified ur is two common hadoop file systems implementations ul li local file system file path li hdfs file system hdfs nn address nn port path ul while uri names flexible requires knowing name address server for convenience one often wants access default system one environment without knowing name address this additional benefit allows one change one default fs e g admin moves application cluster cluster p to facilitate hadoop supports notion default file system the user set default file system although typically set environment via default config a default file system implies default scheme authority slash relative names bar resolved relative default fs similarly user also working directory relative names e names starting slash while working directory generally default fs wd different fs p hence hadoop path names one ul li fully qualified uri scheme authority path li slash relative names path relative default file system li wd relative names path relative working dir ul relative paths scheme scheme foo bar illegal p b the role file context configuration defaults b p the file context provides file namespace context resolving file names also contains umask permissions in sense like per process file related state unix system these two properties ul li default file system e slash li umask ul general obtained default configuration file environment see link configuration no configuration parameters obtained default config far file context layer concerned all file system instances e deployments file systems default properties call server side ss defaults operation like create allow one select many properties either pass explicit parameters use ss properties p the file system related ss defaults ul li home directory default user user name li initial wd local fs li replication factor li block size li buffer size li bytes per checksum used ul p b usage model file context b p example use default config read hadoop config core xml unspecified values come core defaults xml release jar ul li f context file context get file context uses default config default fs li f context create path li f context set working dir path li f context open path ul example get file context specific uri default fs ul li f context file context get file context uri li f context create path ul example file context local file system default ul li f context file context get local fs file context li f context create path li ul example use specific config ignoring hadoop config generally need use config unless ul li config x config some one passed to you li f context get file context config x config x changed passed li f context create path li ul file context log log log factory get log file context fs permission default perm fs permission get default map file context
35	common\src\java\org\apache\hadoop\fs\FileStatus.java	unrelated	package org apache hadoop fs interface represents client side information file file status implements writable comparable path path length boolean isdir short block replication blocksize modification time access time fs permission permission string owner string group path symlink file status false null null null null we deprecate soon file status length boolean isdir block replication constructor file systems symbolic links supported file status length boolean isdir file status length boolean isdir get length file bytes get len is file boolean file is directory boolean directory old instead use explicit link file status file link file status directory link file status symlink link file status directory link file status symlink instead boolean dir is symbolic link boolean symlink get block size file get block size get replication factor file short get replication get modification time file get modification time get access time file get access time get fs permission associated file permissions could determined default permissions equivalent rwxrwxrwx returned fs permission get permission get owner file notion owner file filesystem could determined rare string get owner get group associated file notion group file filesystem could determined rare string get group path get path set path path p these provided values could loaded lazily filesystem e g local file system sets permission protected set permission fs permission permission sets owner protected set owner string owner sets group protected set group string group path get symlink throws io exception set symlink path p writable write data output throws io exception read fields data input throws io exception compare object another object less equal greater specified object type file status compare to object compare object equal another object boolean equals object returns hash code value object defined hash code path name hash code
36	common\src\java\org\apache\hadoop\fs\FileSystem.java	unrelated	package org apache hadoop fs an base fairly generic filesystem it may implemented distributed filesystem local one reflects locally connected disk the local version exists small hadoop instances testing p all user code may potentially use hadoop distributed file system written use file system object the hadoop dfs multi machine system appears single disk it useful fault tolerance potentially large capacity p the local implementation link local file system distributed implementation distributed file system file system extends configured implements closeable string fs default name key string default fs log log log factory get log file system file system cache cache cache new cache the key instance stored cache cache key key recording statistics per file system map class extends file system statistics protected statistics statistics set path delete on exit new tree set path add file system for testing uri uri configuration conf file system get uri uri configuration conf file system get configuration conf throws io exception get default filesystem uri configuration uri get default uri configuration conf set default filesystem uri configuration set default uri configuration conf uri uri set default filesystem uri configuration set default uri configuration conf string uri called new file system instance constructed initialize uri name configuration conf throws io exception returns uri whose scheme authority identify file system uri get uri protected get default port string get canonical service name string get name return get uri string file system get named string name configuration conf update old format filesystem names back compatibility this string fix name string name local file system get local configuration conf returns file system uri scheme authority the scheme file system get uri uri configuration conf throws io exception file system new instance uri uri configuration conf returns file system uri scheme authority the scheme file system new instance uri uri configuration conf throws io exception returns unique configured filesystem implementation file system new instance configuration conf throws io exception local file system new instance local configuration conf close all throws io exception close all for ugi user group information ugi throws io exception path make qualified path path token get delegation token string renewer throws io exception list token get delegation tokens string renewer throws io exception create file provided permission fs data output stream create file system fs create directory provided permission boolean mkdirs file system fs path dir fs permission permission throws io exception file system protected file system protected check path path path block location get file block locations file status file block location get file block locations path p fs server defaults get server defaults throws io exception fs data input stream open path f buffer size fs data input stream open path f throws io exception fs data output stream create path f throws io exception fs data output stream create path f boolean overwrite fs data output stream create path f progressable progress fs data output stream create path f short replication fs data output stream create path f short replication fs data output stream create path f fs data output stream create path
37	common\src\java\org\apache\hadoop\fs\FileUtil.java	unrelated	package org apache hadoop fs a collection file processing util methods file util log log log factory get log file util convert array file status array path array file status objects path stat paths file status stats stats null path ret new path stats length stats length return ret convert array file status array path if stats null return path array file status objects default path return stats null path stat paths file status stats path path stats null else delete directory contents if return false directory may partially deleted if dir symlink file symlink deleted the file pointed symlink deleted if dir symlink directory symlink deleted the directory pointed symlink deleted if dir normal file deleted if dir normal directory dir contents recursively deleted boolean fully delete file dir throws io exception dir delete handle nonempty directory deletion fully delete contents dir return dir delete delete contents directory directory if return false directory may partially deleted if dir symlink directory contents actual directory pointed dir deleted boolean fully delete contents file dir throws io exception boolean deletion succeeded true file contents dir list files contents null return deletion succeeded recursively delete directory fully delete file system fs path dir throws io exception fs delete dir true if destination subdirectory source generate exception check dependencies file system src fs src fs dst fs copy files file systems boolean copy file system src fs path src return copy src fs src dst fs dst delete source true conf boolean copy file system src fs path srcs boolean got exception false boolean return val true string builder exceptions new string builder srcs length check dest directory dst fs exists dst else path src srcs got exception return return val copy files file systems boolean copy file system src fs path src file status file status src fs get file status src return copy src fs file status dst fs dst delete source overwrite conf copy files file systems boolean copy file system src fs file status src status path src src status get path dst check dest src get name dst fs dst overwrite src status directory else delete source else copy files directory one output file merge boolean copy merge file system src fs path src dir dst file check dest src dir get name dst fs dst file false src fs get file status src dir directory output stream dst fs create dst file try finally delete source else copy local files file system boolean copy file src dst check dest src get name dst fs dst false src directory else src file else delete source else copy file system files local files boolean copy file system src fs path src file status filestatus src fs get file status src return copy src fs filestatus dst delete source conf copy file system files local files boolean copy file system src fs file status src status path src src status get path src status directory else delete source else path check dest string src name file system dst fs path dst dst fs
38	common\src\java\org\apache\hadoop\fs\FilterFileSystem.java	unrelated	package org apache hadoop fs a code filter file system code contains file system uses basic file system possibly transforming data along way providing additional functionality the code filter file system code simply overrides methods code file system code versions pass requests contained file system subclasses code filter file system code may methods may also provide additional methods fields filter file system extends file system protected file system fs filter file system filter file system file system fs called new file system instance constructed initialize uri name configuration conf throws io exception returns uri whose scheme authority identify file system uri get uri make sure path specifies file system path make qualified path path file system check path belongs file system protected check path path path block location get file block locations file status file start path resolve path path p throws io exception fs data input stream open path f buffer size throws io exception inherit doc fs data output stream append path f buffer size inherit doc fs data output stream create path f fs permission permission boolean set replication path src short replication throws io exception boolean rename path src path dst throws io exception delete file boolean delete path f boolean recursive throws io exception boolean delete on exit path f throws io exception list files directory file status list status path f throws io exception remote iterator path list corrupt file blocks path path list files block locations directory remote iterator located file status list located status path f throws io exception path get home directory set working directory path new dir path get working directory protected path get initial working directory inherit doc fs status get status path p throws io exception inherit doc boolean mkdirs path f fs permission permission throws io exception copy from local file boolean del src path src path dst copy from local file boolean del src boolean overwrite copy from local file boolean del src boolean overwrite copy to local file boolean del src path src path dst path start local output path fs output file path tmp local file complete local output path fs output file path tmp local file return total size files filesystem get used throws io exception return number bytes large input files optimally get default block size short get default replication file status get file status path f throws io exception inherit doc file checksum get file checksum path f throws io exception inherit doc set verify checksum boolean verify checksum configuration get conf close throws io exception inherit doc set owner path p string username string groupname inherit doc set times path p mtime atime inherit doc set permission path p fs permission permission protected fs data output stream primitive create path f protected boolean primitive mkdir path f fs permission abdolute permission string get canonical service name token get delegation token string renewer throws io exception list token get delegation tokens string renewer throws io exception
39	common\src\java\org\apache\hadoop\fs\FilterFs.java	unrelated	package org apache hadoop fs licensed apache software foundation asf one contributor license agreements see notice file distributed work additional information regarding copyright ownership the asf licenses file apache license version license may use file except compliance license you may obtain copy license http www apache org licenses license unless required applicable law agreed writing software distributed license distributed as is basis without warranties or conditions of any kind either express implied see license specific language governing permissions limitations license a code filter fs code contains file system uses basic file system possibly transforming data along way providing additional functionality the code filter fs code simply overrides methods code abstract file system code versions pass requests contained file system subclasses code filter fs code may methods may also provide additional methods fields filter fs extends abstract file system abstract file system fs protected abstract file system get my fs protected filter fs abstract file system fs throws io exception statistics get statistics path make qualified path path path get initial working directory path get home directory fs data output stream create internal path f boolean delete path f boolean recursive block location get file block locations path f start len file checksum get file checksum path f file status get file status path f file status get file link status path f fs status get fs status path f throws access control exception fs status get fs status throws io exception fs server defaults get server defaults throws io exception path resolve path path p throws file not found exception get uri default port uri get uri check path path path string get uri path path p file status list status path f remote iterator path list corrupt file blocks path path mkdir path dir fs permission permission boolean create parent fs data input stream open path f throws access control exception fs data input stream open path f buffer size rename internal path src path dst rename internal path src path dst set owner path f string username string groupname set permission path f fs permission permission boolean set replication path f short replication set times path f mtime atime set verify checksum boolean verify checksum boolean supports symlinks create symlink path target path link boolean create parent path get link target path f throws io exception string get canonical service name list token get delegation tokens string renewer throws io exception
40	common\src\java\org\apache\hadoop\fs\FsConfig.java	unrelated	package org apache hadoop fs this thin layer manage fs related keys configuration object it provides convenience method set get keys configuration fs config fs config configuration keys default values config file tbd note deprecate keys constants elsewhere the keys string fs replication factor key dfs replication string fs block size key dfs block size the default values default values server default implies use ones target file system files created short fs default replication factor fs default block size string get default fs uri configuration conf string get home dir configuration conf short get default replication factor configuration conf get default block size configuration conf get default io buffersize configuration conf class get impl class uri uri configuration conf set default fs configuration conf string uri set home dir configuration conf string path set default replication factor configuration conf set default block size configuration conf bs set default io buffersize configuration conf bs
41	common\src\java\org\apache\hadoop\fs\FsConstants.java	unrelated	package org apache hadoop fs file system related constants fs constants uri local filesystem uri local fs uri uri create file uri scheme ftp string ftp scheme ftp view fs view fs file system ie mount file system client side uri viewfs uri uri create viewfs string viewfs scheme viewfs
42	common\src\java\org\apache\hadoop\fs\FSDataInputStream.java	unrelated	package org apache hadoop fs utility wraps link fs input stream link data input stream buffers input link buffered input stream fs data input stream extends data input stream fs data input stream input stream seek given offset synchronized seek desired throws io exception get current position input stream get pos throws io exception read bytes given position stream given buffer data end stream reached read position byte buffer offset length read bytes given position stream given buffer continues read code length code bytes read if exception thrown undetermined number bytes buffer may written read fully position byte buffer offset length see link read fully byte read fully position byte buffer seek given position alternate copy data boolean seek to new source target pos throws io exception get reference wrapped input stream used unit tests input stream get wrapped stream
43	common\src\java\org\apache\hadoop\fs\FSDataOutputStream.java	unrelated	package org apache hadoop fs utility wraps link output stream link data output stream buffers output link buffered output stream creates checksum file fs data output stream extends data output stream implements syncable output stream wrapped stream position cache extends filter output stream file system statistics statistics position position cache output stream write b throws io exception write byte b len throws io exception get pos throws io exception close throws io exception fs data output stream output stream throws io exception null fs data output stream output stream file system statistics stats throws io exception stats fs data output stream output stream file system statistics stats super new position cache stats start position wrapped stream get current position output stream get pos throws io exception return position cache get pos close underlying output stream close throws io exception close this invokes position cache close get reference wrapped output stream used unit tests output stream get wrapped stream return wrapped stream sync throws io exception wrapped stream instanceof syncable hflush throws io exception wrapped stream instanceof syncable else hsync throws io exception wrapped stream instanceof syncable else
44	common\src\java\org\apache\hadoop\fs\FSError.java	unrelated	package org apache hadoop fs thrown unexpected filesystem errors presumed reflect disk errors native filesystem fs error extends error serial version uid l fs error throwable cause
45	common\src\java\org\apache\hadoop\fs\FSInputChecker.java	unrelated	package org apache hadoop fs this generic input stream verifying checksums data read user fs input checker extends fs input stream log log log factory get log fs input checker the file name data read protected path file checksum sum boolean verify checksum true max chunk size data bytes checksum eg byte buf buffer non chunk aligned reading byte checksum int buffer checksum ints wrapper checksum buffer pos position reader inside buf count number bytes currently buf num of retries cached file position always multiple max chunk size chunk pos number checksum chunks read user buffer chosen benchmarks higher values reduce cpu usage the size data reads made underlying stream chunks per read max chunk size chunks per read protected checksum size bit checksum constructor protected fs input checker path file num of retries constructor protected fs input checker path file num of retries reads checksum chunks code buf code code offset code checksum code checksum code since checksums disabled two cases implementors need worry need checksum return false len positive value checksum null implementors simply pass underlying data stream b need checksum return true len max chunk size checksum length multiple checksum size implementors read integer number data chunks buf the amount read bounded len checksum length checksum size max chunk size note len may value multiple max chunk size case implementation may return less len the method used implementing read therefore optimized sequential reading protected read chunk pos byte buf offset len return position beginning chunk containing pos protected get chunk position pos return true need checksum verification protected synchronized boolean need checksum read one checksum verified byte stream reached synchronized read throws io exception read checksum verified bytes byte input stream specified byte array starting given offset p this method implements general contract corresponding code link input stream read byte read code method code link input stream code as additional convenience attempts read many bytes possible repeatedly invoking code read code method underlying stream this iterated code read code continues one following conditions becomes true ul li the specified number bytes read li the code read code method underlying stream returns code code indicating end file ul if first code read code underlying stream returns code code indicate end file method returns code code otherwise method returns number bytes actually read stream reached checksum exception checksum error occurs synchronized read byte b len throws io exception fills buffer chunk data no mark supported this method assumes data buffer already read hence pos count fill throws io exception read characters portion array reading underlying stream necessary read byte b len throws io exception read one checksum chunk array b pos it requires least one checksum chunk boundary cur pos cur pos len stops reading last boundary end stream otherwise illegal argument exception thrown this makes sure data read checksum verified data written code code data end stream reached read checksum chunk byte b len throws io exception verify sums byte b read convert checksum byte array this deprecated since since longer use checksum byte checksum synchronized get pos throws io
46	common\src\java\org\apache\hadoop\fs\FSInputStream.java	unrelated	package org apache hadoop fs fs input stream generic old input stream little bit raf style seek ability fs input stream extends input stream seek given offset start file the next read location can seek past end file seek pos throws io exception return current offset start file get pos throws io exception seeks different copy data returns true found new source false otherwise boolean seek to new source target pos throws io exception read position byte buffer offset length read fully position byte buffer offset length read fully position byte buffer
47	common\src\java\org\apache\hadoop\fs\FSOutputSummer.java	unrelated	package org apache hadoop fs this generic output stream generating checksums data written underlying stream fs output summer extends output stream data checksum checksum sum internal buffer storing data checksumed byte buf internal buffer storing checksum byte checksum the number valid bytes buffer count protected fs output summer checksum sum max chunk size checksum size write data chunk code b code staring code offset code length code len code checksum protected write chunk byte b offset len byte checksum throws io exception write one byte synchronized write b throws io exception writes code len code bytes specified byte array starting offset code code generate checksum data chunk p this method stores bytes given array stream buffer gets checksumed the buffer gets checksumed flushed underlying output stream data checksum chunk buffer if buffer empty requested length least large size next checksum chunk size method checksum write chunk directly underlying output stream thus avoids uneccessary data copy synchronized write byte b len throws io exception write portion array flushing underlying stream necessary write byte b len throws io exception forces buffered output bytes checksumed written underlying output stream protected synchronized flush buffer throws io exception forces buffered output bytes checksumed written underlying output stream if keep true state object remains intact protected synchronized flush buffer boolean keep throws io exception return number valid bytes currently buffer protected synchronized get buffered data size generate checksum data chunk output data chunk checksum underlying output stream if keep true keep current checksum intact reset write checksum chunk byte b len boolean keep throws io exception converts checksum integer value byte stream byte convert to byte stream checksum sum checksum size byte byte integer byte bytes resets existing buffer new one specified size protected synchronized reset checksum chunk size
48	common\src\java\org\apache\hadoop\fs\FsServerDefaults.java	unrelated	package org apache hadoop fs provides server default configuration values clients fs server defaults implements writable register ctor block size bytes per checksum write packet size short replication file buffer size fs server defaults fs server defaults block size bytes per checksum get block size get bytes per checksum get write packet size short get replication get file buffer size writable write data output throws io exception read fields data input throws io exception
49	common\src\java\org\apache\hadoop\fs\FsShell.java	unrelated	package org apache hadoop fs provide command line access file system fs shell extends configured implements tool log log log factory get log fs shell file system fs trash trash protected command factory command factory string usage prefix usage hadoop fs generic options default ctor configuration be sure invoke link set conf configuration valid configuration prior running commands fs shell null construct fs shell given configuration commands executed via link run string fs shell configuration conf super conf protected file system get fs throws io exception fs null return fs protected trash get trash throws io exception trash null return trash protected init throws io exception get conf set quiet mode true command factory null protected register commands command factory factory todo dfs admin subclasses fs shell need protect command registration this morph base commands method get class equals fs shell returns trash object associated shell path get current trash dir throws io exception return get trash get current trash dir note usage help inner allow access outer methods access command factory display help commands short usage description protected usage extends fs command string name usage string usage cmd string description protected process raw arguments linked list string args displays short usage commands sans description protected help extends fs command string name help string usage cmd string description protected process raw arguments linked list string args the following helper methods get info they defined outside scope help usage run method needs invoke print usages print usage print stream print info null false print one usage print usage print stream string cmd print info cmd false print helps print help print stream print info null true print one help print help print stream string cmd print info cmd true print info print stream string cmd boolean show help cmd null else print instance usage print stream command instance println usage prefix instance get usage todo eventually auto wrap text matches expected output hdfs tests print instance help print stream command instance boolean first line true string line instance get description split n run run string argv throws exception initialize fs shell init exit code argv length else return exit code display error string cmd string message string line message split n performs necessary cleanup close throws io exception fs null main simple utility methods main string argv throws exception fs shell shell new shell instance res try finally system exit res todo base protected fs shell new shell instance return new fs shell the default ctor signals command executed exist ctor signals specific command exist the latter used commands process commands ex usage help unknown command exception extends illegal argument exception string cmd unknown command exception null unknown command exception string cmd cmd cmd string get message
50	common\src\java\org\apache\hadoop\fs\FsShellPermissions.java	unrelated	package org apache hadoop fs this home file permissions related commands moved separate since fs shell getting large fs shell permissions extends fs command log log fs shell log register commands command factory factory chmod extends fs shell permissions used chown chgrp string allowed chars z a z chown extends fs shell permissions chgrp extends chown
51	common\src\java\org\apache\hadoop\fs\FsStatus.java	unrelated	package org apache hadoop fs this used represent capacity free used space link file system fs status implements writable capacity used remaining construct fs status object using specified statistics fs status capacity used remaining return capacity bytes file system get capacity return number bytes used file system get used return number remaining bytes file system get remaining writable write data output throws io exception read fields data input throws io exception
52	common\src\java\org\apache\hadoop\fs\FsUrlConnection.java	unrelated	package org apache hadoop fs representation url connection open input streams fs url connection extends url connection configuration conf input stream fs url connection configuration conf url url connect throws io exception input stream get input stream throws io exception
53	common\src\java\org\apache\hadoop\fs\FsUrlStreamHandler.java	unrelated	package org apache hadoop fs url stream handler relying file system given configuration handle url protocols fs url stream handler extends url stream handler configuration conf fs url stream handler configuration conf fs url stream handler protected fs url connection open connection url url throws io exception
54	common\src\java\org\apache\hadoop\fs\FsUrlStreamHandlerFactory.java	unrelated	package org apache hadoop fs factory url stream handlers there one handler whose job create url connections a fs url connection relies file system choose appropriate fs implementation before returning handler make sure file system knows implementation requested scheme protocol fs url stream handler factory implements url stream handler factory the configuration holds supported fs implementation names configuration conf this map stores whether protocol know file system map string boolean protocols new hash map string boolean the url stream handler java net url stream handler handler fs url stream handler factory conf new configuration force resolution configuration files required want factory able handle file ur ls conf get class fs file impl null handler new fs url stream handler conf fs url stream handler factory configuration conf conf new configuration conf force resolution configuration files conf get class fs file impl null handler new fs url stream handler conf java net url stream handler create url stream handler string protocol protocols contains key protocol protocols get protocol else
55	common\src\java\org\apache\hadoop\fs\GlobExpander.java	unrelated	package org apache hadoop fs glob expander string with offset string offset string with offset string offset expand globs given code file pattern code collection file patterns expanded set file pattern slash character curly bracket pair list string expand string file pattern throws io exception list string fully expanded new array list string list string with offset expand new array list string with offset expand add new string with offset file pattern expand empty return fully expanded expand leftmost outer curly bracket pair containing slash character code file pattern code list string with offset expand leftmost string with offset string file pattern file pattern with offset leftmost leftmost outer curly containing slash file pattern leftmost curly open string builder prefix new string builder file pattern substring leftmost string builder suffix new string builder list string alts new array list string string builder alt new string builder string builder cur prefix leftmost file pattern length list string with offset exp new array list string with offset string alts return exp finds index leftmost opening curly bracket containing slash character code file pattern code slash character bracket leftmost outer curly containing slash string file pattern curly open leftmost boolean seen slash false offset file pattern length return
56	common\src\java\org\apache\hadoop\fs\GlobFilter.java	unrelated	package org apache hadoop fs a could decide matches glob glob filter implements path filter path filter default filter new path filter path filter user filter default filter glob pattern pattern glob filter string file pattern throws io exception glob filter string file pattern path filter filter throws io exception init string file pattern path filter filter throws io exception boolean pattern boolean accept path path
57	common\src\java\org\apache\hadoop\fs\GlobPattern.java	unrelated	package org apache hadoop fs a posix glob pattern brace expansions glob pattern char backslash pattern compiled boolean wildcard false glob pattern string glob pattern pattern compiled pattern compile string glob pattern boolean matches char sequence set string glob boolean wildcard error string message string pattern pos
58	common\src\java\org\apache\hadoop\fs\HardLink.java	unrelated	package org apache hadoop fs class creating hardlinks supports unix linux win xp vista via cygwin mac os x the hard link formerly inner fs util methods provided blatantly non thread safe to enable volume parallel update snapshots provide threadsafe methods allocate new buffer arrays upon call we also provide api hardlink files directory single command times efficient minimizes impact extra buffer creations hard link enum os type os type os type hard link command getter get hard link command link stats link stats initialize command getters statically use methods without instantiating hard link object hard link os type get os type hard link command getter hard link cg unix extends hard link command getter hard link cg win extends hard link command getter protected get link mult arg length throws io exception protected get max allowed cmd arg length create hard link file file file link name throws io exception create hard link mult file parent dir string file base names protected create hard link mult file parent dir throws io exception get link count file file name throws io exception create io exception failing get link count io exception create io exception file f string message link stats
59	common\src\java\org\apache\hadoop\fs\InvalidPathException.java	unrelated	package org apache hadoop fs path invalid either invalid characters due file system specific reasons invalid path exception extends hadoop illegal argument exception serial version uid l constructs exception specified detail message invalid path exception string path constructs exception specified detail message invalid path exception string path string reason
60	common\src\java\org\apache\hadoop\fs\LocalDirAllocator.java	unrelated	package org apache hadoop fs an implementation round robin scheme disk allocation creating files the way works kept track disk last allocated file write for current request next disk set disks would allocated free space disk sufficient enough accommodate file considered creation if space requirements cannot met next disk order would tried till disk found sufficient capacity once disk sufficient space identified check done make sure disk writable also api provided take space requirements consideration checks whether disk consideration writable used cases file size known apriori an api provided read path created earlier that api works scan disks input pathname this implementation also provides functionality multiple allocators per jvm one unique functionality context like mapred dfs client etc it ensures one instance allocator per context per jvm note the contexts referred actually configuration items defined configuration like mapred local dir want control dir allocations the context strings exactly configuration items this implementation take consideration cases disk becomes read goes space file written disks shared multiple processes latter situation probable in implementation disk referred dir actually points configured directory disk parent file write read allocations local dir allocator a map config item names like mapred local dir dfs client buffer dir instance allocator per context this object make sure exists exactly one instance per jvm map string allocator per context contexts string context cfg item name used size file allocated unknown size unknown create allocator object local dir allocator string context cfg item name this method must used obtain dir allocation context allocator per context obtain context string context cfg item name get path local fs this method used size path get local path for write string path str get path local fs pass size path get local path for write string path str size get path local fs reading we search path get local path to read string path str creates temporary file local fs pass size known file create tmp file for write string path str size method check whether context valid boolean context valid string context cfg item name we search configured dirs file existence boolean exists string path str configuration conf get current directory index allocator per context
61	common\src\java\org\apache\hadoop\fs\LocalFileSystem.java	unrelated	package org apache hadoop fs implement file system api checksumed local filesystem local file system extends checksum file system uri name uri create file random rand new random file system rfs local file system file system get raw local file system file system raw local file system convert path file file path to file path path copy from local file boolean del src path src path dst copy to local file boolean del src path src path dst moves files bad file directory device storage reused boolean report checksum failure path p fs data input stream
62	common\src\java\org\apache\hadoop\fs\LocalFileSystemConfigKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used local file system raw local fs checksum fs local file system config keys extends common configuration keys string local fs block size key file blocksize local fs block size default string local fs replication key file replication short local fs replication default string local fs stream buffer size key local fs stream buffer size default string local fs bytes per checksum key local fs bytes per checksum default string local fs client write packet size key local fs client write packet size default
63	common\src\java\org\apache\hadoop\fs\LocatedFileStatus.java	unrelated	package org apache hadoop fs this defines file status includes file block locations located file status extends file status block location locations located file status file status stat block location locations throws io exception located file status length boolean isdir block location get block locations compare to object compare object equal another object boolean equals object hash code
64	common\src\java\org\apache\hadoop\fs\MD5MD5CRC32FileChecksum.java	unrelated	package org apache hadoop fs md md crc md md crc file checksum extends file checksum length md hash md len bytes per crc crc per block md hash md same null md md crc file checksum create md file checksum md md crc file checksum bytes per crc crc per block md hash md inherit doc string get algorithm name inherit doc get length return length inherit doc byte get bytes inherit doc read fields data input throws io exception inherit doc write data output throws io exception write object xml output write xml outputter xml md md crc file checksum return object represented attributes md md crc file checksum value of attributes attrs inherit doc string string
65	common\src\java\org\apache\hadoop\fs\Options.java	unrelated	package org apache hadoop fs this contains options related file system operations options create opts enum rename
66	common\src\java\org\apache\hadoop\fs\ParentNotDirectoryException.java	unrelated	package org apache hadoop fs indicates parent specified path directory expected parent not directory exception extends io exception serial version uid l parent not directory exception parent not directory exception string msg
67	common\src\java\org\apache\hadoop\fs\Path.java	unrelated	package org apache hadoop fs names file directory link file system path strings use slash directory separator a path absolute begins slash path implements comparable the directory separator slash string separator char separator char string cur dir boolean windows uri uri hierarchical uri resolve child path parent path path string parent string child resolve child path parent path path path parent string child resolve child path parent path path string parent path child resolve child path parent path path path parent path child check path arg string path construct path string path strings ur is unescaped elements additional normalization path string path string construct path uri path uri uri construct path components path string scheme string authority string path initialize string scheme string authority string path string normalize path string path boolean windows drive string path boolean slashed convert uri uri uri return uri return file system owns path file system get file system configuration conf throws io exception is absolute path ie slash relative path part and scheme null and authority null boolean absolute and scheme authority null true path component e directory uri absolute boolean uri path absolute true path component uri absolute there ambiguity an absolute path slash relative name without scheme authority so either method incorrectly named implementation incorrect boolean absolute returns component path string get name returns parent path null root path get parent adds suffix name path path suffix string suffix string string boolean equals object hash code compare to object return number elements path depth returns qualified path object deprecated use link make qualified uri path path make qualified file system fs returns qualified path object path make qualified uri default uri path working dir
68	common\src\java\org\apache\hadoop\fs\PathFilter.java	unrelated	package org apache hadoop fs path filter tests whether specified pathname included pathname list included boolean accept path path
69	common\src\java\org\apache\hadoop\fs\PositionedReadable.java	unrelated	package org apache hadoop fs stream permits positional reading positioned readable read upto specified number bytes given position within file return number bytes read this change current offset file thread safe read position byte buffer offset length read specified number bytes given position within file this change current offset file thread safe read fully position byte buffer offset length read number bytes equalt length buffer given position within file this change current offset file thread safe read fully position byte buffer throws io exception
70	common\src\java\org\apache\hadoop\fs\RawLocalFileSystem.java	unrelated	package org apache hadoop fs implement file system api raw local filesystem raw local file system extends file system uri name uri create file path working dir raw local file system working dir get initial working directory path make absolute path f f absolute else convert path file file path to file path path check path path path absolute return new file path uri get path uri get uri return name initialize uri uri configuration conf throws io exception super initialize uri conf set conf conf tracking file input stream extends file input stream tracking file input stream file f throws io exception read throws io exception read byte data throws io exception read byte data offset length throws io exception for open fs input stream local fs file input stream extends fs input stream file input stream fis position local fs file input stream path f throws io exception seek pos throws io exception get pos throws io exception boolean seek to new source target pos throws io exception just forward fis available throws io exception return fis available close throws io exception fis close boolean mark supported return false read throws io exception read byte b len throws io exception read position byte b len skip n throws io exception fs data input stream open path f buffer size throws io exception exists f return new fs data input stream new buffered fs input stream for create fs output stream local fs file output stream extends output stream file output stream fos local fs file output stream path f boolean append throws io exception just forward fos close throws io exception fos close flush throws io exception fos flush write byte b len throws io exception write b throws io exception inherit doc fs data output stream append path f buffer size exists f get file status f directory return new fs data output stream new buffered output stream inherit doc fs data output stream create path f boolean overwrite buffer size short replication block size progressable progress throws io exception exists f overwrite path parent f get parent parent null mkdirs parent return new fs data output stream new buffered output stream inherit doc fs data output stream create path f fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception fs data output stream create f set permission f permission return boolean rename path src path dst throws io exception path to file src rename to path to file dst return file util copy src dst true get conf boolean delete path p boolean recursive throws io exception file f path to file p f file else recursive f directory return file util fully delete f file status list status path f throws io exception file localf path to file f file status results localf exists localf file string names localf list names null results new file status names length j names length j names length return arrays copy of results j creates specified directory hierarchy does treat existence error boolean mkdirs
71	common\src\java\org\apache\hadoop\fs\RemoteIterator.java	unrelated	package org apache hadoop fs an iterator collection whose elements need fetched remotely remote iterator e returns tt true tt iteration elements boolean next throws io exception returns next element iteration e next throws io exception
72	common\src\java\org\apache\hadoop\fs\Seekable.java	unrelated	package org apache hadoop fs stream permits seeking seekable seek given offset start file the next read location can seek past end file seek pos throws io exception return current offset start file get pos throws io exception seeks different copy data returns true found new source false otherwise boolean seek to new source target pos throws io exception
73	common\src\java\org\apache\hadoop\fs\Syncable.java	unrelated	package org apache hadoop fs this flush sync operation syncable flush data client user buffer after return call new readers see data hflush throws io exception similar posix fsync flush data client user buffer way disk device disk may cache hsync throws io exception
74	common\src\java\org\apache\hadoop\fs\Trash.java	unrelated	package org apache hadoop fs provides trash facility supports pluggable trash policies see implementation configured trash policy details trash extends configured trash policy trash policy configured trash policy instance construct trash accessor trash configuration conf throws io exception construct trash accessor file system provided trash file system fs configuration conf throws io exception in case symlinks mount points one move appropriate trashbin actual volume path p deleted hence get file system fully qualified resolved path move path p trashbin volume boolean move to appropriate trash file system fs path p returns whether trash enabled filesystem boolean enabled move file directory current trash directory boolean move to trash path path throws io exception create trash checkpoint checkpoint throws io exception delete old checkpoint expunge throws io exception get current working directory path get current trash dir get configured trash policy trash policy get trash policy return link runnable periodically empties trash users intended run superuser runnable get emptier throws io exception run emptier main string args throws exception
75	common\src\java\org\apache\hadoop\fs\TrashPolicy.java	unrelated	package org apache hadoop fs this used implementing different trash policies provides factory method create instances configured trash policy trash policy extends configured protected file system fs file system protected path trash path trash directory protected deletion interval deletion interval emptier used setup trash policy must implemented trash policy implementations initialize configuration conf file system fs path home returns whether trash policy enabled filesystem boolean enabled move file directory current trash directory boolean move to trash path path throws io exception create trash checkpoint create checkpoint throws io exception delete old trash checkpoint delete checkpoint throws io exception get current working directory trash policy path get current trash dir return link runnable periodically empties trash users intended run superuser runnable get emptier throws io exception get instance configured trash policy based value configuration paramater fs trash classname trash policy get instance configuration conf file system fs path home
76	common\src\java\org\apache\hadoop\fs\TrashPolicyDefault.java	unrelated	package org apache hadoop fs provides trash feature files moved user trash directory subdirectory home directory named trash files initially moved current sub directory trash directory within sub directory original path preserved periodically one may checkpoint current trash remove older checkpoints this design permits trash management without enumeration full trash content without date support filesystem without clock synchronization trash policy default extends trash policy log log log factory get log trash policy default path current new path current path trash new path trash fs permission permission new fs permission fs action all fs action none fs action none date format checkpoint new simple date format yy m mdd h hmmss msecs per minute path current path homes parent trash policy default trash policy default path home configuration conf throws io exception initialize conf home get file system conf home initialize configuration conf file system fs path home fs fs trash new path home trash homes parent home get parent current new path trash current deletion interval conf get float fs trash interval key path make trash relative path path base path path rm file path return new path base path rm file path uri get path boolean enabled return deletion interval boolean move to trash path path throws io exception enabled path absolute make path absolute fs exists path check path exists string qpath fs make qualified path string qpath starts with trash string trash get parent string starts with qpath path trash path make trash relative path current path path base trash path make trash relative path current path get parent io exception cause null try twice case checkpoint mkdirs rename throw io exception create checkpoint throws io exception fs exists current trash checkpoint path checkpoint synchronized checkpoint fs rename current checkpoint else delete checkpoint throws io exception file status dirs null try catch file not found exception fnfe system current time millis dirs length path get current trash dir return current runnable get emptier throws io exception return new emptier get conf emptier implements runnable configuration conf emptier interval emptier configuration conf throws io exception run ceiling time interval floor time interval
77	common\src\java\org\apache\hadoop\fs\UnresolvedLinkException.java	unrelated	package org apache hadoop fs thrown symbolic link encountered path unresolved link exception extends io exception serial version uid l unresolved link exception unresolved link exception string msg
78	common\src\java\org\apache\hadoop\fs\UnsupportedFileSystemException.java	unrelated	package org apache hadoop fs file system given file system name scheme supported unsupported file system exception extends io exception serial version uid l constructs exception specified detail message unsupported file system exception string message
79	common\src\java\org\apache\hadoop\fs\ftp\FtpConfigKeys.java	unrelated	package org apache hadoop fs ftp this contains constants configuration keys used ftp file system ftp config keys extends common configuration keys string block size key ftp blocksize block size default string replication key ftp replication short replication default string stream buffer size key stream buffer size default string bytes per checksum key bytes per checksum default string client write packet size key client write packet size default protected fs server defaults get server defaults throws io exception
80	common\src\java\org\apache\hadoop\fs\ftp\FTPException.java	unrelated	package org apache hadoop fs ftp a wrap link throwable runtime exception ftp exception extends runtime exception serial version uid l ftp exception string message ftp exception throwable ftp exception string message throwable
81	common\src\java\org\apache\hadoop\fs\ftp\FTPFileSystem.java	unrelated	package org apache hadoop fs ftp p a link file system backed ftp client provided href http commons apache org net apache commons net p ftp file system extends file system log log log factory default buffer size default block size uri uri initialize uri uri configuration conf throws io exception get super initialize uri conf get host information uri overrides info conf string host uri get host host host null conf get fs ftp host null host host null conf set fs ftp host host get port information uri overrides info conf port uri get port port port ftp default port port conf set int fs ftp host port port get user password information uri overrides info conf string user and password uri get user info user and password null string user passwd info user and password split conf set fs ftp user host user passwd info user passwd info length else set conf conf uri uri connect ftp server using configuration parameters ftp client connect throws io exception ftp client client null configuration conf get conf string host conf get fs ftp host port conf get int fs ftp host port ftp default port string user conf get fs ftp user host string password conf get fs ftp password host client new ftp client client connect host port reply client get reply code ftp reply positive completion reply else client login user password else return client logout disconnect given ftp client disconnect ftp client client throws io exception client null resolve given working directory path make absolute path work dir path path path absolute return new path work dir path fs data input stream open path file buffer size throws io exception ftp client client connect path work dir new path client print working directory path absolute make absolute work dir file file status file stat get file status client absolute file stat directory client allocate buffer size path parent absolute get parent change parent directory server only read file server opening input stream as side effect working directory server changed parent directory file the ftp client connection closed close called fs data input stream client change working directory parent uri get path input stream client retrieve file stream file get name fs data input stream fis new fs data input stream new ftp input stream ftp reply positive preliminary client get reply code return fis a stream obtained via call must closed using ap is else invocation block fs data output stream create path file fs permission permission ftp client client connect path work dir new path client print working directory path absolute make absolute work dir file exists client file path parent absolute get parent parent null mkdirs client parent fs permission get default client allocate buffer size change parent directory server only write file server opening output stream as side effect working directory server changed parent directory file the ftp client connection closed close called fs data output stream client change working directory parent uri get path fs data output stream fos new fs data output stream
82	common\src\java\org\apache\hadoop\fs\ftp\FtpFs.java	unrelated	package org apache hadoop fs ftp the ftp fs implementation abstract file system this impl delegates old file system ftp fs extends delegate to file system this constructor signature needed link abstract file system create file system uri configuration ftp fs uri uri configuration conf throws io exception get uri default port fs server defaults get server defaults throws io exception
83	common\src\java\org\apache\hadoop\fs\ftp\FTPInputStream.java	unrelated	package org apache hadoop fs ftp ftp input stream extends fs input stream input stream wrapped stream ftp client client file system statistics stats boolean closed pos ftp input stream input stream stream ftp client client get pos throws io exception we support seek seek pos throws io exception boolean seek to new source target pos throws io exception synchronized read throws io exception synchronized read byte buf len throws io exception synchronized close throws io exception not supported boolean mark supported mark read limit reset throws io exception
84	common\src\java\org\apache\hadoop\fs\kfs\IFSImpl.java	unrelated	package org apache hadoop fs kfs ifs impl boolean exists string path throws io exception boolean directory string path throws io exception boolean file string path throws io exception string readdir string path throws io exception file status readdirplus path path throws io exception mkdirs string path throws io exception rename string source string dest throws io exception rmdir string path throws io exception remove string path throws io exception filesize string path throws io exception short get replication string path throws io exception short set replication string path short replication throws io exception string get data location string path start len throws io exception get modification time string path throws io exception fs data output stream create string path short replication buffer size progressable progress throws io exception fs data input stream open string path buffer size throws io exception fs data output stream append string path buffer size progressable progress throws io exception
85	common\src\java\org\apache\hadoop\fs\kfs\KFSConfigKeys.java	unrelated	package org apache hadoop fs kfs this contains constants configuration keys used kfs file system kfs config keys extends common configuration keys string kfs block size key kfs blocksize kfs block size default string kfs replication key kfs replication short kfs replication default string kfs stream buffer size key kfs stream buffer size default string kfs bytes per checksum key kfs bytes per checksum default string kfs client write packet size key kfs client write packet size default
86	common\src\java\org\apache\hadoop\fs\kfs\KFSImpl.java	unrelated	package org apache hadoop fs kfs kfs impl implements ifs impl kfs access kfs access null file system statistics statistics kfs impl string meta server host meta server port kfs impl string meta server host meta server port boolean exists string path throws io exception boolean directory string path throws io exception boolean file string path throws io exception string readdir string path throws io exception file status readdirplus path path throws io exception mkdirs string path throws io exception rename string source string dest throws io exception rmdir string path throws io exception remove string path throws io exception filesize string path throws io exception short get replication string path throws io exception short set replication string path short replication throws io exception string get data location string path start len throws io exception get modification time string path throws io exception fs data input stream open string path buffer size throws io exception fs data output stream create string path short replication buffer size progressable progress throws io exception fs data output stream append string path buffer size progressable progress throws io exception
87	common\src\java\org\apache\hadoop\fs\kfs\KFSInputStream.java	unrelated	package org apache hadoop fs kfs kfs input stream extends fs input stream res res kfs channel read byte buffer wrap b len use signify eof res statistics null return res
88	common\src\java\org\apache\hadoop\fs\kfs\KFSOutputStream.java	unrelated	package org apache hadoop fs kfs kfs output stream extends output stream string path kfs output channel kfs channel progressable progress reporter kfs output stream kfs access kfs access string path short replication get pos throws io exception write v throws io exception write byte b len throws io exception flush throws io exception synchronized close throws io exception
89	common\src\java\org\apache\hadoop\fs\kfs\KosmosFileSystem.java	unrelated	package org apache hadoop fs kfs a file system backed kfs kosmos file system extends file system return uri return working dir working dir make absolute dir path absolute return new path working dir path path absolute make absolute path res system println calling mkdirs srep res kfs impl mkdirs srep return res path absolute make absolute path system println calling isdir srep path absolute make absolute path path absolute make absolute path throws io exception path parent file get parent parent null mkdirs parent path absolute s make absolute src path absolute d make absolute dst return throws io exception path absolute make absolute path return file util copy local fs src dst del src get conf file util copy src local fs dst del src get conf throws io exception return tmp local file throws io exception move from local file tmp local file fs output file
90	common\src\java\org\apache\hadoop\fs\local\LocalConfigKeys.java	unrelated	package org apache hadoop fs local this contains constants configuration keys used local file system raw local fs checksum fs local config keys extends common configuration keys string block size key file blocksize block size default string replication key file replication short replication default string stream buffer size key file stream buffer size stream buffer size default string bytes per checksum key file bytes per checksum bytes per checksum default string client write packet size key client write packet size default fs server defaults get server defaults throws io exception
91	common\src\java\org\apache\hadoop\fs\local\LocalFs.java	unrelated	package org apache hadoop fs local the local fs implementation checksum fs local fs extends checksum fs local fs configuration conf throws io exception uri syntax exception this constructor signature needed link abstract file system create file system uri configuration local fs uri uri configuration conf throws io exception
92	common\src\java\org\apache\hadoop\fs\local\package-info.java	unrelated	package org apache hadoop fs local
93	common\src\java\org\apache\hadoop\fs\local\RawLocalFs.java	unrelated	package org apache hadoop fs local the raw local fs implementation abstract file system this impl delegates old file system raw local fs extends delegate to file system raw local fs configuration conf throws io exception uri syntax exception fs constants local fs uri conf this constructor signature needed link abstract file system create file system uri configuration raw local fs uri uri configuration conf throws io exception uri syntax exception super uri new raw local file system conf fs constants local fs uri get scheme false get uri default port return no default port file fs server defaults get server defaults throws io exception return local config keys get server defaults boolean supports symlinks return true create symlink path target path link boolean create parent throws io exception string target scheme target uri get scheme target scheme null file equals target scheme throw new io exception unable create symlink non local file create parent mkdir link get parent fs permission get default true nb use create symbolic link java nio file path available try shell exec command shell link command catch uri syntax exception x throw new io exception invalid symlink path x get message catch io exception x throw new io exception unable create symlink x get message returns target given symlink returns empty given path refer symlink error acessing symlink string read link path p nb use read symbolic link java nio file path available could use get canonical path file get target symlink indicate given path refers symlink try string path p uri get path return shell exec command shell read link command path trim catch io exception x return return file status representing given path if path refers symlink return file status representing link rather object link refers file status get file link status path f throws io exception string target read link f try file status fs get file status f if f refers regular file directory equals target return fs otherwise f refers symlink return new file status fs get len catch file not found exception e the exists method file returns false dangling links get file not found exception links exist it also possible raced delete link use read basic file attributes method java nio file attributes available equals target return new file status false fs permission get default f refers file directory exist throw e path get link target path f throws io exception we never get valid local links resolved transparently underlying local file system accessing dangling link result io exception unresolved link exception file context never call function throw new assertion error
94	common\src\java\org\apache\hadoop\fs\permission\AccessControlException.java	unrelated	package org apache hadoop fs permission an exception access control related issues instead access control exception extends io exception required link java io serializable serial version uid l access control exception access control exception string access control exception throwable cause
95	common\src\java\org\apache\hadoop\fs\permission\ChmodParser.java	unrelated	package org apache hadoop fs permission parse permission mode passed chmod command apply mode existing file chmod parser extends permission parser pattern chmod octal pattern pattern chmod normal pattern chmod parser string mode str throws illegal argument exception apply permission specified file determine new mode would short apply new permission file status file
96	common\src\java\org\apache\hadoop\fs\permission\FsAction.java	unrelated	package org apache hadoop fs permission file system actions e g read write etc enum fs action posix style none execute x write w write execute wx read r read execute r x read write rw all rwx retain reference value array fs action vals values symbolic representation string symbol fs action string return true action implies action boolean implies fs action and operation fs action fs action or operation fs action fs action not operation fs action
97	common\src\java\org\apache\hadoop\fs\permission\FsPermission.java	unrelated	package org apache hadoop fs permission a file directory permissions fs permission implements writable log log log factory get log fs permission writable factory factory new writable factory register ctor create immutable link fs permission object fs permission create immutable short permission posix permission style fs action useraction null fs action groupaction null fs action otheraction null boolean sticky bit false fs permission fs permission fs action u fs action g fs action fs permission fs action u fs action g fs action boolean sb fs permission short mode short mode fs permission fs permission fs permission string mode return user link fs action fs action get user action return useraction return group link fs action fs action get group action return groupaction return link fs action fs action get other action return otheraction set fs action u fs action g fs action boolean sb short short n inherit doc write data output throws io exception inherit doc read fields data input throws io exception fs permission read data input throws io exception short short inherit doc boolean equals object obj inherit doc hash code return short inherit doc string string apply umask permission return new one fs permission apply u mask fs permission umask umask property label deprecated key code get u mask method string deprecated umask label dfs umask string umask label default umask fs permission get u mask configuration conf boolean get sticky bit set user file creation mask umask set u mask configuration conf fs permission umask get default permission fs permission get default fs permission value of string unix symbolic permission
98	common\src\java\org\apache\hadoop\fs\permission\PermissionParser.java	unrelated	package org apache hadoop fs permission base parsing either chmod permissions umask permissions includes common code needed either operation implemented umask parser chmod parser permission parser protected boolean symbolic false protected short user mode protected short group mode protected short others mode protected short sticky mode protected char user type protected char group type protected char others type protected char sticky bit type permission parser string mode str pattern symbolic pattern octal apply normal pattern string mode str matcher matcher apply octal pattern string mode str matcher matcher protected combine modes existing boolean exe ok protected combine mode segments char type mode
99	common\src\java\org\apache\hadoop\fs\permission\PermissionStatus.java	unrelated	package org apache hadoop fs permission store permission related information permission status implements writable writable factory factory new writable factory register ctor create immutable link permission status object permission status create immutable string username string groupname fs permission permission permission status constructor permission status string user string group fs permission permission return user name string get user name return username return group name string get group name return groupname return permission fs permission get permission return permission apply umask permission status apply u mask fs permission umask inherit doc read fields data input throws io exception inherit doc write data output throws io exception create initialize link permission status link data input permission status read data input throws io exception serialize link permission status base components write data output inherit doc string string
100	common\src\java\org\apache\hadoop\fs\permission\UmaskParser.java	unrelated	package org apache hadoop fs permission parse umask value provided either octal symbolic format return short value umask values slightly different standard modes cannot specify sticky bit x umask parser extends permission parser pattern chmod octal pattern pattern umask symbolic pattern allow x short umask mode umask parser string mode str throws illegal argument exception to used file directory creation symbolic umask applied relative file mode creation mask permission op characters results clearing corresponding bit mask results bits indicated permission set mask for octal umask specified bits set file mode creation mask short get u mask
101	common\src\java\org\apache\hadoop\fs\s3\Block.java	unrelated	package org apache hadoop fs holds metadata block data stored link file system store block id length block id length get id get length string string
102	common\src\java\org\apache\hadoop\fs\s3\FileSystemStore.java	unrelated	package org apache hadoop fs a facility storing retrieving link i node link block file system store initialize uri uri configuration conf throws io exception string get version throws io exception store i node path path i node inode throws io exception store block block block file file throws io exception boolean inode exists path path throws io exception boolean block exists block id throws io exception i node retrieve i node path path throws io exception file retrieve block block block byte range start throws io exception delete i node path path throws io exception delete block block block throws io exception set path list sub paths path path throws io exception set path list deep sub paths path path throws io exception delete everything used testing purge throws io exception diagnostic method dump i nodes console dump throws io exception
103	common\src\java\org\apache\hadoop\fs\s3\INode.java	unrelated	package org apache hadoop fs holds file metadata including type regular file directory list blocks pointers data i node enum file type file type file types i node directory inode new i node file type directory null file type file type block blocks i node file type file type block blocks block get blocks file type get file type boolean directory boolean file get serialized length input stream serialize throws io exception i node deserialize input stream throws io exception
104	common\src\java\org\apache\hadoop\fs\s3\Jets3tFileSystemStore.java	unrelated	package org apache hadoop fs jets file system store implements file system store string file system name fs string file system value hadoop string file system type name fs type string file system type value block string file system version name fs version string file system version value map string string metadata string path delimiter path separator string block prefix block configuration conf s service service s bucket bucket buffer size log log initialize uri uri configuration conf throws io exception string get version throws io exception delete string key throws io exception delete i node path path throws io exception delete block block block throws io exception boolean inode exists path path throws io exception boolean block exists block id throws io exception input stream get string key boolean check metadata input stream get string key byte range start throws io exception check metadata s object object throws s file system exception i node retrieve i node path path throws io exception file retrieve block block block byte range start file new backup file throws io exception set path list sub paths path path throws io exception set path list deep sub paths path path throws io exception put string key input stream length boolean store metadata store i node path path i node inode throws io exception store block block block file file throws io exception close quietly closeable closeable string path to key path path path key to path string key string block to key block id string block to key block block purge throws io exception dump throws io exception
105	common\src\java\org\apache\hadoop\fs\s3\MigrationTool.java	unrelated	package org apache hadoop fs p this tool migrating data older newer version s filesystem p p all files filesystem migrated writing block metadata datafiles touched p migration tool extends configured implements tool s service service s bucket bucket main string args throws exception run string args throws exception initialize uri uri throws io exception migrate store old store file system store new store s object get string key store unversioned store implements store
106	common\src\java\org\apache\hadoop\fs\s3\S3Credentials.java	unrelated	package org apache hadoop fs p extracts aws credentials filesystem uri configuration p s credentials string access key string secret access key determined initialize uri uri configuration conf string get access key string get secret access key
107	common\src\java\org\apache\hadoop\fs\s3\S3Exception.java	unrelated	package org apache hadoop fs thrown problem communicating amazon s s exception extends io exception serial version uid l s exception throwable
108	common\src\java\org\apache\hadoop\fs\s3\S3FileSystem.java	unrelated	package org apache hadoop fs p a block based link file system backed href http aws amazon com amazon s p s file system extends file system uri uri file system store store path working dir s file system set store initialize s file system file system store store store store uri get uri return uri initialize uri uri configuration conf throws io exception super initialize uri conf store null store initialize uri conf set conf conf uri uri create uri get scheme uri get authority working dir file system store create default store configuration conf file system store store new jets file system store retry policy base policy retry policies retry up to maximum count with fixed sleep map class extends exception retry policy exception to policy map exception to policy map put io exception base policy exception to policy map put s exception base policy retry policy method policy retry policies retry by exception map string retry policy method name to policy map new hash map string retry policy method name to policy map put store block method policy method name to policy map put retrieve block method policy return file system store retry proxy create file system store path get working directory return working dir set working directory path dir working dir make absolute dir path make absolute path path path absolute return new path working dir path boolean mkdirs path path fs permission permission throws io exception path absolute path make absolute path list path paths new array list path absolute path null boolean result true path p paths return result boolean mkdir path path throws io exception path absolute path make absolute path i node inode store retrieve i node absolute path inode null else inode file return true boolean file path path throws io exception i node inode store retrieve i node make absolute path inode null return inode file i node check file path path throws io exception i node inode store retrieve i node make absolute path inode null inode directory return inode file status list status path f throws io exception path absolute path make absolute f i node inode store retrieve i node absolute path inode null inode file array list file status ret new array list file status path p store list sub paths absolute path return ret array new file status this optional operation yet supported fs data output stream append path f buffer size throw new io exception not supported fs data output stream create path file fs permission permission throws io exception i node inode store retrieve i node make absolute file inode null else return new fs data output stream fs data input stream open path path buffer size throws io exception i node inode check file path return new fs data input stream new s input stream get conf store inode boolean rename path src path dst throws io exception path absolute src make absolute src i node src i node store retrieve i node absolute src src i node null path absolute dst make absolute
109	common\src\java\org\apache\hadoop\fs\s3\S3FileSystemConfigKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used file system s file system config keys extends common configuration keys string s block size key blocksize s block size default string s replication key replication short s replication default string s stream buffer size key s stream buffer size default string s bytes per checksum key s bytes per checksum default string s client write packet size key s client write packet size default
110	common\src\java\org\apache\hadoop\fs\s3\S3FileSystemException.java	unrelated	package org apache hadoop fs thrown fatal exception using link s file system s file system exception extends io exception serial version uid l s file system exception string message
111	common\src\java\org\apache\hadoop\fs\s3\S3InputStream.java	unrelated	package org apache hadoop fs s input stream extends fs input stream file system store store block blocks boolean closed file length pos file block file data input stream block stream block end file system statistics stats log log s input stream configuration conf file system store store s input stream configuration conf file system store store synchronized get pos throws io exception synchronized available throws io exception synchronized seek target pos throws io exception synchronized boolean seek to new source target pos throws io exception synchronized read throws io exception synchronized read byte buf len throws io exception synchronized block seek to target throws io exception close throws io exception we support marks boolean mark supported mark read limit reset throws io exception
112	common\src\java\org\apache\hadoop\fs\s3\S3OutputStream.java	unrelated	package org apache hadoop fs s output stream extends output stream configuration conf buffer size file system store store path path block size file backup file output stream backup stream random r new random boolean closed pos file pos bytes written to block byte buf list block blocks new array list block block next block log log s output stream configuration conf file system store store file new backup file throws io exception get pos throws io exception synchronized write b throws io exception synchronized write byte b len throws io exception synchronized flush throws io exception synchronized flush data max pos throws io exception synchronized end block throws io exception synchronized next block output stream throws io exception synchronized internal close throws io exception synchronized close throws io exception
113	common\src\java\org\apache\hadoop\fs\s3\VersionMismatchException.java	unrelated	package org apache hadoop fs thrown hadoop cannot read version data stored link s file system version mismatch exception extends s file system exception serial version uid l version mismatch exception string client version string data version
114	common\src\java\org\apache\hadoop\fs\s3native\FileMetadata.java	unrelated	package org apache hadoop fs native p holds basic metadata file stored link native file system store p file metadata string key length last modified file metadata string key length last modified string get key get length get last modified string string
115	common\src\java\org\apache\hadoop\fs\s3native\Jets3tNativeFileSystemStore.java	unrelated	package org apache hadoop fs native jets native file system store implements native file system store s service service s bucket bucket initialize uri uri configuration conf throws io exception store file string key file file byte md hash store empty file string key throws io exception file metadata retrieve metadata string key throws io exception input stream retrieve string key throws io exception input stream retrieve string key byte range start partial listing list string prefix max listing length partial listing list string prefix max listing length string prior last key partial listing list string prefix string delimiter delete string key throws io exception copy string src key string dst key throws io exception purge string prefix throws io exception dump throws io exception handle service exception string key s service exception e throws io exception handle service exception s service exception e throws io exception
116	common\src\java\org\apache\hadoop\fs\s3native\NativeFileSystemStore.java	unrelated	package org apache hadoop fs native p an abstraction key based link file store p native file system store initialize uri uri configuration conf throws io exception store file string key file file byte md hash throws io exception store empty file string key throws io exception file metadata retrieve metadata string key throws io exception input stream retrieve string key throws io exception input stream retrieve string key byte range start throws io exception partial listing list string prefix max listing length throws io exception partial listing list string prefix max listing length string prior last key boolean recursive delete string key throws io exception copy string src key string dst key throws io exception delete keys given prefix used testing purge string prefix throws io exception diagnostic method dump state console dump throws io exception
117	common\src\java\org\apache\hadoop\fs\s3native\NativeS3FileSystem.java	unrelated	package org apache hadoop fs native p a link file system reading writing files stored href http aws amazon com amazon s unlike link org apache hadoop fs s file system implementation stores files s native form read s tools a note directories s course native support the idiom choose directory created use empty object dirpath folder marker further interoperate s tools also accept following object dirpath denoting directory marker exists objects prefix dirpath directory said exist file name directory marker directory exists file masks directory directory never returned p native s file system extends file system log log log factory get log native s file system string folder suffix folder string path delimiter path separator s max listing length native s fs input stream extends fs input stream native file system store store statistics statistics input stream string key pos native s fs input stream native file system store store statistics statistics input stream string key synchronized read throws io exception synchronized read byte b len close throws io exception synchronized seek pos throws io exception synchronized get pos throws io exception boolean seek to new source target pos throws io exception native s fs output stream extends output stream configuration conf string key file backup file output stream backup stream message digest digest boolean closed native s fs output stream configuration conf file new backup file throws io exception flush throws io exception synchronized close throws io exception write b throws io exception write byte b len throws io exception uri uri native file system store store path working dir native s file system set store initialize native s file system native file system store store store store initialize uri uri configuration conf throws io exception super initialize uri conf store null store initialize uri conf set conf conf uri uri create uri get scheme uri get authority working dir native file system store create default store configuration conf native file system store store new jets native file system store retry policy base policy retry policies retry up to maximum count with fixed sleep map class extends exception retry policy exception to policy map exception to policy map put io exception base policy exception to policy map put s exception base policy retry policy method policy retry policies retry by exception map string retry policy method name to policy map method name to policy map put store file method policy method name to policy map put rename method policy return native file system store string path to key path path path uri get scheme null equals path uri get path path absolute string ret path uri get path substring remove initial slash ret ends with ret index of ret length return ret path key to path string key return new path key path make absolute path path path absolute return new path working dir path this optional operation yet supported fs data output stream append path f buffer size throw new io exception not supported fs data output stream create path f fs permission permission exists f overwrite log debug
118	common\src\java\org\apache\hadoop\fs\s3native\PartialListing.java	unrelated	package org apache hadoop fs native p holds information directory listing link native file system store this includes link file metadata files directories names contained directory p p this listing may returned chunks code prior last key code provided next chunk may requested p partial listing string prior last key file metadata files string common prefixes partial listing string prior last key file metadata files file metadata get files string get common prefixes string get prior last key
119	common\src\java\org\apache\hadoop\fs\s3native\S3NativeFileSystemConfigKeys.java	unrelated	package org apache hadoop fs native this contains constants configuration keys used file system s native file system config keys extends common configuration keys string s native block size key native blocksize s native block size default string s native replication key native replication short s native replication default string s native stream buffer size key s native stream buffer size default string s native bytes per checksum key s native bytes per checksum default string s native client write packet size key s native client write packet size default
120	common\src\java\org\apache\hadoop\fs\shell\Command.java	unrelated	package org apache hadoop fs shell an execution file system command command extends configured default name command string name command usage switches arguments format string usage command description string description protected string args protected string name protected exit code protected num errors protected boolean recursive false protected array list exception exceptions new array list exception log log log factory get log command allows stdout captured necessary print stream system allows stderr captured necessary print stream err system err constructor protected command constructor protected command configuration conf string get command name protected set recursive boolean flag protected boolean recursive protected run path path throws io exception run all run string argv protected exit code for error return protected process options linked list string args throws io exception protected process raw arguments linked list string args throws io exception protected linked list path data expand arguments linked list string args throws io exception protected list path data expand argument string arg throws io exception protected process arguments linked list path data args throws io exception protected process argument path data item throws io exception protected process path argument path data item throws io exception protected process nonexistent path path data item throws io exception protected process paths path data parent path data items throws io exception protected process path path data item throws io exception protected recurse path path data item throws io exception display error exception e display error string message display warning string message string get name set name string name string get usage string get description boolean deprecated string get replacement command string get command field string field
121	common\src\java\org\apache\hadoop\fs\shell\CommandFactory.java	unrelated	package org apache hadoop fs shell search register commands command factory extends configured implements configurable map string class extends command map new hash map string class extends command map string command object map new hash map string command factory constructor commands command factory null factory constructor commands command factory configuration conf super conf invokes register commands command factory given this method abstracts contract factory command do assume directly invoking register commands given effect register commands class registrar class try catch exception e register given handling given list command names add class class extends command cmd class string names string name names map put name cmd class register given object handling given list command names avoid calling method use link add class class string whenever possible avoid startup overhead excessive command object instantiations this method intended handling nested non usable namely help usage add object command cmd object string names string name names returns instance implementing given command the must registered via link add class class string command get instance string cmd return get instance cmd get conf get instance requested command command get instance string cmd name configuration conf conf null throw new null pointer exception configuration null command instance object map get cmd name instance null return instance gets registered commands string get names string names map key set array new string arrays sort names return names
122	common\src\java\org\apache\hadoop\fs\shell\CommandFormat.java	unrelated	package org apache hadoop fs shell parse args command check format args command format min par max par map string boolean options new hash map string boolean boolean ignore unknown opts false command format string n min max string possible opt min max possible opt simple parsing command line arguments command format min max string possible opt min par min max par max string opt possible opt parse parameters starting given position consider using variant directly takes list list string parse string args pos list string parameters new array list string arrays list args parameters sub list pos clear parse parameters return parameters parse parameters given list args the list destructively modified remove options parse list string args pos pos args size psize args size psize min par psize max par return option set boolean get opt string option return options contains key option options get option false returns options set set string get opts set string opt set new hash set string map entry string boolean entry options entry set return opt set used arguments exceed bounds illegal number of arguments exception extends illegal argument exception serial version uid l protected expected protected actual protected illegal number of arguments exception want got string get message used many arguments supplied command too many arguments exception extends illegal number of arguments exception serial version uid l too many arguments exception expected actual string get message used arguments supplied command not enough arguments exception extends illegal number of arguments exception serial version uid l not enough arguments exception expected actual string get message used unsupported option supplied command unknown option exception extends illegal argument exception serial version uid l protected string option null unknown option exception string unknown option string get option
123	common\src\java\org\apache\hadoop\fs\shell\CommandUtils.java	unrelated	package org apache hadoop fs shell command utils string format description string usage string desciptions
124	common\src\java\org\apache\hadoop\fs\shell\CommandWithDestination.java	unrelated	package org apache hadoop fs shell provides argument processing ensure destination valid number source arguments a process paths accepts source resolved target sources resolved children destination directory command with destination extends fs command protected path data dst protected boolean overwrite false this method used enable force f option copying files protected set overwrite boolean flag the last arg expected local path one argument given destination current directory protected get local destination linked list string args throws io exception the last arg expected remote path one argument given destination remote user directory protected get remote destination linked list string args throws io exception protected process arguments linked list path data args throws io exception protected process paths path data parent path data items throws io exception protected process path path data src throws io exception called source target destination pair protected process path path data src path data target throws io exception
125	common\src\java\org\apache\hadoop\fs\shell\CopyCommands.java	unrelated	package org apache hadoop fs shell various commands copy files copy commands register commands command factory factory factory add class merge getmerge factory add class cp cp factory add class copy from local copy from local factory add class copy to local copy to local factory add class get get factory add class put put merge multiple files together merge extends fs command string name getmerge string usage src localdst addnl string description protected path data dst null protected string delimiter null protected process options linked list string args throws io exception protected process path path data src throws io exception cp extends command with destination string name cp string usage src dst string description protected process options linked list string args throws io exception protected process path path data src path data target throws io exception copy local files remote filesystem get extends command with destination string name get string usage string description the prefix tmp file used copy to local it must least three characters required link java io file create temp file string string file string copytolocal prefix copy to local boolean copy crc boolean verify checksum local file system local fs protected process options linked list string args throws io exception protected process path path data src path data target throws io exception copy file to local path data src path target throws io exception copy crc to local path data src path target throws io exception copy local files remote filesystem put extends command with destination string name put string usage localsrc dst string description protected process options linked list string args throws io exception commands operating local paths need glob expansion protected list path data expand argument string arg throws io exception protected process arguments linked list path data args throws io exception protected process path path data src path data target throws io exception copies stdin destination file protected copy from stdin throws io exception copy from local extends put string name copy from local string usage put usage string description identical put command copy to local extends get string name copy to local string usage get usage string description identical get command
126	common\src\java\org\apache\hadoop\fs\shell\Count.java	unrelated	package org apache hadoop fs shell count number directories files bytes quota remaining quota count extends fs command register names count command register commands command factory factory factory add class count count string name count string usage q path string description boolean show quotas constructor count constructor count string cmd pos configuration conf super conf args arrays copy of range cmd pos cmd length protected process options linked list string args command format cf new command format integer max value q cf parse args args empty default path current working directory show quotas cf get opt q protected process path path data src throws io exception content summary summary src fs get content summary src path println summary string show quotas src path
127	common\src\java\org\apache\hadoop\fs\shell\Delete.java	unrelated	package org apache hadoop fs shell classes delete paths delete register commands command factory factory factory add class rm rm factory add class rmdir rmdir factory add class rmr rmr factory add class expunge expunge remove non directory paths rm extends fs command string name rm string usage f r r skip trash src string description boolean skip trash false boolean delete dirs false boolean ignore fnf false protected process options linked list string args throws io exception protected process nonexistent path path data item throws io exception protected process path path data item throws io exception boolean move to trash path data item throws io exception remove path rmr extends rm string name rmr protected process options linked list string args throws io exception string get replacement command remove empty directories rmdir extends fs command string name rmdir string usage string description boolean ignore non empty false protected process options linked list string args throws io exception protected process path path data item throws io exception empty trash expunge extends fs command string name expunge string usage string description empty trash todo probably allow path arguments filesystems protected process options linked list string args throws io exception protected process arguments linked list path data args throws io exception
128	common\src\java\org\apache\hadoop\fs\shell\Display.java	unrelated	package org apache hadoop fs shell display contents files display extends fs command register commands command factory factory factory add class cat cat factory add class text text displays file content stdout cat extends display string name cat string usage ignore crc src string description boolean verify checksum true protected process options linked list string args throws io exception protected process path path data item throws io exception print to stdout input stream throws io exception protected input stream get input stream path data item throws io exception same behavior cat handles zip text record input stream encodings text extends cat string name text string usage cat usage string description protected input stream get input stream path data item throws io exception protected text record input stream extends input stream sequence file reader r writable comparable key writable val data input buffer inbuf data output buffer outbuf text record input stream file status f throws io exception read throws io exception close throws io exception
129	common\src\java\org\apache\hadoop\fs\shell\FsCommand.java	unrelated	package org apache hadoop fs shell base hadoop fs commands may look useful placeholder future functionality act registry fs commands currently used implement unnecessary methods base fs command extends command register commands command factory factory protected fs command protected fs command configuration conf historical method command string get command name method normally invoked runall overridden protected run path path throws io exception run all
130	common\src\java\org\apache\hadoop\fs\shell\FsUsage.java	unrelated	package org apache hadoop fs shell base commands related viewing filesystem usage du df fs usage extends fs command register commands command factory factory factory add class df df factory add class du du factory add class dus dus protected boolean human readable false protected table builder usages table protected string format size size return human readable show size partition filesystem df extends fs usage string name df string usage path string description protected process options linked list string args throws io exception protected process arguments linked list path data args throws io exception protected process path path data item throws io exception show disk usage du extends fs usage string name du string usage path string description show amount space bytes used files n match specified file pattern the following flags optional n rather showing size individual file n matches pattern shows total summary size n formats sizes files human readable fashion n rather number bytes n n note even without option shows size summaries n one level deep directory n the output form n tsize tname full path n protected boolean summary false protected process options linked list string args throws io exception protected process path argument path data item throws io exception protected process path path data item throws io exception show disk usage summary dus extends du string name dus protected process options linked list string args throws io exception string get replacement command creates table aligned values based maximum width column table builder protected boolean header false protected list string rows protected widths protected boolean right align create table w headers table builder columns create table headers table builder object headers change default left align columns set right align indexes add row objects table add row object objects render table stream print to stream print stream number rows excluding header size does table rows boolean empty
131	common\src\java\org\apache\hadoop\fs\shell\Ls.java	unrelated	package org apache hadoop fs shell get listing files match file patterns ls extends fs command register commands command factory factory factory add class ls ls factory add class lsr lsr string name ls string usage r path string description list contents match specified file pattern if n path specified contents user current user n listed directory entries form n tdir name full path dir n file entries form n tfile name full path r n size n n number replicas specified file n size size file bytes n directories listed plain files n formats sizes files human readable fashion n rather number bytes n r recursively list contents directories protected simple date format date format new simple date format yyyy mm dd hh mm protected max repl max len max owner max group protected string line format protected boolean dir recurse protected boolean human readable false protected string format size size return human readable protected process options linked list string args throws io exception command format cf new command format integer max value r cf parse args dir recurse cf get opt set recursive cf get opt r dir recurse human readable cf get opt args empty args add path cur dir protected process path argument path data item throws io exception implicitly recurse cmdline directories dir recurse item stat directory else protected process paths path data parent path data items throws io exception recursive items length adjust column widths items super process paths parent items protected process path path data item throws io exception file status stat item stat string line string format line format println line compute column widths rebuild format adjust column widths path data items path data item items string builder fmt new string builder fmt append permission fmt append max repl fmt append max owner fmt append max group fmt append max len fmt append mod time path line format fmt string max length n object value return math max n value null string value of value length get recursive listing files match file patterns same ls r lsr extends ls string name lsr protected process options linked list string args throws io exception string get replacement command
132	common\src\java\org\apache\hadoop\fs\shell\Mkdir.java	unrelated	package org apache hadoop fs shell create given dir mkdir extends fs command register commands command factory factory string name mkdir string usage path string description protected process options linked list string args protected process path path data item throws io exception protected process nonexistent path path data item throws io exception
133	common\src\java\org\apache\hadoop\fs\shell\MoveCommands.java	unrelated	package org apache hadoop fs shell various commands moving files move commands register commands command factory factory factory add class move from local move from local factory add class move to local move to local factory add class rename mv move local files remote filesystem move from local extends copy from local string name move from local string usage localsrc dst string description protected process path path data src path data target throws io exception move remote files local filesystem move to local extends fs command string name move to local string usage src localdst string description not implemented yet protected process options linked list string args throws io exception move rename paths fileystem rename extends command with destination string name mv string usage src dst string description protected process options linked list string args throws io exception protected process path path data src path data target throws io exception
134	common\src\java\org\apache\hadoop\fs\shell\package-info.java	unrelated	package org apache hadoop fs shell
135	common\src\java\org\apache\hadoop\fs\shell\PathData.java	unrelated	package org apache hadoop fs shell encapsulates path path file status stat file system fs the stat field null path exist path data protected string null path path file status stat file system fs boolean exists creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize path data string path string configuration conf throws io exception creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize path data file local path configuration conf throws io exception creates object wrap given parameters fields path data file system fs path path file status stat convenience ctor looks file status path if path exist status null path data file system fs path path throws io exception creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize if file status null path used initialized path else path used path data file system fs string path string file status stat need method ctor file status get stat file system fs path path throws io exception set stat file status stat convenience ctor extracts path given file status path data file system fs file status stat updates paths file status file status refresh status throws io exception returns list path data objects items contained given directory path data get directory contents throws io exception creates new object child entry directory path data get path data for child path data child throws io exception expand given path glob pattern non existent paths throw exception creation commands like touch mkdir need create the stat field null path exist exist list contain single path data null stat path data expand as glob string pattern configuration conf throws io exception returns printable version path either path given commandline full path string string
136	common\src\java\org\apache\hadoop\fs\shell\PathExceptions.java	unrelated	package org apache hadoop fs shell standardized posix linux style exceptions path related errors returns io exception format path standard error path exceptions eio path io exception extends io exception serial version uid l string eio input output error note really path path buggy return exact used construct path mangles uris authority string operation string path string target path constructor generic i o error exception path io exception string path appends text throwable default error message path io exception string path throwable cause avoid using method use subclass path io exception possible path io exception string path string error protected path io exception string path string error throwable cause format cmd operation path target error string get message path get path return new path path path get target path optional operation preface path set operation string operation optional path exception involved two paths ex copy operation set target path string target path string format path string path enoent path not found exception extends path io exception serial version uid l path not found exception string path eexists path exists exception extends path io exception serial version uid l path exists exception string path protected path exists exception string path string error eisdir path is directory exception extends path exists exception serial version uid l path is directory exception string path enotdir path is not directory exception extends path exists exception serial version uid l path is not directory exception string path generated rm commands path is not empty directory exception extends path exists exception path is not empty directory exception string path eacces path access denied exception extends path io exception serial version uid l path access denied exception string path eperm path permission exception extends path io exception serial version uid l path permission exception string path enotsup path operation exception extends path exists exception serial version uid l path operation exception string path
137	common\src\java\org\apache\hadoop\fs\shell\SetReplication.java	unrelated	package org apache hadoop fs shell modifies replication factor set replication extends fs command register commands command factory factory string name setrep string usage r w rep path file string description protected short new rep protected list path data wait list new linked list path data protected boolean wait opt false protected process options linked list string args throws io exception protected process arguments linked list path data args throws io exception protected process path path data item throws io exception wait files wait list replication number equal rep wait for replication throws io exception
138	common\src\java\org\apache\hadoop\fs\shell\Stat.java	unrelated	package org apache hadoop fs shell print statistics path specified format format sequences b size file blocks n filename block size r replication utc date quot yyyy mm dd hh mm ss quot y milliseconds since january utc stat extends fs command register commands command factory factory string name stat string usage format path string description protected simple date format time fmt default format protected string format protected process options linked list string args throws io exception protected process path path data item throws io exception
139	common\src\java\org\apache\hadoop\fs\shell\Tail.java	unrelated	package org apache hadoop fs shell get listing files match file patterns tail extends fs command register commands command factory factory string name tail string usage f file string description starting offset boolean follow false follow delay milliseconds protected process options linked list string args throws io exception todo hadoop add glob support backwards compat protected list path data expand argument string arg throws io exception protected process path path data item throws io exception dump from offset path data item offset throws io exception
140	common\src\java\org\apache\hadoop\fs\shell\Touchz.java	unrelated	package org apache hadoop fs shell unix touch like commands touch extends fs command register commands command factory factory factory add class touchz touchz re create zero length file specified path this replaced unix like touch files may modified touchz extends touch string name touchz string usage path string description protected process options linked list string args protected process path path data item throws io exception protected process nonexistent path path data item throws io exception touchz path data item throws io exception
141	common\src\java\org\apache\hadoop\fs\viewfs\ChRootedFileSystem.java	unrelated	package org apache hadoop fs viewfs code ch rooted file system code file system root path root base file system example for base file system hdfs nn ch root usr foo members setup shown ul li fs base file system points hdfs nn li li uri hdfs nn user foo li li ch root path part user foo li li working dir directory related ch root li ul the paths resolved follows ch rooted file system ul li absolute path b c resolved user foo b c fs li li relative path x resolved user foo working dir x li ul ch rooted file system extends file system file system fs base file system whose root changed uri uri base uri ch root path ch root path part root root base string ch root path part string path working dir protected file system get my fs return fs protected path full path path path super check path path return path absolute constructor ch rooted file system file system fs path root throws uri syntax exception fs fs fs make qualified root check root valid path fs ch root path part new path root uri get path ch root path part string ch root path part uri get path try initialize fs get uri fs get conf catch io exception e this exception thrown throw new runtime exception this occur we making uri chrooted path e g file chrooted path this questionable since path make qualified uri path ignores path part uri since internal ignore issue make external needs resolved handle two cases scheme scheme authority uri new uri fs get uri string working dir get home directory we use wd fs called new file system instance constructed file system initialize uri name configuration conf throws io exception fs initialize name conf super initialize name conf set conf conf uri get uri return uri path make qualified path path return fs make qualified path not fs make qualified full path path strip root path string strip out root path p throws io exception try check path p catch illegal argument exception e throw new io exception internal error path p string path part p uri get path return path part length ch root path part string length path part protected path get initial working directory choices null user uname strip root fs inital wd only reasonable choice initial wd chrooted fds null default rule wd applied return null path get resolved qualified path path f throws file not found exception return fs make qualified path get home directory return new path user system get property user name make qualified path get working directory return working dir set working directory path new dir working dir new dir absolute new dir new path working dir new dir fs data output stream create path f fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception return fs create full path f permission overwrite buffer size boolean delete path f boolean recursive throws io exception return fs delete full path f
142	common\src\java\org\apache\hadoop\fs\viewfs\ChRootedFs.java	unrelated	package org apache hadoop fs viewfs code chrooted fs code file system root path root base file system example for base file system hdfs nn ch root usr foo members setup shown ul li fs base file system points hdfs nn li li uri hdfs nn user foo li li ch root path part user foo li li working dir directory related ch root li ul the paths resolved follows ch rooted file system ul li absolute path b c resolved user foo b c fs li li relative path x resolved user foo working dir x li ul ch rooted fs extends abstract file system abstract file system fs base file system whose root changed uri uri base uri chroot path ch root path part root root base string ch root path part string protected abstract file system get my fs return fs protected path full path path path super check path path return new path ch root path part string path uri get path ch rooted fs abstract file system fs path root throws uri syntax exception super fs get uri fs get uri get scheme fs fs fs check path root ch root path part new path fs get uri path root ch root path part string ch root path part uri get path we making uri chrooted path e g file chrooted path this questionable since path make qualified uri path ignores path part uri since internal ignore issue make external needs resolved handle two cases scheme scheme authority uri new uri fs get uri string super check path root uri get uri return uri strip root path string strip out root path p try check path p catch illegal argument exception e throw new runtime exception internal error path p string path part p uri get path return path part length ch root path part string length path get home directory return fs get home directory path get initial working directory choices return null strip root fs inital wd only reasonable choice initial wd chrooted fds null return null path get resolved qualified path path f throws file not found exception return fs make qualified fs data output stream create internal path f enum set create flag flag fs permission absolute permission buffer size short replication block size progressable progress bytes per checksum boolean create parent throws io exception unresolved link exception return fs create internal full path f flag boolean delete path f boolean recursive throws io exception unresolved link exception return fs delete full path f recursive block location get file block locations path f start len throws io exception unresolved link exception return fs get file block locations full path f start len file checksum get file checksum path f throws io exception unresolved link exception return fs get file checksum full path f file status get file status path f throws io exception unresolved link exception return fs get file status full path f file status get file link status path f throws io exception unresolved link exception return fs get file link
143	common\src\java\org\apache\hadoop\fs\viewfs\ConfigUtil.java	unrelated	package org apache hadoop fs viewfs utilities config variables view fs see link view fs config util get config variable prefix specified mount table string get config view fs prefix string mount table name get config variable prefix default mount table string get config view fs prefix add link config specified mount table add link configuration conf string mount table name add link config default mount table add link configuration conf string src add config variable homedir default mount table set home dir conf configuration conf add config variable homedir specified mount table set home dir conf configuration conf get value home dir conf value default mount table string get home dir value configuration conf get value home dir conf value specfied mount table string get home dir value configuration conf
144	common\src\java\org\apache\hadoop\fs\viewfs\Constants.java	unrelated	package org apache hadoop fs viewfs config variable prefixes view fs see link org apache hadoop fs viewfs view fs examples the mount table specified config using prefixes see link org apache hadoop fs viewfs config util convenience lib constants prefix config variable prefix view fs mount table string config viewfs prefix fs viewfs mounttable prefix home dir mount table specified hadoop default value user used string config viewfs homedir homedir config variable name default mount table string config viewfs default mount table default config variable full prefix default mount table string config viewfs prefix default mount table config variable specifying simple link string config viewfs link link config variable specifying merge link string config viewfs link merge link merge config variable specifying merge root mount table root another file system string config viewfs link merge slash link merge slash fs permission permission rrr
145	common\src\java\org\apache\hadoop\fs\viewfs\InodeTree.java	unrelated	package org apache hadoop fs viewfs inode tree implements mount table tree inodes it used implement view fs view file system in order use caller must subclass implement methods link get target file system i node dir etc the mountable initialized config variables specified link view fs the three main methods link inode treel configuration constructor link inode tree configuration string constructor link resolve string boolean inode tree t enum result kind internal dir external dir path slash path new path i node dir t root root mount table string homedir prefix homedir config value mount table list mount point t mount points new array list mount point t mount point t string break into path components string path i node t i node dir t extends i node t i node link t extends i node t create link string src string target protected t get target file system uri uri protected t get target file system i node dir t dir protected t get target file system uri merge fs uri list throws unsupported file system exception uri syntax exception protected inode tree configuration config string view name resolve result t resolve result t resolve string p boolean resolve last component list mount point t get mount points string get home dir prefix value
146	common\src\java\org\apache\hadoop\fs\viewfs\ViewFileSystem.java	unrelated	package org apache hadoop fs viewfs view file system extends file system implements client side mount table its spec implementation identical link view fs view file system extends file system access control exception read only mount table string operation return new access control exception access control exception read only mount table string operation return read only mount table operation p string mount point path src src mount uri targets target mount multiple targets imply merge mount mount point path src path uri target ur is path get src uri get targets creation time mount table user group information ugi user group user created mtable uri uri path working dir configuration config inode tree file system fs state fs state ie mount table path home dir null prohibits names contain boolean valid name string src check string tokenizer tokens new string tokenizer src path separator tokens more tokens return true make path absolute get path part pathname checks uri matches file system path part valid name string get uri path path p check path p string make absolute p uri get path valid name return path make absolute path f return f absolute f new path working dir f this constructor signature needed link file system create file system uri configuration after constructor called initialize called view file system throws io exception ugi user group information get current user creation time system current time millis called new file system instance constructed file system initialize uri uri configuration conf super initialize uri conf set conf conf config conf now build client side view e client side mount table config string authority uri get authority try catch uri syntax exception e convenience constructor apps call directly view file system uri uri configuration conf throws io exception initialize uri conf convenience constructor apps call directly view file system configuration conf throws io exception fs constants viewfs uri conf path get trash can location path f throws file not found exception inode tree resolve result file system res return res internal dir null res target file system get home directory uri get uri return uri path resolve path path f inode tree resolve result file system res res internal dir return res target file system resolve path res remaining path path get home directory home dir null return home dir path get working directory return working dir set working directory path new dir get uri path new dir validates path working dir make absolute new dir fs data output stream append path f buffer size inode tree resolve result file system res return res target file system append res remaining path buffer size progress fs data output stream create path f fs permission permission inode tree resolve result file system res try catch file not found exception e assert res remaining path null return res target file system create res remaining path permission boolean delete path f boolean recursive inode tree resolve result file system res if internal dir target mount link ie remaining path slash res internal dir res remaining path inode tree slash path return
147	common\src\java\org\apache\hadoop\fs\viewfs\ViewFs.java	unrelated	package org apache hadoop fs viewfs view fs extends abstract file system implements client side mount table the view fs file system implemented completely memory client side the client side mount table allows client provide customized view file system namespace composed one individual file systems local fs hdfs s fs etc for example one could mount table provides links ul li user hdfs nn containing user dir user li project foo hdfs nn project projects foo li project bar hdfs nn project projects bar li tmp hdfs nn tmp tmp for user xxx ul view fs specified following uri b viewfs b p to use viewfs one would typically set default file system config e fs default name viewfs along mount table config variables described p b config variables specify mount table entries b p the file system initialized standard hadoop config config variables see link fs constants uri scheme constants see link constants config var constants see link config util convenient lib p all mount table config entries view fs prefixed b fs viewfs mounttable b for example example specified following config variables ul li fs viewfs mounttable default link user hdfs nn containing user dir user li fs viewfs mounttable default link project foo hdfs nn project projects foo li fs viewfs mounttable default link project bar hdfs nn project projects bar li fs viewfs mounttable default link tmp hdfs nn tmp tmp for user xxx ul the default mount table authority specified config variables prefixed b fs view fs mounttable default b the authority component uri used specify different mount table for example ul li viewfs sanjay mountable ul initialized fs view fs mounttable sanjay mountable config variables p b merge mounts b note merge mounts implemented yet p one also use merge mounts merge several directories sometimes called union mounts junction mounts literature for example home directories stored say two file systems fit one one could specify mount entry following merges two dirs ul li user hdfs nn user user hdfs nn user user ul such merge link specified following config var used separator links merged ul li fs viewfs mounttable default link merge user hdfs nn user user hdfs nn user user ul a special case merge mount mount table root merged root slash another file system ul li fs viewfs mounttable default link merge slash hdfs nn ul in cases root mount table merged root b hdfs nn b view fs extends abstract file system creation time mount table user group information ugi user group user created mtable configuration config inode tree abstract file system fs state fs state ie mount table path home dir null access control exception read only mount table string operation return new access control exception access control exception read only mount table string operation return read only mount table operation p string mount point path src src mount uri targets target mount multiple targets imply merge mount mount point path src path uri target ur is path get src uri get targets view fs configuration conf throws io exception fs constants
148	common\src\java\org\apache\hadoop\fs\viewfs\ViewFsFileStatus.java	unrelated	package org apache hadoop fs viewfs this needed address problem described link view file system get file status org apache hadoop fs path link view fs get file status org apache hadoop fs path view fs file status extends file status
149	common\src\java\org\apache\hadoop\http\AdminAuthorizedServlet.java	unrelated	package org apache hadoop http general servlet admin authorized admin authorized servlet extends default servlet serial version uid l protected get http servlet request request http servlet response response throws servlet exception io exception
150	common\src\java\org\apache\hadoop\http\FilterContainer.java	unrelated	package org apache hadoop http a container javax servlet filter filter container add filter string name string classname map string string parameters add global filter string name string classname map string string parameters
151	common\src\java\org\apache\hadoop\http\FilterInitializer.java	unrelated	package org apache hadoop http initialize javax servlet filter filter initializer initialize filter filter container init filter filter container container configuration conf
152	common\src\java\org\apache\hadoop\http\HtmlQuoting.java	unrelated	package org apache hadoop http this responsible quoting html characters html quoting byte amp bytes amp get bytes byte apos bytes apos get bytes byte gt bytes gt get bytes byte lt bytes lt get bytes byte quot bytes quot get bytes boolean needs quoting byte data len boolean needs quoting string str quote html chars output stream output byte buffer string quote html chars string item output stream quote output stream output stream string unquote html chars string item main string args throws exception
153	common\src\java\org\apache\hadoop\http\HttpServer.java	pooling	package org apache hadoop http create jetty embedded server answer http requests the primary goal serve status information server there three contexts logs points log directory points common files src webapps jsp server code src webapps name http server implements filter container log log log factory get log http server string filter initializer property string http max threads hadoop http max threads the servlet context attribute daemon configuration gets stored string conf context attribute hadoop conf string admins acl admins acl access control list admins acl protected server web server protected connector listener protected web app context web app context protected boolean find port protected map context boolean default contexts protected list string filter names new array list string max retries string state description alive alive string state description not live live boolean listener started externally same name bind address port find port null http server string name string bind address port boolean find port name bind address port find port new configuration http server string name string bind address port name bind address port find port conf null connector create status server given port the jsp scripts taken src webapps name increment finds free port http server string name string bind address port name bind address port find port conf null null http server string name string bind address port name bind address port find port conf admins acl null create status server given port the jsp scripts taken src webapps name increment finds free port http server string name string bind address port web server new server find port find port admins acl admins acl connector null else web server add connector listener max threads conf get int http max threads if http max threads configured queue thread pool use default value currently queued thread pool thread pool max threads web server set thread pool thread pool string app dir get web apps path name context handler collection contexts new context handler collection web server set handler contexts web app context new web app context web app context set display name wep apps context web app context set context path web app context set war app dir name web app context get servlet context set attribute conf context attribute conf web app context get servlet context set attribute admins acl admins acl web server add handler web app context add default apps contexts app dir conf define filter web app context krb filter add global filter safety quoting input filter get name null filter initializer initializers get filter initializers conf initializers null add default servlets create required listener jetty instance listening port provided this wrapper subclasses must create least one listener connector create base listener configuration conf throws io exception return http server create default channel connector connector create default channel connector select channel connector ret new select channel connector ret set low resource max idle time ret set accept queue size ret set resolve names false ret set use direct buffers false return ret get array filter configuration specified conf filter initializer get filter initializers configuration conf conf null
154	common\src\java\org\apache\hadoop\http\package-info.java	unrelated	package org apache hadoop http
155	common\src\java\org\apache\hadoop\http\lib\StaticUserWebFilter.java	unrelated	package org apache hadoop http lib provides servlet filter pretends authenticate fake user dr who web ui usable secure cluster without authentication static user web filter extends filter initializer string deprecated ugi key dfs web ugi string username key hadoop http staticuser user string username default dr log log log factory get log static user web filter user implements principal string name user string name string get name hash code boolean equals object string string static user filter implements filter user user string username destroy filter servlet request request servlet response response init filter config conf throws servlet exception init filter filter container container configuration conf hash map string string options new hash map string string string username get username from conf conf options put username key username container add filter user filter retrieve username configuration string get username from conf configuration conf string old style ugi conf get deprecated ugi key old style ugi null else
156	common\src\java\org\apache\hadoop\io\AbstractMapWritable.java	unrelated	package org apache hadoop io abstract base map writable sorted map writable unlike org apache nutch crawl map writable allows creation map writable lt writable map writable gt class to id id to class maps travel instead class ids range distinct specific map instance abstract map writable implements writable configurable atomic reference configuration conf class id mappings map class byte to id map new concurrent hash map class byte id class mappings map byte class id to class map new concurrent hash map byte class the number new established constructor volatile byte new classes byte get new classes synchronized add to map class clazz byte id add class maps already present protected synchronized add to map class clazz protected class get class byte id protected byte get id class clazz used child copy constructors protected synchronized copy writable constructor protected abstract map writable configuration get conf set conf configuration conf inherit doc write data output throws io exception inherit doc read fields data input throws io exception
157	common\src\java\org\apache\hadoop\io\ArrayFile.java	unrelated	package org apache hadoop io a dense file based mapping integers values array file extends map file protected array file ctor write new array file writer extends map file writer long writable count new long writable create named file values named writer configuration conf file system fs create named file values named writer configuration conf file system fs append value file synchronized append writable value throws io exception provide access existing array file reader extends map file reader long writable key new long writable construct array reader named file reader file system fs string file positions reader code n code th value synchronized seek n throws io exception read return next value file synchronized writable next writable value throws io exception returns key associated recent call link seek link next writable link get writable synchronized key throws io exception return code n code th value file synchronized writable get n writable value
158	common\src\java\org\apache\hadoop\io\ArrayPrimitiveWritable.java	unrelated	package org apache hadoop io this wrapper it wraps writable implementation around array primitives e g etc optimized wire format without creating new objects per element this wrapper make copy underlying array array primitive writable implements writable component type determined component type value array set operation it must primitive class component type null declared component type need declared using array primitive writable class constructor provide typechecking set operations class declared component type null length object value must array component type length map string class primitive names class get primitive class string name check primitive class component type check declared component type class component type check array object value array primitive writable array primitive writable class component type array primitive writable object value object get return value class get component type return component type class get declared component type return declared component type boolean declared component type class component type set object value internal extends array primitive writable end internal subclass declaration write data output throws io exception read fields data input throws io exception for efficient implementation way around following massive code duplication write boolean array data output throws io exception write char array data output throws io exception write byte array data output throws io exception write short array data output throws io exception write int array data output throws io exception write long array data output throws io exception write float array data output throws io exception write double array data output throws io exception read boolean array data input throws io exception read char array data input throws io exception read byte array data input throws io exception read short array data input throws io exception read int array data input throws io exception read long array data input throws io exception read float array data input throws io exception read double array data input throws io exception
159	common\src\java\org\apache\hadoop\io\ArrayWritable.java	unrelated	package org apache hadoop io a writable arrays containing instances the elements writable must instances if writable input reducer need create subclass sets value proper type for example code int array writable extends array writable int array writable super int writable code array writable implements writable class extends writable value class writable values array writable class extends writable value class array writable class extends writable value class writable values array writable string strings class get value class string strings object array set writable values values values writable get return values read fields data input throws io exception write data output throws io exception
160	common\src\java\org\apache\hadoop\io\BinaryComparable.java	unrelated	package org apache hadoop io interface supported link org apache hadoop io writable comparable types supporting ordering permutation representative set bytes binary comparable implements comparable binary comparable return n st bytes n get bytes valid get length return representative byte array instance byte get bytes compare bytes get bytes compare to binary comparable compare bytes get bytes provided compare to byte len return true bytes get bytes match boolean equals object return hash bytes returned get bytes hash code
161	common\src\java\org\apache\hadoop\io\BloomMapFile.java	unrelated	package org apache hadoop io this extends link map file provides much functionality however uses dynamic bloom filters provide quick membership test keys offers fast version link reader get writable comparable writable operation especially case sparsely populated map file bloom map file log log log factory get log bloom map file string bloom file name bloom hash count delete file system fs string name throws io exception byte byte array for bloom key data output buffer buf writer extends map file writer reader extends map file reader
162	common\src\java\org\apache\hadoop\io\BooleanWritable.java	unrelated	package org apache hadoop io a writable comparable booleans boolean writable implements writable comparable boolean value boolean writable boolean writable boolean value set value set value boolean writable set boolean value value value returns value boolean writable boolean get return value read fields data input throws io exception value read boolean write data output throws io exception write boolean value boolean equals object instanceof boolean writable boolean writable boolean writable return value value hash code return value compare to object boolean value boolean b boolean writable value return b false string string return boolean string get a comparator optimized boolean writable comparator extends writable comparator comparator compare byte b writable comparator define boolean writable new comparator
163	common\src\java\org\apache\hadoop\io\BoundedByteArrayOutputStream.java	unrelated	package org apache hadoop io a byte array backed output stream limit the limit smaller buffer capacity the object reused code reset code api choose different limits round bounded byte array output stream extends output stream byte buffer limit count create bounded byte array output stream specified capacity bounded byte array output stream capacity create bounded byte array output stream specified capacity limit bounded byte array output stream capacity limit write b throws io exception write byte b len throws io exception reset limit reset newlim reset buffer reset return current limit get limit returns underlying buffer data valid link size byte get buffer returns length valid data currently buffer size
164	common\src\java\org\apache\hadoop\io\BytesWritable.java	unrelated	package org apache hadoop io a byte sequence usable key value it resizable distinguishes size seqeunce current capacity the hash function front md buffer the sort order memcmp bytes writable extends binary comparable implements writable comparable binary comparable length bytes byte empty bytes size byte bytes create zero size sequence bytes writable empty bytes create bytes writable using byte array initial value bytes writable byte bytes bytes bytes length create bytes writable using byte array initial value length length use constructor array larger value represents bytes writable byte bytes length bytes bytes size length get copy bytes exactly length data see link get bytes faster access underlying array byte copy bytes byte result new byte size system arraycopy bytes result size return result get data backing bytes writable please use link copy bytes need returned array precisely length data byte get bytes return bytes get data bytes writable byte get return get bytes get current size buffer get length return size get current size buffer get size return get length change size buffer the values old range preserved new values undefined the capacity changed necessary set size size size get capacity size size get capacity maximum size could handled without resizing backing storage get capacity return bytes length change capacity backing storage the data preserved set capacity new cap new cap get capacity set bytes writable contents given new data set bytes writable new data set new data bytes new data size set value copy given byte range set byte new data offset length set size set size length system arraycopy new data offset bytes size inherit javadoc read fields data input throws io exception set size clear old data set size read int read fully bytes size inherit javadoc write data output throws io exception write int size write bytes size hash code return super hash code are two byte sequences equal boolean equals object right obj right obj instanceof bytes writable return false generate stream bytes hex pairs separated string string string builder sb new string builder size idx idx size idx return sb string a comparator optimized bytes writable comparator extends writable comparator comparator compare buffers serialized form compare byte b register comparator writable comparator define bytes writable new comparator
165	common\src\java\org\apache\hadoop\io\ByteWritable.java	unrelated	package org apache hadoop io a writable comparable single byte byte writable implements writable comparable byte value byte writable byte writable byte value set value set value byte writable set byte value value value return value byte writable byte get return value read fields data input throws io exception value read byte write data output throws io exception write byte value returns true iff code code byte writable value boolean equals object instanceof byte writable byte writable byte writable return value value hash code return value compares two byte writables compare to object value value value byte writable value return value value value value string string return byte string value a comparator optimized byte writable comparator extends writable comparator comparator compare byte b register comparator writable comparator define byte writable new comparator
166	common\src\java\org\apache\hadoop\io\Closeable.java	unrelated	package org apache hadoop io closeable extends java io closeable
167	common\src\java\org\apache\hadoop\io\CompressedWritable.java	unrelated	package org apache hadoop io a base writables store compressed lazily inflate field access this useful large objects whose fields altered map reduce operation leaving field data compressed makes copying instance one file another much faster compressed writable implements writable non null compressed field data instance byte compressed compressed writable read fields data input throws io exception must called methods access fields ensure data uncompressed protected ensure inflated subclasses implement instead link read fields data input protected read fields compressed data input write data output throws io exception subclasses implement instead link write data output protected write compressed data output throws io exception
168	common\src\java\org\apache\hadoop\io\DataInputBuffer.java	unrelated	package org apache hadoop io a reusable link data input implementation reads memory buffer p this saves memory creating new data input stream byte array input stream time data read p typical usage something like following pre data input buffer buffer new data input buffer loop condition byte data get data data length get data length buffer reset data data length read buffer using data input methods pre data input buffer extends data input stream buffer extends byte array input stream buffer reset byte input start length byte get data return buf get position return pos get length return count buffer buffer constructs new empty buffer data input buffer new buffer data input buffer buffer buffer super buffer buffer buffer resets data buffer reads reset byte input length buffer reset input length resets data buffer reads reset byte input start length buffer reset input start length byte get data return buffer get data returns current position input get position return buffer get position returns length input get length return buffer get length
169	common\src\java\org\apache\hadoop\io\DataInputByteBuffer.java	unrelated	package org apache hadoop io data input byte buffer extends data input stream buffer extends input stream byte scratch new byte byte buffer buffers new byte buffer bidx pos length read read byte b len reset byte buffer buffers get position get length byte buffer get data buffer buffers data input byte buffer new buffer data input byte buffer buffer buffers super buffers buffers buffers reset byte buffer input buffers reset input byte buffer get data return buffers get data get position return buffers get position get length return buffers get length
170	common\src\java\org\apache\hadoop\io\DataOutputBuffer.java	unrelated	package org apache hadoop io a reusable link data output implementation writes memory buffer p this saves memory creating new data output stream byte array output stream time data written p typical usage something like following pre data output buffer buffer new data output buffer loop condition buffer reset write buffer using data output methods byte data buffer get data data length buffer get length write data ultimate destination pre data output buffer extends data output stream buffer extends byte array output stream byte get data return buf get length return count buffer buffer size write data input len throws io exception buffer buffer constructs new empty buffer data output buffer new buffer data output buffer size new buffer size data output buffer buffer buffer super buffer buffer buffer returns current contents buffer data valid link get length byte get data return buffer get data returns length valid data currently buffer get length return buffer get length resets buffer empty data output buffer reset written buffer reset return writes bytes data input directly buffer write data input length throws io exception buffer write length write file stream write to output stream throws io exception buffer write to
171	common\src\java\org\apache\hadoop\io\DataOutputByteBuffer.java	unrelated	package org apache hadoop io data output byte buffer extends data output stream buffer extends output stream byte b new byte boolean direct list byte buffer active new array list byte buffer list byte buffer inactive new linked list byte buffer size length byte buffer current buffer size boolean direct write b write byte b write byte b len get buffer newsize byte buffer get data get length reset buffer buffers data output byte buffer data output byte buffer size size false data output byte buffer size boolean direct new buffer size direct data output byte buffer buffer buffers super buffers buffers buffers byte buffer get data return buffers get data get length return buffers get length reset written buffers reset
172	common\src\java\org\apache\hadoop\io\DataOutputOutputStream.java	unrelated	package org apache hadoop io output stream implementation wraps data output data output output stream extends output stream data output construct output stream given data output if already output stream simply returns otherwise wraps output stream output stream construct output stream data output data output output stream data output write b throws io exception write byte b len throws io exception write byte b throws io exception
173	common\src\java\org\apache\hadoop\io\DefaultStringifier.java	unrelated	package org apache hadoop io default stringifier default implementation link stringifier stringifies objects using base encoding serialized version objects the link serializer link deserializer obtained link serialization factory br default stringifier offers convenience methods store load objects configuration default stringifier t implements stringifier t string separator serializer t serializer deserializer t deserializer data input buffer buf data output buffer buf default stringifier configuration conf class t c t string string str throws io exception string string t obj throws io exception close throws io exception k store configuration conf k item string key name throws io exception k k load configuration conf string key name k store array configuration conf k items k k load array configuration conf string key name
174	common\src\java\org\apache\hadoop\io\DoubleWritable.java	unrelated	package org apache hadoop io writable double values double writable implements writable comparable value double writable double writable value set value read fields data input throws io exception value read double write data output throws io exception write double value set value value value get return value returns true iff code code double writable value boolean equals object instanceof double writable double writable double writable return value value hash code return double to long bits value compare to object double writable double writable return value value value value string string return double string value a comparator optimized double writable comparator extends writable comparator comparator compare byte b register comparator writable comparator define double writable new comparator
175	common\src\java\org\apache\hadoop\io\EnumSetWritable.java	unrelated	package org apache hadoop io a writable wrapper enum set enum set writable e extends enum e extends abstract collection e implements writable configurable enum set e value transient class e element type transient configuration conf enum set writable iterator e iterator return value iterator size return value size boolean add e e value null return value add e construct new enum set writable if tt value tt argument null size zero tt element type tt argument must null if argument tt value tt size bigger zero argument tt element type tt used enum set writable enum set e value class e element type set value element type construct new enum set writable argument tt value tt null empty enum set writable enum set e value value null reset enum set writable specified tt value value tt element type tt if tt value tt argument null size zero tt element type tt argument must null if argument tt value tt size bigger zero argument tt element type tt used set enum set e value class e element type value null value size value value value null value size else element type null return value enum set writable enum set e get return value inherit doc read fields data input throws io exception length read int length else length else inherit doc write data output throws io exception value null else returns true code code enum set writable value null boolean equals object null instanceof enum set writable enum set writable enum set writable value value value null value must null reach return value equals value returns elements underlying enum set wriable it may return null class e get element type return element type inherit doc hash code value null return value hash code inherit doc string string value null return value string inherit doc configuration get conf return conf inherit doc set conf configuration conf conf conf writable factories set factory enum set writable new writable factory
176	common\src\java\org\apache\hadoop\io\FloatWritable.java	unrelated	package org apache hadoop io a writable comparable floats float writable implements writable comparable value float writable float writable value set value set value float writable set value value value return value float writable get return value read fields data input throws io exception value read float write data output throws io exception write float value returns true iff code code float writable value boolean equals object instanceof float writable float writable float writable return value value hash code return float to int bits value compares two float writables compare to object value value value float writable value return value value value value string string return float string value a comparator optimized float writable comparator extends writable comparator comparator compare byte b register comparator writable comparator define float writable new comparator
177	common\src\java\org\apache\hadoop\io\GenericWritable.java	unrelated	package org apache hadoop io a wrapper writable instances p when two sequence files key type different value types mapped reduce multiple value types allowed in case help wrap instances different types p p compared code object writable code much effective code object writable code append declaration string output file every key value pair p p generic writable implements link configurable configured framework the configuration passed wrapped objects implementing link configurable deserialization p use br write generic object extends generic writable br implements method code get types code defines wrapped generic object application attention defined code get types code method must implement code writable code br br the code looks like blockquote pre generic object extends generic writable class classes class type class type class type protected class get types return classes pre blockquote generic writable implements writable configurable byte not set byte type not set writable instance configuration conf null set writable obj writable get string string read fields data input throws io exception write data output throws io exception protected class extends writable get types configuration get conf set conf configuration conf
178	common\src\java\org\apache\hadoop\io\InputBuffer.java	unrelated	package org apache hadoop io a reusable link input stream implementation reads memory buffer p this saves memory creating new input stream byte array input stream time data read p typical usage something like following pre input buffer buffer new input buffer loop condition byte data get data data length get data length buffer reset data data length read buffer using input stream methods pre input buffer extends filter input stream buffer extends byte array input stream buffer reset byte input start length get position return pos get length return count buffer buffer constructs new empty buffer input buffer new buffer input buffer buffer buffer super buffer buffer buffer resets data buffer reads reset byte input length buffer reset input length resets data buffer reads reset byte input start length buffer reset input start length returns current position input get position return buffer get position returns length input get length return buffer get length
179	common\src\java\org\apache\hadoop\io\IntWritable.java	unrelated	package org apache hadoop io a writable comparable ints int writable implements writable comparable value int writable int writable value set value set value int writable set value value value return value int writable get return value read fields data input throws io exception value read int write data output throws io exception write int value returns true iff code code int writable value boolean equals object instanceof int writable int writable int writable return value value hash code return value compares two int writables compare to object value value value int writable value return value value value value string string return integer string value a comparator optimized int writable comparator extends writable comparator comparator compare byte b register comparator writable comparator define int writable new comparator
180	common\src\java\org\apache\hadoop\io\IOUtils.java	unrelated	package org apache hadoop io an utility i o related functionality io utils copy bytes input stream output stream buff size boolean close copy bytes input stream output stream buff size copy bytes input stream output stream configuration conf copy bytes input stream output stream configuration conf boolean close copy bytes input stream output stream count read fully input stream byte buf skip fully input stream len throws io exception cleanup log log java io closeable closeables close stream java io closeable stream close socket socket sock null output stream extends output stream
181	common\src\java\org\apache\hadoop\io\LongWritable.java	unrelated	package org apache hadoop io a writable comparable longs long writable implements writable comparable value long writable long writable value set value set value long writable set value value value return value long writable get return value read fields data input throws io exception value read long write data output throws io exception write long value returns true iff code code long writable value boolean equals object instanceof long writable long writable long writable return value value hash code return value compares two long writables compare to object value value value long writable value return value value value value string string return long string value a comparator optimized long writable comparator extends writable comparator comparator compare byte b a decreasing comparator optimized long writable decreasing comparator extends comparator compare writable comparable writable comparable b compare byte b byte b register default comparator writable comparator define long writable new comparator
182	common\src\java\org\apache\hadoop\io\MapFile.java	unrelated	package org apache hadoop io a file based map keys values p a map directory containing two files code data code file containing keys values map smaller code index code file containing fraction keys the fraction determined link writer get index interval p the index file read entirely memory thus key implementations try keep small p map files created adding entries order to maintain large database perform updates copying previous version database merging sorted change list create new version database new file sorting large change lists done link sequence file sorter map file log log log factory get log map file the name index file string index file name index the name data file string data file name data protected map file ctor writes new map writer implements java io closeable sequence file writer data sequence file writer index string index interval io map index interval index interval size long writable position new long writable following fields used checking key order writable comparator comparator data input buffer buf new data input buffer data output buffer buf new data output buffer writable comparable last key what position bytes wrote got last index last index pos what size last wrote index set min value ensure index position zero mid key throw exception case last index key count long min value create named map keys named writer configuration conf file system fs string dir name create named map keys named writer configuration conf file system fs string dir name create named map keys named writer configuration conf file system fs string dir name create named map keys named writer configuration conf file system fs string dir name create named map using named key comparator writer configuration conf file system fs string dir name create named map using named key comparator writer configuration conf file system fs string dir name create named map using named key comparator writer configuration conf file system fs string dir name create named map using named key comparator writer configuration conf file system fs string dir name options superset sequence file writer options option extends sequence file writer option key class option extends options class option comparator option implements option option key class class extends writable comparable value option comparator writable comparator value sequence file writer option value class class value sequence file writer option compression compression type type sequence file writer option compression compression type type sequence file writer option progressable progressable value writer configuration conf the number entries added index entry added get index interval return index interval sets index interval set index interval interval index interval interval sets index interval stores conf set index interval configuration conf interval close map synchronized close throws io exception append key value pair map the key must greater equal previous key added map synchronized append writable comparable key writable val check key writable comparable key throws io exception provide access existing map reader implements java io closeable number index entries skip entry zero default setting values larger zero facilitate opening large map files using less memory index skip writable comparator comparator writable
183	common\src\java\org\apache\hadoop\io\MapWritable.java	unrelated	package org apache hadoop io a writable map map writable extends abstract map writable implements map writable writable map writable writable instance default constructor map writable copy constructor map writable map writable inherit doc clear inherit doc boolean contains key object key inherit doc boolean contains value object value inherit doc set map entry writable writable entry set inherit doc boolean equals object obj inherit doc writable get object key inherit doc hash code inherit doc boolean empty inherit doc set writable key set inherit doc writable put writable key writable value inherit doc put all map extends writable extends writable inherit doc writable remove object key inherit doc size inherit doc collection writable values writable inherit doc write data output throws io exception inherit doc read fields data input throws io exception
184	common\src\java\org\apache\hadoop\io\MD5Hash.java	unrelated	package org apache hadoop io a writable md hash values md hash implements writable comparable md hash md len thread local message digest digester factory new thread local message digest protected message digest initial value byte digest constructs md hash md hash digest new byte md len constructs md hash hex md hash string hex set digest hex constructs md hash specified value md hash byte digest digest length md len digest digest javadoc writable read fields data input throws io exception read fully digest constructs reads returns instance md hash read data input throws io exception md hash result new md hash result read fields return result javadoc writable write data output throws io exception write digest copy contents another instance instance set md hash system arraycopy digest digest md len returns digest bytes byte get digest return digest construct hash value byte array md hash digest byte data return digest data data length create thread local md digester message digest get digester message digest digester digester factory get digester reset return digester construct hash value content input stream md hash digest input stream throws io exception byte buffer new byte message digest digester get digester n n read buffer return new md hash digester digest construct hash value byte array md hash digest byte data start len byte digest message digest digester get digester digester update data start len digest digester digest return new md hash digest construct hash value string md hash digest string return digest utf get bytes construct hash value string md hash digest utf utf return digest utf get bytes utf get length construct half sized version md fits half digest value return value return bit digest md quarter digest value return value returns true iff code code md hash whose digest contains values boolean equals object instanceof md hash md hash md hash return arrays equals digest digest returns hash code value object only uses first bytes since md evenly distributed hash code return quarter digest compares object specified object order compare to md hash return writable comparator compare bytes digest md len a writable comparator optimized md hash keys comparator extends writable comparator comparator compare byte b register comparator writable comparator define md hash new comparator char hex digits b c e f returns representation object string string string builder buf new string builder md len md len return buf string sets digest value hex set digest string hex hex length md len byte digest new byte md len md len digest digest char to nibble char c c c else c c f else c a c f else
185	common\src\java\org\apache\hadoop\io\MultipleIOException.java	unrelated	package org apache hadoop io encapsulate list link io exception link io exception multiple io exception extends io exception require link java io serializable serial version uid l list io exception exceptions constructor use link create io exception list multiple io exception list io exception exceptions list io exception get exceptions return exceptions a convenient method create link io exception io exception create io exception list io exception exceptions
186	common\src\java\org\apache\hadoop\io\NullWritable.java	unrelated	package org apache hadoop io singleton writable data null writable implements writable comparable null writable this new null writable null writable ctor returns single instance null writable get return this string string return null hash code return compare to object instanceof null writable return boolean equals object return instanceof null writable read fields data input throws io exception write data output throws io exception a comparator quot optimized quot null writable comparator extends writable comparator comparator compare buffers serialized form compare byte b register comparator writable comparator define null writable new comparator
187	common\src\java\org\apache\hadoop\io\ObjectWritable.java	unrelated	package org apache hadoop io a polymorphic writable writes instance name handles arrays strings primitive types without writable wrapper object writable implements writable configurable class declared class object instance configuration conf object writable object writable object instance set instance object writable class declared class object instance declared class declared class instance instance return instance null none object get return instance return meant class get declared class return declared class reset instance set object instance declared class instance get class instance instance string string return ow declared class value instance read fields data input throws io exception read object conf write data output throws io exception write object instance declared class conf map string class primitive names new hash map string class primitive names put boolean boolean type primitive names put byte byte type primitive names put char character type primitive names put short short type primitive names put integer type primitive names put long type primitive names put float type primitive names put double type primitive names put void type null instance extends configured implements writable class declared class null instance super null null instance class declared class configuration conf read fields data input throws io exception write data output throws io exception write link writable link string primitive type array preceding write object data output object instance write object instance declared class conf false write link writable link string primitive type array preceding usages set false inter cluster file persisted output usages preserve ability interchange files clusters may running version software sometime consider removing parameter always using compact format write object data output object instance throws io exception instance null null special case must come writing declared class if eligible array primitives wrap array primitive writable internal wrapper allow compact arrays declared class array utf write string declared class get name always write declared declared class array non primitive non compact array else declared class array primitive writable internal else declared class string string else declared class primitive primitive type else declared class enum enum else writable assignable from declared class writable else message assignable from declared class else read link writable link string primitive type array preceding object read object data input configuration conf throws io exception return read object null conf read link writable link string primitive type array preceding object read object data input object writable object writable configuration conf throws io exception string name utf read string class declared class primitive names get name declared class null object instance declared class primitive primitive types else declared class array array else declared class array primitive writable internal else declared class string string else declared class enum enum else message assignable from declared class else writable object writable null store values return instance try instantiate protocol buffer given message given input stream message try instantiate protobuf try catch invocation target exception e catch illegal access exception iae method get static protobuf method class declared class string method try catch exception e find load given name tt name tt first finding specified tt conf tt if specified tt conf tt
188	common\src\java\org\apache\hadoop\io\OutputBuffer.java	unrelated	package org apache hadoop io a reusable link output stream implementation writes memory buffer p this saves memory creating new output stream byte array output stream time data written p typical usage something like following pre output buffer buffer new output buffer loop condition buffer reset write buffer using output stream methods byte data buffer get data data length buffer get length write data ultimate destination pre output buffer extends filter output stream buffer extends byte array output stream byte get data return buf get length return count reset count write input stream len throws io exception buffer buffer constructs new empty buffer output buffer new buffer output buffer buffer buffer super buffer buffer buffer returns current contents buffer data valid link get length byte get data return buffer get data returns length valid data currently buffer get length return buffer get length resets buffer empty output buffer reset buffer reset return writes bytes input stream directly buffer write input stream length throws io exception buffer write length
189	common\src\java\org\apache\hadoop\io\RawComparator.java	unrelated	package org apache hadoop io p a link comparator operates directly byte representations objects p raw comparator t extends comparator t compare two objects binary b first object b second object compare byte b byte b
190	common\src\java\org\apache\hadoop\io\SecureIOUtils.java	unrelated	package org apache hadoop io this provides secure ap is opening creating files local disk the main issue tries handle symlink traversal br an example attack ol li malicious user removes task syslog file puts link job token file target user li li malicious user tries open syslog file via servlet tasktracker li li the tasktracker unaware symlink simply streams contents job token file the malicious user access potentially sensitive map outputs etc target user job li ol a similar attack possible involving task log truncation case due insecure write file br secure io utils boolean skip security file system raw filesystem file input stream open for read file f string expected owner file input stream force secure open for read file f string expected owner file output stream insecure create for write file f file output stream create for write file f permissions throws io exception check stat file f string owner string group already exists exception extends io exception
191	common\src\java\org\apache\hadoop\io\SequenceFile.java	pooling	package org apache hadoop io code sequence file code flat files consisting binary key value pairs p code sequence file code provides link writer link reader link sorter writing reading sorting respectively p there three code sequence file code code writer code based link compression type used compress key value pairs ol li code writer code uncompressed records li li code record compress writer code record compressed files compress values li li code block compress writer code block compressed files keys values collected blocks separately compressed the size block configurable ol p the actual compression algorithm used compress key values specified using appropriate link compression codec p p the recommended way use tt create writer tt methods provided code sequence file code chose preferred format p p the link reader acts bridge read code sequence file code formats p id formats sequence file formats p essentially different formats code sequence file code depending code compression type code specified all share href header common header described id header sequence file header ul li version bytes magic header b seq b followed byte actual version number e g seq seq li li key class name key li li value class name value li li compression a boolean specifies compression turned keys values file li li block compression a boolean specifies block compression turned keys values file li li compression codec code compression codec code used compression keys values compression enabled li li metadata link metadata file li li sync a sync marker denote end header li ul id uncompressed format uncompressed sequence file format ul li href header header li li record ul li record length li li key length li li key li li value li ul li li a sync marker every code code bytes li ul id record compressed format record compressed sequence file format ul li href header header li li record ul li record length li li key length li li key li li compressed value li ul li li a sync marker every code code bytes li ul id block compressed format block compressed sequence file format ul li href header header li li record block ul li uncompressed number records block li li compressed key lengths block size li li compressed key lengths block li li compressed keys block size li li compressed keys block li li compressed value lengths block size li li compressed value lengths block li li compressed values block size li li compressed values block li ul li li a sync marker every block li ul p the compressed blocks key lengths value lengths consist actual lengths individual keys values encoded zero compressed integer format p sequence file log log log factory get log sequence file sequence file ctor byte block compress version byte byte custom compress version byte byte version with metadata byte byte version new byte sync escape length sync entries sync hash size number bytes hash sync size sync hash size escape hash the number bytes sync points sync interval sync size enum compression type compression type get default compression
192	common\src\java\org\apache\hadoop\io\SetFile.java	unrelated	package org apache hadoop io a file based set keys set file extends map file protected set file ctor write new set file writer extends map file writer create named set keys named writer file system fs string dir name class extends writable comparable key class throws io exception create set naming element compression type writer configuration conf file system fs string dir name create set naming element comparator compression type writer configuration conf file system fs string dir name append key set the key must strictly greater previous key added set append writable comparable key throws io exception provide access existing set file reader extends map file reader construct set reader named set reader file system fs string dir name configuration conf throws io exception construct set reader named set using named comparator reader file system fs string dir name writable comparator comparator configuration conf javadoc inherited boolean seek writable comparable key read next key set code key code returns true key exists false end set boolean next writable comparable key read matching key set code key code returns code key code null match exists writable comparable get writable comparable key
193	common\src\java\org\apache\hadoop\io\SortedMapWritable.java	unrelated	package org apache hadoop io a writable sorted map sorted map writable extends abstract map writable implements sorted map writable comparable writable sorted map writable comparable writable instance default constructor sorted map writable copy constructor sorted map writable sorted map writable inherit doc comparator super writable comparable comparator inherit doc writable comparable first key inherit doc sorted map writable comparable writable head map writable comparable key inherit doc writable comparable last key inherit doc sorted map writable comparable writable sub map writable comparable key writable comparable key inherit doc sorted map writable comparable writable tail map writable comparable key inherit doc clear inherit doc boolean contains key object key inherit doc boolean contains value object value inherit doc set java util map entry writable comparable writable entry set inherit doc writable get object key inherit doc boolean empty inherit doc set writable comparable key set inherit doc writable put writable comparable key writable value inherit doc put all map extends writable comparable extends writable inherit doc writable remove object key inherit doc size inherit doc collection writable values inherit doc read fields data input throws io exception inherit doc write data output throws io exception
194	common\src\java\org\apache\hadoop\io\Stringifier.java	unrelated	package org apache hadoop io stringifier offers two methods convert object representation restore object given representation stringifier t extends java io closeable string string t obj throws io exception t string string str throws io exception close throws io exception
195	common\src\java\org\apache\hadoop\io\Text.java	unrelated	package org apache hadoop io this stores text using standard utf encoding it provides methods serialize deserialize compare texts byte level the type length integer serialized using zero compressed format p in addition provides methods traversal without converting byte array p also includes utilities serializing deserialing coding decoding checking byte array contains valid utf code calculating length encoded text extends binary comparable implements writable comparable binary comparable thread local charset encoder encoder factory new thread local charset encoder thread local charset decoder decoder factory new thread local charset decoder protected charset decoder initial value byte empty bytes new byte byte bytes length text bytes empty bytes construct text string set construct another text text text utf set utf construct byte array text byte utf set utf get copy bytes exactly length data see link get bytes faster access underlying array byte copy bytes byte result new byte length system arraycopy bytes result length return result returns raw bytes however data link get length valid please use link copy bytes need returned array precisely length data byte get bytes return bytes returns number bytes byte array get length return length returns unicode scalar value bit integer value character code position code note method avoids using converter string instatiation position invalid points trailing byte char at position position length return position return duh byte buffer bb byte buffer byte buffer wrap bytes position position return bytes to code point bb slice find string return find finds occurence code code backing buffer starting position code start code the starting position measured bytes return value terms byte position buffer the backing buffer converted operation utf buffer found find string start try catch character coding exception e set contain contents set string try catch character coding exception e set utf byte array set byte utf set utf utf length copy text set text set get bytes get length set text range bytes set byte utf start len set capacity len false system arraycopy utf start bytes len length len append range bytes end given text append byte utf start len set capacity length len true system arraycopy utf start bytes length len length len clear empty clear length sets capacity text object em least em code len code bytes if current buffer longer capacity existing content buffer unchanged if code len code larger current capacity text object capacity increased match set capacity len boolean keep data bytes null bytes length len convert text back string string try catch character coding exception e deserialize read fields data input throws io exception new length writable utils read v int set capacity new length false read fully bytes new length length new length skips one text input skip data input throws io exception length writable utils read v int writable utils skip fully length serialize write object length uses zero compressed encoding write data output throws io exception writable utils write v int length write bytes length returns true iff code code text contents boolean equals object instanceof text return false hash code return super hash code a writable comparator
196	common\src\java\org\apache\hadoop\io\TwoDArrayWritable.java	unrelated	package org apache hadoop io a writable d arrays containing matrix instances two d array writable implements writable class value class writable values two d array writable class value class two d array writable class value class writable values object array set writable values values values writable get return values read fields data input throws io exception write data output throws io exception
197	common\src\java\org\apache\hadoop\io\UTF8.java	unrelated	package org apache hadoop io a writable comparable strings uses utf encoding p also includes utilities efficiently reading writing utf utf implements writable comparable log log log factory get log utf data input buffer ibuf new data input buffer thread local data output buffer obuf factory new thread local data output buffer protected data output buffer initial value byte empty bytes new byte byte bytes empty bytes length utf set construct given utf string set construct given utf utf utf set utf the raw bytes byte get bytes return bytes the number bytes encoded get length return length set contain contents set string length xffff maybe length utf length compute length length xffff check length bytes null length bytes length grow buffer try avoid sync allocations catch io exception e set contain contents set utf length length bytes null length bytes length grow buffer system arraycopy bytes bytes length read fields data input throws io exception length read unsigned short bytes null bytes length length read fully bytes length skips one utf input skip data input throws io exception length read unsigned short writable utils skip fully length write data output throws io exception write short length write bytes length compare two utf compare to object utf utf return writable comparator compare bytes bytes length convert string string string string builder buffer new string builder length try catch io exception e return buffer string returns true iff code code utf contents boolean equals object instanceof utf utf utf length length else hash code return writable comparator hash bytes bytes length a writable comparator optimized utf keys comparator extends writable comparator comparator compare byte b register comparator writable comparator define utf new comparator static utilities from here down these probably used much anymore might removed convert utf encoded byte array byte get bytes string byte result new byte utf length try avoid sync allocations catch io exception e return result read utf encoded string read string data input throws io exception bytes read unsigned short string builder buffer new string builder bytes read chars buffer bytes return buffer string read chars data input string builder buffer n bytes throws io exception data output buffer obuf obuf factory get obuf reset obuf write n bytes byte bytes obuf get data n bytes write utf encoded write string data output string throws io exception length xffff maybe len utf length len xffff check length write short len write chars length return len returns number bytes required write utf length string length length utf length length return utf length write chars data output throws io exception end start length start end
198	common\src\java\org\apache\hadoop\io\VersionedWritable.java	unrelated	package org apache hadoop io a base writables provides version checking p this useful may evolve instances written old version may still processed new version to handle situation link read fields data input implementations catch link version mismatch exception versioned writable implements writable return version number current implementation byte get version javadoc writable write data output throws io exception javadoc writable read fields data input throws io exception
199	common\src\java\org\apache\hadoop\io\VersionMismatchException.java	unrelated	package org apache hadoop io thrown link versioned writable read fields data input version object read match current implementation version returned link versioned writable get version version mismatch exception extends io exception byte expected version byte found version version mismatch exception byte expected version in byte found version in returns representation object string string
200	common\src\java\org\apache\hadoop\io\VIntWritable.java	unrelated	package org apache hadoop io a writable comparable integer values stored variable length format such values take one five bytes smaller values take fewer bytes v int writable implements writable comparable value v int writable v int writable value set value set value v int writable set value value value return value v int writable get return value read fields data input throws io exception write data output throws io exception returns true iff code code v int writable value boolean equals object hash code compares two v int writables compare to object string string
201	common\src\java\org\apache\hadoop\io\VLongWritable.java	unrelated	package org apache hadoop io a writable comparable longs variable length format such values take one five bytes smaller values take fewer bytes v long writable implements writable comparable value v long writable v long writable value set value set value long writable set value value value return value long writable get return value read fields data input throws io exception write data output throws io exception returns true iff code code v long writable value boolean equals object hash code compares two v long writables compare to object string string
202	common\src\java\org\apache\hadoop\io\Writable.java	unrelated	package org apache hadoop io a serializable object implements simple efficient serialization protocol based link data input link data output p any code key code code value code type hadoop map reduce framework implements p p implementations typically implement code read data input code method constructs new instance calls link read fields data input returns instance p p example p p blockquote pre my writable implements writable some data counter timestamp write data output throws io exception write int counter write long timestamp read fields data input throws io exception counter read int timestamp read long my writable read data input throws io exception my writable w new my writable w read fields return w pre blockquote p writable write data output throws io exception read fields data input throws io exception
203	common\src\java\org\apache\hadoop\io\WritableComparable.java	unrelated	package org apache hadoop io a link writable also link comparable p code writable comparable code compared typically via code comparator code any type used code key code hadoop map reduce framework implement p p note code hash code code frequently used hadoop partition keys it important implementation hash code returns result across different instances jvm note also default code hash code code implementation code object code b b satisfy property p p example p p blockquote pre my writable comparable implements writable comparable some data counter timestamp write data output throws io exception write int counter write long timestamp read fields data input throws io exception counter read int timestamp read long compare to my writable comparable w value value value int writable value return value lt value value value hash code prime result result prime result counter result prime result timestamp timestamp gt gt gt return result pre blockquote p writable comparable t extends writable comparable t
204	common\src\java\org\apache\hadoop\io\WritableComparator.java	unrelated	package org apache hadoop io a comparator link writable comparable p this base implemenation uses natural ordering to define alternate orderings link compare writable comparable writable comparable p one may optimize compare intensive operations overriding link compare byte byte static utility methods provided assist optimized implementations method writable comparator implements raw comparator hash map class writable comparator comparators new hash map class writable comparator registry get comparator link writable comparable implementation synchronized writable comparator get class extends writable comparable c writable comparator comparator comparators get c comparator null return comparator force initialization members as java referencing force initialize since requires initialized declare comparators force initialization happen force init class cls try catch class not found exception e register optimized comparator link writable comparable implementation comparators registered method must thread safe synchronized define class c comparators put c comparator class extends writable comparable key class writable comparable key writable comparable key data input buffer buffer construct link writable comparable implementation protected writable comparator class extends writable comparable key class key class false protected writable comparator class extends writable comparable key class key class key class create instances else returns writable comparable implementation class extends writable comparable get key class return key class construct new link writable comparable instance writable comparable new key return reflection utils new instance key class null optimization hook override make sequence file sorter scream p the default implementation reads data two link writable comparable using link writable read fields data input calls link compare writable comparable writable comparable compare byte b byte b try catch io exception e return compare key key compare compare two writable comparables p the default implementation uses natural ordering calling link comparable compare to object compare writable comparable writable comparable b return compare to b compare object object b return compare writable comparable writable comparable b lexicographic order binary data compare bytes byte b end end j end j end j return compute hash binary data hash bytes byte bytes offset length hash offset offset length return hash compute hash binary data hash bytes byte bytes length return hash bytes bytes length parse unsigned short byte array read unsigned short byte bytes start return bytes start xff parse integer byte array read int byte bytes start return bytes start xff parse byte array read float byte bytes start return float bits to float read int bytes start parse byte array read long byte bytes start return read int bytes start parse byte array read double byte bytes start return double bits to double read long bytes start reads zero compressed encoded byte array returns read v long byte bytes start throws io exception len bytes start len boolean negative len len negative len len start len bytes length idx idx len idx return negative l reads zero compressed encoded integer byte array returns read v int byte bytes start throws io exception return read v long bytes start
205	common\src\java\org\apache\hadoop\io\WritableFactories.java	unrelated	package org apache hadoop io factories non writables defining factory permits link object writable able construct instances non writable factories hash map class writable factory class to factory new hash map class writable factory writable factories singleton define factory synchronized set factory class c writable factory factory class to factory put c factory define factory synchronized writable factory get factory class c return class to factory get c create new instance defined factory writable new instance class extends writable c configuration conf writable factory factory writable factories get factory c factory null else create new instance defined factory writable new instance class extends writable c return new instance c null
206	common\src\java\org\apache\hadoop\io\WritableFactory.java	unrelated	package org apache hadoop io a factory writable writable factory return new instance writable new instance
207	common\src\java\org\apache\hadoop\io\WritableName.java	unrelated	package org apache hadoop io utility permit renaming writable implementation without invalidiating files contain name writable name hash map string class name to class hash map class string class to name define important types writable name ctor set name known something synchronized set name class writable class string name add alternate name synchronized add name class writable class string name return name default link class get name synchronized string get name class writable class return name default link class name string synchronized class get class string name configuration conf
208	common\src\java\org\apache\hadoop\io\WritableUtils.java	unrelated	package org apache hadoop io writable utils byte read compressed byte array data input throws io exception skip compressed byte array data input throws io exception write compressed byte array data output ugly utility maybe someone else better string read compressed string data input throws io exception write compressed string data output string throws io exception write string network int n followed n bytes alternative bit read write utf encoding standard write string data output string throws io exception read string network int n followed n bytes alternative bit read write utf encoding standard string read string data input throws io exception write string array nework int n followed int n byte array strings could generalised using introspection write string array data output string throws io exception write string array nework int n followed int n byte array compressed strings handles also null arrays null values could generalised using introspection write compressed string array data output string throws io exception write string array nework int n followed int n byte array strings could generalised using introspection actually bit string read string array data input throws io exception write string array nework int n followed int n byte array strings could generalised using introspection handles null arrays null values string read compressed string array data input throws io exception test utility method display byte array display byte array byte record make copy writable object using serialization buffer t extends writable t clone t orig configuration conf make copy writable object using serialiation buffer clone into writable dst writable src throws io exception serializes integer binary stream zero compressed encoding for one byte used actual value for values first byte value indicates whether integer positive negative number bytes follow if first byte value v following integer positive number bytes follow v if first byte value v following integer negative number bytes follow v bytes stored high non zero byte first order write v int data output stream throws io exception serializes binary stream zero compressed encoding for one byte used actual value for values first byte value indicates whether positive negative number bytes follow if first byte value v following positive number bytes follow v if first byte value v following negative number bytes follow v bytes stored high non zero byte first order write v long data output stream throws io exception reads zero compressed encoded input stream returns read v long data input stream throws io exception reads zero compressed encoded integer input stream returns read v int data input stream throws io exception given first byte vint vlong determine sign boolean negative v int byte value parse first byte vint vlong determine number bytes decode v int size byte value get encoded length integer stored variable length format get v int size read enum value data input enums read written using string values t extends enum t t read enum data input class t enum type writes string value enum data output write enum data output enum enum val skip len number bytes input stream skip fully data input len throws
209	common\src\java\org\apache\hadoop\io\compress\BlockCompressorStream.java	unrelated	package org apache hadoop io compress a link org apache hadoop io compress compressor stream works block based based compression algorithms opposed stream based compression algorithms it noted wrapper guarantee blocks sized compressor if link org apache hadoop io compress compressor requires buffering effect meaningful compression responsible block compressor stream extends compressor stream the maximum size input data compressed account overhead compression algorithm max input size create link block compressor stream algorithm given buffer size block compressor stream output stream compressor compressor create link block compressor stream given output stream compressor use default buffer size compression overhead buffer size bytes bytes zlib algorithm block compressor stream output stream compressor compressor write data provided compression codec compressing buffer size less compression overhead specified construction block each block contains uncompressed length block followed one length prefixed blocks compressed data write byte b len throws io exception finish throws io exception protected compress throws io exception raw write int v throws io exception
210	common\src\java\org\apache\hadoop\io\compress\BlockDecompressorStream.java	unrelated	package org apache hadoop io compress a link org apache hadoop io compress decompressor stream works block based based compression algorithms opposed stream based compression algorithms block decompressor stream extends decompressor stream original block size uncompressed bytes create link block decompressor stream block decompressor stream input stream decompressor decompressor create link block decompressor stream block decompressor stream input stream decompressor decompressor throws io exception protected block decompressor stream input stream throws io exception protected decompress byte b len throws io exception protected get compressed data throws io exception reset state throws io exception raw read int throws io exception
211	common\src\java\org\apache\hadoop\io\compress\BZip2Codec.java	unrelated	package org apache hadoop io compress this provides compression output stream compression input stream compression decompression currently dont implementation compressor decompressor interfaces methods compression codec compressor decompressor type argument throw unsupported operation exception b zip codec implements splittable compression codec string header bz header len header length string sub header sub header len sub header length creates new instance b zip codec b zip codec creates compression output stream b zip the output stream throws io exception compression output stream create output stream output stream creates compressor using given output stream compression output stream create output stream output stream this functionality currently supported class extends org apache hadoop io compress compressor get compressor type this functionality currently supported compressor create compressor creates compression input stream used read uncompressed data the input stream throws io exception compression input stream create input stream input stream this functionality currently supported compression input stream create input stream input stream split compression input stream create input stream input stream seekable in this functionality currently supported class extends org apache hadoop io compress decompressor get decompressor type this functionality currently supported decompressor create decompressor bz recognized default extension compressed b zip files string get default extension b zip compression output stream extends end b zip compression output stream b zip compression input stream extends end b zip compression input stream
212	common\src\java\org\apache\hadoop\io\compress\CodecPool.java	pooling	package org apache hadoop io compress a global compressor decompressor pool used save reuse possibly native compression decompression codecs codec pool log log log factory get log codec pool a global compressor pool used save expensive construction destruction possibly native decompression codecs map class compressor list compressor compressor pool a global decompressor pool used save expensive construction destruction possibly native decompression codecs map class decompressor list decompressor decompressor pool t t borrow map class t list t pool t payback map class t list t pool t codec get link compressor given link compression codec pool new one code compressor code code compression codec code pool new one compressor get compressor compression codec codec configuration conf compressor get compressor compression codec codec get link decompressor given link compression codec pool new one code decompressor code code compression codec code pool new one decompressor get decompressor compression codec codec return link compressor pool return compressor compressor compressor return link decompressor pool pool return decompressor decompressor decompressor
213	common\src\java\org\apache\hadoop\io\compress\CompressionCodec.java	unrelated	package org apache hadoop io compress this encapsulates streaming compression decompression pair compression codec compression output stream create output stream output stream throws io exception compression output stream create output stream output stream throws io exception class extends compressor get compressor type compressor create compressor compression input stream create input stream input stream throws io exception compression input stream create input stream input stream throws io exception class extends decompressor get decompressor type decompressor create decompressor string get default extension
214	common\src\java\org\apache\hadoop\io\compress\CompressionCodecFactory.java	unrelated	package org apache hadoop io compress a factory find correct codec given filename compression codec factory log log log factory get log compression codec factory get name a map reversed filename suffixes codecs this probably overkill maps small automatically supports finding longest matching suffix sorted map string compression codec codecs null map string compression codec codecs by name null a map names codecs hash map string compression codec codecs by class name null add codec compression codec codec string suffix codec get default extension codecs put new string builder suffix reverse string codec codecs by class name put codec get class get canonical name codec string codec name codec get class get simple name codecs by name put codec name lower case codec codec name ends with codec print extension map string string string builder buf new string builder iterator map entry string compression codec itr buf append itr next buf append return buf string get list codecs listed configuration set list class extends compression codec get codec classes configuration conf string codecs string conf get io compression codecs codecs string null else sets list codec configuration set codec classes configuration conf string builder buf new string builder iterator class itr iterator itr next conf set io compression codecs buf string find codecs specified config value io compression codecs register defaults gzip zip compression codec factory configuration conf codecs new tree map string compression codec codecs by class name new hash map string compression codec codecs by name new hash map string compression codec list class extends compression codec codec classes get codec classes conf codec classes null else find relevant compression codec given file based filename suffix compression codec get codec path file compression codec result null codecs null return result find relevant compression codec codec canonical name compression codec get codec by class name string classname codecs by class name null return codecs by class name get classname compression codec get codec by name string codec name class extends compression codec get codec class by name string codec name removes suffix filename string remove suffix string filename string suffix filename ends with suffix return filename a little test program main string args throws exception configuration conf new configuration compression codec factory factory new compression codec factory conf boolean encode false args length
215	common\src\java\org\apache\hadoop\io\compress\CompressionInputStream.java	unrelated	package org apache hadoop io compress a compression input stream p implementations assumed buffered this permits clients reposition underlying input stream call link reset state without also synchronize client buffers compression input stream extends input stream implements seekable the input stream compressed protected input stream protected max available data l create compression input stream reads decompressed bytes given stream protected compression input stream input stream throws io exception close throws io exception read bytes stream made prevent leakage underlying stream read byte b len throws io exception reset decompressor initial state discard buffered data underlying stream may repositioned reset state throws io exception this method returns current position stream get pos throws io exception this method current supported seek pos throws unsupported operation exception this method current supported boolean seek to new source target pos throws unsupported operation exception
216	common\src\java\org\apache\hadoop\io\compress\CompressionOutputStream.java	unrelated	package org apache hadoop io compress a compression output stream compression output stream extends output stream the output stream compressed protected output stream create compression output stream writes compressed bytes given stream protected compression output stream output stream close throws io exception flush throws io exception write compressed bytes stream made prevent leakage underlying stream write byte b len throws io exception finishes writing compressed data output stream without closing underlying stream finish throws io exception reset compression initial state does reset underlying stream reset state throws io exception
217	common\src\java\org\apache\hadoop\io\compress\Compressor.java	unrelated	package org apache hadoop io compress specification stream based compressor plugged link compression output stream compress data this modelled link java util zip deflater compressor sets input data compression this called whenever needs input returns code true code indicating input data required set input byte b len returns true input data buffer empty set input called provide input set input called order provide input boolean needs input sets preset dictionary compression a preset dictionary used history buffer predetermined set dictionary byte b len return number uncompressed bytes input far get bytes read return number compressed bytes output far get bytes written when called indicates compression end current contents input buffer finish returns true end compressed data output stream reached data output stream reached boolean finished fills specified buffer compressed data returns actual number bytes compressed data a return value indicates needs input called order determine input data required compress byte b len throws io exception resets compressor new set input data processed reset closes compressor discards unprocessed input end prepare compressor used new stream settings defined given configuration reinit configuration conf
218	common\src\java\org\apache\hadoop\io\compress\CompressorStream.java	unrelated	package org apache hadoop io compress compressor stream extends compression output stream protected compressor compressor protected byte buffer protected boolean closed false compressor stream output stream compressor compressor buffer size compressor stream output stream compressor compressor allow derived directly set underlying stream protected compressor stream output stream write byte b len throws io exception protected compress throws io exception finish throws io exception reset state throws io exception close throws io exception byte one byte new byte write b throws io exception
219	common\src\java\org\apache\hadoop\io\compress\Decompressor.java	unrelated	package org apache hadoop io compress specification stream based de compressor plugged link compression input stream compress data this modelled link java util zip inflater decompressor sets input data decompression this called link needs input returns code true code indicating input data required both native non native versions various decompressors require data passed via code b code remain unmodified caller explicitly notified via link needs input buffer may safely modified with requirement extra buffer copy avoided set input byte b len returns true input data buffer empty link set input byte called provide input link set input byte called order provide input boolean needs input sets preset dictionary compression a preset dictionary used history buffer predetermined set dictionary byte b len returns code true code preset dictionary needed decompression boolean needs dictionary returns true end decompressed data output stream reached data output stream reached boolean finished fills specified buffer uncompressed data returns actual number bytes uncompressed data a return value indicates link needs input called order determine input data required decompress byte b len throws io exception returns number bytes remaining compressed data buffer typically called decompressor finished decompressing current gzip stream k member get remaining resets decompressor input output buffers new set input data processed reset closes decompressor discards unprocessed input end
220	common\src\java\org\apache\hadoop\io\compress\DecompressorStream.java	unrelated	package org apache hadoop io compress decompressor stream extends compression input stream protected decompressor decompressor null protected byte buffer protected boolean eof false protected boolean closed false last bytes sent decompressor stream input stream decompressor decompressor throws io exception decompressor stream input stream decompressor decompressor throws io exception allow derived directly set underlying stream protected decompressor stream input stream throws io exception byte one byte new byte read throws io exception read byte b len throws io exception protected decompress byte b len throws io exception protected get compressed data throws io exception protected check stream throws io exception reset state throws io exception byte skip bytes new byte skip n throws io exception available throws io exception close throws io exception boolean mark supported synchronized mark readlimit synchronized reset throws io exception
221	common\src\java\org\apache\hadoop\io\compress\DefaultCodec.java	unrelated	package org apache hadoop io compress default codec implements configurable compression codec log log log factory get log default codec configuration conf set conf configuration conf configuration get conf compression output stream create output stream output stream throws io exception compression output stream create output stream output stream throws io exception class extends compressor get compressor type compressor create compressor compression input stream create input stream input stream throws io exception compression input stream create input stream input stream throws io exception class extends decompressor get decompressor type decompressor create decompressor string get default extension
222	common\src\java\org\apache\hadoop\io\compress\DeflateCodec.java	unrelated	package org apache hadoop io compress alias default codec enable codec discovery deflate name deflate codec extends default codec
223	common\src\java\org\apache\hadoop\io\compress\DoNotPool.java	pooling	package org apache hadoop io compress this marker annotation marks compressor decompressor type pooled do not pool
224	common\src\java\org\apache\hadoop\io\compress\GzipCodec.java	unrelated	package org apache hadoop io compress this creates gzip compressors decompressors gzip codec extends default codec protected gzip output stream extends compressor stream compression output stream create output stream output stream compression output stream create output stream output stream throws io exception compressor create compressor class extends compressor get compressor type compression input stream create input stream input stream throws io exception compression input stream create input stream input stream throws io exception decompressor create decompressor class extends decompressor get decompressor type string get default extension gzip zlib compressor extends zlib compressor gzip zlib decompressor extends zlib decompressor
225	common\src\java\org\apache\hadoop\io\compress\SnappyCodec.java	unrelated	package org apache hadoop io compress this creates snappy compressors decompressors snappy codec implements configurable compression codec configuration conf set conf configuration conf configuration get conf boolean native snappy loaded configuration conf compression output stream create output stream output stream compression output stream create output stream output stream class extends compressor get compressor type compressor create compressor compression input stream create input stream input stream compression input stream create input stream input stream class extends decompressor get decompressor type decompressor create decompressor string get default extension
226	common\src\java\org\apache\hadoop\io\compress\SplitCompressionInputStream.java	unrelated	package org apache hadoop io compress an input stream covering range compressed data the start end offsets requested client may modified codec fit block boundaries algorithm dependent requirements split compression input stream start end split compression input stream input stream start end protected set start start protected set end end after calling create input stream values start end might change so method used get new value start get adjusted start after calling create input stream values start end might change so method used get new value end get adjusted end
227	common\src\java\org\apache\hadoop\io\compress\SplittableCompressionCodec.java	unrelated	package org apache hadoop io compress this meant implemented compression codecs capable compress de compress stream starting arbitrary position especially process de compressing stream starting arbitrary position challenging most codecs able successfully de compress stream start beginning till end one reasons stored state beginning stream crucial de compression yet codecs save whole state beginning stream hence used de compress stream starting arbitrary points this meant used codecs such codecs highly valuable especially context hadoop input compressed file split hence worked multiple machines parallel splittable compression codec extends compression codec during decompression data read decompressor two modes namely continuous blocked few codecs e g b zip capable compressing data blocks decompressing blocks in blocked reading mode codecs inform end block events caller while continuous mode caller codecs unaware blocks uncompressed data spilled like continuous stream enum read mode continuous byblock create stream dictated read mode this method used codecs wants ability work underlying stream positions underlying codec underlying codec compressed stream block boundaries split compression input stream create input stream input stream seekable in
228	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2Constants.java	unrelated	package org apache hadoop io compress bzip base compress decompress holds common arrays data p this historical purposes you need use p b zip constants base block size max alpha size max code len runa runb n groups g size n iters max selectors g size num overshoot bytes end of block end of stream this array really again historical purposes p fixme this array package location since could modified malicious code p r nums
229	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2DummyCompressor.java	unrelated	package org apache hadoop io compress bzip this dummy compressor b zip b zip dummy compressor implements compressor compress byte b len throws io exception end finish boolean finished get bytes read get bytes written boolean needs input reset set dictionary byte b len set input byte b len reinit configuration conf
230	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2DummyDecompressor.java	unrelated	package org apache hadoop io compress bzip this dummy decompressor b zip b zip dummy decompressor implements decompressor decompress byte b len throws io exception end boolean finished boolean needs dictionary boolean needs input get remaining reset set dictionary byte b len set input byte b len
231	common\src\java\org\apache\hadoop\io\compress\bzip2\CBZip2InputStream.java	unrelated	package org apache hadoop io compress bzip an input stream decompresses b zip format without file header chars read stream p the decompression requires large amounts memory thus call link close close method soon possible force tt cb zip input stream tt release allocated memory see link cb zip output stream cb zip output stream information memory usage p p tt cb zip input stream tt reads bytes compressed source stream via single byte link java io input stream read read method exclusively thus consider use buffered source stream p p this ant code enhanced de compress blocks bzip data current position stream important statistic hadoop for example line record reader solely depend current position stream know progess the notion position becomes complicated compressed files the hadoop splitting done terms compressed file but compressed file deflates large amount data so handled problem following way on object creation time find next block start delimiter once marker found stream stops discard read compressed data process position updated e caller find stream location at point ready actual reading e decompression data the subsequent read calls give data the position updated caller read current block bytes in block reading position updated we update postion block boundaries p p instances threadsafe p cb zip input stream extends input stream implements b zip constants block delimiter x l start block eos delimiter x l end bzip stream delimiter bit length read mode read mode read mode continuous the variable records current advertised position stream reported bytes read from compressed stream l the following variable keep record compressed bytes read bytes read from compressed stream l boolean lazy initialization false byte array new byte index last char block block size last last index zptr original sorting orig ptr always range the current block size number block size k boolean block randomised false bs buff bs live crc crc new crc n in use buffered input stream current char enum state state current state state start block state stored block crc stored combined crc computed block crc computed combined crc boolean skip result false used skip to next marker boolean skip decompression false variables used setup methods exclusively su count su ch su ch prev su su j su r n to go su r t pos su pos char su z all memory intensive stuff this field initialized init block cb zip input stream data data this method reports processed bytes far please note statistic updated block boundaries stream initiated byblock mode get processed byte count protected update processed byte count count update reported byte count count this method reads byte compressed stream whenever need read underlying compressed stream method called instead directly calling read method underlying compressed stream this method important record keeping statistic many bytes read compressed stream read a byte input stream stream throws io exception this method tries find marker passed first parameter stream it find bit patterns length bits specifically method used cb zip input stream find end block eob delimiter stream starting current position stream if marker found stream position right marker end
232	common\src\java\org\apache\hadoop\io\compress\bzip2\CBZip2OutputStream.java	unrelated	package org apache hadoop io compress bzip an output stream compresses b zip format without file header chars another stream p the compression requires large amounts memory thus call link close close method soon possible force tt cb zip output stream tt release allocated memory p p you shrink amount allocated memory maybe raise compression speed choosing lower blocksize turn may cause lower compression ratio you avoid unnecessary memory allocation avoiding using blocksize bigger size input p p you compute memory usage compressing following formula p pre lt code gt k blocksize lt code gt pre p to get memory required decompression link cb zip input stream cb zip input stream use p pre lt code gt k blocksize lt code gt pre table width border colgroup col width col width col width colgroup tr th colspan memory usage blocksize th tr tr th align right blocksize th th align right compression br memory usage th th align right decompression br memory usage th tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr table p for decompression tt cb zip input stream tt allocates less memory bzipped input smaller one block p p instances threadsafe p p todo update b zip p cb zip output stream extends output stream implements b zip constants the minimum supported blocksize tt tt min blocksize the maximum supported blocksize tt tt max blocksize this constant accessible subclasses historical purposes if know means need protected setmask this constant accessible subclasses historical purposes if know means need protected clearmask setmask this constant accessible subclasses historical purposes if know means need protected greater icost this constant accessible subclasses historical purposes if know means need protected lesser icost this constant accessible subclasses historical purposes if know means need protected small thresh this constant accessible subclasses historical purposes if know means need protected depth thresh this constant accessible subclasses historical purposes if know means need protected work factor this constant accessible subclasses historical purposes if know means need p if ever unlucky improbable enough get stack overflow whilst sorting increase following constant try in practice i never seen stack go elems following limit seems generous p protected qsort stack size knuth increments seem work better incerpi
233	common\src\java\org\apache\hadoop\io\compress\bzip2\CRC.java	unrelated	package org apache hadoop io compress bzip a simple hold calculate crc sanity checking data crc crc table x x c db x b e crc initialise crc get final crc get global crc set global crc new crc update crc ch update crc ch repeat global crc
234	common\src\java\org\apache\hadoop\io\compress\bzip2\package-info.java	unrelated	package org apache hadoop io compress bzip
235	common\src\java\org\apache\hadoop\io\compress\snappy\LoadSnappy.java	unrelated	package org apache hadoop io compress snappy determines snappy native library available loads available load snappy log log log factory get log load snappy get name boolean available false boolean loaded false returns snappy native library loaded code false code boolean available returns snappy native library loaded code false code boolean loaded
236	common\src\java\org\apache\hadoop\io\compress\snappy\SnappyCompressor.java	unrelated	package org apache hadoop io compress snappy a link compressor based snappy compression algorithm http code google com p snappy snappy compressor implements compressor log log default direct buffer size hack use global lock jni layer class clazz snappy compressor direct buffer size buffer compressed direct buf null uncompressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finish finished bytes read l bytes written l creates new compressor snappy compressor direct buffer size creates new compressor default buffer size snappy compressor sets input data compression this called whenever needs input returns code true code indicating input data required synchronized set input byte b len if write would exceed capacity direct buffers set aside loaded function compressed data consumed synchronized set input from saved data does nothing synchronized set dictionary byte b len returns true input data buffer empty set input called provide input set input called order provide input synchronized boolean needs input when called indicates compression end current contents input buffer synchronized finish returns true end compressed data output stream reached data output stream reached synchronized boolean finished fills specified buffer compressed data returns actual number bytes compressed data a return value indicates needs input called order determine input data required synchronized compress byte b len resets compressor new set input data processed synchronized reset prepare compressor used new stream settings defined given configuration synchronized reinit configuration conf return number bytes given compressor since last reset synchronized get bytes read return number bytes consumed callers compress since last reset synchronized get bytes written closes compressor discards unprocessed input synchronized end native init i ds native compress bytes direct
237	common\src\java\org\apache\hadoop\io\compress\snappy\SnappyDecompressor.java	unrelated	package org apache hadoop io compress snappy a link decompressor based snappy compression algorithm http code google com p snappy snappy decompressor implements decompressor log log default direct buffer size hack use global lock jni layer class clazz snappy decompressor direct buffer size buffer compressed direct buf null compressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finished creates new compressor snappy decompressor direct buffer size creates new decompressor default buffer size snappy decompressor sets input data decompression this called link needs input returns code true code indicating input data required both native non native versions various decompressors require data passed via code b code remain unmodified caller explicitly notified via link needs input buffer may safely modified with requirement extra buffer copy avoided synchronized set input byte b len if write would exceed capacity direct buffers set aside loaded function compressed data consumed synchronized set input from saved data does nothing synchronized set dictionary byte b len returns true input data buffer empty link set input byte called provide input link set input byte called order provide input synchronized boolean needs input returns code false code synchronized boolean needs dictionary returns true end decompressed data output stream reached data output stream reached synchronized boolean finished fills specified buffer uncompressed data returns actual number bytes uncompressed data a return value indicates link needs input called order determine input data required synchronized decompress byte b len returns code code synchronized get remaining synchronized reset resets decompressor input output buffers new set input data processed synchronized end native init i ds native decompress bytes direct
238	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInGzipDecompressor.java	pooling	package org apache hadoop io compress zlib a link decompressor based popular gzip compressed file format http www gzip org built in gzip decompressor implements decompressor gzip magic id x b f read le short gzip deflate method gzip flagbit header crc x gzip flagbit extra field x gzip flagbit filename x gzip flagbit comment x gzip flagbits reserved xe true nowrap inflater handle raw deflate stream inflater inflater new inflater true byte user buf null user buf off user buf len byte local buf new byte local buf off header bytes read trailer bytes read num extra field bytes remaining pure java crc crc new pure java crc boolean extra field false boolean filename false boolean comment false boolean header crc false gzip state label state the current state gzip decoder external inflater context technically variables local buf header crc also part state enum merely label enum gzip state label creates new pure java gzip decompressor built in gzip decompressor inherit doc synchronized boolean needs input inherit doc in case input data includes gzip header trailer bytes handle execute state deflate stream bytes hand inflater note this code assumes data passed via b remains unmodified signal safe modify via needs input the alternative would require additional buffer copy even bulk deflate stream performance hit want absorb decompressor documents requirement synchronized set input byte b len decompress data gzip header deflate stream gzip trailer provided buffer from caller perspective state machine lives the code written never return decompress data remaining user buf unless finished state data beyond current gzip member e g within concatenated gzip stream if ever changes link needs input also need modified e uncomment user buf len condition the actual deflate stream processing decompression handled java inflater unlike gzip header trailer code execute methods deflate stream never copied inflater operates directly user buffer synchronized decompress byte b len throws io exception parse gzip header assuming appropriate state in order deal degenerate cases e g user buffer one byte copy header bytes another buffer filename comment extra field bytes simply skipped p see http www ietf org rfc rfc txt gzip spec note version gzip date least supports fhcrc header crc flagbit instead implementation treats multi file continuation flag also support sun jdk v supports header crc however execute header state throws io exception parse gzip trailer assuming appropriate state in order deal degenerate cases e g user buffer one byte copy trailer bytes em local buffer p see http www ietf org rfc rfc txt gzip spec execute trailer state throws io exception returns total number compressed bytes input far including gzip header trailer bytes p synchronized get bytes read returns number bytes remaining input buffer normally called finished true determine amount post gzip stream data note finished state concatenated data end current gzip stream never return non zero value unless called link set input byte b len link decompress byte b len that link decompress byte b len always returns zero except finished state concatenated data p synchronized get remaining inherit doc synchronized boolean needs dictionary inherit doc
239	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInZlibDeflater.java	unrelated	package org apache hadoop io compress zlib a wrapper around java util zip deflater make conform org apache hadoop io compress compressor built in zlib deflater extends deflater implements compressor log log log factory get log built in zlib deflater built in zlib deflater level boolean nowrap built in zlib deflater level built in zlib deflater synchronized compress byte b len reinit compressor given configuration it reset compressor compression level compression strategy different tt zlib compressor tt tt built in zlib deflater tt support three kind compression strategy filtered huffman only default strategy it use default strategy default configured compression strategy supported reinit configuration conf
240	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInZlibInflater.java	unrelated	package org apache hadoop io compress zlib a wrapper around java util zip inflater make conform org apache hadoop io compress decompressor built in zlib inflater extends inflater implements decompressor built in zlib inflater boolean nowrap built in zlib inflater synchronized decompress byte b len
241	common\src\java\org\apache\hadoop\io\compress\zlib\package-info.java	unrelated	package org apache hadoop io compress zlib
242	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibCompressor.java	unrelated	package org apache hadoop io compress zlib a link compressor based popular zlib compression algorithm http www zlib net zlib compressor implements compressor log log log factory get log zlib compressor default direct buffer size hack use global lock jni layer class clazz zlib compressor stream compression level level compression strategy strategy compression header window bits direct buffer size byte user buf null user buf off user buf len buffer uncompressed direct buf null uncompressed direct buf off uncompressed direct buf len boolean keep uncompressed buf false buffer compressed direct buf null boolean finish finished the compression level zlib library enum compression level the compression level zlib library enum compression strategy the type header compressed data enum compression header boolean native zlib loaded false boolean native zlib loaded protected construct compression level level compression strategy strategy creates new compressor default compression level compressed data generated zlib format zlib compressor creates new compressor taking settings configuration zlib compressor configuration conf creates new compressor using specified compression level compressed data generated zlib format zlib compressor compression level level compression strategy strategy prepare compressor used new stream settings defined given configuration it reset compressor compression level compression strategy synchronized reinit configuration conf synchronized set input byte b len copy enough data user buf uncompressed direct buf synchronized set input from saved data synchronized set dictionary byte b len synchronized boolean needs input synchronized finish synchronized boolean finished synchronized compress byte b len returns total number compressed bytes output far synchronized get bytes written returns total number uncompressed bytes input far p synchronized get bytes read synchronized reset synchronized end check stream native init i ds native init level strategy window bits native set dictionary strm byte b native deflate bytes direct native get bytes read strm native get bytes written strm native reset strm native end strm
243	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibDecompressor.java	unrelated	package org apache hadoop io compress zlib a link decompressor based popular zlib compression algorithm http www zlib net zlib decompressor implements decompressor default direct buffer size hack use global lock jni layer class clazz zlib decompressor stream compression header header direct buffer size buffer compressed direct buf null compressed direct buf off compressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finished boolean need dict the headers detect compressed data enum compression header boolean native zlib loaded false boolean native zlib loaded creates new decompressor zlib decompressor compression header header direct buffer size zlib decompressor synchronized set input byte b len synchronized set input from saved data synchronized set dictionary byte b len synchronized boolean needs input synchronized boolean needs dictionary synchronized boolean finished synchronized decompress byte b len returns total number uncompressed bytes output far synchronized get bytes written returns total number compressed bytes input far p synchronized get bytes read returns number bytes remaining input buffers normally called finished true determine amount post gzip stream data p synchronized get remaining resets everything including input buffers user direct p synchronized reset synchronized end protected finalize check stream native init i ds native init window bits native set dictionary strm byte b native inflate bytes direct native get bytes read strm native get bytes written strm native get remaining strm native reset strm native end strm
244	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibFactory.java	unrelated	package org apache hadoop io compress zlib a collection factories create right zlib gzip compressor decompressor instances zlib factory log log log factory get log zlib factory boolean native zlib loaded false native code loader native code loaded native zlib loaded zlib compressor native zlib loaded zlib decompressor native zlib loaded native zlib loaded log info successfully loaded initialized native zlib library else log warn failed load initialize native zlib library check native zlib code loaded initialized correctly loaded job loaded job else code false code boolean native zlib loaded configuration conf return native zlib loaded conf get boolean common configuration keys io native lib available key common configuration keys io native lib available default return appropriate type zlib compressor class extends compressor get zlib compressor type configuration conf return native zlib loaded conf zlib compressor built in zlib deflater return appropriate implementation zlib compressor compressor get zlib compressor configuration conf return native zlib loaded conf new zlib compressor conf new built in zlib deflater zlib factory get compression level conf compression level return appropriate type zlib decompressor class extends decompressor get zlib decompressor type configuration conf return native zlib loaded conf zlib decompressor built in zlib inflater return appropriate implementation zlib decompressor decompressor get zlib decompressor configuration conf return native zlib loaded conf new zlib decompressor new built in zlib inflater set compression strategy configuration conf compression strategy strategy conf set enum zlib compress strategy strategy compression strategy get compression strategy configuration conf return conf get enum zlib compress strategy compression strategy default strategy set compression level configuration conf compression level level conf set enum zlib compress level level compression level get compression level configuration conf return conf get enum zlib compress level compression level default compression
245	common\src\java\org\apache\hadoop\io\file\tfile\BCFile.java	unrelated	package org apache hadoop io file tfile block compressed file underlying physical storage layer t file bc file provides basic block level compression data block meta blocks it separated t file may used block compressed file implementation bc file current version bc file impl increment major minor made enough changes version api version new version short short log log log factory get log bc file prevent instantiation bc file objects bc file nothing bc file writer entry point creating new bc file writer implements closeable fs data output stream configuration conf single meta block containing index compressed data blocks data index data index index meta blocks meta index meta index boolean blk in progress false boolean meta blk seen false boolean closed false error count reusable buffers bytes writable fs output buffer call back register block block closed block register intermediate maintain state writable compression block w block state access point stuff data block todo change data output stream something else tracks size instead currently wrap around row block size greater gb block appender extends data output stream constructor fs output stream name compression algorithm used data blocks writer fs data output stream fout string compression name close bc file writer attempting use writer calling code close code allowed may lead undetermined results close throws io exception algorithm get default compression algorithm block appender prepare meta block string name algorithm compress algo create meta block obtain output stream adding data block there one block appender stream active time regular blocks may created first meta blocks the caller must call block appender close conclude block creation the name meta block the name must conflict existing meta blocks the name compression algorithm used if meta block name already exists block appender prepare meta block string name string compression name create meta block obtain output stream adding data block the meta block compressed compression algorithm data blocks there one block appender stream active time regular blocks may created first meta blocks the caller must call block appender close conclude block creation the name meta block the name must conflict existing meta blocks if meta block name already exists block appender prepare meta block string name throws io exception create data block obtain output stream adding data block there one block appender stream active time data blocks may created first meta blocks the caller must call block appender close conclude block creation block appender prepare data block throws io exception callback make sure meta block added internal list stream closed meta block register implements block register callback make sure data block added internal list closed data block register implements block register bc file reader read file data meta blocks reader implements closeable fs data input stream configuration conf data index data index index meta blocks meta index meta index version version intermediate maintain state readable compression block r block state access point read block block reader extends data input stream constructor fs input stream length corresponding file reader fs data input stream fin file length configuration conf get name default compression algorithm string get default compression
246	common\src\java\org\apache\hadoop\io\file\tfile\BoundedRangeFileInputStream.java	unrelated	package org apache hadoop io file tfile bounded range f ile input stream abstracts contiguous region hadoop fs data input stream regular input stream one create multiple bounded range file input stream top fs data input stream would interfere bounded range file input stream extends input stream fs data input stream pos end mark byte one byte new byte constructor the fs data input stream connect begining offset region length region the actual length region may smaller begin length goes beyond end fs input stream bounded range file input stream fs data input stream offset available throws io exception read throws io exception read byte b throws io exception read byte b len throws io exception we may skip beyond end file skip n throws io exception synchronized mark readlimit synchronized reset throws io exception boolean mark supported close
247	common\src\java\org\apache\hadoop\io\file\tfile\ByteArray.java	unrelated	package org apache hadoop io file tfile adaptor wrap byte array backed objects including java byte array raw comparable objects byte array implements raw comparable byte buffer offset len byte array bytes writable byte array byte buffer byte array byte buffer offset len byte buffer offset size
248	common\src\java\org\apache\hadoop\io\file\tfile\Chunk.java	unrelated	package org apache hadoop io file tfile several related support chunk encoded sub streams top regular stream chunk prevent instantiation chunk nothing decoding chain chunks encoded chunk encoder single chunk encoder chunk decoder extends input stream data input stream null boolean last chunk remain boolean closed chunk decoder reset data input stream stream constructor the source input stream contains chunk encoded data stream chunk decoder data input stream have reached last chunk boolean last chunk throws io exception how many bytes remain current chunk get remain throws io exception reading length next chunk data available read length throws io exception check whether reach end stream case available greater true otherwise i o errors boolean check eof throws io exception this method never blocks caller returning mean reach end stream available read throws io exception read byte b throws io exception read byte b len throws io exception skip n throws io exception boolean mark supported boolean closed close throws io exception chunk encoder encoding output data chain chunks following sequences len byte len len byte len len n byte len n where len len len n lengths data chunks non terminal chunks lengths negated non terminal chunks cannot length all lengths range integer max value encoded utils v int format chunk encoder extends output stream the data output stream connects data output stream the internal buffer used know advertised size byte buf the number valid bytes buffer this value always range tt tt tt buf length tt elements tt buf tt tt buf count tt contain valid byte data count constructor underlying output stream user supplied buffer the buffer would used exclusively chunk encoder life cycle chunk encoder data output stream byte buf write chunk the chunk buffer offset chunk buffer beginning chunk is last call flush buffer write chunk byte chunk offset len boolean last write chunk concatenation internal buffer plus user supplied data this never last block user supplied data buffer offset user data buffer user data buffer size write buf data byte data offset len flush internal buffer is last call flush buffer flush buffer throws io exception write b throws io exception write byte b throws io exception write byte b len throws io exception flush throws io exception close throws io exception encode whole stream single chunk expecting know size chunk front single chunk encoder extends output stream the data output stream connects data output stream the remaining bytes written remain boolean closed false constructor underlying output stream the total bytes written single chunk i o error occurs single chunk encoder data output stream size write b throws io exception write byte b throws io exception write byte b len throws io exception flush throws io exception close throws io exception
249	common\src\java\org\apache\hadoop\io\file\tfile\CompareUtils.java	unrelated	package org apache hadoop io file tfile compare utils prevent instantiation compare utils nothing a comparator compare anything implements link raw comparable using customized comparator bytes comparator implements raw comparator object cmp bytes comparator raw comparator object cmp compare raw comparable raw comparable compare byte len byte b interface objects single integer magnitude scalar magnitude scalar long implements scalar magnitude scalar long magnitude scalar comparator implements comparator scalar serializable compare scalar scalar memcmp raw comparator implements compare byte b byte b compare object object
250	common\src\java\org\apache\hadoop\io\file\tfile\Compression.java	pooling	package org apache hadoop io file tfile compression related stuff compression log log log factory get log compression prevent instantiation compression nothing finish on flush compression stream extends filter output stream finish on flush compression stream compression output stream cout write byte b len throws io exception flush throws io exception compression algorithms enum algorithm lzo t file compression lzo gz t file compression gz none t file compression none we require compression related settings configured statically configuration object protected configuration conf new configuration string compress name data input buffer size absorb small reads application data ibuf size data output buffer size absorb small writes application data obuf size string conf lzo class algorithm string name compression codec get codec throws io exception input stream create decompression stream output stream create compression stream boolean supported compressor get compressor throws io exception return compressor compressor compressor decompressor get decompressor throws io exception return decompressor decompressor decompressor string get name algorithm get compression algorithm by name string compress name algorithm algos algorithm get enum constants algorithm algos throw new illegal argument exception string get supported algorithms algorithm algos algorithm get enum constants array list string ret new array list string algorithm algos return ret array new string ret size
251	common\src\java\org\apache\hadoop\io\file\tfile\MetaBlockAlreadyExists.java	unrelated	package org apache hadoop io file tfile exception meta block name already exists meta block already exists extends io exception constructor message meta block already exists string
252	common\src\java\org\apache\hadoop\io\file\tfile\MetaBlockDoesNotExist.java	unrelated	package org apache hadoop io file tfile exception no meta block given name meta block does not exist extends io exception constructor message meta block does not exist string
253	common\src\java\org\apache\hadoop\io\file\tfile\RawComparable.java	unrelated	package org apache hadoop io file tfile interface objects compared link raw comparator this useful places need single object reference specify range bytes byte array link comparable link collections binary search java util list object comparator the actual comparison among raw comparable requires external raw comparator applications responsibility ensure two raw comparable supposed semantically comparable raw comparator raw comparable get underlying byte array byte buffer get offset first byte byte array offset get size byte range byte array size
254	common\src\java\org\apache\hadoop\io\file\tfile\SimpleBufferedOutputStream.java	unrelated	package org apache hadoop io file tfile a simplified buffered output stream borrowed buffer allow users see much data buffered simple buffered output stream extends filter output stream protected byte buf borrowed buffer protected count bytes used buffer constructor simple buffered output stream output stream byte buf flush buffer throws io exception write b throws io exception write byte b len throws io exception synchronized flush throws io exception get size internal buffer used size
255	common\src\java\org\apache\hadoop\io\file\tfile\TFile.java	unrelated	package org apache hadoop io file tfile a t file container key value pairs both keys values type less bytes keys restricted kb value length restricted practically limited available disk storage t file provides following features ul li block compression li named meta data blocks li sorted unsorted keys li seek key file offset ul the memory footprint t file includes following ul li some constant overhead reading writing compressed block ul li each compressed block requires one compression decompression codec i o li temporary space buffer key li temporary space buffer value t file writer values chunk encoded buffer one chunk user data by default chunk buffer mb reading chunked value require additional memory ul li t file index proportional total number data blocks the total amount memory needed hold index estimated avg key size num blocks li meta block index proportional total number meta blocks the total amount memory needed hold index meta blocks estimated avg meta block name num meta block ul p the behavior t file customized following variables configuration ul li b tfile io chunk size b value chunk size integer bytes default mb values length less chunk size guaranteed known value length read time see link t file reader scanner entry value length known li b tfile fs output buffer size b buffer size used fs data output stream integer bytes default kb li b tfile fs input buffer size b buffer size used fs data input stream integer bytes default kb ul p suggestions performance optimization ul li minimum block size we recommend setting minimum block size kb mb general usage larger block size preferred files primarily sequential access however would lead inefficient random access data decompress smaller blocks good random access require memory hold block index may slower create must flush compressor stream conclusion data block leads fs i o flush further due internal caching compression codec smallest possible block size would around kb kb li the current implementation offer true multi threading reading the implementation uses fs data input stream seek read shown much faster positioned read call single thread mode however also means multiple threads attempt access t file using multiple scanners simultaneously actual i o carried sequentially even access different dfs blocks li compression codec use none data compressable compressable i mean compression ratio least generally use lzo starting point experimenting gz overs slightly better compression ratio lzo requires x cpu compress x cpu decompress comparing lzo li file system buffering underlying fs data input stream fs data output stream already adequately buffered applications reads writes keys values large buffers reduce sizes input output buffering t file layer setting configuration parameters tfile fs input buffer size tfile fs output buffer size ul some design rationale behind t file found href https issues apache org jira browse hadoop hadoop t file log log log factory get log t file string chunk buf size attr tfile io chunk size string fs input buf size attr string fs output buf size attr get chunk buffer size configuration conf ret conf get int chunk buf
256	common\src\java\org\apache\hadoop\io\file\tfile\TFileDumper.java	unrelated	package org apache hadoop io file tfile dumping information t file t file dumper log log log factory get log t file dumper t file dumper enum align dump information t file path t file print stream output information the configuration object dump info string file print stream configuration conf
257	common\src\java\org\apache\hadoop\io\file\tfile\Utils.java	unrelated	package org apache hadoop io file tfile supporting utility used t file shared users t file utils prevent instantiation utils utils nothing encoding integer variable length encoding format synonymous code utils write v long n code output stream the integer encoded write v int data output n throws io exception write v long n encoding long integer variable length encoding format ul li n encode one byte actual value otherwise li n encode two bytes byte n byte n xff otherwise li n in encode three bytes byte n byte n xff byte n xff otherwise li n encode four bytes byte n byte n xff byte n xff byte n xff otherwise li n encode five bytes byte byte n xff byte n xff byte n xff byte n xff li n encode six bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff li n encode seven bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff li n encode eight bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff li n encode nine bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff ul output stream integer number write v long data output n throws io exception n n un n n n many bytes need represent number sign bit len long size long number of leading zeros un first byte n len switch len decoding variable length integer synonymous code utils read v long code input stream read v int data input throws io exception ret read v long ret integer max value ret integer min value return ret decoding variable length integer suppose value first byte fb following bytes nb ul li fb return fb li fb return fb nb xff li fb return fb nb xff nb xff li fb return fb nb xff nb xff nb xff li fb return interpret nb fb signed big endian integer input stream read v long data input throws io exception first byte read byte first byte switch first byte write string v int n followed n bytes text format write string data output string throws io exception null else read string v int n followed n bytes text format the input stream string read string data input throws io exception length read v int length return null byte buffer new byte length read fully buffer return text decode buffer a generic version we suggest applications built top t file use maintain version information meta blocks a version number consists major version minor version the suggested usage major minor version number increment major version number new storage format backward compatible increment minor version otherwise version implements comparable version short major short minor construct version object reading input stream input stream version data input throws io exception constructor major version minor version version
258	common\src\java\org\apache\hadoop\io\nativeio\Errno.java	unrelated	package org apache hadoop io nativeio enum representing posix errno values enum errno eperm enoent esrch eintr eio enxio e big enoexec ebadf echild eagain enomem eacces efault enotblk ebusy eexist exdev enodev enotdir eisdir einval enfile emfile enotty etxtbsy efbig enospc espipe erofs emlink epipe edom erange unknown
259	common\src\java\org\apache\hadoop\io\nativeio\NativeIO.java	unrelated	package org apache hadoop io nativeio jni wrappers various native io related calls available java these functions generally used alongside fallback another portable mechanism native io flags open call bits fcntl o rdonly o wronly o rdwr o creat o excl o noctty o trunc o append o nonblock o sync o async o fsync o sync o ndelay o nonblock log log log factory get log native io boolean native loaded false boolean workaround non thread safe passwd calls false string workaround non threadsafe calls key hadoop workaround non threadsafe getpwuid boolean workaround non threadsafe calls default false native code loader native code loaded return true jni based native io extensions available boolean available return native code loader native code loaded native loaded wrapper around open native file descriptor open string path flags mode throws io exception wrapper around fstat native stat fstat file descriptor fd throws io exception wrapper around chmod native chmod string path mode throws io exception initialize jni method id id cache native init native result type fstat call stat string owner group mode mode constants s ifmt type file s ififo named pipe fifo s ifchr character special s ifdir directory s ifblk block special s ifreg regular s iflnk symbolic link s ifsock socket s ifwht whiteout s isuid set user id execution s isgid set group id execution s isvtx save swapped text even use s irusr read permission owner s iwusr write permission owner s ixusr execute search permission owner stat string owner string group mode string string string get owner string get group get mode
260	common\src\java\org\apache\hadoop\io\nativeio\NativeIOException.java	unrelated	package org apache hadoop io nativeio an exception generated call native io code these exceptions simply wrap errno result codes native io exception extends io exception serial version uid l errno errno native io exception string msg errno errno errno get errno string string
261	common\src\java\org\apache\hadoop\io\retry\DefaultFailoverProxyProvider.java	unrelated	package org apache hadoop io retry an implementation link failover proxy provider nothing event failover always returns proxy object default failover proxy provider implements failover proxy provider object proxy class iface default failover proxy provider class iface object proxy class get interface object get proxy perform failover object current proxy
262	common\src\java\org\apache\hadoop\io\retry\FailoverProxyProvider.java	unrelated	package org apache hadoop io retry an implementer capable providing proxy objects use ipc communication potentially modifying objects creating entirely new ones event certain types failures the determination whether fail handled link retry policy failover proxy provider get proxy object used next failover event occurs object get proxy called whenever associated link retry policy determines error warrants failing failover event perform failover object current proxy return reference provider proxy objects actually implement if methods annotated link idempotent fact passed link retry policy retry exception boolean method error use determining whether failover attempted link failover proxy provider get proxy class get interface
263	common\src\java\org\apache\hadoop\io\retry\Idempotent.java	unrelated	package org apache hadoop io retry used mark certain methods idempotent therefore warrant retried failover idempotent
264	common\src\java\org\apache\hadoop\io\retry\package-info.java	unrelated	package org apache hadoop io retry
265	common\src\java\org\apache\hadoop\io\retry\RetryInvocationHandler.java	unrelated	package org apache hadoop io retry retry invocation handler implements invocation handler log log log factory get log retry invocation handler failover proxy provider proxy provider retry policy default policy map string retry policy method name to policy map object current proxy retry invocation handler failover proxy provider proxy provider retry invocation handler failover proxy provider proxy provider object invoke object proxy method method object args object invoke method method method object args throws throwable
266	common\src\java\org\apache\hadoop\io\retry\RetryPolicies.java	unrelated	package org apache hadoop io retry p a collection useful implementations link retry policy p retry policies log log log factory get log retry policies p try fail throwing exception this corresponds retry mechanism place p retry policy try once then fail new try once then fail p try fail silently code code methods throwing exception non code code methods p retry policy try once dont fail new try once dont fail p keep trying forever p retry policy retry forever new retry forever p keep trying limited number times waiting fixed time attempts fail throwing exception p retry policy retry up to maximum count with fixed sleep max retries sleep time time unit time unit return new retry up to maximum count with fixed sleep max retries sleep time time unit p keep trying maximum time waiting fixed time attempts fail throwing exception p retry policy retry up to maximum time with fixed sleep max time sleep time time unit time unit return new retry up to maximum time with fixed sleep max time sleep time time unit p keep trying limited number times waiting growing amount time attempts fail throwing exception the time attempts code sleep time code mutliplied number tries far p retry policy retry up to maximum count with proportional sleep max retries sleep time time unit time unit return new retry up to maximum count with proportional sleep max retries sleep time time unit p keep trying limited number times waiting growing amount time attempts fail throwing exception the time attempts code sleep time code mutliplied random number range number retries p retry policy exponential backoff retry return new exponential backoff retry max retries sleep time time unit p set default policy explicit handlers specific exceptions p retry policy retry by exception retry policy default policy return new exception dependent retry default policy exception to policy map p a retry policy remote exception set default policy explicit handlers specific exceptions p retry policy retry by remote exception return new remote exception dependent retry default policy exception to policy map retry policy failover on network exception max failovers return failover on network exception try once then fail max failovers retry policy failover on network exception return new failover on network exception retry fallback policy max failovers try once then fail implements retry policy retry action retry exception e retries failovers try once dont fail implements retry policy retry action retry exception e retries failovers retry forever implements retry policy retry action retry exception e retries failovers retry limited implements retry policy max retries sleep time time unit time unit retry limited max retries sleep time time unit time unit retry action retry exception e retries failovers protected calculate sleep time retries retry up to maximum count with fixed sleep extends retry limited retry up to maximum count with fixed sleep max retries sleep time time unit time unit protected calculate sleep time retries retry up to maximum time with fixed sleep extends retry up to maximum count with fixed sleep retry up to maximum time with fixed sleep
267	common\src\java\org\apache\hadoop\io\retry\RetryPolicy.java	unrelated	package org apache hadoop io retry p specifies policy retrying method failures implementations immutable p retry policy returned link retry policy retry exception boolean enum retry action p determines whether framework retry method given exception number retries made operation far p different backend implementation reasonably retried failover know previous attempt reached server code false code method retried fail exception methods method failed retried retry action retry exception e retries failovers
268	common\src\java\org\apache\hadoop\io\retry\RetryProxy.java	unrelated	package org apache hadoop io retry p a factory creating retry proxies p retry proxy p create proxy implementation using retry policy method p object create class iface object implementation return retry proxy create iface create proxy implementations using given link failover proxy provider retry policy method object create class iface failover proxy provider proxy provider return proxy new proxy instance create proxy implementation using set retry policies specified method name if retry policy defined method default link retry policies try once then fail used object create class iface object implementation return retry proxy create iface create proxy implementations using given link failover proxy provider set retry policies specified method name if retry policy defined method default link retry policies try once then fail used object create class iface failover proxy provider proxy provider return proxy new proxy instance
269	common\src\java\org\apache\hadoop\io\serializer\Deserializer.java	unrelated	package org apache hadoop io serializer p provides facility deserializing objects type t link input stream p p deserializers stateful must buffer input since producers may read input calls link deserialize object p deserializer t p prepare deserializer reading p open input stream throws io exception p deserialize next object underlying input stream if object code code non null deserializer may set internal state next object read input stream otherwise object code code null new deserialized object created p t deserialize t throws io exception p close underlying input stream clear resources p close throws io exception
270	common\src\java\org\apache\hadoop\io\serializer\DeserializerComparator.java	unrelated	package org apache hadoop io serializer p a link raw comparator uses link deserializer deserialize objects compared standard link comparator used compare p p one may optimize compare intensive operations using custom implementation link raw comparator operates directly byte representations p deserializer comparator t implements raw comparator t input buffer buffer new input buffer deserializer t deserializer t key t key protected deserializer comparator deserializer t deserializer compare byte b byte b
271	common\src\java\org\apache\hadoop\io\serializer\JavaSerialization.java	unrelated	package org apache hadoop io serializer p an experimental link serialization java link serializable p java serialization implements serialization serializable java serialization deserializer t extends serializable implements deserializer t object input stream ois open input stream throws io exception t deserialize t object throws io exception close throws io exception java serialization serializer implements serializer serializable object output stream oos open output stream throws io exception serialize serializable object throws io exception close throws io exception boolean accept class c return serializable assignable from c deserializer serializable get deserializer class serializable c return new java serialization deserializer serializable serializer serializable get serializer class serializable c return new java serialization serializer
272	common\src\java\org\apache\hadoop\io\serializer\JavaSerializationComparator.java	unrelated	package org apache hadoop io serializer p a link raw comparator uses link java serialization link deserializer deserialize objects compared via link comparable interfaces p java serialization comparator t extends serializable comparable t extends deserializer comparator t java serialization comparator throws io exception compare t t
273	common\src\java\org\apache\hadoop\io\serializer\Serialization.java	unrelated	package org apache hadoop io serializer p encapsulates link serializer link deserializer pair p serialization t allows clients test whether link serialization supports given boolean accept class c serializer t get serializer class t c deserializer t get deserializer class t c
274	common\src\java\org\apache\hadoop\io\serializer\SerializationFactory.java	unrelated	package org apache hadoop io serializer p a factory link serialization p serialization factory extends configured log log log factory get log serialization factory get name list serialization serializations new array list serialization p serializations found reading code io serializations code property code conf code comma delimited list classnames p serialization factory configuration conf super conf string serializer name conf get strings io serializations new string writable serialization get name avro specific serialization get name avro reflect serialization get name add conf serializer name add configuration conf string serialization name try class extends serialization serializion class class extends serialization conf get class by name serialization name serializations add serialization reflection utils new instance serializion class get conf catch class not found exception e log warn serialization found e t serializer t get serializer class t c return get serialization c get serializer c t deserializer t get deserializer class t c return get serialization c get deserializer c t serialization t get serialization class t c serialization serialization serializations serialization accept c return serialization t serialization return null
275	common\src\java\org\apache\hadoop\io\serializer\Serializer.java	unrelated	package org apache hadoop io serializer p provides facility serializing objects type t link output stream p p serializers stateful must buffer output since producers may write output calls link serialize object p serializer t p prepare serializer writing p open output stream throws io exception p serialize code code underlying output stream p serialize t throws io exception p close underlying output stream clear resources p close throws io exception
276	common\src\java\org\apache\hadoop\io\serializer\WritableSerialization.java	unrelated	package org apache hadoop io serializer a link serialization link writable delegates link writable write java io data output link writable read fields java io data input writable serialization extends configured implements serialization writable writable deserializer extends configured implements deserializer writable class writable class data input stream data in writable deserializer configuration conf class c open input stream writable deserialize writable w throws io exception close throws io exception writable serializer extends configured implements serializer writable data output stream data out open output stream serialize writable w throws io exception close throws io exception boolean accept class c return writable assignable from c serializer writable get serializer class writable c return new writable serializer deserializer writable get deserializer class writable c return new writable deserializer get conf c
277	common\src\java\org\apache\hadoop\io\serializer\avro\AvroReflectSerializable.java	unrelated	package org apache hadoop io serializer avro tag avro reflect serializable classes implementing serialized deserialized using link avro reflect serialization avro reflect serializable
278	common\src\java\org\apache\hadoop\io\serializer\avro\AvroReflectSerialization.java	unrelated	package org apache hadoop io serializer avro serialization avro reflect for accepted serialization must either package list configured via code avro reflect pkgs code implement link avro reflect serializable avro reflect serialization extends avro serialization object string avro reflect packages avro reflect pkgs set string packages synchronized boolean accept class c get packages datum reader get reader class object clazz schema get schema object datum writer get writer class object clazz
279	common\src\java\org\apache\hadoop\io\serializer\avro\AvroSerialization.java	unrelated	package org apache hadoop io serializer avro base providing serialization avro types avro serialization t extends configured implements serialization t string avro schema key avro schema deserializer t get deserializer class t c serializer t get serializer class t c schema get schema t datum writer t get writer class t clazz datum reader t get reader class t clazz avro serializer implements serializer t avro deserializer implements deserializer t
280	common\src\java\org\apache\hadoop\io\serializer\avro\AvroSpecificSerialization.java	unrelated	package org apache hadoop io serializer avro serialization avro specific this serialization used generated avro specific compiler avro specific serialization boolean accept class c datum reader get reader class specific record clazz schema get schema specific record datum writer get writer class specific record clazz
281	common\src\java\org\apache\hadoop\ipc\AvroRpcEngine.java	unrelated	package org apache hadoop ipc tunnel avro format rpc requests hadoop link rpc connection this give cross language wire compatibility since hadoop rpc wire format non standard permit use avro protocol versioning features inter java rp cs avro rpc engine implements rpc engine log log log factory get log rpc version implementation tunnel rpc engine engine new writable rpc engine tunnel avro rpc request response hadoop rpc tunnel protocol extends versioned protocol writable rpc engine expects version id every protocol version id l all avro methods responses go buffer list writable call buffer list writable request throws io exception a writable holds list byte buffer the avro rpc transceiver basic unit data transfer buffer list writable implements writable list byte buffer buffers buffer list writable required rpc writables buffer list writable list byte buffer buffers read fields data input throws io exception write data output throws io exception an avro rpc transceiver tunnels client requests hadoop rpc client transceiver extends transceiver tunnel protocol tunnel inet socket address remote client transceiver inet socket address addr string get remote name return remote string list byte buffer transceive list byte buffer request list byte buffer read buffers throws io exception write buffers list byte buffer buffers throws io exception close throws io exception construct client side proxy object implements named protocol talking server named address t protocol proxy t get proxy class t protocol client version throws io exception return new protocol proxy t protocol stop proxy stop proxy object proxy try catch io exception e invoker implements invocation handler closeable client transceiver tx specific requestor requestor invoker class protocol inet socket address addr close throws io exception protected specific requestor create requestor class protocol return new reflect requestor protocol transeiver protected responder create responder class iface object impl return new reflect responder iface impl an avro rpc responder process requests passed via hadoop rpc tunnel responder implements tunnel protocol responder responder tunnel responder class iface object impl get protocol version string protocol version throws io exception protocol signature get protocol signature buffer list writable call buffer list writable request object call method method object params throw new unsupported operation exception construct server protocol implementation instance listening port address rpc server get server class iface object impl string bind address return engine get server tunnel protocol
282	common\src\java\org\apache\hadoop\ipc\AvroSpecificRpcEngine.java	unrelated	package org apache hadoop ipc avro rpc engine uses avro specific ap is the protocols generated via avro idl needs use engine avro specific rpc engine extends avro rpc engine protected specific requestor create requestor class protocol protected responder create responder class iface object impl
283	common\src\java\org\apache\hadoop\ipc\Client.java	authenticate	package org apache hadoop ipc a client ipc service ipc calls take single link writable parameter return link writable value a service runs port defined parameter value client log log hashtable connection id connection connections class extends writable value class call values counter counter call ids atomic boolean running new atomic boolean true client runs configuration conf socket factory socket factory create sockets ref count string ping interval name ipc ping interval default ping interval min ping call id set ping interval configuration conf ping interval get ping interval configuration conf get timeout configuration conf synchronized inc count synchronized dec count synchronized boolean zero reference a call waiting value call thread reads responses notifies callers each connection owns connection extends thread call implementation used parallel calls parallel call extends call result collector parallel calls parallel results construct ipc client whose values given link writable client class extends writable value class configuration conf client class extends writable value class configuration conf return socket factory client socket factory get socket factory stop threads related client no calls may made stop make call passing code param code ipc server running writable call writable param inet socket address address throws interrupted exception io exception make call passing code param code ipc server running writable call writable param inet socket address addr make call passing code param code ipc server running writable call writable param inet socket address addr writable call writable param inet socket address addr make call passing code param code ipc server defined writable call writable param connection id remote id io exception wrap exception inet socket address addr writable call writable params inet socket address addresses writable call writable params inet socket address addresses makes set calls parallel each parameter sent writable call writable params inet socket address addresses unit testing set connection id get connection ids get connection pool create new one add connection get connection connection id remote id connection id
284	common\src\java\org\apache\hadoop\ipc\ClientCache.java	pooling	package org apache hadoop ipc cache client using socket factory hash key client cache map socket factory client clients construct cache ipc client user provided socket factory cached client exists synchronized client get client configuration conf construct cache ipc client default socket factory default value class cached client exists synchronized client get client configuration conf construct cache ipc client user provided socket factory cached client exists default response type object writable synchronized client get client configuration conf socket factory factory stop rpc client connection a rpc client closed reference count becomes zero stop client client client
285	common\src\java\org\apache\hadoop\ipc\ConnectionHeader.java	authenticate	package org apache hadoop ipc the ipc connection header sent client server connection establishment connection header implements writable log log log factory get log connection header string protocol user group information ugi null auth method auth method connection header create new link connection header given code protocol code link user group information server server connection header string protocol user group information ugi auth method auth method read fields data input throws io exception write data output throws io exception string get protocol user group information get ugi string string
286	common\src\java\org\apache\hadoop\ipc\package-info.java	unrelated	package org apache hadoop ipc
287	common\src\java\org\apache\hadoop\ipc\ProtocolProxy.java	unrelated	package org apache hadoop ipc wraps around server proxy containing list supported methods a list methods value null indicates client server protocol protocol proxy t class t protocol t proxy hash set integer server methods null boolean support server method check boolean server methods fetched false protocol proxy class t protocol t proxy fetch server methods method method throws io exception t get proxy synchronized boolean method supported string method name throws io exception
288	common\src\java\org\apache\hadoop\ipc\ProtocolSignature.java	unrelated	package org apache hadoop ipc protocol signature implements writable register ctor writable factories set factory version methods null array method hash codes default constructor protocol signature constructor protocol signature version method hashcodes version version methods method hashcodes get version return version get methods return methods read fields data input throws io exception version read long boolean methods read boolean methods write data output throws io exception write long version methods null else calculate method hash code considering method name returning type parameter types get fingerprint method method hashcode method get name hash code hashcode hashcode method get return type get name hash code class type method get parameter types return hashcode convert array method array hash codes get fingerprints method methods methods null hash codes new methods length methods length return hash codes get hash code array methods methods sorted hashcode calculated so returned value irrelevant method order array get fingerprint method methods return get fingerprint get fingerprints methods get hash code array hashcodes hashcodes sorted hashcode calculated so returned value irrelevant hashcode order array get fingerprint hashcodes arrays sort hashcodes return arrays hash code hashcodes protocol sig fingerprint protocol signature signature fingerprint protocol sig fingerprint protocol signature sig fingerprint a cache maps protocol name signature finger print hash map string protocol sig fingerprint protocol fingerprint cache return protocol signature finger print cache protocol sig fingerprint get sig fingerprint string protocol name protocol get name synchronized protocol fingerprint cache get server protocol signature protocol signature get protocol signature try get finger print signature cache protocol sig fingerprint sig get sig fingerprint protocol server version check client side protocol matches one server side client methods hash code sig fingerprint return sig signature get server protocol signature protocol signature get protocol signature versioned protocol server class extends versioned protocol inter try catch exception e server version server get protocol version protocol client version return protocol signature get protocol signature
289	common\src\java\org\apache\hadoop\ipc\RemoteException.java	unrelated	package org apache hadoop ipc remote exception extends io exception for java io serializable serial version uid l string name remote exception string name string msg string get class name if remote exception wraps one lookup types return exception p unwraps io exception io exception unwrap remote exception class lookup types instantiate return exception wrapped remote exception p this unwraps code throwable code constructor taking code string code parameter otherwise returns io exception unwrap remote exception io exception instantiate exception class extends io exception cls create remote exception attributes remote exception value of attributes attrs string string
290	common\src\java\org\apache\hadoop\ipc\RPC.java	unrelated	package org apache hadoop ipc a simple rpc mechanism a protocol java all parameters return types must one ul li primitive type code boolean code code byte code code char code code short code code code code code code code code code code code li li link string li li link writable li li array types li ul all methods protocol throw io exception no field data protocol instance transmitted rpc log log log factory get log rpc rpc ctor cache rpc engines protocol map class rpc engine protocol engines new hash map class rpc engine track rpc engine used proxy stop proxy map class rpc engine proxy engines new hash map class rpc engine string engine prop rpc engine set protocol use non default rpc engine set protocol engine configuration conf conf set class engine prop protocol get name engine rpc engine return rpc engine configured handle protocol synchronized rpc engine get protocol engine class protocol rpc engine engine protocol engines get protocol engine null return engine return rpc engine handles proxy object synchronized rpc engine get proxy engine object proxy return proxy engines get proxy get class a version mismatch rpc protocol version mismatch extends io exception serial version uid string name client version server version version mismatch string name client version string get interface name get client version get server version get proxy connection remote server t t wait for proxy return wait for protocol proxy protocol client version addr conf get proxy get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t wait for protocol proxy class t protocol return wait for protocol proxy get proxy connection remote server t t wait for proxy class t protocol client version return wait for protocol proxy protocol client version addr get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t wait for protocol proxy class t protocol return wait for protocol proxy protocol client version addr conf conn timeout get proxy connection remote server t t wait for proxy class t protocol return wait for protocol proxy protocol client version addr get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t wait for protocol proxy class t protocol start time system current time millis io exception ioe true construct client side proxy object implements named protocol talking server named address t t get proxy class t protocol return get protocol proxy get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t get protocol proxy class t protocol user group information ugi user group information get current user return get protocol proxy protocol client version addr ugi conf factory construct client side proxy object implements named protocol talking server named address t t get proxy class t protocol return get protocol proxy get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t get protocol proxy class t protocol return get protocol proxy construct client side proxy implements named protocol talking server
291	common\src\java\org\apache\hadoop\ipc\RpcClientException.java	unrelated	package org apache hadoop ipc indicates exception rpc client rpc client exception extends rpc exception serial version uid l constructs exception specified detail message rpc client exception string message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc client exception string message throwable cause
292	common\src\java\org\apache\hadoop\ipc\RpcEngine.java	unrelated	package org apache hadoop ipc an rpc implementation rpc engine construct client side proxy object t protocol proxy t get proxy class t protocol stop proxy stop proxy object proxy expert make multiple parallel calls set servers object call method method object params inet socket address addrs construct server protocol implementation instance rpc server get server class protocol object instance string bind address
293	common\src\java\org\apache\hadoop\ipc\RpcException.java	unrelated	package org apache hadoop ipc indicates exception execution remote procedure call rpc exception extends io exception serial version uid l constructs exception specified detail message rpc exception string message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc exception string message throwable cause
294	common\src\java\org\apache\hadoop\ipc\RpcServerException.java	unrelated	package org apache hadoop ipc indicates exception rpc server rpc server exception extends rpc exception serial version uid l constructs exception specified detail message rpc server exception string message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc server exception string message throwable cause
295	common\src\java\org\apache\hadoop\ipc\Server.java	authenticate	package org apache hadoop ipc an ipc service ipc calls take single link writable parameter return link writable value a service runs port defined parameter value server boolean authorize boolean security enabled byte buffer header byte buffer wrap hrpc get bytes introduce ping server throw away rp cs introduce protocol rpc connection header introduced sasl security layer introduced use link array primitive writable internal object writable efficiently transmit arrays primitives byte current version initial resp buf size log log log factory get log server log auditlog string auth failed for auth failed string auth successfull for auth successfull thread local server server new thread local server map string class protocol cache class get protocol class string protocol name configuration conf throws class not found exception returns server instance called null may called server get this set call object handler invokes rpc reset thread local call cur call new thread local call returns remote side ip address invoked inside rpc inet address get remote ip returns remote address invoked inside rpc string get remote address return true invocation rpc boolean rpc invocation string bind address port port listen handler count number handler threads read threads number read threads class extends writable param class call parameters max idle time maximum idle time threshold idle connections number idle connections max connections to nuke max number protected rpc metrics rpc metrics protected rpc detailed metrics rpc detailed metrics configuration conf secret manager token identifier secret manager service authorization manager service authorization manager new service authorization manager max queue size max resp size socket send buffer size boolean tcp no delay t disable nagle algorithm volatile boolean running true true server runs blocking queue call call queue queued calls list connection connection list maintain list client connections listener listener null responder responder null num connections handler handlers null bind server socket socket inet socket address address rpc metrics get rpc metrics refresh service acl configuration conf policy provider provider service authorization manager get service authorization manager a call queued handling call listens socket creates jobs handler threads listener extends thread sends responses rpc back clients responder extends thread reads calls connection queues handling connection handles queued calls handler extends thread protected server string bind address port constructs server listening named port address parameters passed must protected server string bind address port close connection connection connection setup response byte array output stream response throws io exception wrap with sasl byte array output stream response call call configuration get conf unit testing called server started disable security unit testing called server started enable security sets socket buffer size used responding rp cs set socket send buf size size socket send buffer size size starts service must called calls handled synchronized start stops service no new calls handled called synchronized stop wait server stopped synchronized join throws interrupted exception synchronized inet socket address get listener address writable call writable param receive time throws io exception called call writable call class protocol throws io exception authorize user group information user get num open connections get call queue len get max queue
296	common\src\java\org\apache\hadoop\ipc\StandbyException.java	unrelated	package org apache hadoop ipc thrown remote server active server set servers subset may active standby exception extends exception serial version uid x ad l standby exception string msg
297	common\src\java\org\apache\hadoop\ipc\Status.java	unrelated	package org apache hadoop ipc status hadoop ipc call enum status success error fatal state status state
298	common\src\java\org\apache\hadoop\ipc\UnexpectedServerException.java	unrelated	package org apache hadoop ipc indicates rpc server encountered undeclared exception service unexpected server exception extends rpc exception serial version uid l constructs exception specified detail message unexpected server exception string message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown unexpected server exception string message throwable cause
299	common\src\java\org\apache\hadoop\ipc\VersionedProtocol.java	unrelated	package org apache hadoop ipc superclass protocols use hadoop rpc subclasses also supposed version id field versioned protocol get protocol version string protocol protocol signature get protocol signature string protocol
300	common\src\java\org\apache\hadoop\ipc\WritableRpcEngine.java	unrelated	package org apache hadoop ipc an rpc engine implementation writable data writable rpc engine implements rpc engine log log log factory get log rpc writable rpc version updated change format rpc messages writable rpc version l a method invocation including method name parameters invocation implements writable configurable string method name class parameter classes object parameters configuration conf client version client methods hash this could different writable rpc version received server client using different version rpc version invocation invocation method method object parameters the name method invoked string get method name return method name the parameter class get parameter classes return parameter classes the parameter instances object get parameters return parameters get protocol version get client methods hash returns rpc version used client get rpc version read fields data input throws io exception write data output throws io exception string string set conf configuration conf configuration get conf client cache clients new client cache invoker implements invocation handler client connection id remote id client client boolean closed false invoker class protocol object invoke object proxy method method object args close ipc client responsible invoker rp cs synchronized close unit testing client get client configuration conf return clients get client conf construct client side proxy object implements named protocol talking server named address t protocol proxy t get proxy class t protocol client version throws io exception t proxy t proxy new proxy instance protocol get class loader return new protocol proxy t protocol proxy true stop proxy release invoker resource stop proxy object proxy invoker proxy get invocation handler proxy close expert make multiple parallel calls set servers object call method method object params throws io exception interrupted exception invocation invocations new invocation params length params length client client clients get client conf try writable wrapped values method get return type void type object values values length return values finally construct server protocol implementation instance listening port address server get server class protocol throws io exception return new server instance conf bind address port num handlers an rpc server server extends rpc server object instance boolean verbose construct rpc server server object instance configuration conf string bind address port string name base string name construct rpc server server object instance configuration conf string bind address port writable call class protocol writable param received time throws io exception log string value value null value length log info value
301	common\src\java\org\apache\hadoop\ipc\metrics\package-info.java	unrelated	package org apache hadoop ipc metrics
302	common\src\java\org\apache\hadoop\ipc\metrics\RpcDetailedMetrics.java	unrelated	package org apache hadoop ipc metrics this maintaining rpc method related statistics publishing metrics interfaces rpc detailed metrics log log log factory get log rpc detailed metrics metrics registry registry string name rpc detailed metrics port string name return name rpc detailed metrics create port init class protocol add processing time string name processing time shutdown
303	common\src\java\org\apache\hadoop\ipc\metrics\RpcMetrics.java	authenticate	package org apache hadoop ipc metrics this maintaining various rpc statistics publishing metrics interfaces rpc metrics log log log factory get log rpc metrics server server metrics registry registry string name rpc metrics server server string name return name rpc metrics create server server mutable counter int rpc authentication failures mutable counter int rpc authentication successes mutable counter int rpc authorization failures mutable counter int rpc authorization successes public instrumentation methods could extracted decide custom instrumentation la job tracker instrumenation the methods override comment candidates methods instrumentation incr authentication failures incr authentication successes incr authorization successes incr authorization failures shutdown incr sent bytes count incr received bytes count add rpc queue time q time add rpc processing time processing time
304	common\src\java\org\apache\hadoop\jmx\JMXJsonServlet.java	unrelated	package org apache hadoop jmx this servlet based jmx proxy servlet tomcat it rewritten read output json format really close original provides read web access jmx p this servlet generally placed jmx url http server it provides read access jmx metrics the optional code qry code parameter may used query subset jmx beans this query functionality provided link m bean server query names object name javax management query exp method p for example code http jmx qry hadoop code return hadoop metrics exposed jmx p the optional code get code parameter used query specific attribute jmx bean the format url code http jmx get mx bean name attribute name code p for example code http jmx get hadoop service name node name name node info cluster id code return cluster id namenode mxbean p if code qry code code get code parameter formatted correctly bad request http response code returned p if resouce mbean attribute found sc not found http response code returned p the return format json form p code pre beans name bean name pre code p the servlet attempts convert jmx beans json each bean attributes converted json object member if attribute boolean number array converted json equivalent if value link composite data converted json object keys name json member value converted following rules if value link tabular data converted array link composite data elements contains all objects converted output the bean name modeler type returned beans jmx json servlet extends http servlet log log log factory get log jmx json servlet serial version uid l instance variables m bean server protected transient m bean server bean server null public methods initialize servlet init throws servlet exception retrieve m bean server bean server management factory get platform m bean server process get request specified resource the servlet request processing the servlet response creating get http servlet request request http servlet response response try do authorization http server administrator access get servlet context request response return response set content type application json charset utf print writer writer response get writer json factory json factory new json factory json generator jg json factory create json generator writer jg use default pretty printer jg write start object bean server null jg write string field result error jg write string field message no m bean server could found jg close log error no m bean server could found response set status http servlet response sc not found return query per mbean attribute string getmethod request get parameter get getmethod null string split strings getmethod split split strings length jg write string field result error jg write string field message query format expected jg close response set status http servlet response sc bad request return list beans jg new object name split strings split strings response jg close return query per mbean string qry request get parameter qry qry null qry list beans jg new object name qry null response jg close catch io exception e log error caught exception processing jmx request e response set status http servlet response sc internal server error
305	common\src\java\org\apache\hadoop\jmx\package-info.java	unrelated	package org apache hadoop jmx
306	common\src\java\org\apache\hadoop\log\EventCounter.java	unrelated	package org apache hadoop log a log j appender simply counts logging events three levels fatal error warn the name used log j properties event counter extends org apache hadoop log metrics event counter
307	common\src\java\org\apache\hadoop\log\LogLevel.java	unrelated	package org apache hadoop log change log level runtime log level string usages n usages n java log level get name getlevel host port name n java log level get name setlevel host port name level n a command line implementation main string args args length getlevel equals args else args length setlevel equals args system err println usages system exit process string urlstring try catch io exception ioe string marker output pattern tag pattern compile a servlet implementation servlet extends http servlet serial version uid l get http servlet request request http servlet response response string forms n br hr get set process org apache log j logger log string level process java util logging logger log string level
308	common\src\java\org\apache\hadoop\log\metrics\EventCounter.java	unrelated	package org apache hadoop log metrics a log j appender simply counts logging events three levels fatal error warn the name used log j properties event counter extends appender skeleton fatal error warn info event counts event counts counts new event counts get fatal get error get warn get info append logging event event close boolean requires layout
309	common\src\java\org\apache\hadoop\metrics\ContextFactory.java	unrelated	package org apache hadoop metrics factory creating metrics context objects to obtain instance use code get factory code method context factory string properties file string context class suffix string default context classname context factory factory null map string object attribute map new hash map string object map string metrics context context map used contexts context factory cannot created map string metrics context null context map creates new instance context factory protected context factory object get attribute string attribute name string get attribute names set attribute string attribute name object value remove attribute string attribute name synchronized metrics context get context string ref name string context name synchronized metrics context get context string context name synchronized collection metrics context get all contexts synchronized metrics context get null context string context name synchronized context factory get factory throws io exception set attributes throws io exception
310	common\src\java\org\apache\hadoop\metrics\MetricsContext.java	unrelated	package org apache hadoop metrics the main metrics package metrics context default period seconds data sent metrics system default period initialize context init string context name context factory factory returns context name string get context name starts restarts monitoring emitting metrics records updated start monitoring throws io exception stops monitoring this free data implementation may buffered sending next timer event it ok call code start monitoring code calling stop monitoring returns true monitoring currently progress boolean monitoring stops monitoring also frees buffered data returning object initial state close creates new metrics record instance given code record name code throws exception metrics implementation configured fixed set record names code record name code set metrics record create record string record name registers callback called regular time intervals determined implementation specific configuration metrics records return register updater updater updater removes callback exists unregister updater updater updater returns timer period get period retrieves records managed metrics context useful monitoring systems polling based map string collection output record get all records
311	common\src\java\org\apache\hadoop\metrics\MetricsException.java	unrelated	package org apache hadoop metrics general purpose unchecked metrics exception metrics exception extends runtime exception serial version uid l creates new instance metrics exception metrics exception creates new instance metrics exception metrics exception string message
312	common\src\java\org\apache\hadoop\metrics\MetricsRecord.java	unrelated	package org apache hadoop metrics a named optionally tagged set records sent metrics system p a record name identifies kind data reported for example program reporting statistics relating disks computer might use record name disk stats p a record zero tags a tag name value to continue example disk stats record might use tag named disk name identify particular disk sometimes useful one tag might also disk type value ide scsi whatever p a record also zero metrics these named values reported metrics system in disk stats example possible metric names would disk percent full disk percent busy kb read per second etc p the general procedure using metrics record fill tag metric values call code update code pass record client library metric data immediately sent metrics system time code update code called an internal table maintained identified record name this table columns corresponding tag metric names rows corresponding unique set tag values an update either modifies existing row table adds new row set tag values different rows note tags one row table p once row added table data sent metrics system every timer period whether updated since previous timer period if inappropriate example metrics reported transient object application code remove code method used remove row thus stop data sent p note code update code method atomic this means safe different threads updating metric more precisely ok different threads call code update code metrics record instances set tag names tag values different threads b b use metrics record instance time metrics record returns record name string get record name sets named tag specified value the tag value may null treated empty string set tag string tag name string tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name short tag value sets named tag specified value set tag string tag name byte tag value removes tag specified name remove tag string tag name sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name short metric value sets named metric specified value conflicts configuration set metric string metric name byte metric value sets named metric specified value conflicts configuration set metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name short metric value increments named metric specified value conflicts configuration incr metric string metric name byte metric value increments named metric specified value conflicts configuration incr metric string metric name metric value updates table buffered data sent periodically if tag values match existing row row updated otherwise new row added update removes buffered data table rows tags equal tags set record
313	common\src\java\org\apache\hadoop\metrics\MetricsServlet.java	unrelated	package org apache hadoop metrics a servlet print metrics data by default servlet returns textual representation promises made parseability users use format json parseable output metrics servlet extends http servlet a helper hold tag map metric map tags metrics pair implements json convertible tag map tag map metric map metric map tags metrics pair tag map tag map metric map metric map json map map converts json providing array json output collects metric data returns map context name record name tag tag value metric metric value the values either string number the value implemented list tags metrics pair map string map string list tags metrics pair make map map string map string list tags metrics pair map metrics context context contexts return map get http servlet request request http servlet response response do authorization http server administrator access get servlet context request string format request get parameter format collection metrics context contexts json equals format else prints metrics data multi line text form print map print writer map string map string list tags metrics pair map map entry string map string list tags metrics pair context map entry set indent print writer indent indent
314	common\src\java\org\apache\hadoop\metrics\MetricsUtil.java	unrelated	package org apache hadoop metrics utility simplify creation reporting hadoop metrics for examples usage see name node metrics metrics util log log metrics util metrics context get context string context name metrics context get context string ref name string context name metrics record create record metrics context context string get host name
315	common\src\java\org\apache\hadoop\metrics\Updater.java	unrelated	package org apache hadoop metrics call back see code metrics context register updater code updater timer based call back metric library updates metrics context context
316	common\src\java\org\apache\hadoop\metrics\file\FileContext.java	unrelated	package org apache hadoop metrics file metrics context writing metrics file p this configured setting context factory attributes turn usually configured properties file all attributes prefixed context name for example properties file might contain pre context name file name tmp metrics log context name period pre file context extends abstract metrics context configuration attribute names protected string file name property file name protected string period property period file file null file metrics written print writer writer null creates new instance file context file context init string context name context factory factory string get file name start monitoring stop monitoring emit record string context name string record name output record rec flush
317	common\src\java\org\apache\hadoop\metrics\ganglia\GangliaContext.java	unrelated	package org apache hadoop metrics ganglia context sending metrics ganglia ganglia context extends abstract metrics context string period property period string servers property servers string units property units string slope property slope string tmax property tmax string dmax property dmax string default units string default slope default tmax default dmax default port buffer size per libgmond c log log log factory get log get class map class string type table new hash map class string protected byte buffer new byte buffer size protected offset protected list extends socket address metrics servers map string string units table map string string slope table map string string tmax table map string string dmax table protected datagram socket datagram socket creates new instance ganglia context ganglia context init string context name context factory factory method close datagram socket close emit record string context name string record name throws io exception protected emit metric string name string type string value throws io exception protected string get units string metric name protected get slope string metric name protected get tmax string metric name protected get dmax string metric name puts buffer first writing size followed bytes padded necessary multiple protected xdr string pads buffer zero bytes nearest multiple pad puts integer buffer bytes big endian protected xdr
318	common\src\java\org\apache\hadoop\metrics\ganglia\GangliaContext31.java	unrelated	package org apache hadoop metrics ganglia context sending metrics ganglia version x slightly different wire portal compared x ganglia context extends ganglia context string host name unknown example com log log init string context name context factory factory protected emit metric string name string type string value
319	common\src\java\org\apache\hadoop\metrics\jvm\EventCounter.java	unrelated	package org apache hadoop metrics jvm a log j appender simply counts logging events three levels fatal error warn event counter extends org apache hadoop log metrics event counter
320	common\src\java\org\apache\hadoop\metrics\jvm\JvmMetrics.java	unrelated	package org apache hadoop metrics jvm singleton reports java virtual machine metrics metrics api any application create instance order emit java vm metrics jvm metrics implements updater
321	common\src\java\org\apache\hadoop\metrics\jvm\package-info.java	unrelated	package org apache hadoop metrics jvm
322	common\src\java\org\apache\hadoop\metrics\spi\AbstractMetricsContext.java	unrelated	package org apache hadoop metrics spi the main service provider interface this extended order integrate metrics api specific metrics client library p this implements internal table metric data timer data sent metrics system subclasses must code emit record code method order transmit data p abstract metrics context implements metrics context period metrics context default period timer timer null set updater updaters new hash set updater volatile boolean monitoring false context factory factory null string context name null tag map extends tree map string object metric map extends tree map string number record map extends hash map tag map metric map map string record map buffered data new hash map string record map protected abstract metrics context init string context name context factory factory protected string get attribute string attribute name protected map string string get attribute table string table name string get context name context factory get context factory synchronized start monitoring synchronized stop monitoring boolean monitoring synchronized close synchronized metrics record create record string record name protected metrics record new record string record name synchronized register updater updater updater synchronized unregister updater updater updater synchronized clear updaters synchronized start timer synchronized stop timer timer event throws io exception synchronized emit records throws io exception synchronized map string collection output record get all records protected emit record string context name string record name protected flush throws io exception protected update metrics record impl record synchronized record map get record map string record name number sum number number b protected remove metrics record impl record get period protected set period period protected parse and set period string attribute name
323	common\src\java\org\apache\hadoop\metrics\spi\CompositeContext.java	unrelated	package org apache hadoop metrics spi composite context extends abstract metrics context log log log factory get log composite context string arity label arity string sub fmt sub array list metrics context subctxt new array list metrics context composite context init string context name context factory factory super init context name factory n kids try catch exception e n kids metrics record new record string record name return metrics record proxy new proxy instance protected emit record string context name string record name metrics context ctxt subctxt protected flush throws io exception metrics context ctxt subctxt start monitoring throws io exception metrics context ctxt subctxt stop monitoring metrics context ctxt subctxt return true subcontexts monitoring boolean monitoring boolean ret true metrics context ctxt subctxt return ret close metrics context ctxt subctxt register updater updater updater metrics context ctxt subctxt unregister updater updater updater metrics context ctxt subctxt metrics record delegator implements invocation handler method get record name init method method init method string record name array list metrics record subrecs metrics record delegator string record name array list metrics context ctxts object invoke object p method object args throws throwable
324	common\src\java\org\apache\hadoop\metrics\spi\MetricsRecordImpl.java	unrelated	package org apache hadoop metrics spi an implementation metrics record keeps back pointer context created delegates back code update code code remove code metrics record impl implements metrics record tag map tag table new tag map map string metric value metric table new linked hash map string metric value string record name abstract metrics context context creates new instance file record protected metrics record impl string record name abstract metrics context context returns record name string get record name sets named tag specified value set tag string tag name string tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name short tag value sets named tag specified value set tag string tag name byte tag value removes tag specified name remove tag string tag name sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name short metric value sets named metric specified value conflicts configuration set metric string metric name byte metric value sets named metric specified value conflicts configuration set metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name short metric value increments named metric specified value conflicts configuration incr metric string metric name byte metric value increments named metric specified value conflicts configuration incr metric string metric name metric value set absolute string metric name number metric value set increment string metric name number metric value updates table buffered data sent periodically if tag values match existing row row updated otherwise new row added update removes row exists buffered data table tags equal tags set record remove tag map get tag table map string metric value get metric table
325	common\src\java\org\apache\hadoop\metrics\spi\MetricValue.java	unrelated	package org apache hadoop metrics spi a number either absolute incremental amount metric value boolean absolute false boolean increment true boolean increment number number creates new instance metric value metric value number number boolean increment boolean increment boolean absolute number get number
326	common\src\java\org\apache\hadoop\metrics\spi\NoEmitMetricsContext.java	unrelated	package org apache hadoop metrics spi a metrics context emit data unlike null context with update save retrieval get all records this useful want support link metrics servlet emit metrics way no emit metrics context extends abstract metrics context string period property period creates new instance null context with update thread no emit metrics context init string context name context factory factory do nothing version emit record protected emit record string context name string record name
327	common\src\java\org\apache\hadoop\metrics\spi\NullContext.java	unrelated	package org apache hadoop metrics spi null metrics context metrics context nothing used default context performance data emitted configuration data found null context extends abstract metrics context creates new instance null context null context do nothing version start monitoring start monitoring do nothing version emit record protected emit record string context name string record name do nothing version update protected update metrics record impl record do nothing version remove protected remove metrics record impl record
328	common\src\java\org\apache\hadoop\metrics\spi\NullContextWithUpdateThread.java	unrelated	package org apache hadoop metrics spi a null context thread calling periodically monitoring started this keeps data sampled correctly in respects like null context no data emitted this suitable monitoring systems like jmx reads metrics someone reads data jmx the default impl start stop monitoring abstract metrics context good enough null context with update thread extends abstract metrics context string period property period creates new instance null context with update thread null context with update thread init string context name context factory factory do nothing version emit record protected emit record string context name string record name do nothing version update protected update metrics record impl record do nothing version remove protected remove metrics record impl record
329	common\src\java\org\apache\hadoop\metrics\spi\OutputRecord.java	unrelated	package org apache hadoop metrics spi represents record metric data sent metrics system output record tag map tag map metric map metric map creates new instance output record output record tag map tag map metric map metric map returns set tag names set string get tag names returns tag object string integer short byte object get tag string name returns set metric names set string get metric names returns metric object float integer short byte number get metric string name returns copy record tags tag map get tags copy returns copy record metrics metric map get metrics copy
330	common\src\java\org\apache\hadoop\metrics\spi\Util.java	unrelated	package org apache hadoop metrics spi static utility methods util this intended instantiated util parses space comma separated sequence server specifications form hostname hostname port if specs null defaults localhost default port list inet socket address parse string specs default port list inet socket address result new array list inet socket address specs null else return result
331	common\src\java\org\apache\hadoop\metrics\util\MBeanUtil.java	unrelated	package org apache hadoop metrics util this util provides method register m bean using standard naming convention described doc link link register m bean string string object m bean util object name register m bean string service name unregister m bean object name mbean name object name get m bean name string service name
332	common\src\java\org\apache\hadoop\metrics\util\MetricsBase.java	unrelated	package org apache hadoop metrics util this base metrics metrics base string no description no description string name string description protected metrics base string nam protected metrics base string nam string desc push metric metrics record mr string get name return name string get description return description
333	common\src\java\org\apache\hadoop\metrics\util\MetricsDynamicMBeanBase.java	unrelated	package org apache hadoop metrics util this base facilitates creating dynamic mbeans automatically metrics the metrics constructors registers metrics registry different categories metrics differnt registry name node metrics data node metrics then m bean created passing registry constructor the m bean registered using mbean name example metrics holder metrics new metrics holder metrics registry metrics test m bean m bean new metrics test m bean metrics mregistry object name mbean name m bean util register m bean service foo test statistics m bean metrics dynamic m bean base implements dynamic m bean string avg time avg time string min time min time string max time max time string num ops num ops string reset all min max op reset all min max metrics registry metrics registry m bean info mbean info map string metrics base metrics rate attribute mod num entries in registry string mbean description protected metrics dynamic m bean base metrics registry mr string m bean description update mbean info if metrics list changed create m bean info object get attribute string attribute name throws attribute not found exception attribute list get attributes string attribute names m bean info get m bean info object invoke string action name object parms string signature set attribute attribute attribute attribute list set attributes attribute list attributes
334	common\src\java\org\apache\hadoop\metrics\util\MetricsIntValue.java	unrelated	package org apache hadoop metrics util the metrics int value metric time varied changes set each time value set published next update call metrics int value extends metrics base log log value boolean changed metrics int value string nam metrics registry registry string description metrics int value string nam metrics registry registry synchronized set new value synchronized get synchronized push metric metrics record mr
335	common\src\java\org\apache\hadoop\metrics\util\MetricsLongValue.java	unrelated	package org apache hadoop metrics util the metrics long value metric time varied changes set each time value set published next update call metrics long value extends metrics base value boolean changed metrics long value string nam metrics registry registry string description metrics long value string nam metrics registry registry synchronized set new value synchronized get synchronized push metric metrics record mr
336	common\src\java\org\apache\hadoop\metrics\util\MetricsRegistry.java	unrelated	package org apache hadoop metrics util this registry metrics related set metrics declared holding registered registry metrics also stored holding metrics registry map string metrics base metrics list new hash map string metrics base metrics registry size synchronized add string metrics name metrics base metrics obj synchronized metrics base get string metrics name synchronized collection string get key list synchronized collection metrics base get metrics list
337	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingInt.java	unrelated	package org apache hadoop metrics util the metrics time varying int metric naturally varies time e g number files created the metrics accumulated interval set metrics config file metrics published end interval reset zero hence counter value current interval note one wants time associated metric use metrics time varying int extends metrics base log log current value previous interval value metrics time varying int string nam metrics time varying int string nam metrics registry registry synchronized inc incr synchronized inc synchronized interval heart beat synchronized push metric metrics record mr synchronized get previous interval value synchronized get current interval value
338	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingLong.java	unrelated	package org apache hadoop metrics util licensed apache software foundation asf one contributor license agreements see notice file distributed work additional information regarding copyright ownership the asf licenses file apache license version license may use file except compliance license you may obtain copy license http www apache org licenses license unless required applicable law agreed writing software distributed license distributed as is basis without warranties or conditions of any kind either express implied see license specific language governing permissions limitations license the metrics time varying long metric naturally varies time e g number files created the metrics accumulated interval set metrics config file metrics published end interval reset zero hence counter value current interval note one wants time associated metric use metrics time varying long extends metrics base log log current value previous interval value metrics time varying long string nam metrics registry registry string description metrics time varying long string nam metrics registry registry synchronized inc incr synchronized inc synchronized interval heart beat synchronized push metric metrics record mr synchronized get previous interval value synchronized get current interval value
339	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingRate.java	unrelated	package org apache hadoop metrics util the metrics time varying rate rate based metric naturally varies time e g time taken create file the rate averaged interval heart beat interval set metrics config file this also keeps track min max rates along method reset min max metrics time varying rate extends metrics base log log metrics min max metrics current data metrics previous interval data min max min max metrics time varying rate string nam metrics registry registry string description metrics time varying rate string nam metrics registry registry synchronized inc num ops time synchronized inc time synchronized interval heart beat synchronized push metric metrics record mr synchronized get previous interval num ops synchronized get previous interval average time synchronized get min time synchronized get max time synchronized reset min max
340	common\src\java\org\apache\hadoop\metrics\util\package-info.java	unrelated	package org apache hadoop metrics util
341	common\src\java\org\apache\hadoop\metrics2\AbstractMetric.java	unrelated	package org apache hadoop metrics the immutable metric abstract metric implements metrics info metrics info info construct metric protected abstract metric metrics info info protected metrics info info get value metric number value get type metric metric type type accept visitor visit metrics visitor visitor
342	common\src\java\org\apache\hadoop\metrics2\MetricsCollector.java	unrelated	package org apache hadoop metrics the metrics collector metrics collector add metrics record metrics record builder add record string name add metrics record metrics record builder add record metrics info info
343	common\src\java\org\apache\hadoop\metrics2\MetricsException.java	unrelated	package org apache hadoop metrics a general metrics exception wrapper metrics exception extends runtime exception serial version uid l construct exception message metrics exception string message construct exception message cause metrics exception string message throwable cause construct exception cause metrics exception throwable cause
344	common\src\java\org\apache\hadoop\metrics2\MetricsFilter.java	unrelated	package org apache hadoop metrics the metrics filter metrics filter implements metrics plugin whether accept name boolean accepts string name whether accept tag boolean accepts metrics tag tag whether accept tags boolean accepts iterable metrics tag tags whether accept record boolean accepts metrics record record
345	common\src\java\org\apache\hadoop\metrics2\MetricsInfo.java	unrelated	package org apache hadoop metrics interface provide immutable meta info metrics metrics info string name string description
346	common\src\java\org\apache\hadoop\metrics2\MetricsPlugin.java	unrelated	package org apache hadoop metrics the plugin metrics framework metrics plugin initialize plugin init subset configuration conf
347	common\src\java\org\apache\hadoop\metrics2\MetricsRecord.java	unrelated	package org apache hadoop metrics an immutable snapshot metrics timestamp metrics record get timestamp metrics timestamp string name string description string context get tags record note returning collection instead iterable need use tags keys hence collection hash code etc maps collection metrics tag tags get metrics record iterable abstract metric metrics
348	common\src\java\org\apache\hadoop\metrics2\MetricsRecordBuilder.java	unrelated	package org apache hadoop metrics the metrics record builder metrics record builder add metrics tag metrics record builder tag metrics info info string value add immutable metrics tag object metrics record builder add metrics tag tag add pre made immutable metric object metrics record builder add abstract metric metric set context tag metrics record builder set context string value add integer metric metrics record builder add counter metrics info info value add metric metrics record builder add counter metrics info info value add integer gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value metrics collector parent syntactic sugar add multiple records collector one liner metrics collector end record return parent
349	common\src\java\org\apache\hadoop\metrics2\MetricsSink.java	unrelated	package org apache hadoop metrics the metrics sink metrics sink extends metrics plugin put metrics record sink put metrics metrics record record flush buffered metrics flush
350	common\src\java\org\apache\hadoop\metrics2\MetricsSource.java	unrelated	package org apache hadoop metrics the metrics source metrics source get metrics source get metrics metrics collector collector boolean
351	common\src\java\org\apache\hadoop\metrics2\MetricsSystem.java	unrelated	package org apache hadoop metrics the metrics system metrics system implements metrics system mx bean metrics system init string prefix register metrics source annotations source object t t register string name string desc t source register metrics source deriving name description object t t register t source return register null null source metrics source get source string name register metrics sink t extends metrics sink t register string name string desc t sink register callback jmx events register callback callback shutdown metrics system completely usually server shutdown the metrics system mx bean unregistered boolean shutdown the metrics system callback needed proxies callback called start pre start called start post start called stop pre stop called stop post stop convenient implementing callback abstract callback implements callback
352	common\src\java\org\apache\hadoop\metrics2\MetricsSystemMXBean.java	unrelated	package org apache hadoop metrics the jmx metrics system metrics system mx bean start metrics system start stop metrics system stop start metrics m beans start metrics m beans stop metrics m beans note stop metrics system control m bean e stop metrics m beans avoided get config turn config attribute support multiple line values jconsole string current config
353	common\src\java\org\apache\hadoop\metrics2\MetricsTag.java	unrelated	package org apache hadoop metrics immutable tag metrics grouping host queue username etc metrics tag implements metrics info metrics info info string value construct tag name description value metrics tag metrics info info string value metrics info info get value tag string value
354	common\src\java\org\apache\hadoop\metrics2\MetricsVisitor.java	unrelated	package org apache hadoop metrics a visitor metrics metrics visitor callback integer value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback integer value counters counter metrics info info value callback value counters counter metrics info info value
355	common\src\java\org\apache\hadoop\metrics2\MetricType.java	unrelated	package org apache hadoop metrics enum metric type a monotonically increasing metric used calculate throughput counter an arbitrary varying metric gauge
356	common\src\java\org\apache\hadoop\metrics2\package-info.java	unrelated	package org apache hadoop metrics
357	common\src\java\org\apache\hadoop\metrics2\annotation\Metric.java	unrelated	package org apache hadoop metrics annotation annotation single metric metric enum type shorthand optional name description string value default string default string sample name default ops string value name default time boolean always default false type type default type default
358	common\src\java\org\apache\hadoop\metrics2\annotation\Metrics.java	unrelated	package org apache hadoop metrics annotation annotation group metrics metrics string name default string default string context
359	common\src\java\org\apache\hadoop\metrics2\annotation\package-info.java	unrelated	package org apache hadoop metrics annotation
360	common\src\java\org\apache\hadoop\metrics2\filter\AbstractPatternFilter.java	unrelated	package org apache hadoop metrics filter base pattern based filters abstract pattern filter extends metrics filter protected string include key protected string exclude key exclude protected string include tags key tags protected string exclude tags key exclude tags pattern pattern pattern exclude pattern map string pattern tag patterns map string pattern exclude tag patterns pattern tag pattern pattern compile w abstract pattern filter init subset configuration conf set include pattern pattern pattern set exclude pattern pattern exclude pattern set include tag pattern string name pattern pattern set exclude tag pattern string name pattern pattern boolean accepts metrics tag tag boolean accepts iterable metrics tag tags boolean accepts string name protected pattern compile string
361	common\src\java\org\apache\hadoop\metrics2\filter\GlobFilter.java	unrelated	package org apache hadoop metrics filter a glob pattern filter metrics the name used metrics config files glob filter extends abstract pattern filter protected pattern compile string
362	common\src\java\org\apache\hadoop\metrics2\filter\package-info.java	unrelated	package org apache hadoop metrics filter
363	common\src\java\org\apache\hadoop\metrics2\filter\RegexFilter.java	unrelated	package org apache hadoop metrics filter a regex pattern filter metrics regex filter extends abstract pattern filter protected pattern compile string
364	common\src\java\org\apache\hadoop\metrics2\impl\AbstractMetricsRecord.java	unrelated	package org apache hadoop metrics impl abstract metrics record implements metrics record should make sense time record used key
365	common\src\java\org\apache\hadoop\metrics2\impl\MBeanInfoBuilder.java	unrelated	package org apache hadoop metrics impl helper build m bean info metrics records m bean info builder implements metrics visitor string name description list m bean attribute info attrs iterable metrics record impl recs cur rec no m bean info builder string name string desc m bean info builder reset iterable metrics record impl recs m bean attribute info new attr info string name string desc string type m bean attribute info new attr info metrics info info string type gauge metrics info info value gauge metrics info info value gauge metrics info info value gauge metrics info info value counter metrics info info value counter metrics info info value string get attr name string name m bean info get
366	common\src\java\org\apache\hadoop\metrics2\impl\MetricCounterInt.java	unrelated	package org apache hadoop metrics impl metric counter int extends abstract metric value metric counter int metrics info info value integer value metric type type visit metrics visitor visitor
367	common\src\java\org\apache\hadoop\metrics2\impl\MetricCounterLong.java	unrelated	package org apache hadoop metrics impl metric counter long extends abstract metric value metric counter long metrics info info value long value metric type type visit metrics visitor visitor
368	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeDouble.java	unrelated	package org apache hadoop metrics impl metric gauge double extends abstract metric value metric gauge double metrics info info value double value metric type type visit metrics visitor visitor
369	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeFloat.java	unrelated	package org apache hadoop metrics impl metric gauge float extends abstract metric value metric gauge float metrics info info value float value metric type type visit metrics visitor visitor
370	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeInt.java	unrelated	package org apache hadoop metrics impl metric gauge int extends abstract metric value metric gauge int metrics info info value integer value metric type type visit metrics visitor visitor
371	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeLong.java	unrelated	package org apache hadoop metrics impl metric gauge long extends abstract metric value metric gauge long metrics info info value long value metric type type visit metrics visitor visitor
372	common\src\java\org\apache\hadoop\metrics2\impl\MetricsBuffer.java	unrelated	package org apache hadoop metrics impl an immutable element sink queues metrics buffer implements iterable metrics buffer entry iterable entry mutable metrics buffer iterable metrics buffer entry mutable mutable mutable iterator entry iterator return mutable iterator entry string source name iterable metrics record impl records entry string name iterable metrics record impl records string name iterable metrics record impl records
373	common\src\java\org\apache\hadoop\metrics2\impl\MetricsBufferBuilder.java	unrelated	package org apache hadoop metrics impl builder immutable metrics buffers metrics buffer builder extends array list metrics buffer entry serial version uid l boolean add string name iterable metrics record impl records metrics buffer get
374	common\src\java\org\apache\hadoop\metrics2\impl\MetricsCollectorImpl.java	unrelated	package org apache hadoop metrics impl metrics collector impl implements metrics collector list metrics record builder impl rbs lists new array list metrics filter record filter metric filter metrics record builder impl add record metrics info info metrics record builder impl add record string name list metrics record impl get records iterator metrics record builder impl iterator clear rbs clear metrics collector impl set record filter metrics filter rf metrics collector impl set metric filter metrics filter mf
375	common\src\java\org\apache\hadoop\metrics2\impl\MetricsConfig.java	unrelated	package org apache hadoop metrics impl metrics configuration metrics system impl metrics config extends subset configuration log log log factory get log metrics config string default file name hadoop metrics properties string prefix default string period key period period default seconds string queue capacity key queue capacity queue capacity default string retry delay key retry delay retry delay default seconds string retry backoff key retry backoff retry backoff default back factor string retry count key retry count retry count default string jmx cache ttl key jmx cache ttl string start mbeans key source start mbeans string plugin urls key plugin urls string context key context string name key name string desc key description string source key source string sink key sink string metric filter key metric filter string record filter key record filter string source filter key source filter pattern instance regex pattern compile splitter splitter splitter trim results class loader plugin loader metrics config configuration c string prefix super c prefix lower case locale us metrics config create string prefix return load first prefix hadoop metrics prefix lower case locale us metrics config create string prefix string file names return load first prefix file names load configuration list files first successful load metrics config load first string prefix string file names string fname file names try catch configuration exception e log warn cannot locate configuration tried default empty configuration return new metrics config new properties configuration prefix metrics config subset string prefix return new metrics config prefix return sub configs instance specified config assuming format specified follows pre type instance option value pre note special default instance excluded result map string metrics config get instance configs string type map string metrics config map maps new hash map metrics config sub subset type string key sub keys matcher matcher instance regex matcher key matcher matches return map iterable string keys return new iterable string iterator string iterator will poke parents defaults object get property string key object value super get property key value null log debug enabled return get parent get property key starts with prefix default key log debug enabled log debug returning value key key return value t extends metrics plugin t get plugin string name string cls name get class name name cls name null return null try class cls class name cls name true get plugin loader t plugin t cls new instance plugin init name empty subset name return plugin catch exception e throw new metrics config exception error creating plugin cls name e string get class name string prefix string key prefix empty prefix string cls name get string key log debug cls name cls name null cls name empty return null return cls name class loader get plugin loader plugin loader null return plugin loader class loader default loader get class get class loader object purls super get property plugin urls key purls null return default loader iterable string jars splitter split string purls len iterables size jars len url urls new url len try catch exception e log debug enabled plugin loader privileged new
376	common\src\java\org\apache\hadoop\metrics2\impl\MetricsConfigException.java	unrelated	package org apache hadoop metrics impl the metrics configuration runtime exception metrics config exception extends metrics exception serial version uid l metrics config exception string message metrics config exception string message throwable cause metrics config exception throwable cause
377	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordBuilderImpl.java	unrelated	package org apache hadoop metrics impl metrics record builder impl extends metrics record builder metrics collector parent timestamp metrics info rec info list abstract metric metrics list metrics tag tags metrics filter record filter metric filter boolean acceptable metrics record builder impl metrics collector parent metrics info info metrics collector parent return parent metrics record builder impl tag metrics info info string value metrics record builder impl add metrics tag tag metrics record builder impl add abstract metric metric metrics record builder impl add counter metrics info info value metrics record builder impl add counter metrics info info value metrics record builder impl add gauge metrics info info value metrics record builder impl add gauge metrics info info value metrics record builder impl add gauge metrics info info value metrics record builder impl add gauge metrics info info value metrics record builder impl set context string value metrics record impl get record list metrics tag tags list abstract metric metrics
378	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordFiltered.java	unrelated	package org apache hadoop metrics impl metrics record filtered extends abstract metrics record metrics record delegate metrics filter filter metrics record filtered metrics record delegate metrics filter filter
379	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordImpl.java	unrelated	package org apache hadoop metrics impl metrics record impl extends abstract metrics record protected string default context default timestamp metrics info info list metrics tag tags iterable abstract metric metrics construct metrics record metrics record impl metrics info info timestamp metrics info info list metrics tag tags
380	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSinkAdapter.java	unrelated	package org apache hadoop metrics impl an adapter metrics sink associated filters metrics sink adapter implements sink queue consumer metrics buffer log log log factory get log metrics sink adapter string name description context metrics sink sink metrics filter source filter record filter metric filter sink queue metrics buffer queue thread sink thread volatile boolean stopping false volatile boolean error false period first retry delay retry count retry backoff metrics registry registry new metrics registry sinkadapter mutable stat latency mutable counter int dropped mutable gauge int qsize metrics sink adapter string name string description metrics sink sink boolean put metrics metrics buffer buffer logical time publish metrics from queue consume metrics buffer buffer start stop string name string description snapshot metrics record builder rb boolean metrics sink sink
381	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSourceAdapter.java	unrelated	package org apache hadoop metrics impl an adapter metrics source associated filter jmx impl metrics source adapter implements dynamic m bean log log log factory get log metrics source adapter string prefix name metrics source source metrics filter record filter metric filter hash map string attribute attr cache m bean info builder info builder iterable metrics tag injected tags iterable metrics record impl last recs jmx cache ts jmx cache ttl m bean info info cache object name mbean name boolean start m beans metrics source adapter string prefix string name string description metrics source adapter string prefix string name string description start synchronized object get attribute string attribute set attribute attribute attribute synchronized attribute list get attributes string attributes attribute list set attributes attribute list attributes object invoke string action name object params string signature synchronized m bean info get m bean info synchronized update jmx cache iterable metrics record impl get metrics metrics collector impl builder synchronized stop synchronized start m beans synchronized stop m beans update info cache update attr cache string tag name string name rec no set attr cache tag metrics tag tag rec no string metric name string name rec no set attr cache metric abstract metric metric rec no string name metrics source source
382	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSystemImpl.java	unrelated	package org apache hadoop metrics impl a base metrics system singletons metrics system impl extends metrics system implements metrics source log log log factory get log metrics system impl string ms name metrics system string ms stats name ms name sub stats string ms stats desc metrics system metrics string ms control name ms name sub control string ms init mode key hadoop metrics init mode enum init mode normal standby map string metrics source adapter sources map string metrics source sources map string metrics sink adapter sinks map string metrics sink sinks list callback callbacks metrics collector impl collector metrics registry registry new metrics registry ms name list metrics tag injected tags things changed init start stop string prefix metrics filter source filter metrics config config map string metrics config source configs sink configs boolean monitoring false timer timer period seconds logical time number timer invocations period object name mbean name boolean publish self metrics true metrics source adapter sys source ref count mini cluster mode metrics system impl string prefix metrics system impl synchronized metrics system init string prefix synchronized start synchronized stop t register string name string desc t source synchronized register source string name string desc metrics source source t register string name string description t sink synchronized register sink string name string desc metrics sink sink synchronized register callback callback synchronized start metrics m beans synchronized stop metrics m beans synchronized string current config synchronized start timer synchronized timer event synchronized metrics buffer sample metrics snapshot metrics metrics source adapter sa synchronized publish metrics metrics buffer buffer synchronized stop timer synchronized stop sources synchronized stop sinks synchronized configure string prefix synchronized configure system synchronized configure sinks metrics sink adapter new sink string name string desc metrics sink sink metrics sink adapter new sink string name string desc configure sources clear configs string get hostname register system source synchronized get metrics metrics collector builder boolean init system m bean synchronized boolean shutdown metrics source get source string name init mode init mode
383	common\src\java\org\apache\hadoop\metrics2\impl\MsInfo.java	unrelated	package org apache hadoop metrics impl metrics system related metrics info instances enum ms info implements metrics info num active sources number active metrics sources num all sources number registered metrics sources num active sinks number active metrics sinks num all sinks number registered metrics sinks context metrics context hostname local hostname session id session id process name process name string desc ms info string desc
384	common\src\java\org\apache\hadoop\metrics2\impl\package-info.java	unrelated	package org apache hadoop metrics impl
385	common\src\java\org\apache\hadoop\metrics2\impl\SinkQueue.java	unrelated	package org apache hadoop metrics impl a half blocking nonblocking producers blocking consumers queue metrics sinks new elements dropped queue full preserve interesting elements onset queue filling events sink queue t consumer t a fixed size circular buffer minimize garbage t data head head position tail tail position size number elements thread current consumer null sink queue capacity synchronized boolean enqueue t e consume one element block queue empty only one consumer time allowed consume consumer t consumer throws interrupted exception consume elements block queue empty consume all consumer t consumer throws interrupted exception dequeue one element head queue block queue empty synchronized t dequeue throws interrupted exception synchronized t wait for data throws interrupted exception synchronized check consumer synchronized set consumer lock synchronized clear consumer lock synchronized t dequeue synchronized t front synchronized t back synchronized clear synchronized size capacity
386	common\src\java\org\apache\hadoop\metrics2\lib\DefaultMetricsFactory.java	unrelated	package org apache hadoop metrics lib experimental extend metrics dynamically enum default metrics factory instance singleton mutable metrics factory mmf impl mutable metrics factory get annotated metrics factory synchronized t t get instance class t cls synchronized set instance mutable metrics factory factory
387	common\src\java\org\apache\hadoop\metrics2\lib\DefaultMetricsSystem.java	unrelated	package org apache hadoop metrics lib the default metrics system singleton enum default metrics system instance singleton metrics system impl new metrics system impl volatile boolean mini cluster mode false unique names bean names new unique names unique names source names new unique names convenience method initialize metrics system metrics system initialize string prefix synchronized metrics system init string prefix metrics system instance shutdown metrics system shutdown synchronized shutdown instance metrics system set instance metrics system ms synchronized metrics system set impl metrics system ms synchronized metrics system get impl return impl set mini cluster mode boolean choice boolean mini cluster mode object name new m bean name string name string source name string name boolean dup ok synchronized object name new object name string name synchronized string new source name string name boolean dup ok
388	common\src\java\org\apache\hadoop\metrics2\lib\Interns.java	unrelated	package org apache hadoop metrics lib helpers create interned metrics info interns log log log factory get log interns a simple intern cache two keys avoid creating new combined key objects lookup cache with keys k k v map k map k v k map protected boolean expire key at size protected boolean expire key at size protected v new value k k k k synchronized v add k k k k sanity limits case misuse abuse max info names max info descs distinct per name enum info instance cache with keys string string metrics info cache get metric info object metrics info info string name string description return info instance cache add name description sanity limits max tag names max tag values distinct per name enum tags instance cache with keys metrics info string metrics tag cache get metrics tag metrics tag tag metrics info info string value return tags instance cache add info value get metrics tag metrics tag tag string name string description string value return tags instance cache add info name description value
389	common\src\java\org\apache\hadoop\metrics2\lib\MethodMetric.java	unrelated	package org apache hadoop metrics lib metric generated method mostly used annotation method metric extends mutable metric log log log factory get log method metric object obj method method metrics info info mutable metric impl method metric object obj method method metrics info info metric type type obj check not null obj object method check arg method method get parameter types length method set accessible true info check not null info info impl new impl check not null type metric type mutable metric new impl metric type metric type class res type method get return type switch metric type case counter return new counter res type case gauge return new gauge res type case default return res type string new tag res type new gauge res type case tag return new tag res type default check arg metric type false unsupported metric type return null mutable metric new counter class type int type long type return new mutable metric try catch exception ex throw new metrics exception unsupported counter type type get name boolean int class type boolean ret type integer type type integer return ret boolean long class type return type long type type long boolean float class type return type float type type float boolean double class type return type double type type double mutable metric new gauge class int long float double return new mutable metric try catch exception ex throw new metrics exception unsupported gauge type get name mutable metric new tag class res type res type string return new mutable metric try catch exception ex throw new metrics exception unsupported tag type res type get name impl snapshot builder metrics info metric info method method return interns info name from method metric method get name string name from method method string method name method get name method name starts with get return string utils capitalize method name substring return string utils capitalize method name
390	common\src\java\org\apache\hadoop\metrics2\lib\MetricsAnnotations.java	unrelated	package org apache hadoop metrics lib metrics annotation helpers metrics annotations make metrics source annotated object metrics source make source object source metrics source builder new source builder object source
391	common\src\java\org\apache\hadoop\metrics2\lib\MetricsInfoImpl.java	unrelated	package org apache hadoop metrics lib making implementing metric info little easier metrics info impl implements metrics info string name description metrics info impl string name string description
392	common\src\java\org\apache\hadoop\metrics2\lib\MetricsRegistry.java	unrelated	package org apache hadoop metrics lib an optional metrics registry creating maintaining collection metrics mutables making writing metrics source easier metrics registry map string mutable metric metrics map maps new linked hash map map string metrics tag tags map maps new linked hash map metrics info metrics info metrics registry string name metrics registry metrics info info metrics info info synchronized mutable metric get string name synchronized metrics tag get tag string name mutable counter int new counter string name string desc val synchronized mutable counter int new counter metrics info info val mutable counter long new counter string name string desc val synchronized mutable counter long new counter metrics info info val mutable gauge int new gauge string name string desc val synchronized mutable gauge int new gauge metrics info info val mutable gauge long new gauge string name string desc val synchronized mutable gauge long new gauge metrics info info val synchronized mutable stat new stat string name string desc mutable stat new stat string name string desc mutable rate new rate string name mutable rate new rate string name string description mutable rate new rate string name string desc boolean extended synchronized mutable rate new rate string name string desc synchronized add string name mutable metric metric synchronized add string name value metrics registry set context string name metrics registry tag string name string description string value metrics registry tag string name string description string value synchronized metrics registry tag metrics info info string value boolean metrics registry tag metrics info info string value collection metrics tag tags collection mutable metric metrics check metric name string name check tag name string name synchronized snapshot metrics record builder builder boolean
393	common\src\java\org\apache\hadoop\metrics2\lib\MetricsSourceBuilder.java	unrelated	package org apache hadoop metrics lib helper build metrics source object annotations metrics source builder log log log factory get log metrics source builder object source mutable metrics factory factory metrics registry registry metrics info info boolean at metric false boolean registry false metrics source builder object source mutable metrics factory factory metrics source build metrics info info metrics registry init registry object source add object source field field add object source method method
394	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounter.java	unrelated	package org apache hadoop metrics lib the mutable counter monotonically increasing metric mutable counter extends mutable metric metrics info info protected mutable counter metrics info info protected metrics info info increment metric value incr
395	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounterInt.java	unrelated	package org apache hadoop metrics lib a mutable counter implementing metrics sources mutable counter int extends mutable counter volatile value mutable counter int metrics info info init value synchronized incr increment value delta synchronized incr delta value snapshot metrics record builder builder boolean
396	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounterLong.java	unrelated	package org apache hadoop metrics lib a mutable counter mutable counter long extends mutable counter volatile value mutable counter long metrics info info init value synchronized incr increment value delta synchronized incr delta value snapshot metrics record builder builder boolean
397	common\src\java\org\apache\hadoop\metrics2\lib\MutableGauge.java	unrelated	package org apache hadoop metrics lib the mutable gauge metric mutable gauge extends mutable metric metrics info info protected mutable gauge metrics info info protected metrics info info increment value metric incr decrement value metric decr
398	common\src\java\org\apache\hadoop\metrics2\lib\MutableGaugeInt.java	unrelated	package org apache hadoop metrics lib a mutable gauge mutable gauge int extends mutable gauge volatile value mutable gauge int metrics info info init value value synchronized incr increment delta synchronized incr delta synchronized decr decrement delta synchronized decr delta set value metric set value snapshot metrics record builder builder boolean
399	common\src\java\org\apache\hadoop\metrics2\lib\MutableGaugeLong.java	unrelated	package org apache hadoop metrics lib a mutable gauge mutable gauge long extends mutable gauge volatile value mutable gauge long metrics info info init value value synchronized incr increment delta synchronized incr delta synchronized decr decrement delta synchronized decr delta set value metric set value snapshot metrics record builder builder boolean
400	common\src\java\org\apache\hadoop\metrics2\lib\MutableMetric.java	unrelated	package org apache hadoop metrics lib the mutable metric mutable metric volatile boolean changed true get snapshot metric snapshot metrics record builder builder boolean get snapshot metric changed snapshot metrics record builder builder set changed flag mutable operations protected set changed changed true clear changed flag snapshot operations protected clear changed changed false boolean changed return changed
401	common\src\java\org\apache\hadoop\metrics2\lib\MutableMetricsFactory.java	unrelated	package org apache hadoop metrics lib mutable metrics factory log log log factory get log mutable metrics factory mutable metric new for field field field metric annotation mutable metric new for method object source method method metric annotation override handle custom mutable metrics fields protected mutable metric new for field field field metric annotation override handle custom mutable metrics methods protected mutable metric new for method object source method method protected metrics info get info metric annotation field field protected string get name field field protected metrics info get info metric annotation method method protected metrics info get info class cls metrics annotation protected string get name method method protected metrics info get info metric annotation string default name
402	common\src\java\org\apache\hadoop\metrics2\lib\MutableRate.java	unrelated	package org apache hadoop metrics lib a convenient mutable metric throughput measurement mutable rate extends mutable stat mutable rate string name string description boolean extended
403	common\src\java\org\apache\hadoop\metrics2\lib\MutableRates.java	unrelated	package org apache hadoop metrics lib helper manage group mutable rate metrics mutable rates extends mutable metric log log log factory get log mutable rates metrics registry registry set class protocol cache sets new hash set mutable rates metrics registry registry init class protocol add string name elapsed snapshot metrics record builder rb boolean
404	common\src\java\org\apache\hadoop\metrics2\lib\MutableStat.java	unrelated	package org apache hadoop metrics lib a mutable metric stats useful keeping throughput latency stats mutable stat extends mutable metric metrics info num info metrics info avg info metrics info stdev info metrics info min info metrics info max info metrics info min info metrics info max info sample stat interval stat new sample stat sample stat prev stat new sample stat sample stat min max min max new sample stat min max num samples boolean extended false construct sample statistics metric mutable stat string name string description construct snapshot stat metric extended stat default mutable stat string name string description add number samples sum running stat synchronized add num samples sum add snapshot metric synchronized add value synchronized snapshot metrics record builder builder boolean sample stat last stat reset time min max metric reset min max
405	common\src\java\org\apache\hadoop\metrics2\lib\package-info.java	unrelated	package org apache hadoop metrics lib
406	common\src\java\org\apache\hadoop\metrics2\lib\UniqueNames.java	unrelated	package org apache hadoop metrics lib generates predictable user friendly unique names unique names count string base name value count string name value joiner joiner joiner map string count map maps new hash map synchronized string unique name string name count c map get name c null c base name equals name c new count name true
407	common\src\java\org\apache\hadoop\metrics2\sink\FileSink.java	unrelated	package org apache hadoop metrics sink a metrics sink writes file file sink implements metrics sink string filename key filename print writer writer init subset configuration conf put metrics metrics record record flush
408	common\src\java\org\apache\hadoop\metrics2\sink\package-info.java	unrelated	package org apache hadoop metrics sink
409	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\AbstractGangliaSink.java	unrelated	package org apache hadoop metrics sink ganglia this base ganglia sink using metrics lot code derived org apache hadoop metrics ganglia ganglia context as per documentation sink implementations worry thread safety hence code written thread safety modified case assumption changes future abstract ganglia sink implements metrics sink log log log factory get log get class string default units default tmax default dmax ganglia slope default slope ganglia slope default port string servers property servers buffer size per libgmond c string support sparse metrics property supportsparse boolean support sparse metrics default false string equal string host name unknown example com datagram socket datagram socket list extends socket address metrics servers byte buffer new byte buffer size offset boolean support sparse metrics support sparse metrics default protected ganglia metric visitor ganglia metric visitor subset configuration conf map string ganglia conf ganglia conf map ganglia conf default ganglia conf new ganglia conf enum ganglia slope enum ganglia conf type init subset configuration conf flush load configurations conf type load ganglia conf ganglia conf type gtype protected ganglia conf get ganglia conf for metric string metric name protected string get host name protected xdr string pads buffer zero bytes nearest multiple pad protected xdr protected emit to ganglia hosts throws io exception reset buffer protected boolean support sparse metrics set datagram socket datagram socket datagram socket
410	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaConf.java	unrelated	package org apache hadoop metrics sink ganglia used store ganglia properties ganglia conf string units abstract ganglia sink default units ganglia slope slope dmax abstract ganglia sink default dmax tmax abstract ganglia sink default tmax string string string get units set units string units ganglia slope get slope set slope ganglia slope slope get dmax set dmax dmax get tmax set tmax tmax
411	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaMetricVisitor.java	unrelated	package org apache hadoop metrics sink ganglia since implementations metric hence use visitor figure type slope metric counters positive slope ganglia metric visitor implements metrics visitor string int string float string double string type ganglia slope slope string get type return type null others ganglia slope get slope return slope gauge metrics info info value metric gauge int type int slope null set null cannot figure metric gauge metrics info info value metric gauge long type float slope null set null cannot figure metric gauge metrics info info value metric gauge float type float slope null set null cannot figure metric gauge metrics info info value metric gauge double type double slope null set null cannot figure metric counter metrics info info value metric counter int type int counters positive slope slope ganglia slope positive counter metrics info info value metric counter long type float counters positive slope slope ganglia slope positive
412	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaSink30.java	unrelated	package org apache hadoop metrics sink ganglia this code supports ganglia ganglia sink extends abstract ganglia sink log log log factory get log get class metrics cache metrics cache new metrics cache put metrics metrics record record calculate slope properties metric ganglia slope calculate slope ganglia conf g conf the method sends metrics ganglia servers the method taken org apache hadoop metrics ganglia ganglia context minimal changes order keep sync protected emit metric string group name string name string type
413	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaSink31.java	unrelated	package org apache hadoop metrics sink ganglia this code supports ganglia ganglia sink extends ganglia sink log log log factory get log get class the method sends metrics ganglia servers the method taken org apache hadoop metrics ganglia ganglia context minimal changes order keep sync protected emit metric string group name string name string type
414	common\src\java\org\apache\hadoop\metrics2\source\JvmMetrics.java	unrelated	package org apache hadoop metrics source jvm logging related metrics mostly used various servers part metrics export jvm metrics implements metrics source enum singleton m memory mx bean memory mx bean management factory get memory mx bean list garbage collector mx bean gc beans thread mx bean thread mx bean management factory get thread mx bean string process name session id map string metrics info gc info cache maps new hash map jvm metrics string process name string session id jvm metrics create string process name string session id jvm metrics init singleton string process name string session id get metrics metrics collector collector boolean get memory usage metrics record builder rb get gc usage metrics record builder rb synchronized metrics info get gc info string gc name get thread usage metrics record builder rb get event counters metrics record builder rb
415	common\src\java\org\apache\hadoop\metrics2\source\JvmMetricsInfo.java	unrelated	package org apache hadoop metrics source jvm logging related metrics info instances enum jvm metrics info implements metrics info jvm metrics jvm related metrics etc record info metrics mem non heap used m non heap memory used mb mem non heap committed m non heap memory committed mb mem heap used m heap memory used mb mem heap committed m heap memory committed mb gc count total gc count gc time millis total gc time milliseconds threads new number new threads threads runnable number runnable threads threads blocked number blocked threads threads waiting number waiting threads threads timed waiting number timed waiting threads threads terminated number terminated threads log fatal total number fatal log events log error total number error log events log warn total number warning log events log info total number info log events string desc jvm metrics info string desc desc desc return objects string helper
416	common\src\java\org\apache\hadoop\metrics2\util\Contracts.java	unrelated	package org apache hadoop metrics util additional helpers besides guava preconditions programming contract contracts contracts check argument false conditions t t check arg t arg boolean expression object msg check argument false conditions check arg arg boolean expression object msg check argument false conditions check arg arg boolean expression object msg check argument false conditions check arg arg boolean expression object msg check argument false conditions check arg arg boolean expression object msg
417	common\src\java\org\apache\hadoop\metrics2\util\MBeans.java	unrelated	package org apache hadoop metrics util this util provides method register m bean using standard naming convention described doc link link register string string object m beans log log log factory get log m beans object name register string service name string name name unregister object name mbean name object name get m bean name string service name string name name
418	common\src\java\org\apache\hadoop\metrics2\util\MetricsCache.java	unrelated	package org apache hadoop metrics util a metrics cache sinks support sparse updates metrics cache log log log factory get log metrics cache max recs per name default map string record cache map maps new hash map max recs per name record cache serial version uid l boolean got overflow false protected boolean remove eldest entry map entry collection metrics tag cached record record map string string tags maps new hash map map string abstract metric metrics maps new hash map lookup tag value string get tag string key lookup metric value number get metric string key lookup metric instance abstract metric get metric instance string key set map entry string string tags set map entry string number metrics set map entry string abstract metric metrics entry set metrics cache max recs per name default construct metrics cache metrics cache max recs per name max recs per name max recs per name update cache return current cached record record update metrics record mr boolean including tags string name mr name record cache record cache map get name record cache null collection metrics tag tags mr tags record record record cache get tags record null abstract metric mr metrics including tags return record update cache return current cache record record update metrics record mr return update mr false get cached record record get string name collection metrics tag tags record cache rc map get name rc null return null return rc get tags
419	common\src\java\org\apache\hadoop\metrics2\util\package-info.java	unrelated	package org apache hadoop metrics util
420	common\src\java\org\apache\hadoop\metrics2\util\SampleStat.java	unrelated	package org apache hadoop metrics util helper compute running sample stats sample stat min max minmax new min max num samples construct new running sample stat sample stat reset num samples minmax reset we want reuse object sometimes reset num samples num samples num samples minmax reset minmax copy values saves object creation gc copy to sample stat reset num samples minmax add sample running stat sample stat add x minmax add x return add x add sample partial sum running stat note min max evaluated using method sample stat add n samples x num samples n samples num samples else return num samples return num samples mean return num samples variance return num samples num samples stddev return math sqrt variance min return minmax min max return minmax max helper keep running min max min max min double max value max double min value add value min return min max return max reset reset min max
421	common\src\java\org\apache\hadoop\metrics2\util\Servers.java	unrelated	package org apache hadoop metrics util helpers handle server addresses servers this intended instantiated servers parses space comma separated sequence server specifications form hostname hostname port if specs null defaults localhost default port list inet socket address parse string specs default port list inet socket address result lists new array list specs null else return result
422	common\src\java\org\apache\hadoop\net\CachedDNSToSwitchMapping.java	unrelated	package org apache hadoop net a cached implementation dns to switch mapping takes raw dns to switch mapping stores resolved network location cache the following calls resolved network location get location cache cached dns to switch mapping implements dns to switch mapping map string string cache new concurrent hash map string string protected dns to switch mapping raw mapping cached dns to switch mapping dns to switch mapping raw mapping returns hosts names cached previously list string get uncached hosts list string names caches resolved hosts cache resolved hosts list string uncached hosts returns cached resolution list hostnames addresses returns null names currently cache list string get cached hosts list string names resolves host names adds cache unlike resolve method hide unknown host exceptions list string resolve valid hosts list string names list string resolve list string names
423	common\src\java\org\apache\hadoop\net\DNS.java	unrelated	package org apache hadoop net a provides direct reverse lookup functionalities allowing querying specific network interfaces nameservers dns log log log factory get log dns string cached hostname resolve local hostname string cached host address resolve local host ip address string localhost localhost string reverse dns inet address host ip string ns string get i ps string str interface string get default ip string str interface string get hosts string str interface string nameserver string resolve local hostname string resolve local host ip address string get hosts string str interface string get default host string str interface string nameserver string get default host string str interface
424	common\src\java\org\apache\hadoop\net\DNSToSwitchMapping.java	unrelated	package org apache hadoop net an implemented allow pluggable dns name ip address rack id resolvers dns to switch mapping resolves list dns names ip addresses returns back list switch information network paths one one correspondence must maintained elements lists consider element argument list x com the switch information returned must network path form foo rack root foo switch rack connected note hostname ip address part returned path the network topology cluster would determine number components network path list string resolve list string names resolves list dns names ip addresses returns back list switch information network paths one one correspondence must maintained elements lists consider element argument list x com the switch information returned must network path form foo rack root foo switch rack connected note hostname ip address part returned path the network topology cluster would determine number components network path unlike resolve names must resolvable list string resolve valid hosts list string names
425	common\src\java\org\apache\hadoop\net\NetUtils.java	unrelated	package org apache hadoop net net utils log log log factory get log net utils map string string host to resolved get socket factory given according configuration parameter tt hadoop rpc socket factory lt class name gt tt when parameter exists fall back default socket factory configured tt hadoop rpc socket factory default tt if default socket factory configured fall back jvm default socket factory socket factory get socket factory configuration conf socket factory factory null string prop value prop value null prop value length factory null return factory get default socket factory specified configuration parameter tt hadoop rpc socket factory default tt jvm default socket factory configuration contain default socket factory property socket factory get default socket factory configuration conf string prop value conf get hadoop rpc socket factory default prop value null prop value length return get socket factory from property conf prop value get socket factory corresponding given proxy uri if given proxy uri corresponds absence configuration parameter returns null if uri malformed raises exception socket factory instantiate assumed non null non empty socket factory get socket factory from property try catch class not found exception cnfe util method build socket addr either host post fs host port path inet socket address create socket addr string target return create socket addr target util method build socket addr either host host post fs host port path inet socket address create socket addr string target target null colon index target index of colon index default port string hostname port target contains else port get static resolution hostname null return new inet socket address hostname port adds resolution host this used setting hostnames names fake point well known host for e g testcases require daemons different hostnames running machine in order create connections daemons one set mappings hostnames localhost link net utils get static resolution string used query actual hostname add static resolution string host string resolved name synchronized host to resolved retrieves resolved name passed host the resolved name must set earlier using link net utils add static resolution string string string get static resolution string host synchronized host to resolved this used get resolutions added using link net utils add static resolution string string the return value list element contains array string form string hostname string resolved hostname list string get all static resolutions synchronized host to resolved return returns inet socket address client use connect server server get listener address correct server binds this returns port get listener address returns port inet socket address get connect address server server inet socket address addr server get listener address addr get address get host address equals return addr same get input stream socket socket get so timeout br br from documentation link get input stream socket br returns input stream socket if socket associated socket channel returns link socket input stream given timeout if socket channel link socket get input stream returned in later case timeout argument ignored timeout set link socket set so timeout applies reads br br any socket created using socket factories returned link net utils must use
426	common\src\java\org\apache\hadoop\net\NetworkTopology.java	unrelated	package org apache hadoop net the represents cluster computer tree hierarchical network topology for example cluster may consists many data centers filled racks computers in network topology leaves represent data nodes computers inner nodes represent switches routers manage traffic data centers racks network topology string default rack default rack default host level log log log factory get log network topology inner node represent switch router data center rack different leave node non null children inner node extends node base array list node children new array list node num of leaves construct inner node path like inner node string path construct inner node name network location inner node string name string location construct inner node name network location parent level inner node string name string location inner node parent level get children collection node get children return children return number children node get num of children judge node represents rack return true child children inner nodes boolean rack judge node ancestor node n boolean ancestor node n judge node parent node n boolean parent node n return child name node ancestor node n string get next ancestor name node n add node n subtree node boolean add node n remove node n subtree node boolean remove node n end remove given node representation return reference node node get loc string loc get leaf index leaf subtree excluded node node get leaf leaf index node excluded node get num of leaves end inner node inner node cluster map new inner node inner node root root num of racks rack counter read write lock netlock network topology netlock new reentrant read write lock add leaf node update node counter rack counter necessary node added add node node node null return node instanceof inner node netlock write lock lock try finally remove node update node counter rack counter necessary node removed remove node node node null return node instanceof inner node log info removing node node base get path node netlock write lock lock try finally check tree contains node node node boolean contains node node node null return false netlock read lock lock try finally return false given representation node return reference path like representation node node get node string loc netlock read lock lock try finally return total number racks get num of racks netlock read lock lock try finally return total number nodes get num of leaves netlock read lock lock try finally return distance two nodes it assumed distance one node parent the distance two nodes calculated summing distances closest common ancestor node node belong cluster get distance node node node node node node node n node n node dis netlock read lock lock try finally n null n null return dis check two nodes rack node node belong cluster boolean on same rack node node node node node null node null netlock read lock lock try finally random r new random randomly choose one node scope scope starts choose one nodes except ones scope otherwise choose one scope node choose random string scope netlock read lock lock try finally node choose
427	common\src\java\org\apache\hadoop\net\Node.java	unrelated	package org apache hadoop net the defines node network topology a node may leave representing data node inner node representing datacenter rack each data name location network decided syntax similar file name for example data node name hostname port located rack orange datacenter dog representation network location dog orange node return representation node network location string get network location set node network location set network location string location return node name string get name return node parent node get parent set node parent set parent node parent return node level tree e g root tree returns children return get level set node level tree set level
428	common\src\java\org\apache\hadoop\net\NodeBase.java	unrelated	package org apache hadoop net a base implements node node base implements node char path separator string path separator str string root representation root protected string name host port protected string location representation node location protected level level tree node resides protected node parent parent default constructor node base construct node path concatenation node location path seperator name node base string path construct node name location node base string name string location construct node name location node base string name string location node parent level set node name location set string name string location return node name string get name return name return node network location string get network location return location set node network location set network location string location location location return node path string get path node node return node representation string string normalize path string normalize string path return node parent node get parent return parent set node parent set parent node parent return node level tree e g root tree returns children return get level return level set node level tree set level level
429	common\src\java\org\apache\hadoop\net\ScriptBasedMapping.java	unrelated	package org apache hadoop net this implements link dns to switch mapping using script configured via net topology script file name script based mapping extends cached dns to switch mapping implements configurable script based mapping script must accept least many args min allowable args default arg count string script filename key string script arg count key script based mapping configuration conf configuration get conf set conf configuration conf raw script based mapping implements dns to switch mapping string script name configuration conf max args max hostnames per call script log log set conf configuration conf configuration get conf raw script based mapping list string resolve list string names list string resolve valid hosts list string names string run resolve command list string args
430	common\src\java\org\apache\hadoop\net\SocketInputStream.java	unrelated	package org apache hadoop net this implements input stream timeout reading this sets non blocking flag socket channel so create object read link socket get input stream write link socket get output stream associated socket throw illegal blocking mode exception please use link socket output stream writing socket input stream extends input stream reader reader reader extends socket io with timeout readable byte channel channel reader readable byte channel channel timeout throws io exception perform io byte buffer buf throws io exception create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking channel reading also link selectable channel the channel configured non blocking socket input stream readable byte channel channel timeout socket io with timeout check channel validity channel reader new reader channel timeout same socket input stream socket get channel timeout br br create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket input stream socket socket timeout socket get channel timeout same socket input stream socket get channel socket get so timeout br br create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket input stream socket socket throws io exception socket get channel socket get so timeout read throws io exception allocation removed required probably need optimize encourage single byte read byte buf new byte ret read buf ret ret return ret read byte b len throws io exception return read byte buffer wrap b len synchronized close throws io exception close channel since socket get input stream close closes socket reader channel close reader close returns underlying channel used inputstream this useful certain cases like channel link file channel transfer from readable byte channel readable byte channel get channel return reader channel readable byte channel boolean open return reader open read byte buffer dst throws io exception return reader io dst selection key op read waits underlying channel ready reading the timeout specified stream applies wait select channel times i o error occurs wait for readable throws io exception reader wait for io selection key op read
431	common\src\java\org\apache\hadoop\net\SocketIOWithTimeout.java	pooling	package org apache hadoop net this supports input output streams socket channels these streams timeout socket io with timeout this intentionally package log log log factory get log socket io with timeout selectable channel channel timeout boolean closed false selector pool selector new selector pool a timeout value implies wait ever we value timeout implies zero wait e read write returns immediately this set channel non blocking socket io with timeout selectable channel channel timeout check channel validity channel channel channel timeout timeout set non blocking channel configure blocking false close closed true boolean open return closed channel open selectable channel get channel return channel utility function check channel ok mainly throw io exception instead runtime exception case mismatch this mismatch occur many runtime reasons check channel validity object channel throws io exception channel null channel instanceof selectable channel performs actual io operations this expected block channel drained completely we wait io required perform io byte buffer buf throws io exception performs one io returns number bytes read written it waits specified timeout if channel read timeout socket timeout exception thrown selection key op read reading selection key op write writing io byte buffer buf ops throws io exception for one thread allowed if user want read write multiple threads multiple streams could created in case multiple threads work well underlying channel supports buf remaining buf remaining return reach the contract similar link socket channel connect socket address timeout connect socket channel channel boolean blocking on channel blocking blocking on try catch io exception e finally this similar link io byte buffer except perform i o it waits channel ready i o specified ops select channel times i o error occurs wait for io ops throws io exception selector select channel ops timeout string timeout exception string selectable channel channel string waiting for switch ops case selection key op read case selection key op write case selection key op connect default return timeout millis timeout this maintains pool selectors these selectors closed idle unused seconds selector pool selector info provider info idle timeout seconds provider info provider list null waits channel given timeout using one cached selectors it also removes cached selectors idle seconds select selectable channel channel ops timeout takes one selector end lru list free selectors if selectors awailable creates new selector also invokes trim idle selectors synchronized selector info get selectable channel channel puts selector back end lru list free selectos also invokes trim idle selectors synchronized release selector info info closes selectors idle idle timeout sec it traverse whole list one crossed timeout trim idle selectors
432	common\src\java\org\apache\hadoop\net\SocketOutputStream.java	unrelated	package org apache hadoop net this implements output stream timeout writing this sets non blocking flag socket channel so creating object read link socket get input stream write link socket get output stream associated socket throw llegal blocking mode exception please use link socket input stream reading socket output stream extends output stream writer writer writer extends socket io with timeout writable byte channel channel writer writable byte channel channel timeout throws io exception perform io byte buffer buf throws io exception create new ouput stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking channel writing also link selectable channel the channel configured non blocking socket output stream writable byte channel channel timeout socket io with timeout check channel validity channel writer new writer channel timeout same socket output stream socket get channel timeout br br create new ouput stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket output stream socket socket timeout socket get channel timeout write b throws io exception if need optimize allocation probably need optimize encourage single byte writes byte buf new byte buf byte b write buf write byte b len throws io exception byte buffer buf byte buffer wrap b len buf remaining synchronized close throws io exception close channel since socket get ouput stream close closes socket writer channel close writer close returns underlying channel used stream this useful certain cases like channel link file channel transfer to writable byte channel writable byte channel get channel return writer channel writable byte channle boolean open return writer open write byte buffer src throws io exception return writer io src selection key op write waits underlying channel ready writing the timeout specified stream applies wait select channel times i o error occurs wait for writable throws io exception writer wait for io selection key op write transfers data file channel using link file channel transfer to writable byte channel similar read fully waits till requested amount data transfered if end input file reached requested number bytes transfered if channel blocks transfer longer timeout stream link file channel transfer to writable byte channel transfer to fully file channel file ch position count count
433	common\src\java\org\apache\hadoop\net\SocksSocketFactory.java	unrelated	package org apache hadoop net specialized socket factory create sockets socks proxy socks socket factory extends socket factory implements configuration conf proxy proxy default empty constructor use reflection api socks socket factory constructor supplied proxy socks socket factory proxy proxy socket create socket throws io exception socket create socket inet address addr port throws io exception socket create socket inet address addr port socket create socket string host port throws io exception socket create socket string host port hash code boolean equals object obj configuration get conf set conf configuration conf set proxy socket factory described parameter set proxy string proxy str
434	common\src\java\org\apache\hadoop\net\StandardSocketFactory.java	unrelated	package org apache hadoop net specialized socket factory create sockets socks proxy standard socket factory extends socket factory default empty constructor use reflection api standard socket factory socket create socket throws io exception socket create socket inet address addr port throws io exception socket create socket inet address addr port socket create socket string host port throws io exception socket create socket string host port boolean equals object obj hash code
435	common\src\java\org\apache\hadoop\record\BinaryRecordInput.java	unrelated	package org apache hadoop record binary record input implements record input data input binary index implements index nelems binary index nelems boolean done incr binary record input set data input data input inp inp thread local b in new thread local get thread local record input supplied data input binary record input get data input inp binary record input bin binary record input b in get bin set data input inp return bin creates new instance binary record input binary record input input stream strm new data input stream strm creates new instance binary record input binary record input data input din din byte read byte string tag throws io exception return read byte boolean read bool string tag throws io exception return read boolean read int string tag throws io exception return utils read v int read long string tag throws io exception return utils read v long read float string tag throws io exception return read float read double string tag throws io exception return read double string read string string tag throws io exception return utils binary string buffer read buffer string tag throws io exception len utils read v int byte barr new byte len read fully barr return new buffer barr start record string tag throws io exception op end record string tag throws io exception op index start vector string tag throws io exception return new binary index read int tag end vector string tag throws io exception op index start map string tag throws io exception return new binary index read int tag end map string tag throws io exception op
436	common\src\java\org\apache\hadoop\record\BinaryRecordOutput.java	unrelated	package org apache hadoop record binary record output implements record output data output binary record output set data output data output thread local b out new thread local get thread local record output supplied data output binary record output get data output creates new instance binary record output binary record output output stream creates new instance binary record output binary record output data output write byte byte b string tag throws io exception write bool boolean b string tag throws io exception write int string tag throws io exception write long string tag throws io exception write float f string tag throws io exception write double string tag throws io exception write string string string tag throws io exception write buffer buffer buf string tag start record record r string tag throws io exception end record record r string tag throws io exception start vector array list v string tag throws io exception end vector array list v string tag throws io exception start map tree map v string tag throws io exception end map tree map v string tag throws io exception
437	common\src\java\org\apache\hadoop\record\Buffer.java	unrelated	package org apache hadoop record a byte sequence used java native type buffer it resizable distinguishes count seqeunce current capacity buffer implements comparable cloneable number valid bytes bytes count backing store buffer byte bytes null create zero count sequence buffer create buffer using byte array initial value buffer byte bytes create buffer using byte range initial value buffer byte bytes offset length use specified bytes array underlying sequence set byte bytes copy specified byte array buffer replaces current buffer copy byte bytes offset length get data buffer byte get get current count buffer get count get capacity maximum count could handled without resizing backing storage get capacity change capacity backing storage the data preserved new capacity get count set capacity new capacity reset buffer size reset change capacity backing store current count buffer truncate append specified bytes buffer append byte bytes offset length append specified bytes buffer append byte bytes inherit javadoc hash code define sort order buffer negative smaller compare to object inherit javadoc boolean equals object inheric javadoc string string convert byte buffer specific character encoding string string string charset name inherit javadoc object clone throws clone not supported exception
438	common\src\java\org\apache\hadoop\record\CsvRecordInput.java	unrelated	package org apache hadoop record csv record input implements record input pushback reader stream csv index implements index boolean done incr throw exception on error string tag throws io exception throw new io exception error deserializing tag string read field string tag throws io exception try catch io exception ex creates new instance csv record input csv record input input stream try catch unsupported encoding exception ex byte read byte string tag throws io exception return byte read long tag boolean read bool string tag throws io exception string sval read field tag return t equals sval true false read int string tag throws io exception return read long tag read long string tag throws io exception string sval read field tag try catch number format exception ex read float string tag throws io exception return read double tag read double string tag throws io exception string sval read field tag try catch number format exception ex string read string string tag throws io exception string sval read field tag return utils csv string sval buffer read buffer string tag throws io exception string sval read field tag return utils csv buffer sval start record string tag throws io exception tag null equals tag end record string tag throws io exception char c char stream read tag null equals tag c c char stream read c return index start vector string tag throws io exception char c char stream read char c char stream read c v c return new csv index end vector string tag throws io exception char c char stream read c c char stream read c return index start map string tag throws io exception char c char stream read char c char stream read c c return new csv index end map string tag throws io exception char c char stream read c c char stream read c return
439	common\src\java\org\apache\hadoop\record\CsvRecordOutput.java	unrelated	package org apache hadoop record csv record output implements record output print stream stream boolean first true throw exception on error string tag throws io exception print comma unless first creates new instance csv record output csv record output output stream write byte byte b string tag throws io exception write bool boolean b string tag throws io exception write int string tag throws io exception write long string tag throws io exception write float f string tag throws io exception write double string tag throws io exception write string string string tag throws io exception write buffer buffer buf string tag start record record r string tag throws io exception end record record r string tag throws io exception start vector array list v string tag throws io exception end vector array list v string tag throws io exception start map tree map v string tag throws io exception end map tree map v string tag throws io exception
440	common\src\java\org\apache\hadoop\record\Index.java	unrelated	package org apache hadoop record interface acts iterator deserializing maps the deserializer returns instance record uses read vectors maps an example usage follows code index idx start vector idx done read element vector idx incr code index boolean done incr
441	common\src\java\org\apache\hadoop\record\Record.java	unrelated	package org apache hadoop record abstract extended generated record implements writable comparable cloneable serialize record output rout string tag deserialize record input rin string tag inheric javadoc compare to object peer throws class cast exception serialize record output rout throws io exception deserialize record input rin throws io exception inherit javadoc write data output throws java io io exception inherit javadoc read fields data input din throws java io io exception inherit javadoc string string
442	common\src\java\org\apache\hadoop\record\RecordComparator.java	unrelated	package org apache hadoop record a raw record comparator base record comparator extends writable comparator protected record comparator class extends writable comparable record class inheric java doc compare byte b byte b synchronized define class c record comparator comparator
443	common\src\java\org\apache\hadoop\record\RecordInput.java	unrelated	package org apache hadoop record interface deserializers implement record input read byte serialized record byte read byte string tag throws io exception read boolean serialized record boolean read bool string tag throws io exception read integer serialized record read int string tag throws io exception read integer serialized record read long string tag throws io exception read single precision serialized record read float string tag throws io exception read precision number serialized record read double string tag throws io exception read utf encoded serialized record string read string string tag throws io exception read byte array serialized record buffer read buffer string tag throws io exception check mark start serialized record start record string tag throws io exception check mark end serialized record end record string tag throws io exception check mark start serialized vector index start vector string tag throws io exception check mark end serialized vector end vector string tag throws io exception check mark start serialized map index start map string tag throws io exception check mark end serialized map end map string tag throws io exception
444	common\src\java\org\apache\hadoop\record\RecordOutput.java	unrelated	package org apache hadoop record interface alll serializers implement record output write byte serialized record write byte byte b string tag throws io exception write boolean serialized record write bool boolean b string tag throws io exception write integer serialized record write int string tag throws io exception write integer serialized record write long string tag throws io exception write single precision serialized record write float f string tag throws io exception write precision floating point number serialized record write double string tag throws io exception write unicode serialized record write string string string tag throws io exception write buffer serialized record write buffer buffer buf string tag mark start record serialized start record record r string tag throws io exception mark end serialized record end record record r string tag throws io exception mark start vector serialized start vector array list v string tag throws io exception mark end serialized vector end vector array list v string tag throws io exception mark start map serialized start map tree map string tag throws io exception mark end serialized map end map tree map string tag throws io exception
445	common\src\java\org\apache\hadoop\record\Utils.java	unrelated	package org apache hadoop record various utility functions hadooop record i o runtime utils cannot create new instance utils utils char hexchars string xml string string c char ch string xml string string string csv string string string csv string string throws io exception string xml buffer buffer buffer xml buffer string string csv buffer buffer buf converts csv serialized representation buffer new buffer buffer csv buffer string utf len for code point cpt throws io exception b integer parse int b integer parse int b integer parse int b integer parse int b integer parse int b integer parse int b integer parse int b integer parse int write utf cpt byte bytes offset binary string data output string str boolean valid code point cpt utf to code point b b b b utf to code point b b b utf to code point b b check b b throws io exception string binary string data input din throws io exception parse byte array read float byte bytes start parse byte array read double byte bytes start reads zero compressed encoded byte array returns read v long byte bytes start throws io exception reads zero compressed encoded integer byte array returns read v int byte bytes start throws io exception reads zero compressed encoded stream return read v long data input throws io exception reads zero compressed encoded integer stream returns read v int data input throws io exception get encoded length integer stored variable length format get v int size serializes binary stream zero compressed encoding for one byte used actual value for values first byte value indicates whether positive negative number bytes follow if first byte value v following positive number bytes follow v if first byte value v following negative number bytes follow v bytes stored high non zero byte first order write v long data output stream throws io exception serializes binary stream zero compressed encoding write v int data output stream throws io exception lexicographic order binary data compare bytes byte b
446	common\src\java\org\apache\hadoop\record\XmlRecordInput.java	unrelated	package org apache hadoop record xml deserializer xml record input implements record input value string type string buffer sb value string add chars char buf offset len string get value return sb string string get type return type xml parser extends default handler boolean chars valid false array list value val list xml parser array list value vlist start document throws sax exception end document throws sax exception start element string ns end element string ns characters char buf offset len xml index implements index boolean done incr array list value val list v len v idx value next throws io exception v idx v len else creates new instance xml record input xml record input input stream try catch exception ex byte read byte string tag throws io exception value v next ex equals v get type return byte parse byte v get value boolean read bool string tag throws io exception value v next boolean equals v get type return equals v get value read int string tag throws io exception value v next equals v get type return integer parse int v get value read long string tag throws io exception value v next ex equals v get type return long parse long v get value read float string tag throws io exception value v next ex equals v get type return float parse float v get value read double string tag throws io exception value v next equals v get type return double parse double v get value string read string string tag throws io exception value v next equals v get type return utils xml string v get value buffer read buffer string tag throws io exception value v next equals v get type return utils xml buffer v get value start record string tag throws io exception value v next equals v get type end record string tag throws io exception value v next equals v get type index start vector string tag throws io exception value v next array equals v get type return new xml index end vector string tag throws io exception index start map string tag throws io exception return start vector tag end map string tag throws io exception end vector tag
447	common\src\java\org\apache\hadoop\record\XmlRecordOutput.java	unrelated	package org apache hadoop record xml serializer xml record output implements record output print stream stream indent stack string compound stack put indent add indent close indent print begin envelope string tag print end envelope string tag inside vector string tag outside vector string tag throws io exception inside map string tag outside map string tag throws io exception inside record string tag outside record string tag throws io exception creates new instance xml record output xml record output output stream write byte byte b string tag throws io exception write bool boolean b string tag throws io exception write int string tag throws io exception write long string tag throws io exception write float f string tag throws io exception write double string tag throws io exception write string string string tag throws io exception write buffer buffer buf string tag start record record r string tag throws io exception end record record r string tag throws io exception start vector array list v string tag throws io exception end vector array list v string tag throws io exception start map tree map v string tag throws io exception end map tree map v string tag throws io exception
448	common\src\java\org\apache\hadoop\record\compiler\CGenerator.java	unrelated	package org apache hadoop record compiler c code generator front end hadoop record i o c generator extends code generator c generator generate c code this method creates requested file spits file level elements statements etc record level code generated j record gen code string name array list j file ilist
449	common\src\java\org\apache\hadoop\record\compiler\CodeBuffer.java	unrelated	package org apache hadoop record compiler a wrapper around string buffer automatically indentation code buffer array list character start markers new array list character array list character end markers new array list character add markers char ch char ch level num spaces boolean first char true string buffer sb creates new instance code buffer code buffer code buffer string code buffer num spaces string append string append char ch raw append char ch string string
450	common\src\java\org\apache\hadoop\record\compiler\CodeGenerator.java	unrelated	package org apache hadoop record compiler code generator factory base hadoop record i o translators different translators register creation methods factory code generator hash map string code generator generators register string lang code generator gen code generator get string lang gen code string file
451	common\src\java\org\apache\hadoop\record\compiler\Consts.java	unrelated	package org apache hadoop record compiler definitions record i o compiler consts cannot create new instance consts prefix use variables generated string rio prefix rio vars used generated string rti var rio prefix rec type info string rti filter rio prefix rti filter string rti filter fields rio prefix rti filter fields string record output rio prefix string record input rio prefix string tag rio prefix tag
452	common\src\java\org\apache\hadoop\record\compiler\CppGenerator.java	unrelated	package org apache hadoop record compiler c code generator front end hadoop record i o cpp generator extends code generator cpp generator generate c code this method creates requested file spits file level elements statements etc record level code generated j record gen code string name array list j file ilist
453	common\src\java\org\apache\hadoop\record\compiler\JavaGenerator.java	unrelated	package org apache hadoop record compiler java code generator front end hadoop record i o java generator extends code generator java generator generate java code records this method front end j record since one file generated record gen code string name array list j file ilist
454	common\src\java\org\apache\hadoop\record\compiler\JBoolean.java	unrelated	package org apache hadoop record compiler j boolean extends j type java boolean extends j type java type java boolean gen compare to code buffer cb string fname string string get type id object string gen hash code code buffer cb string fname in binary format boolean written byte true false gen slurp bytes code buffer cb string b string string in binary format boolean written byte true false gen compare bytes code buffer cb cpp boolean extends cpp type cpp boolean string get type id object string creates new instance j boolean j boolean set java type new java boolean set cpp type new cpp boolean set c type new c type string get signature return z
455	common\src\java\org\apache\hadoop\record\compiler\JBuffer.java	unrelated	package org apache hadoop record compiler code generator buffer type j buffer extends j comp type java buffer extends java comp type java buffer string get type id object string gen compare to code buffer cb string fname string gen equals code buffer cb string fname string peer gen hash code code buffer cb string fname gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp buffer extends cpp comp type cpp buffer gen get set code buffer cb string fname string get type id object string creates new instance j buffer j buffer set java type new java buffer set cpp type new cpp buffer set c type new c comp type string get signature return b
456	common\src\java\org\apache\hadoop\record\compiler\JByte.java	unrelated	package org apache hadoop record compiler code generator byte type j byte extends j type java byte extends java type java byte string get type id object string gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp byte extends cpp type cpp byte string get type id object string j byte set java type new java byte set cpp type new cpp byte set c type new c type string get signature return b
457	common\src\java\org\apache\hadoop\record\compiler\JCompType.java	unrelated	package org apache hadoop record compiler abstract base compound types ustring buffer vector map record j comp type extends j type java comp type extends java type cpp comp type extends cpp type c comp type extends c type
458	common\src\java\org\apache\hadoop\record\compiler\JDouble.java	unrelated	package org apache hadoop record compiler j double extends j type java double extends java type java double string get type id object string gen hash code code buffer cb string fname gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp double extends cpp type cpp double string get type id object string creates new instance j double j double set java type new java double set cpp type new cpp double set c type new c type string get signature return
459	common\src\java\org\apache\hadoop\record\compiler\JField.java	unrelated	package org apache hadoop record compiler a thin wrappper around record field j field t string name t type creates new instance j field j field string name t type string get name t get type
460	common\src\java\org\apache\hadoop\record\compiler\JFile.java	unrelated	package org apache hadoop record compiler container hadoop record ddl the main components file filename list included files records defined file j file possibly full name file string name ordered list included files array list j file incl files ordered list records declared file array list j record records creates new instance j file j file string name array list j file incl files strip pathname components return basename string get name generate record code given language language lowercase gen code string language string dest dir array list string options
461	common\src\java\org\apache\hadoop\record\compiler\JFloat.java	unrelated	package org apache hadoop record compiler j float extends j type java float extends java type java float string get type id object string gen hash code code buffer cb string fname gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp float extends cpp type cpp float string get type id object string creates new instance j float j float set java type new java float set cpp type new cpp float set c type new c type string get signature return f
462	common\src\java\org\apache\hadoop\record\compiler\JInt.java	unrelated	package org apache hadoop record compiler code generator type j int extends j type java int extends java type java int string get type id object string gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp int extends cpp type cpp int string get type id object string creates new instance j int j int set java type new java int set cpp type new cpp int set c type new c type string get signature return
463	common\src\java\org\apache\hadoop\record\compiler\JLong.java	unrelated	package org apache hadoop record compiler code generator type j long extends j type java long extends java type java long string get type id object string gen hash code code buffer cb string fname gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp long extends cpp type cpp long string get type id object string creates new instance j long j long set java type new java long set cpp type new cpp long set c type new c type string get signature return
464	common\src\java\org\apache\hadoop\record\compiler\JMap.java	unrelated	package org apache hadoop record compiler j map extends j comp type level string get level return integer string level incr level level decr level level string get id string id return id get level j type key type j type value type java map extends java comp type j type java type key j type java type value java map j type java type key j type java type value string get type id object string gen set rti filter code buffer cb map string integer nested struct map gen compare to code buffer cb string fname string gen read method code buffer cb string fname string tag boolean decl gen write method code buffer cb string fname string tag gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp map extends cpp comp type j type cpp type key j type cpp type value cpp map j type cpp type key j type cpp type value string get type id object string gen set rti filter code buffer cb creates new instance j map j map j type j type set java type new java map get java type get java type set cpp type new cpp map get cpp type get cpp type set c type new c type key type value type string get signature return key type get signature value type get signature
465	common\src\java\org\apache\hadoop\record\compiler\JRecord.java	unrelated	package org apache hadoop record compiler j record extends j comp type java record extends java comp type string full name string name string module array list j field java type fields java record string name array list j field j type flist string get type id object string gen set rti filter code buffer cb map string integer nested struct map type info filter see similar one record since store type infos array lists thsi search o n squared we faster also store map type info index since setup rti fields called deserializing sticking former code easier gen setup rti fields code buffer cb gen read method code buffer cb string fname string tag boolean decl gen write method code buffer cb string fname string tag gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb gen code string dest dir array list string options throws io exception cpp record extends cpp comp type string full name string name string module array list j field cpp type fields cpp record string name array list j field j type flist string get type id object string string gen decl string fname gen set rti filter code buffer cb gen setup rti fields code buffer cb gen code file writer hh file writer cc array list string options c record extends c comp type string signature creates new instance j record j record string name array list j field j type flist set java type new java record name flist set cpp type new cpp record name flist set c type new c record precompute signature idx name last index of string rec name name substring idx string builder sb new string builder sb append l append rec name append iterator j field j type flist iterator next sb append signature sb string string get signature return signature gen cpp code file writer hh file writer cc array list string options throws io exception cpp record get cpp type gen code hh cc options gen java code string dest dir array list string options throws io exception java record get java type gen code dest dir options
466	common\src\java\org\apache\hadoop\record\compiler\JString.java	unrelated	package org apache hadoop record compiler j string extends j comp type java string extends java comp type java string string get type id object string gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb gen clone code buffer cb string fname cpp string extends cpp comp type cpp string string get type id object string creates new instance j string j string set java type new java string set cpp type new cpp string set c type new c comp type string get signature return
467	common\src\java\org\apache\hadoop\record\compiler\JType.java	unrelated	package org apache hadoop record compiler abstract base types supported hadoop record i o j type string camel case string name java type java type cpp type cpp type c type c type java type cpp type c type string get signature set java type java type j type java type get java type set cpp type cpp type cpp type cpp type get cpp type set c type c type c type c type get c type
468	common\src\java\org\apache\hadoop\record\compiler\JVector.java	unrelated	package org apache hadoop record compiler j vector extends j comp type level string get id string id return id get level string get level return integer string level incr level level decr level level j type type java vector extends java comp type j type java type element java vector j type java type string get type id object string gen set rti filter code buffer cb map string integer nested struct map gen compare to code buffer cb string fname string gen read method code buffer cb string fname string tag boolean decl gen write method code buffer cb string fname string tag gen slurp bytes code buffer cb string b string string gen compare bytes code buffer cb cpp vector extends cpp comp type j type cpp type element cpp vector j type cpp type string get type id object string gen set rti filter code buffer cb creates new instance j vector j vector j type type set java type new java vector get java type set cpp type new cpp vector get cpp type set c type new c comp type string get signature return type get signature
469	common\src\java\org\apache\hadoop\record\compiler\ant\RccTask.java	unrelated	package org apache hadoop record compiler ant hadoop record compiler ant task p this task takes given record definition files compiles java c files it user compile generated files p the task requires code file code nested fileset element specified optional attributes code language code set output language default java code destdir code name destination directory generated java c code default code failonerror code specifies error handling behavior default true p usage pre lt recordcc destdir basedir gensrc language java gt lt fileset jr gt lt recordcc gt pre rcc task extends task string language java file src file dest new file array list file set filesets new array list file set boolean fail on error true creates new instance rcc task rcc task sets output language option set language string language sets record definition file attribute set file file file given multiple files via fileset set error handling behavior set failonerror boolean flag sets directory output files generated set destdir file dir adds fileset consist one files add fileset file set set invoke hadoop record compiler record definition file execute throws build exception compile file file throws build exception
470	common\src\java\org\apache\hadoop\record\compiler\generated\ParseException.java	unrelated	package org apache hadoop record compiler generated this exception thrown parse errors encountered you explicitly create objects exception type calling method generate parse exception generated parser you modify customize error reporting mechanisms retain fields parse exception extends exception parse exception token current token val parse exception parse exception string message protected boolean special constructor token current token expected token sequences string token image string get message protected string eol system get property line separator n protected string add escapes string str
471	common\src\java\org\apache\hadoop\record\compiler\generated\Rcc.java	unrelated	package org apache hadoop record compiler generated rcc implements rcc constants string language java string dest dir array list string rec files new array list string array list string cmdargs new array list string j file cur file hashtable string j record rec tab string cur dir string cur file name string cur module name main string args usage driver string args j file input throws parse exception j file include throws parse exception array list j record module throws parse exception string module name throws parse exception array list j record record list throws parse exception j record record throws parse exception j field j type field throws parse exception j type type throws parse exception j map map throws parse exception j vector vector throws parse exception rcc token manager token source simple char stream jj input stream token token jj nt jj ntk jj gen jj la new jj la jj la jj la jj la rcc java io input stream stream rcc java io input stream stream string encoding re init java io input stream stream re init java io input stream stream string encoding rcc java io reader stream re init java io reader stream rcc rcc token manager tm re init rcc token manager tm token jj consume token kind throws parse exception token get next token token get token index jj ntk java util vector jj expentries new java util vector jj expentry jj kind parse exception generate parse exception enable tracing disable tracing
472	common\src\java\org\apache\hadoop\record\compiler\generated\RccConstants.java	unrelated	package org apache hadoop record compiler generated rcc constants eof module tkn record tkn include tkn byte tkn boolean tkn int tkn long tkn float tkn double tkn ustring tkn buffer tkn vector tkn map tkn lbrace tkn rbrace tkn lt tkn gt tkn semicolon tkn comma tkn dot tkn cstring tkn ident tkn default within one line comment within multi line comment string token image
473	common\src\java\org\apache\hadoop\record\compiler\generated\RccTokenManager.java	unrelated	package org apache hadoop record compiler generated rcc token manager implements rcc constants java io print stream debug stream system set debug stream java io print stream ds debug stream ds jj move string literal dfa return jj move nfa jj check n add state jjrounds state jjround jjstate set jjnew state cnt state jjrounds state jjround jj add states start end jjstate set jjnew state cnt jjnext states start start end jj check n add two states state state jj check n add state jj check n add state jj check n add states start end jj check n add jjnext states start start end jj check n add states start jj check n add jjnext states start jj check n add jjnext states start jj move nfa start state cur pos next states starts at jjnew state cnt jjstate set start state j kind x fffffff jjround x fffffff cur char else cur char else kind x fffffff cur pos jjnew state cnt starts at jjnew state cnt starts at try cur char input stream read char catch java io io exception e return cur pos jj stop string literal dfa pos active switch pos case active xfff l l return case active xfff l l return case active x ef l l active x l l return case active x l l active x cb l l return case active x l l active x l l return case active x l l active x l l return default return jj start nfa pos active return jj move nfa jj stop string literal dfa pos active pos jj stop at pos pos kind jjmatched kind kind jjmatched pos pos return pos jj start nfa with states pos kind state jjmatched kind kind jjmatched pos pos try cur char input stream read char catch java io io exception e return pos return jj move nfa state pos jj move string literal dfa switch cur char case return jj stop at pos case return jj stop at pos case return jj move string literal dfa x l case return jj stop at pos case return jj stop at pos case return jj stop at pos case return jj move string literal dfa x c l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj move string literal dfa x l case return jj stop at pos case return jj stop at pos default return jj move nfa jj move string literal dfa active try cur char input stream read char catch java io io exception e jj stop string literal dfa active return switch cur char case active x l l break case active x l l break case return jj move string literal
474	common\src\java\org\apache\hadoop\record\compiler\generated\SimpleCharStream.java	unrelated	package org apache hadoop record compiler generated an implementation char stream stream assumed contain ascii characters without unicode processing simple char stream boolean flag false bufsize available token begin bufpos protected bufline protected bufcolumn protected column protected line protected boolean prev char is cr false protected boolean prev char is lf false protected java io reader input stream protected char buffer protected max next char ind protected buf protected tab size protected set tab size tab size protected get tab size return tab size protected expand buff boolean wrap around protected fill buff throws java io io exception char begin token throws java io io exception protected update line column char c char read char throws java io io exception get end column get end line get begin column get begin line backup amount simple char stream java io reader dstream startline simple char stream java io reader dstream startline simple char stream java io reader dstream re init java io reader dstream startline re init java io reader dstream startline re init java io reader dstream simple char stream java io input stream dstream string encoding startline simple char stream java io input stream dstream startline simple char stream java io input stream dstream string encoding startline simple char stream java io input stream dstream startline simple char stream java io input stream dstream string encoding throws java io unsupported encoding exception simple char stream java io input stream dstream re init java io input stream dstream string encoding startline re init java io input stream dstream startline re init java io input stream dstream string encoding throws java io unsupported encoding exception re init java io input stream dstream re init java io input stream dstream string encoding startline re init java io input stream dstream startline string get image char get suffix len done method adjust line column numbers start token adjust begin line column new line new col
475	common\src\java\org\apache\hadoop\record\compiler\generated\Token.java	unrelated	package org apache hadoop record compiler generated describes input token stream token an integer describes kind token this numbering system determined java cc parser table numbers stored file constants java kind begin line begin column describe position first character token end line end column describe position last character token begin line begin column end line end column the image token string image a reference next regular non special token input stream if last token input stream token manager read tokens beyond one field set null this true token also regular token otherwise see description contents field token next this field used access special tokens occur prior token immediately preceding regular non special token if special tokens field set null when one special token field refers last special tokens turn refers next previous special token special token field first special token whose special token field null the next fields special tokens refer special tokens immediately follow without intervening regular token if token field null token special token returns image string string return image returns new token object default however want create return subclass objects based value kind simply add cases switch special cases for example subclass token called id token want create kind id simlpy add something like case my parser constants id return new id token following switch statement then cast matched token variable appropriate type use lexical actions token new token kind switch kind
476	common\src\java\org\apache\hadoop\record\compiler\generated\TokenMgrError.java	unrelated	package org apache hadoop record compiler generated token mgr error extends error ordinals various reasons error type thrown lexical error occured lexical error an attempt wass made create second instance token manager static lexer error tried change invalid lexical state invalid lexical state detected bailed infinite loop token manager loop detected indicates reason exception thrown it one values error code replaces unprintable characters espaced unicode escaped equivalents given protected string add escapes string str returns detailed message error thrown token manager indicate lexical error parameters eof seen indicates eof caused lexicl error cur lex state lexical state error occured error line line number error occured error column column number error occured error after prefix seen error occured curchar offending character note you customize lexical error message modifying method protected string lexical error boolean eof seen lex state error line error column string error after char cur char you also modify body method customize error messages for example cases like loop detected invalid lexical state end users concern return something like internal error please file bug report method cases release version parser string get message constructors various flavors follow token mgr error token mgr error string message reason token mgr error boolean eof seen lex state error line error column string error after char cur char reason
477	common\src\java\org\apache\hadoop\record\meta\FieldTypeInfo.java	unrelated	package org apache hadoop record meta represents type information field made id name type type id object field type info string field id type id type id construct filed type info given field name type field type info string field id type id type id field id field id type id type id get field type id object type id get type id return type id get field id name string get field id return field id write record output rout string tag throws io exception rout write string field id tag type id write rout tag two field type infos equal ach fields matches boolean equals object instanceof field type info field type info fti field type info first check field id matches field id equals fti field id see type id matches return type id equals fti type id we use basic hashcode implementation since likely used hashmap key hash code return type id hash code field id hash code boolean equals field type info ti first check field id matches field id equals ti field id see type id matches return type id equals ti type id
478	common\src\java\org\apache\hadoop\record\meta\MapTypeID.java	unrelated	package org apache hadoop record meta represents type id map map type id extends type id type id type id key type id type id value map type id type id type id key type id type id value super rio type map type id key type id key type id value type id value get type id map key element type id get key type id return type id key get type id map value element type id get value type id return type id value write record output rout string tag throws io exception rout write byte type val tag type id key write rout tag type id value write rout tag two map type i ds equal constituent elements type boolean equals object super equals map type id mti map type id return type id key equals mti type id key we use basic hashcode implementation since likely used hashmap key hash code return type id key hash code type id value hash code
479	common\src\java\org\apache\hadoop\record\meta\RecordTypeInfo.java	unrelated	package org apache hadoop record meta a record type information object read write type information record comprises metadata record well collection type information field record record type info extends org apache hadoop record record string name a record type info really wrapper around struct type id struct type id tid a record type info object collection type info objects fields array list field type info type infos new array list field type info keep hashmap record names type information need set filters reading nested structs this map used deserialization map string record type info rt is new hash map string record type info create empty record type info object record type info tid new struct type id create record type info object representing record given name record type info string name name name tid new struct type id constructor record type info string name struct type id stid tid stid name name return name record string get name return name set name record set name string name name name add field add field string field name type id tid tid get field type infos add new field type info field name tid add all collection field type info tis tid get field type infos add all tis return collection field type infos collection field type info get field type infos return tid get field type infos return type info nested record we consider nesting one level record type info get nested struct type info string name struct type id stid tid find struct name null stid return null return new record type info name stid serialize type information record serialize record output rout string tag throws io exception write header version info rout start record tag rout write string name tag tid write rest rout tag rout end record tag deserialize type information record deserialize record input rin string tag throws io exception read header version info rin start record tag name name rin read string tag tid read rin tag rin end record tag this implement comparable meant used anything besides de serializing so always throw exception not implemented always returns another record type info passed compare to object peer throws class cast exception peer instanceof record type info throw new unsupported operation exception compare to supported
480	common\src\java\org\apache\hadoop\record\meta\StructTypeID.java	unrelated	package org apache hadoop record meta represents type id struct type id extends type id array list field type info type infos new array list field type info struct type id create struct type id based record type info record struct type id record type info rti add field type info ti collection field type info get field type infos return struct typei d given field struct type id find struct string name write record output rout string tag throws io exception writes rest excluding type value as optimization method directly called rti top level record write byte indicating since top level records always structs write rest record output rout string tag throws io exception deserialize called rti read record input rin string tag throws io exception generic reader reads next type info object stream returns field type info generic read type info record input rin string tag throws io exception generic reader reads next type id object stream returns type id generic read type id record input rin string tag throws io exception boolean equals object hash code return super hash code
481	common\src\java\org\apache\hadoop\record\meta\TypeID.java	unrelated	package org apache hadoop record meta represents type id basic types type id constants representing idl types support rio type byte bool byte buffer byte byte byte double byte float byte int byte long byte map byte string byte struct byte vector constant basic types share type id bool type id new type id rio type bool type id buffer type id new type id rio type buffer type id byte type id new type id rio type byte type id double type id new type id rio type double type id float type id new type id rio type float type id int type id new type id rio type int type id long type id new type id rio type long type id string type id new type id rio type string protected byte type val create type id object type id byte type val type val type val get type value one constants rio type byte get type val return type val serialize type id object write record output rout string tag throws io exception rout write byte type val tag two base type i ds equal refer type boolean equals object null get class get class type id type id type id return type val type id type val we use basic hashcode implementation since likely used hashmap key hash code see effectve java joshua bloch return type val
482	common\src\java\org\apache\hadoop\record\meta\Utils.java	unrelated	package org apache hadoop record meta various utility functions hadooop record i o platform utils cannot create new instance utils utils read skip bytes stream based type skip record input rin string tag type id type id throws io exception
483	common\src\java\org\apache\hadoop\record\meta\VectorTypeID.java	unrelated	package org apache hadoop record meta represents type id vector vector type id extends type id type id type id element vector type id type id type id element super rio type vector type id element type id element type id get element type id return type id element write record output rout string tag throws io exception rout write byte type val tag type id element write rout tag two vector type i ds equal constituent elements type boolean equals object super equals vector type id vti vector type id return type id element equals vti type id element we use basic hashcode implementation since likely used hashmap key hash code return type id element hash code
484	common\src\java\org\apache\hadoop\security\AccessControlException.java	unrelated	package org apache hadoop security an exception access control related issues access control exception required link java io serializable serial version uid l access control exception access control exception string super access control exception throwable cause
485	common\src\java\org\apache\hadoop\security\AnnotatedSecurityInfo.java	unrelated	package org apache hadoop security constructs security info annotations provided protocol annotated security info extends security info kerberos info get kerberos info class protocol configuration conf token info get token info class protocol configuration conf
486	common\src\java\org\apache\hadoop\security\Credentials.java	unrelated	package org apache hadoop security a provides facilities reading writing secret keys tokens credentials implements writable log log log factory get log credentials map text byte secret keys map new hash map text byte map text token extends token identifier token map byte get secret key text alias token extends token identifier get token text alias add token text alias token extends token identifier collection token extends token identifier get all tokens number of tokens number of secret keys add secret key text alias byte key credentials read token storage file path filename configuration conf throws io exception read token storage stream data input stream throws io exception byte token storage magic hdts get bytes byte token storage version write token storage to stream data output stream os write token storage file path filename write data output throws io exception read fields data input throws io exception add all credentials
487	common\src\java\org\apache\hadoop\security\GroupMappingServiceProvider.java	unrelated	package org apache hadoop security an implementation user groups mapping service used link groups group mapping service provider get various group memberships given user returns empty list case non existing user list string get groups string user throws io exception refresh cache groups user mapping cache groups refresh throws io exception caches group user information cache groups add list string groups throws io exception
488	common\src\java\org\apache\hadoop\security\Groups.java	unrelated	package org apache hadoop security a user groups mapping service link groups allows server get various group memberships given user via link get groups string call thus ensuring consistent user groups mapping protects vagaries different mappings servers clients hadoop cluster groups log log log factory get log groups group mapping service provider impl map string cached groups user to groups map new concurrent hash map string cached groups cache timeout groups configuration conf impl cache timeout log debug enabled get group memberships given user list string get groups string user throws io exception return cached value available cached groups groups user to groups map get user system current time millis cache value expired groups null groups get timestamp cache timeout create cache user groups groups new cached groups impl get groups user user to groups map put user groups log debug enabled return groups get groups refresh user groups mappings refresh log info clearing user to groups map cache try catch io exception e user to groups map clear add groups cache cache groups add list string groups try catch io exception e class hold cached groups cached groups timestamp list string groups create initialize group cache cached groups list string groups returns time last cache update get timestamp get list cached groups list string get groups groups groups null get groups used map user groups groups get user to groups mapping service return get user to groups mapping service new configuration get groups used map user groups synchronized groups get user to groups mapping service configuration conf groups null return groups
489	common\src\java\org\apache\hadoop\security\JniBasedUnixGroupsMapping.java	unrelated	package org apache hadoop security a jni based implementation link group mapping service provider invokes lib c calls get group memberships given user jni based unix groups mapping implements group mapping service provider log log native string get group for user string user list string get groups string user throws io exception cache groups refresh throws io exception cache groups add list string groups throws io exception
490	common\src\java\org\apache\hadoop\security\JniBasedUnixGroupsNetgroupMapping.java	unrelated	package org apache hadoop security a jni based implementation link group mapping service provider invokes lib c calls get group memberships given user jni based unix groups netgroup mapping extends jni based unix groups mapping log log log factory get log native string get users for netgroup jni string group gets unix groups netgroups user it gets unix groups returned id gn returns netgroups used ac ls way get netgroups given user see documentation getent netgroup list string get groups string user throws io exception refresh netgroup cache cache groups refresh throws io exception add group cache netgroups cached cache groups add list string groups throws io exception calls jni function get users netgroup since c functions reentrant need make synchronized see documentation setnetgrent getnetgrent endnetgrent protected synchronized list string get users for netgroup string netgroup
491	common\src\java\org\apache\hadoop\security\KerberosInfo.java	authenticate	package org apache hadoop security indicates kerberos related information used kerberos info key getting server kerberos principal name configuration string server principal string client principal default
492	common\src\java\org\apache\hadoop\security\KerberosName.java	authenticate	package org apache hadoop security this implements parsing handling kerberos principal names in particular splits apart translates local operating system names kerberos name the first component name string service name the second component name it may null string host name the realm name string realm pattern name parser pattern parameter pattern pattern rule parser pattern non simple pattern pattern compile list rule rules string default realm config kerb conf kerberos name string name string get default realm string string string get service name string get host name string get realm rule list rule parse rules string rules set configuration configuration conf throws io exception bad format string extends io exception no matching rule extends io exception string get short name throws io exception print rules throws io exception main string args throws exception
493	common\src\java\org\apache\hadoop\security\Krb5AndCertsSslSocketConnector.java	authenticate	package org apache hadoop security extend jetty link ssl socket connector optionally also provide kerberos ized ssl sockets the change behavior superclass longer honor requests turn need authentication running kerberos support krb and certs ssl socket connector extends ssl socket connector list string krb cipher suites log log log factory string remote principal remote principal enum mode krb certs both support kerberos certificates boolean use krb boolean use certs krb and certs ssl socket connector krb and certs ssl socket connector mode mode if using certs set passwords random gibberish else jetty actually prompt user set passwords protected ssl server socket factory create factory throws exception non javadoc protected server socket new server socket string host port backlog customize end point endpoint request request throws io exception log if debug string krb ssl filter implements filter
494	common\src\java\org\apache\hadoop\security\NetgroupCache.java	unrelated	package org apache hadoop security class caches netgroups inverts group user map user group map primarily intented use netgroups returned getent netgrgoup returns group user mapping netgroup cache log log log factory get log netgroup cache boolean netgroup to users map updated true map string set string netgroup to users map map string set string user to netgroups map get netgroups given user get netgroups string user get list cached netgroups list string get netgroup names returns true given netgroup cached boolean cached string group clear cache clear add group cache add string group list string users
495	common\src\java\org\apache\hadoop\security\RefreshUserMappingsProtocol.java	unrelated	package org apache hadoop security protocol use refresh user mappings protocol extends versioned protocol version initial version version id l refresh user group mappings refresh user to groups mappings throws io exception refresh superuser proxy group list refresh super user groups configuration throws io exception
496	common\src\java\org\apache\hadoop\security\SaslInputStream.java	unrelated	package org apache hadoop security a sasl input stream composed input stream sasl server sasl client read methods return data read underlying input stream additionally processed sasl server sasl client object the sasl server sasl client object must fully initialized used sasl input stream sasl input stream extends input stream log log log factory get log sasl input stream data input stream stream should wrap communication channel boolean use wrap data read underlying input stream processed sasl byte sasl token sasl client sasl client sasl server sasl server byte length buf new byte buffer holding data processed sasl read byte obuffer position next new byte ostart position last new byte ofinish unsigned bytes to int byte buf buf length result return result read data get processed br entry condition ostart ofinish br exit condition ostart ofinish br return ofinish ostart many bytes data could later absolutely data read more data throws io exception try catch eof exception e try catch sasl exception se ostart obuffer null else return ofinish disposes system resources security sensitive information sasl might using sasl error occurs dispose sasl throws sasl exception sasl client null sasl server null constructs sasl input stream input stream sasl server br note specified input stream sasl server null null pointer exception may thrown later used input stream processed initialized sasl server object sasl input stream input stream stream sasl server sasl server stream new data input stream stream sasl server sasl server sasl client null string qop string sasl server get negotiated property sasl qop use wrap qop null auth equals ignore case qop constructs sasl input stream input stream sasl client br note specified input stream sasl client null null pointer exception may thrown later used input stream processed initialized sasl client object sasl input stream input stream stream sasl client sasl client stream new data input stream stream sasl server null sasl client sasl client string qop string sasl client get negotiated property sasl qop use wrap qop null auth equals ignore case qop reads next byte data input stream the value byte returned code code range code code code code if byte available end stream reached value code code returned this method blocks input data available end stream detected exception thrown p reached i o error occurs read throws io exception use wrap ostart ofinish return obuffer ostart xff reads code b length code bytes data input stream array bytes p the code read code method code input stream code calls code read code method three arguments arguments code b code code code code b length code buffer data read data end stream reached i o error occurs read byte b throws io exception return read b b length reads code len code bytes data input stream array bytes this method blocks input available if first argument code null code code len code bytes read discarded buffer data read start offset data maximum number bytes read data end stream reached i o error occurs read byte b len throws io exception use wrap ostart ofinish len available ofinish ostart
497	common\src\java\org\apache\hadoop\security\SaslOutputStream.java	unrelated	package org apache hadoop security a sasl output stream composed output stream sasl server sasl client write methods first process data writing underlying output stream the sasl server sasl client object must fully initialized used sasl output stream sasl output stream extends output stream output stream stream processed data ready written byte sasl token sasl client sasl client sasl server sasl server buffer holding one byte incoming data byte ibuffer new byte boolean use wrap constructs sasl output stream output stream sasl server br note specified output stream sasl server null null pointer exception may thrown later used output stream processed initialized sasl server object sasl output stream output stream stream sasl server sasl server constructs sasl output stream output stream sasl client br note specified output stream sasl client null null pointer exception may thrown later used output stream processed initialized sasl client object sasl output stream output stream stream sasl client sasl client disposes system resources security sensitive information sasl might using sasl error occurs dispose sasl throws sasl exception writes specified byte output stream code byte code i o error occurs write b throws io exception writes code b length code bytes specified byte array output stream p the code write code method code sasl output stream code calls code write code method three arguments three arguments code b code code code code b length code data code b code null i o error occurs write byte b throws io exception writes code len code bytes specified byte array starting offset code code output stream data start offset data number bytes write i o error occurs write byte buf len throws io exception flushes output stream i o error occurs flush throws io exception closes output stream releases system resources associated stream i o error occurs close throws io exception
498	common\src\java\org\apache\hadoop\security\SaslRpcClient.java	authenticate	package org apache hadoop security a utility encapsulates sasl logic rpc client sasl rpc client log log log factory get log sasl rpc client sasl client sasl client sasl rpc client auth method method read status data input stream stream throws io exception boolean sasl connect input stream s output stream s input stream get input stream input stream throws io exception output stream get output stream output stream throws io exception release resources used wrapped sasl client dispose throws sasl exception sasl client callback handler implements callback handler
499	common\src\java\org\apache\hadoop\security\SaslRpcServer.java	authenticate	package org apache hadoop security a utility dealing sasl rpc server sasl rpc server log log log factory get log sasl rpc server string sasl default realm default map string string sasl props switch to simple auth enum quality of protection init configuration conf string encode identifier byte identifier byte decode identifier string identifier t extends token identifier t get identifier string id char encode password byte password splitting fully qualified kerberos name parts string split kerberos name string full name enum sasl status authentication method enum auth method callback handler sasl digest md mechanism sasl digest callback handler implements callback handler callback handler sasl gssapi kerberos mechanism sasl gss callback handler implements callback handler
500	common\src\java\org\apache\hadoop\security\SecurityInfo.java	unrelated	package org apache hadoop security interface used rpc get security information given protocol security info get kerberos info given protocol kerberos info get kerberos info class protocol configuration conf get token info given protocol token info get token info class protocol configuration conf
501	common\src\java\org\apache\hadoop\security\SecurityUtil.java	authenticate	package org apache hadoop security security util log log log factory get log security util string hostname pattern host find original tgt within current subject credentials cross realm tgt form krbtgt two com one com may present tgt found kerberos ticket get tgt from subject throws io exception subject current subject get subject access controller get context current null set kerberos ticket tickets current kerberos ticket tickets throw new io exception failed find tgt current subject tgs must server principal form krbtgt foo foo boolean tgs principal kerberos principal principal principal null principal get name equals krbtgt principal get realm return false check whether server principal tgs principal kinit done protected boolean original tgt kerberos ticket ticket return tgs principal ticket get server explicitly pull service ticket specified host this solves problem java kerberos ssl problem client cannot authenticate cross realm service it necessary clients making kerberized https requests call method target url ensure cross realm environment remote host successfully authenticated this method internal hadoop used applications this method considered stable open removed java behavior changed fetch service ticket url remote host throws io exception user group information security enabled string service name host remote host get host log debug enabled credentials service cred null try catch exception e service cred null subject get subject access controller get context get private credentials convert kerberos principal name pattern valid kerberos principal names it replaces hostname pattern hostname fully qualified domain name if hostname null uses dynamically looked fqdn current host instead kerberos principal name conf value convert fully qualified domain name used substitution string get server principal string principal config string components get components principal config components null components length else convert kerberos principal name pattern valid kerberos principal names this method similar link get server principal string string except reverse dns lookup addr hostname done necessary param addr null default behavior using local hostname addr null kerberos principal name pattern convert inet address host used substitution string get server principal string principal config string components get components principal config components null components length else string get components string principal config principal config null return principal config split string replace pattern string components string hostname string fqdn hostname fqdn null fqdn equals fqdn equals return components fqdn components string get local host name throws unknown host exception return inet address get local host get canonical host name login principal specified config substitute host user kerberos principal name dynamically looked fully qualified domain name current host conf use key look keytab file conf key look user kerberos principal name conf login configuration conf login conf keytab file key user name key get local host name login principal specified config substitute host user kerberos principal name hostname if non secure mode return if keytab available bail exception conf use key look keytab file conf key look user kerberos principal name conf hostname use substitution login configuration conf user group information security enabled string keytab filename conf get keytab file key keytab filename null keytab filename length string principal config conf get user name key system
502	common\src\java\org\apache\hadoop\security\ShellBasedUnixGroupsMapping.java	unrelated	package org apache hadoop security a simple shell based implementation link group mapping service provider exec code groups code shell command fetch group memberships given user shell based unix groups mapping implements group mapping service provider log log returns list groups user list string get groups string user throws io exception caches groups need provider cache groups refresh throws io exception adds groups cache need provider cache groups add list string groups throws io exception get current user group list unix running command groups note for non existing user return empty list list string get unix groups string user throws io exception
503	common\src\java\org\apache\hadoop\security\ShellBasedUnixGroupsNetgroupMapping.java	unrelated	package org apache hadoop security a simple shell based implementation link group mapping service provider exec code groups code shell command fetch group memberships given user shell based unix groups netgroup mapping extends shell based unix groups mapping log log get unix groups parent netgroups given user list string get groups string user throws io exception refresh netgroup cache cache groups refresh throws io exception add group cache netgroups cached cache groups add list string groups throws io exception gets users netgroup protected list string get users for netgroup string netgroup calls shell get users netgroup calling getent netgroup low level function returns protected string exec shell get user for netgroup string netgroup
504	common\src\java\org\apache\hadoop\security\User.java	authenticate	package org apache hadoop security save full short name user principal this allows us single type always look picking user names user implements principal string full name string short name volatile authentication method auth method null volatile login context login null volatile last login user string name user string name authentication method auth method login context login get full name user string get name get user name first string get short name boolean equals object hash code string string set authentication method authentication method auth method authentication method get authentication method returns login object login context get login set login object set login login context login set last login time set last login time get time last login get last login
505	common\src\java\org\apache\hadoop\security\UserGroupInformation.java	authenticate	package org apache hadoop security user group information hadoop this wraps around jaas subject provides methods determine user username groups it supports windows unix kerberos login modules user group information log log log factory get log user group information ticket renew window f ugi metrics hadoop login module implements login module metrics track ugi activity ugi metrics metrics ugi metrics create are variables depend configuration initialized boolean initialized false should use kerberos configuration boolean use kerberos server side groups fetching service groups groups the configuration use configuration conf leave minutes relogin attempts min time before relogin l environment variable pointing token cache file string hadoop token file location synchronized ensure initialized synchronized initialize configuration conf synchronized init ugi configuration conf set configuration configuration conf boolean security enabled user group information login user null string keytab principal null string keytab file null subject subject all non fields must read caches come subject user user boolean keytab boolean krb tkt string os login module name class extends principal os principal class boolean windows real user implements principal hadoop configuration login context get login set login login context login user group information subject subject boolean kerberos credentials user group information get current user throws io exception synchronized user group information get login user throws io exception boolean from keytab synchronized kerberos ticket get tgt get refresh time kerberos ticket tgt spawn thread periodic renewals kerberos credentials spawn auto renewal thread for user creds synchronized login user from keytab string user synchronized relogin from keytab throws io exception synchronized relogin from ticket cache throws io exception synchronized user group information login user from keytab and return ugi string user boolean sufficient time elapsed synchronized boolean login keytab based throws io exception user group information create remote user string user enum authentication method user group information create proxy user string user user group information get real user testing groups extends groups user group information create user for testing string user user group information create proxy user for testing string user string get short user name string get user name synchronized boolean add token identifier token identifier token id synchronized set token identifier get token identifiers synchronized boolean add token token extends token identifier token synchronized collection token extends token identifier get tokens synchronized string get group names string string synchronized set authentication method authentication method auth method synchronized authentication method get authentication method authentication method get real authentication method boolean equals object hash code protected subject get subject t t as privileged action t action t t as privileged exception action t action print throws io exception main string args throws exception system println getting ugi current user
506	common\src\java\org\apache\hadoop\security\authorize\AccessControlList.java	unrelated	package org apache hadoop security authorize class representing configured access control list access control list implements writable register ctor indicates acl represents access users string wildcard acl value initial capacity set users granted access set string users set groups granted access set string groups whether users granted access boolean allowed groups groups mapping groups get user to groups mapping service new configuration this constructor exists primarily access control list writable access control list construct new acl string representation the string comma separated list users groups the user list comes first separated space followed group list for e g user user group group access control list string acl string build acl given format user user n group group n build acl string acl string checks whether acl contains wildcard boolean wild card acl value string acl string boolean all allowed add user names users allowed service the user name add user string user add group names groups allowed service the group name add group string group remove user names users allowed service the user name remove user string user remove group names groups allowed service the group name remove group string group get names users allowed service set string get users get names user groups allowed service set string get groups boolean user allowed user group information ugi cleanup list remove empty strings trim leading trailing spaces cleanup list list string list add list set add to set set string set list string list returns descriptive way users groups part acl use link get acl string get exact string given constructor access control list create new instance string string returns access control list string used building new instance sending constructor link access control list string get acl string serializes access control list object write data output throws io exception deserializes access control list object read fields data input throws io exception returns comma separated concatenated single string set users string get users string returns comma separated concatenated single string set groups string get groups string returns comma separated concatenated single string strings given set string get string set string strings
507	common\src\java\org\apache\hadoop\security\authorize\AuthorizationException.java	unrelated	package org apache hadoop security authorize an exception authorization related issues this em em provide stack trace security purposes authorization exception extends access control exception serial version uid l authorization exception authorization exception string message authorization exception throwable cause stack trace element stack trace new stack trace element stack trace element get stack trace print stack trace print stack trace print stream print stack trace print writer
508	common\src\java\org\apache\hadoop\security\authorize\PolicyProvider.java	unrelated	package org apache hadoop security authorize link policy provider provides link service definitions security link policy effect hadoop policy provider configuration key link policy provider implementation string policy provider config a default link policy provider without defined services policy provider default policy provider get link service definitions link policy provider service get services
509	common\src\java\org\apache\hadoop\security\authorize\ProxyUsers.java	unrelated	package org apache hadoop security authorize proxy users string conf hosts hosts string conf groups groups string conf hadoop proxyuser hadoop proxyuser string conf hadoop proxyuser re hadoop proxyuser boolean init false list groups hosts per proxyuser map string collection string proxy groups map string collection string proxy hosts reread conf get new values hadoop proxyuser groups hosts refresh super user groups configuration refresh configuration synchronized refresh super user groups configuration configuration conf returns configuration key effective user groups allowed superuser string get proxy superuser group conf key string user name return configuration key superuser ip addresses string get proxy superuser ip conf key string user name authorize superuser as synchronized authorize user group information user return true configuration specifies special configuration value indicating group host list allowed use configuration boolean wildcard list collection string list
510	common\src\java\org\apache\hadoop\security\authorize\RefreshAuthorizationPolicyProtocol.java	unrelated	package org apache hadoop security authorize protocol used refresh authorization policy use currently refresh authorization policy protocol extends versioned protocol version initial version version id l refresh service level authorization policy effect refresh service acl throws io exception
511	common\src\java\org\apache\hadoop\security\authorize\Service.java	unrelated	package org apache hadoop security authorize an definition em service em related service level authorization hadoop each service defines configuration key also necessary link permission required access service service string key class protocol service string key class protocol get configuration key service string get service key get protocol service class get protocol
512	common\src\java\org\apache\hadoop\security\authorize\ServiceAuthorizationManager.java	unrelated	package org apache hadoop security authorize an authorization manager handles service level authorization incoming service requests service authorization manager string hadoop policy file hadoop policy xml map class access control list protocol to acl configuration key controlling service level authorization hadoop link common configuration keys hadoop security authorization instead string service authorization config log auditlog string authz successfull for authorization successfull string authz failed for authorization failed authorize user access protocol used authorize user group information user synchronized refresh configuration conf package protected use tests set class get protocols with acls
513	common\src\java\org\apache\hadoop\security\token\SecretManager.java	authenticate	package org apache hadoop security token the server side secret manager token type secret manager t extends token identifier the token invalid message explains invalid token extends io exception invalid token string msg create password given identifier identifier may modified inside method protected byte create password t identifier retrieve password given token identifier should check date registry make sure token expired revoked returns relevant password byte retrieve password t identifier throws invalid token create empty token identifier t create identifier the name hashing algorithm string default hmac algorithm hmac sha the length random keys use key length a thread local store macs thread local mac thread local mac new thread local mac protected mac initial value key generator use key generator key gen try catch no such algorithm exception nsa generate new random secret key protected secret key generate secret secret key key synchronized key gen return key compute hmac identifier using secret key return output password protected byte create password byte identifier mac mac thread local mac get try catch invalid key exception ike return mac final identifier convert byte secret key protected secret key create secret key byte key return new secret key spec key default hmac algorithm
514	common\src\java\org\apache\hadoop\security\token\Token.java	authenticate	package org apache hadoop security token the client side form token token t extends token identifier implements writable byte identifier byte password text kind text service construct token given token identifier secret manager type token identifier token t id secret manager t mgr construct token components token byte identifier byte password text kind text service default constructor token get token identifier byte get identifier get token password secret byte get password get token kind text get kind get service token supposed used text get service set service token supposed used set service text new service inherit doc read fields data input throws io exception inherit doc write data output throws io exception generate url quoted base encoded serialized form writable string encode writable writable obj throws io exception modify writable value new value decode writable writable obj encode token url safe string encode to url string throws io exception decode given url safe token decode from url string string new value throws io exception boolean equals object right hash code add binary buffer string builder buffer byte bytes string string
515	common\src\java\org\apache\hadoop\security\token\TokenIdentifier.java	authenticate	package org apache hadoop security token an identifier identifies token may contain information token including kind type token identifier implements writable get token kind text get kind get ugi username encoded token identifier empty null user group information get user get bytes token identifier byte get bytes
516	common\src\java\org\apache\hadoop\security\token\TokenInfo.java	authenticate	package org apache hadoop security token indicates token related information used token info the type token selector used class extends token selector extends token identifier value
517	common\src\java\org\apache\hadoop\security\token\TokenSelector.java	authenticate	package org apache hadoop security token select token type t tokens use named service t extends token identifier token selector t extends token identifier token t select token text service
518	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenIdentifier.java	unrelated	package org apache hadoop security token delegation abstract delegation token identifier extends token identifier byte version text owner text renewer text real user issue date max date sequence number master key id abstract delegation token identifier abstract delegation token identifier text owner text renewer text real user text get kind get username encoded token identifier user group information get user text get renewer set issue date issue date get issue date set max date max date get max date set sequence number seq num get sequence number set master key id new id get master key id boolean equal object object b inherit doc boolean equals object obj inherit doc hash code read fields data input throws io exception throw new io exception unknown version delegation token write data output throws io exception string string
519	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenSecretManager.java	unrelated	package org apache hadoop security token delegation abstract delegation token secret manager token ident extends abstract delegation token identifier extends secret manager token ident log log log factory cache currently valid tokens mapping delegation token identifier delegation token information protected object lock protected map token ident delegation token information current tokens sequence number create delegation token identifier protected object lock protected delegation token sequence number access keys protected object lock protected map integer delegation key keys access current id protected object lock protected current id access current key protected object lock delegation key current key key update interval token max lifetime token remover scan interval token renew interval thread token remover thread protected volatile boolean running abstract delegation token secret manager delegation key update interval key update interval delegation key update interval token max lifetime delegation token max lifetime token renew interval delegation token renew interval token remover scan interval delegation token remover scan interval called object used start threads throws io exception update current key synchronized add previously used master key cache nn restarts called activate synchronized add key delegation key key throws io exception running safety check key get key id current id keys put key get key id key synchronized delegation key get all keys return keys values array new delegation key protected log update master key delegation key key throws io exception return update current master key this called start threads token remover thread created token remover thread afterwards update current key throws io exception log info updating current master key generating delegation tokens create new current key estimated expiry date new current id synchronized delegation key new key new delegation key new current id system log must invoked outside lock log update master key new key synchronized update current master key generating delegation tokens it called token remover thread roll master key throws io exception synchronized update current key synchronized remove expired keys system current time millis iterator map entry integer delegation key keys entry set protected synchronized byte create password token ident identifier log info creating password identifier identifier sequence num system current time millis sequence num delegation token sequence number identifier set issue date identifier set max date token max lifetime identifier set master key id current id identifier set sequence number sequence num byte password create password identifier get bytes current key get key current tokens put identifier new delegation token information return password synchronized byte retrieve password token ident identifier delegation token information info current tokens get identifier info null system current time millis info get renew date return info get password renew delegation token synchronized renew token token token ident token system current time millis byte array input stream buf new byte array input stream token get identifier data input stream new data input stream buf token ident id create identifier id read fields log info token renewal requested identifier id id get max date id get renewer null equals id get renewer string id get renewer string equals renewer delegation key key keys get id get master key id key null
520	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenSelector.java	unrelated	package org apache hadoop security token delegation look tokens find first delegation token matches service return abstract delegation token selector token ident extends abstract delegation token identifier text kind name protected abstract delegation token selector text kind name token token ident select token text service
521	common\src\java\org\apache\hadoop\security\token\delegation\DelegationKey.java	unrelated	package org apache hadoop security token delegation key used generating verifying delegation tokens delegation key implements writable key id expiry date byte key bytes null delegation key delegation key key id expiry date secret key key get key id get expiry date secret key get key set expiry date expiry date write data output throws io exception read fields data input throws io exception
522	common\src\java\org\apache\hadoop\tools\GetGroupsBase.java	unrelated	package org apache hadoop tools base hdfs mr implementations tools fetch display groups users belong get groups base extends configured implements tool print stream protected get groups base configuration conf protected get groups base configuration conf print stream run string args throws exception protected inet socket address get protocol address configuration conf get user mappings protocol get ugm protocol throws io exception
523	common\src\java\org\apache\hadoop\tools\GetUserMappingsProtocol.java	unrelated	package org apache hadoop tools protocol implemented name node job tracker maps users groups get user mappings protocol extends versioned protocol version initial version version id l get groups mapped given user string get groups for user string user throws io exception
524	common\src\java\org\apache\hadoop\util\AsyncDiskService.java	pooling	package org apache hadoop util this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion files we move files to be deleted folder asychronously deleting make sure caller run faster async disk service log log log factory get log async disk service thread pool core pool size core threads per volume thread pool maximum pool size maximum threads per volume thread pool keep alive time threads core pool size threads keep alive seconds thread group thread group new thread group async disk service thread factory thread factory hash map string thread pool executor executors async disk service string volumes throws io exception synchronized execute string root runnable task synchronized shutdown synchronized boolean await termination milliseconds synchronized list runnable shutdown now
525	common\src\java\org\apache\hadoop\util\CyclicIteration.java	unrelated	package org apache hadoop util provide cyclic link iterator link navigable map the link iterator navigates entries map according map ordering if link iterator hits last entry map continue first entry cyclic iteration k v implements iterable map entry k v navigable map k v navigablemap navigable map k v tailmap construct link iterable object link iterator created iterating given link navigable map the iteration begins starting key exclusively cyclic iteration navigable map k v navigablemap k startingkey navigablemap null navigablemap empty else inherit doc iterator map entry k v iterator return new cyclic iterator an link iterator link cyclic iteration cyclic iterator implements iterator map entry k v boolean hasnext iterator map entry k v the first entry begin map entry k v first the next entry map entry k v next cyclic iterator map entry k v next entry inherit doc boolean next inherit doc map entry k v next not supported remove
526	common\src\java\org\apache\hadoop\util\Daemon.java	unrelated	package org apache hadoop util a thread called link thread set daemon boolean true daemon extends thread set daemon true always daemon provide factory named daemon threads use executor services constructors daemon factory extends daemon implements thread factory thread new thread runnable runnable runnable runnable null construct daemon thread daemon super construct daemon thread daemon runnable runnable super runnable runnable runnable set name object runnable string construct daemon thread part specified thread group daemon thread group group runnable runnable super group runnable runnable runnable set name object runnable string runnable get runnable return runnable
527	common\src\java\org\apache\hadoop\util\DataChecksum.java	unrelated	package org apache hadoop util this provides inteface utilities processing checksums dfs data transfers data checksum implements checksum misc constants header len byte type byte len checksum types checksum null checksum crc checksum crc c checksum null size checksum crc size checksum crc c size data checksum new data checksum type bytes per checksum data checksum new data checksum byte bytes offset data checksum new data checksum data input stream write header data output stream byte get header type size checksum summer bytes per checksum sum data checksum checksum type checksum checksum accessors get checksum type get checksum size get bytes per checksum get num bytes in sum size of integer integer size byte size get checksum header size checksum interface just wrapper around member summer get value reset update byte b len update b verify chunked sums byte buffer data byte buffer checksums throws checksum exception verify chunked sums calculate chunked sums byte buffer data byte buffer checksums calculate chunked sums checksum null implements checksum
528	common\src\java\org\apache\hadoop\util\DiskChecker.java	unrelated	package org apache hadoop util class provides utility functions checking disk problem disk checker disk error exception extends io exception disk error exception string msg disk out of space exception extends io exception disk out of space exception string msg the semantics mkdirs with exists check method different mkdirs method provided sun java io file following way while creating non existent parent directories method checks existence directories mkdir fails point since directory might created process if mkdir exists check fails seemingly non existent directory signal error sun mkdir would signal error return false directory attempting create already exists mkdir fails boolean mkdirs with exists check file dir dir mkdir dir exists file canon dir null try catch io exception e string parent canon dir get parent return parent null create directory exist check dir file dir throws disk error exception mkdirs with exists check dir dir directory dir read dir write create directory check permissions already exists the semantics mkdirs with exists and permission check method different mkdirs method provided sun java io file following way while creating non existent parent directories method checks existence directories mkdir fails point since directory might created process if mkdir exists check fails seemingly non existent directory signal error sun mkdir would signal error return false directory attempting create already exists mkdir fails mkdirs with exists and permission check file directory local fs path to file dir boolean created false directory exists created local fs get file status dir get permission equals expected create local directory necessary check permissions also ensure read written check dir local file system local fs path dir throws disk error exception io exception mkdirs with exists and permission check local fs dir expected file status stat local fs get file status dir fs permission actual stat get permission stat directory fs action user actual get user action user implies fs action read user implies fs action write user implies fs action execute
529	common\src\java\org\apache\hadoop\util\GenericOptionsParser.java	unrelated	package org apache hadoop util code generic options parser code utility parse command line arguments generic hadoop framework code generic options parser code recognizes several standarad command line arguments enabling applications easily specify namenode jobtracker additional configuration resources etc id generic options generic options p the supported generic options p p blockquote pre conf lt configuration file gt specify configuration file d lt property value gt use value given property fs lt local namenode port gt specify namenode jt lt local jobtracker port gt specify job tracker files lt comma separated list files gt specify comma separated files copied map reduce cluster libjars lt comma separated list jars gt specify comma separated jar files classpath archives lt comma separated list archives gt specify comma separated archives unarchived compute machines pre blockquote p p the general command line syntax p p tt pre bin hadoop command generic options command options pre tt p p generic command line arguments strong might strong modify code configuration code objects given constructors p p the functionality implemented using commons cli p p examples p p blockquote pre bin hadoop dfs fs darwin ls data list data directory dfs namenode darwin bin hadoop dfs d fs default name darwin ls data list data directory dfs namenode darwin bin hadoop dfs conf core site xml conf hdfs site xml ls data list data directory dfs multiple conf files specified bin hadoop job d mapred job tracker darwin submit job xml submit job job tracker darwin bin hadoop job jt darwin submit job xml submit job job tracker darwin bin hadoop job jt local submit job xml submit job local runner bin hadoop jar libjars testlib jar archives test tgz files file txt inputjar args job submission libjars files archives pre blockquote p generic options parser log log log factory get log generic options parser configuration conf command line command line create options parser given options parse args generic options parser options opts string args create options parser parse args generic options parser string args create code generic options parser code parse generic hadoop arguments the array arguments generic arguments obtained link get remaining args generic options parser configuration conf string args create code generic options parser code parse given options well generic hadoop options the resulting code command line code object obtained link get command line generic options parser configuration conf returns array strings containing application specific arguments strong empty array strong command line defined string get remaining args get modified configuration configuration get configuration returns commons cli code command line code object process parsed arguments note if object created link generic options parser configuration string returned object contain parsed generic options parsed options descriptor command line get command line specify properties generic option options build general options options opts modify configuration according user specified generic options process general options configuration conf if libjars set conf parse libjars url get lib jars configuration conf throws io exception takes input comma separated list files verifies exist it defaults file files specified scheme returns paths uri converted defaulting file so input
530	common\src\java\org\apache\hadoop\util\GenericsUtil.java	unrelated	package org apache hadoop util contains utility methods dealing java generics generics util returns class object type code class lt t gt code argument type code t code t class t get class t class t clazz class t get class return clazz converts given code list lt t gt code array code t code t t array class t c list t list t ta t array new instance c list size list size return ta converts given code list lt t gt code array code t code use link array class list list may empty t t array list t list return array get class list get list
531	common\src\java\org\apache\hadoop\util\HeapSort.java	unrelated	package org apache hadoop util an implementation core algorithm heap sort heap sort implements indexed sorter heap sort heap indexed sortable b sort given range items using heap sort inherit doc sort indexed sortable p r inherit doc sort indexed sortable p r
532	common\src\java\org\apache\hadoop\util\HostsFileReader.java	unrelated	package org apache hadoop util keeps track datanodes tasktrackers allowed connect namenode jobtracker hosts file reader set string includes set string excludes string includes file string excludes file log log log factory get log hosts file reader hosts file reader string file read file to set string filename set string set throws io exception synchronized refresh throws io exception synchronized set string get hosts synchronized set string get excluded hosts synchronized set includes file string includes file synchronized set excludes file string excludes file synchronized update file names string includes file
533	common\src\java\org\apache\hadoop\util\IndexedSortable.java	unrelated	package org apache hadoop util interface collections capable sorted link indexed sorter algorithms indexed sortable compare items given addresses consistent semantics link java util comparator compare object object compare j swap items given addresses swap j
534	common\src\java\org\apache\hadoop\util\IndexedSorter.java	unrelated	package org apache hadoop util interface sort algorithms accepting link indexed sortable items a sort algorithm implementing may link indexed sortable compare link indexed sortable swap items range indices effect sort across range indexed sorter sort items accessed given indexed sortable given range logical indices from perspective sort algorithm index inclusive r exclusive addressable entry sort indexed sortable r same link sort indexed sortable indicate progress periodically sort indexed sortable r progressable rep
535	common\src\java\org\apache\hadoop\util\LineReader.java	unrelated	package org apache hadoop util a provides line reader input stream depending constructor used lines either terminated ul li one following n lf r cr r n cr lf li li em em custom byte sequence delimiter li ul in cases eof also terminates otherwise unterminated line line reader default buffer size buffer size default buffer size input stream byte buffer number bytes real data buffer buffer length current position buffer buffer posn byte cr r byte lf n the line delimiter byte record delimiter bytes line reader input stream line reader input stream buffer size line reader input stream configuration conf throws io exception line reader input stream byte record delimiter bytes line reader input stream buffer size line reader input stream configuration conf close throws io exception read line text str max line length read default line text str max line length max bytes to consume throws io exception read custom line text str max line length max bytes to consume read line text str max line length throws io exception read line text str throws io exception
536	common\src\java\org\apache\hadoop\util\MergeSort.java	unrelated	package org apache hadoop util an implementation core algorithm merge sort merge sort reusable int writables int writable i new int writable int writable j new int writable comparator algo use comparator int writable comparator merge sort comparator int writable comparator merge sort src dest low high swap x b
537	common\src\java\org\apache\hadoop\util\NativeCodeLoader.java	unrelated	package org apache hadoop util a helper load native hadoop code e libhadoop this handles fallback either bundled libhadoop linux default java implementations appropriate native code loader log log boolean native code loaded false check native hadoop code loaded platform else code false code boolean native code loaded return native hadoop libraries present used job used job code false code otherwise boolean get load native libraries configuration conf set native hadoop libraries present used job set load native libraries configuration conf
538	common\src\java\org\apache\hadoop\util\Options.java	unrelated	package org apache hadoop util this allows generic access variable length type safe parameter lists options string option class option boolean option integer option long option path option fs data input stream option fs data output stream option progressable option base t extends base t get option class t cls base opts t t prepend options t old opts t new opts
539	common\src\java\org\apache\hadoop\util\PlatformName.java	unrelated	package org apache hadoop util a helper getting build info java vm platform name string platform name system get property os name string get platform name main string args
540	common\src\java\org\apache\hadoop\util\PrintJarMainClass.java	unrelated	package org apache hadoop util a micro application prints main name jar file print jar main class main string args
541	common\src\java\org\apache\hadoop\util\PriorityQueue.java	scheduler	package org apache hadoop util a priority queue maintains partial ordering elements least element always found constant time put pop require log size time priority queue t t heap size max size determines ordering objects priority queue subclasses protected boolean less than object object b subclass constructors must call protected initialize max size size heap size max size heap t new object heap size max size max size adds object priority queue log size time if one tries add objects max size initialize runtime exception array index out of bound thrown put t element size heap size element heap adds element priority queue log size time either priority queue full less than element top boolean insert t element size max size else size less than element top else returns least element priority queue constant time t top size else removes returns least element priority queue log size t pop size else should called object top changes values still log n worst case least twice fast pre pq top change pq adjust top pre instead pre pq pop change pq push pre adjust top heap returns number elements currently stored priority queue size return size removes entries priority queue clear size size heap size j j less than node heap j heap k j k size less than heap k heap j j size less than heap j node j k
542	common\src\java\org\apache\hadoop\util\ProgramDriver.java	unrelated	package org apache hadoop util a driver used run programs added program driver a description program based human readable description map string program description programs program driver programs new tree map string program description program description class param types new class string create description example program program description class main class invoke example application given arguments invoke string args string get description method main string description print usage map string program description programs system println valid program names map entry string program description item programs entry set this method adds classed repository add class string name class main class string description throws throwable programs put name new program description main class description this driver example programs it looks first command line argument tries find example program name if found calls main method rest command line arguments driver string args throws throwable make sure gave us program name args length and good program description pgm programs get args pgm null remove leading argument call main string new args new string args length args length pgm invoke new args return
543	common\src\java\org\apache\hadoop\util\Progress.java	unrelated	package org apache hadoop util utility assist generation progress reports applications build hierarchy link progress instances modelling phase execution the root constructed link progress nodes sub phases created calling link add phase progress log log log factory get log progress string status progress current phase array list progress phases new array list progress progress parent each phase different progress weightage for example map task map phase accounts sort phase user needs give weightages parameters phases adding phases progress object wants give weightage phases so nodes added without specifying weightage means fixed weightage phases boolean fixed weightage for all phases false progress per phase f array list float progress weightages for phases new array list float creates new root node progress adds named node tree progress add phase string status adds node tree gives equal weightage phases synchronized progress add phase adds new phase caller needs set progress weightage synchronized progress add new phase adds named node specified progress weightage tree progress add phase string status weightage adds node specified progress weightage tree synchronized progress add phase weightage adds n nodes tree gives equal weightage phases synchronized add phases n returns progress weightage given phase progress weightage get progress weightage phase num synchronized progress get parent return parent synchronized set parent progress parent parent parent called execution move next phase level tree synchronized start next phase returns current sub node executing synchronized progress phase completes node moving parent node next child complete called execution leaf node set progress synchronized set progress returns overall progress root method probably need synchronized get internal synchronized node parent never changes still hurt synchronized get returns progress node get would give overall progress root node given current node synchronized get progress computes progress node synchronized get internal synchronized set status string status string string synchronized string string builder buffer
544	common\src\java\org\apache\hadoop\util\Progressable.java	unrelated	package org apache hadoop util a facility reporting progress p clients applications use provided code progressable code explicitly report progress hadoop framework this especially lieu reported progress framework assume error occured time operation p progressable report progress hadoop framework progress
545	common\src\java\org\apache\hadoop\util\ProtoUtil.java	unrelated	package org apache hadoop util proto util read variable length integer format proto bufs encodes read raw varint data input throws io exception
546	common\src\java\org\apache\hadoop\util\PureJavaCrc32.java	unrelated	package org apache hadoop util a pure java implementation crc checksum uses polynomial built native crc this avoid jni overhead certain uses checksumming many small pieces data checksummed succession the current version x x fast sun native java util zip crc java pure java crc implements checksum current crc value bit flipped crc create new pure java crc object pure java crc inherit doc get value inherit doc reset inherit doc update byte b len inherit doc update b crc lookup tables generated polynomial x edb see also test pure java crc table t new t new t new t new t new t new t new t new
547	common\src\java\org\apache\hadoop\util\PureJavaCrc32C.java	unrelated	package org apache hadoop util a pure java implementation crc checksum uses crc c polynomial polynomial used scsi implemented many intel chipsets supporting sse pure java crc c implements checksum current crc value bit flipped crc create new pure java crc object pure java crc c inherit doc get value inherit doc reset inherit doc update byte b len inherit doc update b crc polynomial tables generated java cp build test build org apache hadoop util test pure java crc table f b t new t new t new t new t new t new t new t new
548	common\src\java\org\apache\hadoop\util\QuickSort.java	unrelated	package org apache hadoop util an implementation core algorithm quick sort quick sort implements indexed sorter indexed sorter alt new heap sort quick sort fix indexed sortable p r deepest recursion giving heapsort returns ceil log n protected get max depth x sort given range items using quick sort inherit doc if recursion depth falls link get max depth switch link heap sort sort indexed sortable p r inherit doc sort indexed sortable p r sort internal indexed sortable p r
549	common\src\java\org\apache\hadoop\util\ReflectionUtils.java	unrelated	package org apache hadoop util general reflection utils reflection utils class empty array new class volatile serialization factory serial factory null cache constructors pins garbage collected reflection utils collected map class constructor constructor cache new concurrent hash map class constructor check set configuration necessary set conf object object configuration conf conf null this code support backward compatibility break compile time dependency core mapred this made deprecated along mapred package hadoop should removed mapred package removed set job conf object object configuration conf if job conf job configurable classpath and object type job configurable and conf type job conf invoke configure object try catch class not found exception e catch exception e create object given initialize conf t t new instance class t class configuration conf t result try catch exception e set conf result conf return result thread mx bean thread bean management factory get thread mx bean set contention tracing boolean val thread bean set thread contention monitoring enabled val string get task name id string name name null return id name print thread information stack traces print thread info print writer stream stack depth boolean contention thread bean thread contention monitoring enabled thread ids thread bean get all thread ids stream println process thread dump title stream println thread ids length active threads tid thread ids stream flush previous log time log current thread stacks info level log thread info log log boolean dump stack false log info enabled return correctly typed link class given object t class t get class t return class t get class methods support testing clear cache constructor cache clear get cache size return constructor cache size a pair input output buffers use clone writables copy in copy out buffer data output buffer buffer new data output buffer data input buffer buffer new data input buffer move data output buffer input buffer move data allocate buffer thread tries clone objects thread local copy in copy out buffer clone buffers serialization factory get factory configuration conf serial factory null return serial factory make copy writable object using serialization buffer t t copy configuration conf copy in copy out buffer buffer clone buffers get buffer buffer reset serialization factory factory get factory conf class t cls class t src get class serializer t serializer factory get serializer cls serializer open buffer buffer serializer serialize src buffer move data deserializer t deserializer factory get deserializer cls deserializer open buffer buffer dst deserializer deserialize dst return dst clone writable into writable dst copy in copy out buffer buffer clone buffers get buffer buffer reset src write buffer buffer buffer move data dst read fields buffer buffer
550	common\src\java\org\apache\hadoop\util\RunJar.java	unrelated	package org apache hadoop util run hadoop job jar run jar pattern matches pattern match any pattern compile unpack jar file directory this version unpacks files inside jar regardless filename un jar file jar file file dir throws io exception un jar jar file dir match any unpack matching files jar entries inside jar match given pattern skipped un jar file jar file file dir pattern unpack regex throws io exception jar file jar new jar file jar file try finally ensure existence given directory ensure directory file dir throws io exception dir mkdirs dir directory run hadoop job jar if main jar manifest must provided command line main string args throws throwable string usage run jar jar file main class args args length first arg string file name args first arg file file new file file name string main class name null jar file jar file try catch io exception io manifest manifest jar file get manifest manifest null jar file close main class name null main class name main class name replace all file tmp dir new file new configuration get hadoop tmp dir ensure directory tmp dir file work dir file create temp file hadoop unjar tmp dir work dir delete ensure directory work dir runtime get runtime add shutdown hook new thread un jar file work dir array list url path new array list url path add new file work dir uri url path add file uri url path add new file work dir uri url file libs new file work dir lib list files libs null class loader loader thread current thread set context class loader loader class main class class name main class name true loader method main main class get method main new class string new args arrays list args try catch invocation target exception e
551	common\src\java\org\apache\hadoop\util\ServicePlugin.java	unrelated	package org apache hadoop util service plug service plug ins may used expose functionality datanodes namenodes using arbitrary rpc protocols plug ins instantiated service instance notified service life cycle events using methods defined service plug ins started service instance started stopped service instance stopped service plugin extends closeable this method invoked service instance started start object service this method invoked service instance shut stop
552	common\src\java\org\apache\hadoop\util\ServletUtil.java	unrelated	package org apache hadoop util servlet util initial html header print writer init html servlet response response string title get parameter servlet request return null parameter contains white spaces string get parameter servlet request request string name string html tail hr n html footer added jsps string html footer generate percentage graph returns html representation string percentage graph perc width throws io exception generate percentage graph returns html representation string percentage graph perc width throws io exception
553	common\src\java\org\apache\hadoop\util\Shell.java	unrelated	package org apache hadoop util a base running unix command code shell code used run unix commands like code du code code df code it also offers facilities gate commands time intervals shell log log log factory get log shell unix command get current user name string user name command whoami unix command get current user groups list string get groups command unix command get given user groups list string get groups for user command string user unix command get given netgroup user list string get users for netgroup command string netgroup unix command set permission string set permission command chmod unix command set owner string set owner command chown string set group command chgrp unix command create link string link command ln unix command get link target string read link command readlink return unix command get permission information string get get permission command time executing script would timedout protected time out interval l if script timed atomic boolean timed out unix command get ulimit process string ulimit command ulimit string get ulimit memory command memory limit string get ulimit memory command configuration conf set true windows platforms boolean windows borrowed path windows interval refresh interval msec last time last time command performed map string string environment env command execution file dir process process sub process used execute command exit code if script finished executing volatile atomic boolean completed shell shell interval set environment command protected set environment map string string env set working directory protected set working directory file dir check see command needs executed execute needed protected run throws io exception run command run command throws io exception return array containing command name parameters protected string get exec string parse execution result protected parse exec result buffered reader lines throws io exception get current sub process executing given command process get process get exit code get exit code exit code exception extends io exception shell command executor extends shell boolean timed out set timed out string exec command string cmd throws io exception string exec command map string string env string cmd string exec command map string string env string cmd throws io exception shell timeout timer task extends timer task
554	common\src\java\org\apache\hadoop\util\StringUtils.java	unrelated	package org apache hadoop util general utils string utils decimal format decimal format make representation exception string stringify exception throwable e string writer stm new string writer print writer wrt new print writer stm e print stack trace wrt wrt close return stm string given full hostname return word upto first dot string simple hostname string full hostname offset full hostname index of offset return full hostname decimal format one decimal new decimal format given integer return approximate human readable format it uses bases k g string human readable int number abs number math abs number result number string suffix abs number else abs number else abs number else return one decimal format result suffix format percentage presentation user string format percent done digits decimal format percent format new decimal format scale math pow digits rounded math floor done scale percent format set decimal separator always shown false percent format set minimum fraction digits digits percent format set maximum fraction digits digits return percent format format rounded scale given array strings return comma separated list elements otherwise string array to string string strs strs length return string builder sbuf new string builder sbuf append strs idx idx strs length idx return sbuf string given array bytes convert bytes hex representation bytes string byte to hex string byte bytes start end bytes null string builder new string builder start end return string same byte to hex string bytes bytes length string byte to hex string byte bytes return byte to hex string bytes bytes length given hexstring return byte array corresponding the size byte array therefore hex length byte hex string to byte string hex byte bts new byte hex length bts length return bts string uri to string uri uris uris null string builder ret new string builder uris string uris length return ret string uri to uri string str str null uri uris new uri str length str length return uris path to path string str str null path p new path str length str length return p given finish start time milliseconds returns string format xhrs ymins z sec time difference two times if finish time comes start time negative valeus x y z wil return string format time diff finish time start time time diff finish time start time return format time time diff given time milliseconds returns string format xhrs ymins z sec string format time time diff string builder buf new string builder hours time diff rem time diff minutes rem rem rem seconds rem hours minutes return sec difference buf append seconds buf append sec return buf string formats time ms appends difference finish time start time returned format time diff if finish time empty returned start time difference appended return value string get formatted time with diff date format date format string builder buf new string builder finish time return buf string returns arraylist strings string get strings string str collection string values get string collection str values size return values array new string values size returns collection strings collection string get string collection string
555	common\src\java\org\apache\hadoop\util\Tool.java	unrelated	package org apache hadoop util a tool supports handling generic command line options p code tool code standard map reduce tool application the tool application delegate handling href doc root org apache hadoop util generic options parser html generic options standard command line options link tool runner run tool string handle custom arguments p p here typical code tool code implemented p p blockquote pre my app extends configured implements tool run string args throws exception code configuration code processed code tool runner code configuration conf get conf create job conf using processed code conf code job conf job new job conf conf my app process custom command line options path new path args path new path args specify various job specific parameters job set job name app job set input path job set output path job set mapper class my mapper job set reducer class my reducer submit job poll progress job complete job client run job job return main string args throws exception let code tool runner code handle generic command line options res tool runner run new configuration new my app args system exit res pre blockquote p tool extends configurable run string args throws exception
556	common\src\java\org\apache\hadoop\util\ToolRunner.java	unrelated	package org apache hadoop util a utility help run link tool p code tool runner code used run implementing code tool code it works conjunction link generic options parser parse href doc root org apache hadoop util generic options parser html generic options generic hadoop command line arguments modifies code configuration code code tool code the application specific options passed along without modified p tool runner runs given code tool code link tool run string parsing given generic arguments uses given code configuration code builds one null sets code tool code configuration possibly modified version code conf code run configuration conf tool tool string args runs code tool code code configuration code equivalent code run tool get conf tool args code run tool tool string args prints generic command line argurments usage information print generic command usage print stream
557	common\src\java\org\apache\hadoop\util\UTF8ByteArrayUtils.java	unrelated	package org apache hadoop util utf byte array utils find first occurrence given byte b utf encoded find byte byte utf start end byte b find first occurrence given bytes b utf encoded find bytes byte utf start end byte b find nth occurrence given byte b utf encoded find nth byte byte utf start length byte b n find nth occurrence given byte b utf encoded find nth byte byte utf byte b n
558	common\src\java\org\apache\hadoop\util\VersionInfo.java	unrelated	package org apache hadoop util this finds package info hadoop hadoop version annotation information version info log log log factory get log version info package package hadoop version annotation version package get package string get version string get revision string get branch string get date string get user string get url string get src checksum string get build version main string args
559	common\src\java\org\apache\hadoop\util\XMLUtils.java	unrelated	package org apache hadoop util general xml utilities xml utils transform input xml given stylesheet transform
560	common\src\java\org\apache\hadoop\util\bloom\BloomFilter.java	unrelated	package org apache hadoop util bloom implements bloom filter defined bloom p the bloom filter data structure introduced adopted networking research community past decade thanks bandwidth efficiencies offers transmission set membership information networked hosts a sender encodes information bit vector bloom filter compact conventional representation computation space costs construction linear number elements the receiver uses filter test whether various elements members set though filter occasionally return false positive never return false negative when creating filter sender choose desired point trade false positive rate size p originally created href http www one lab org european commission one lab project bloom filter extends filter byte bitvalues new byte the bit vector bit set bits default constructor use read fields bloom filter constructor link org apache hadoop util hash hash bloom filter vector size nb hash hash type add key key filter filter boolean membership test key key filter filter xor filter filter string string get vector size writable write data output throws io exception read fields data input throws io exception get n bytes end
561	common\src\java\org\apache\hadoop\util\bloom\CountingBloomFilter.java	unrelated	package org apache hadoop util bloom implements counting bloom filter defined fan et al to n paper p a counting bloom filter improvement standard bloom filter allows dynamic additions deletions set membership information this achieved use counting vector instead bit vector p originally created href http www one lab org european commission one lab project counting bloom filter extends filter storage counting buckets buckets we using bit buckets bucket count bucket max value default constructor use read fields counting bloom filter constructor link org apache hadoop util hash hash counting bloom filter vector size nb hash hash type returns number bit words would take hold vector size buckets buckets words vector size return vector size add key key removes specified key counting bloom filter p b invariant b nothing happens specified key belong counter bloom filter delete key key filter filter boolean membership test key key this method calculates approximate count key e many times key added filter this allows filter used approximate code key gt count code map p note due bucket size filter inserting key times cause overflow filter positions associated key significantly increase error rate keys for reason filter used store small count values code lt n lt lt code returned code v count code probability equal error rate filter code v gt count code otherwise additionally filter experienced underflow result link delete key operation return value may lower code count code probability false negative rate filter approximate count key key filter filter xor filter filter string string writable write data output throws io exception read fields data input throws io exception
562	common\src\java\org\apache\hadoop\util\bloom\DynamicBloomFilter.java	unrelated	package org apache hadoop util bloom implements dynamic bloom filter defined infocom paper p a dynamic bloom filter dbf makes use code code bit matrix code code rows standard bloom filter the creation process dbf iterative at start dbf code code bit matrix e composed single standard bloom filter it assumes code n sub r sub code elements recorded initial bit vector code n sub r sub n code code n code cardinality set code a code record filter p as size code a code grows execution application several keys must inserted dbf when inserting key dbf one must first get active bloom filter matrix a bloom filter active number recorded keys code n sub r sub code strictly less current cardinality code a code code n code if active bloom filter found key inserted code n sub r sub code incremented one on hand active bloom filter new one created e new row added matrix according current size code a code element added new bloom filter code n sub r sub code value new bloom filter set one a given key said belong dbf code k code positions set one one matrix rows p originally created href http www one lab org european commission one lab project dynamic bloom filter extends filter threshold maximum number key record dynamic bloom filter row nr the number keys recorded current standard active bloom filter current nb record the matrix bloom filter bloom filter matrix zero args constructor serialization dynamic bloom filter constructor p builds empty dynamic bloom filter link org apache hadoop util hash hash dynamic bloom filter row dynamic bloom filter vector size nb hash hash type nr add key key filter filter boolean membership test key key filter filter xor filter filter string string writable write data output throws io exception read fields data input throws io exception adds new row dynamic bloom filter add row returns active standard bloom filter dynamic bloom filter bloom filter get active standard bf
563	common\src\java\org\apache\hadoop\util\bloom\Filter.java	unrelated	package org apache hadoop util bloom defines general behavior filter p a filter data structure aims offering lossy summary set code a code the key idea map entries code a code also called keys several positions vector use several hash functions p typically filter implemented bloom filter bloom filter extension p it must extended order define real behavior filter implements writable version negative accommodate old format the vector size filter protected vector size the hash function used map key several positions vector protected hash function hash the number hash function consider protected nb hash type hashing function use protected hash type protected filter constructor protected filter vector size nb hash hash type adds key filter add key key determines wether specified key belongs filter boolean membership test key key peforms logical and filter specified filter p b invariant b the result assigned filter filter filter peforms logical or filter specified filter p b invariant b the result assigned filter filter filter peforms logical xor filter specified filter p b invariant b the result assigned filter xor filter filter performs logical not filter p the result assigned filter adds list keys filter add list key keys end add adds collection keys filter add collection key keys end add adds array keys filter add key keys end add writable write data output throws io exception read fields data input throws io exception end
564	common\src\java\org\apache\hadoop\util\bloom\HashFunction.java	unrelated	package org apache hadoop util bloom implements hash object returns certain number hashed values hash function the number hashed values nb hash the maximum highest returned value max value hashing algorithm use hash hash function constructor p builds hash function must obey given maximum number returned values highest value hash function max value nb hash hash type clears hash function a noop clear hashes specified key several integers hash key k
565	common\src\java\org\apache\hadoop\util\bloom\Key.java	unrelated	package org apache hadoop util bloom the general behavior key must stored filter key implements writable comparable key byte value key byte bytes the weight associated key p b invariant b specified instance code key code default weight weight default constructor use read fields key constructor p builds key default weight key byte value constructor p builds key specified weight key byte value weight set byte value weight byte get bytes get weight increments weight key specified value increment weight weight increments weight key one increment weight boolean equals object hash code writable write data output throws io exception read fields data input throws io exception comparable compare to key
566	common\src\java\org\apache\hadoop\util\bloom\RemoveScheme.java	unrelated	package org apache hadoop util bloom defines different remove scheme retouched bloom filters p originally created href http www one lab org european commission one lab project remove scheme random selection p the idea randomly select bit reset short random minimum fn selection p the idea select bit reset generate minimum number false negative short minimum fn maximum fp selection p the idea select bit reset remove maximum number false positive short maximum fp ratio selection p the idea select bit reset time remove maximum number false positve minimizing amount false negative generated short ratio
567	common\src\java\org\apache\hadoop\util\bloom\RetouchedBloomFilter.java	unrelated	package org apache hadoop util bloom implements retouched bloom filter defined co next paper p it allows removal selected false positives cost introducing random false negatives benefit eliminating random false positives time p originally created href http www one lab org european commission one lab project retouched bloom filter extends bloom filter implements remove scheme key list vector element list vector defined paper false positives list key fp vector key list vector keys recorded filter list key key vector ratio vector ratio random rand default constructor use read fields retouched bloom filter constructor link org apache hadoop util hash hash retouched bloom filter vector size nb hash hash type add key key adds false positive information retouched bloom filter p b invariant b false positive code null code nothing happens add false positive key key adds collection false positive information retouched bloom filter add false positive collection key coll adds list false positive information retouched bloom filter add false positive list key keys adds array false positive information retouched bloom filter add false positive key keys performs selective clearing given key selective clearing key k short scheme random remove chooses bit position minimizes number false negative generated minimum fn remove chooses bit position maximizes number false positive removed maximum fp remove chooses bit position minimizes number false negative generated maximizing number false positive removed ratio remove clears specified bit bit vector keeps date key list vectors clear bit index removes given key filer remove key key k list key vector computes ratio a fp compute ratio get weight list key key list creates initialises various vectors create vector writable write data output throws io exception read fields data input throws io exception
568	common\src\java\org\apache\hadoop\util\hash\Hash.java	unrelated	package org apache hadoop util hash this represents common api hashing functions hash constant denote invalid hash type invalid hash constant denote link jenkins hash jenkins hash constant denote link murmur hash murmur hash parse hash type string name get hash type configuration conf hash get instance type hash get instance configuration conf hash byte bytes hash byte bytes initval hash byte bytes length initval
569	common\src\java\org\apache\hadoop\util\hash\JenkinsHash.java	unrelated	package org apache hadoop util hash produces bit hash hash table lookup pre lookup c bob jenkins may public domain you use free purpose it domain it warranty pre function compares others crc md etc dr dobbs article jenkins hash extends hash int mask x ffffffff l byte mask x ff l jenkins hash instance new jenkins hash hash get instance rot val pos taken hashlittle hash variable length key bit value return value two keys differing one two bits totally different hash values p the best hash table sizes powers there need mod prime mod sooo slow if need less bits use bitmask for example need bits code hashmask code in case hash table hashsize elements p if hashing n strings byte k like n hash k p by bob jenkins bob jenkins burtleburtle net you may use code way wish educational commercial it free p use hash table lookup anything one collision acceptable do not use cryptographic purposes hash byte key nbytes initval compute hash specified file main string args throws io exception
570	common\src\java\org\apache\hadoop\util\hash\MurmurHash.java	unrelated	package org apache hadoop util hash this fast non cryptographic hash suitable general hash based lookup see http murmurhash googlepages com details p the c version murmur hash found site ported java andrzej bialecki ab getopt org p murmur hash extends hash murmur hash instance new murmur hash hash get instance hash byte data length seed
571	hdfs\src\ant\org\apache\hadoop\ant\DfsTask.java	unrelated	package org apache hadoop ant link org apache hadoop fs fs shell fs shell wrapper ant task dfs task extends task default sink link java lang system system link java lang system err system err output stream null out new output stream fs shell shell new fs shell protected ant class loader confloader protected output stream null out protected output stream err null out set ant protected string cmd protected linked list string argv new linked list string protected string outprop protected string errprop protected boolean failonerror true saved ant context print stream ant out print stream ant err sets command run link org apache hadoop fs fs shell fs shell set cmd string cmd sets argument list string comma separated values set args string args sets property system written if property defined task executed updated set out string outprop sets property system err written if property name property system two interlaced if property defined task executed updated set err string errprop sets path parent last class loader intended used link org apache hadoop conf configuration configuration parent class loaders set conf string confpath sets property controlling whether link org apache tools ant build exception build exception thrown command returns value less zero throws exception set failonerror boolean failonerror save current values system system err configure output streams fs shell protected push context create appropriate output properties respective output restore system system err release resources created class loaders aid garbage collection protected pop context case dfs task overridden protected post cmd exit code invoke link org apache hadoop fs fs shell main fs shell main cursory checks configuration execute throws build exception
572	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsBaseConditional.java	unrelated	package org apache hadoop ant condition this wrapper around link org apache hadoop ant dfs task implements ant gt link org apache tools ant taskdefs condition condition condition hdfs tests so one test conditions like code condition property precond hadoop exists file file a hadoop exists file file b hadoop sizezero file file b condition this define property precond file a exists file b zero length dfs base conditional extends org apache hadoop ant dfs task protected boolean result string file init args set file string file protected char get flag protected post cmd exit code boolean eval
573	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsExists.java	unrelated	package org apache hadoop ant condition dfs exists extends dfs base conditional protected char flag e protected char get flag return flag
574	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsIsDir.java	unrelated	package org apache hadoop ant condition dfs is dir extends dfs base conditional protected char flag protected char get flag return flag
575	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsZeroLen.java	unrelated	package org apache hadoop ant condition dfs zero len extends dfs base conditional protected char flag z protected char get flag return flag
576	hdfs\src\java\org\apache\hadoop\fs\Hdfs.java	unrelated	package org apache hadoop fs hdfs extends abstract file system dfs client dfs boolean verify checksum true hdfs configuration init this constructor signature needed link abstract file system create file system uri configuration must hdfs hdfs uri uri configuration conf throws io exception uri syntax exception super uri fs constants hdfs uri scheme true name node default port uri get scheme equals ignore case fs constants hdfs uri scheme string host uri get host host null inet socket address namenode name node get address uri get authority dfs new dfs client namenode conf get statistics get uri default port return name node default port fs data output stream create internal path f return new fs data output stream dfs primitive create get uri path f boolean delete path f boolean recursive return dfs delete get uri path f recursive block location get file block locations path p start len return dfs get block locations get uri path p start len file checksum get file checksum path f return dfs get file checksum get uri path f file status get file status path f hdfs file status fi dfs get file info get uri path f fi null else file status get file link status path f hdfs file status fi dfs get file link info get uri path f fi null else file status make qualified hdfs file status f path parent nb symlink made fully qualified file context return new file status f get len f dir f get replication located file status make qualified located return new located file status f get len f dir f get replication fs status get fs status throws io exception return dfs get disk status fs server defaults get server defaults throws io exception return dfs get server defaults remote iterator located file status list located status return new dir listing iterator located file status p true remote iterator file status list status iterator path f throws access control exception file not found exception unresolved link exception io exception return new dir listing iterator file status f false this defines iterator returns file status file subdirectory directory need location status contains block location file throws runtime exception error cause dir listing iterator t extends file status implements remote iterator t directory listing listing string src boolean need location status dir listing iterator path p boolean need location boolean next throws io exception get next item list hdfs file status get next throws io exception file status list status path f string src get uri path f fetch first batch entries directory directory listing listing dfs list paths listing null directory exist hdfs file status partial listing listing get partial listing listing more got entries directory the directory size big needs fetch estimate total number entries directory total num entries array list file status listing add first batch entries array list hdfs file status file status partial listing fetch entries listing more return listing array new file status listing size inherit doc remote iterator path list corrupt file blocks path path throws io exception return
577	hdfs\src\java\org\apache\hadoop\hdfs\BlockMissingException.java	unrelated	package org apache hadoop hdfs this exception thrown read encounters block locations associated block missing exception extends io exception serial version uid l string filename offset an exception indicates file corrupted block missing exception string filename string description offset returns name corrupted file string get file returns offset file corrupted get offset
578	hdfs\src\java\org\apache\hadoop\hdfs\BlockReader.java	pooling	package org apache hadoop hdfs this wrapper around connection datanode understands checksum offset etc terminology dl dt block dt dd the hdfs block typically large mb dd dt chunk dt dd a block divided chunks comes checksum we want transfers chunk aligned able verify checksums dd dt packet dt dd a grouping chunks used transport it contains header followed checksum data followed real data dd dl please see data node rpc specification block reader extends fs input checker socket dn sock sending status code e g checksum ok read data input stream data checksum checksum offset block last chunk received last chunk offset last chunk len last seq no offset block reader wants actually read start offset offset block first chunk may less start offset first chunk offset bytes per checksum checksum size the total number bytes need transfer dn this amount user requested plus padding beginning read begin chunk boundary bytes needed to finish boolean eos false boolean sent status code false byte skip buf null byte buffer checksum bytes null amount unread data current received packet data left fs input checker input stream java io input stream read used dfs input stream read this violates one rule checksum error read modify user buffer successful read first reads data user buffer checks checksum synchronized read byte buf len synchronized skip n throws io exception read throws io exception boolean seek to new source target pos throws io exception seek pos throws io exception protected get chunk position pos makes sure checksum bytes enough capacity limit set number checksum bytes needed read adjust checksum bytes data len protected synchronized read chunk pos byte buf offset block reader string file string bpid block id block reader new block reader socket sock string file java doc required block reader new block reader socket sock string file create new block reader specifically satisfy read this method also sends op read block request block reader new block reader socket sock string file synchronized close throws io exception kind like read fully only reads much possible and allows use protected read fully read all byte buf offset len throws io exception take socket used talk dn socket take socket whether block reader reached end input stream successfully sent status code back datanode boolean sent status code when reader reaches end read sends status response e g checksum ok dn failure could lead dn closing connection open affect data correctness send read result socket sock status status code file name print accessing block directly servlets string get file name inet socket address
579	hdfs\src\java\org\apache\hadoop\hdfs\ByteRangeInputStream.java	unrelated	package org apache hadoop hdfs to support http byte streams new connection http server needs created time this hides complexity multiple connections client whenever seek called new connection made successive read the normal input stream functions connected currently active input stream byte range input stream extends fs input stream url opener enum stream status protected input stream protected url opener original url protected url opener resolved url protected start pos protected current pos protected filelength stream status status stream status seek byte range input stream url url byte range input stream url opener url opener r input stream get input stream throws io exception update boolean eof n read throws io exception seek pos throws io exception get pos throws io exception boolean seek to new source target pos throws io exception
580	hdfs\src\java\org\apache\hadoop\hdfs\CorruptFileBlockIterator.java	unrelated	package org apache hadoop hdfs provides iterator list corrupt file blocks this used distributed file system hdfs corrupt file block iterator implements remote iterator path dfs client dfs string path string files null file idx string cookie null path next path null calls made corrupt file block iterator dfs client dfs path path throws io exception get calls made string path string path path path path string load next throws io exception boolean next path next throws io exception
581	hdfs\src\java\org\apache\hadoop\hdfs\DeprecatedUTF8.java	unrelated	package org apache hadoop hdfs a simple wrapper around link org apache hadoop io utf this used absolutely necessary use link org apache hadoop io utf the difference using require suppress warning annotation avoid javac warning instead deprecation implied name this treated package hdfs deprecated utf extends org apache hadoop io utf deprecated utf construct given deprecated utf string construct given deprecated utf deprecated utf utf the following two mostly commonly used methods string read string data input throws io exception write string data output string throws io exception
582	hdfs\src\java\org\apache\hadoop\hdfs\DFSClient.java	unrelated	package org apache hadoop hdfs dfs client connect hadoop filesystem perform basic file tasks it uses client protocol communicate name node daemon connects directly data nodes read write block data hadoop dfs users obtain instance distributed file system uses dfs client handle filesystem tasks dfs client implements fs constants java io closeable log log log factory get log dfs client server defaults validity period l hour tcp window size kb client protocol namenode client protocol rpc namenode user group information ugi volatile boolean client running true volatile fs server defaults server defaults volatile server defaults last update string client name configuration conf socket factory socket factory replace datanode on failure dtp replace datanode on failure file system statistics stats hdfs timeout timeout value dfs operation lease renewer leaserenewer socket cache socket cache conf dfs client conf dfs client configuration conf max block acquire failures conf time io buffer size bytes per checksum write packet size socket timeout socket cache capacity wait time window msec block missing exception caught time window n cached conn retry n block write retry n block write locate following retry default block size prefetch size short default replication string task id fs permission u mask conf configuration conf conf get conf return dfs client conf a map file names link dfs output stream objects currently written client note file written single client map string dfs output stream files being written same name node get address conf conf dfs client configuration conf throws io exception name node get address conf conf same name node addr conf null dfs client inet socket address name node addr configuration conf name node addr conf null same name node addr null conf stats dfs client inet socket address name node addr configuration conf throws io exception name node addr null conf stats create new dfs client connected given name node addr rpc namenode exactly one name node addr rpc namenode must null dfs client inet socket address name node addr client protocol rpc namenode throws io exception copy required dfs client configuration dfs client conf new conf conf conf conf stats stats socket factory net utils get socket factory conf client protocol dtp replace datanode on failure replace datanode on failure get conf the hdfs timeout currently ipc timeout hdfs timeout client get timeout conf ugi user group information get current user string authority name node addr null null leaserenewer lease renewer get instance authority ugi client name leaserenewer get client name dfs client conf task id socket cache new socket cache dfs client conf socket cache capacity name node addr null rpc namenode null else name node addr null rpc namenode null else return number times client go back namenode retrieve block locations reading get max block acquire failures return dfs client conf max block acquire failures return timeout clients use writing datanodes get datanode write timeout num nodes return dfs client conf conf time get datanode read timeout num nodes return dfs client conf socket timeout get hdfs timeout return hdfs timeout string get client name return client name check open
583	hdfs\src\java\org\apache\hadoop\hdfs\DFSConfigKeys.java	heartbeat	package org apache hadoop hdfs this contains constants configuration keys used hdfs dfs config keys extends common configuration keys string dfs block size key dfs blocksize dfs block size default string dfs replication key dfs replication short dfs replication default string dfs stream buffer size key dfs stream buffer size dfs stream buffer size default string dfs bytes per checksum key dfs bytes per checksum dfs bytes per checksum default string dfs client write packet size key dfs client write packet size dfs client write packet size default string dfs client write replace datanode on failure enable key dfs client block write replace datanode failure enable boolean dfs client write replace datanode on failure enable default true string dfs client write replace datanode on failure policy key dfs client block write replace datanode failure policy string dfs client write replace datanode on failure policy default default string dfs client socket cache capacity key dfs client socketcache capacity dfs client socket cache capacity default string dfs namenode backup address key dfs namenode backup address string dfs namenode backup address default localhost string dfs namenode backup http address key dfs namenode backup http address string dfs namenode backup http address default string dfs namenode backup service rpc address key dfs namenode backup dnrpc address string dfs datanode balance bandwidthpersec key dfs datanode balance bandwidth per sec dfs datanode balance bandwidthpersec default string dfs namenode http address key dfs namenode http address string dfs namenode http address default string dfs namenode rpc address key dfs namenode rpc address string dfs namenode service rpc address key dfs namenode servicerpc address string dfs namenode max objects key dfs namenode max objects dfs namenode max objects default string dfs namenode safemode extension key dfs namenode safemode extension dfs namenode safemode extension default string dfs namenode safemode threshold pct key dfs namenode safemode threshold pct dfs namenode safemode threshold pct default f set slightly smaller value dfs namenode safemode threshold pct default populate needed replication queues exiting safe mode string dfs namenode repl queue threshold pct key string dfs namenode safemode min datanodes key dfs namenode safemode min datanodes dfs namenode safemode min datanodes default string dfs namenode secondary http address key dfs namenode secondary http address string dfs namenode secondary http address default string dfs namenode checkpoint period key dfs namenode checkpoint period dfs namenode checkpoint period default string dfs namenode checkpoint size key dfs namenode checkpoint size dfs namenode checkpoint size default string dfs namenode upgrade permission key dfs namenode upgrade permission dfs namenode upgrade permission default string dfs namenode heartbeat recheck interval key dfs namenode heartbeat recheck interval dfs namenode heartbeat recheck interval default string dfs client https keystore resource key dfs client https keystore resource string dfs client https keystore resource default ssl client xml string dfs client https need auth key dfs client https need auth boolean dfs client https need auth default false string dfs client cached conn retry key dfs client cached conn retry dfs client cached conn retry default string dfs namenode accesstime precision key dfs namenode accesstime precision dfs namenode
584	hdfs\src\java\org\apache\hadoop\hdfs\DFSInputStream.java	unrelated	package org apache hadoop hdfs dfs input stream provides bytes named file it handles negotiation namenode various datanodes necessary dfs input stream extends fs input stream socket cache socket cache dfs client dfs client boolean closed false string src prefetch size block reader block reader null boolean verify checksum located blocks located blocks null last block being written length datanode info current node null located block current located block null pos block end this variable tracks number failures since start recent user facing operation that say reset whenever user makes call stream point retry logic failure count exceeds threshold errors thrown back operation specifically counts number times client gone back namenode get new list block locations capped max block acquire failures failures time window xxx use cocurrent hash map temp fix need fix parallel accesses dfs input stream ptreads properly concurrent hash map datanode info datanode info dead nodes buffersize byte one byte buf new byte used read n cached conn retry add to dead nodes datanode info dn info dead nodes put dn info dn info dfs input stream dfs client dfs client string src buffersize boolean verify checksum dfs client dfs client verify checksum verify checksum buffersize buffersize src src socket cache dfs client socket cache prefetch size dfs client get conf prefetch size time window dfs client get conf time window n cached conn retry dfs client get conf n cached conn retry open info grab open file info namenode synchronized open info throws io exception unresolved link exception located blocks new info dfs client call get block locations dfs client namenode src prefetch size dfs client log debug enabled new info null located blocks null located blocks new info last block being written length located blocks last block complete current node null read block length one datanodes read block length located block locatedblock throws io exception locatedblock null locatedblock get locations length replica not found count locatedblock get locations length datanode info datanode locatedblock get locations namenode told us locations none know replica means hit race pipeline creation start end require exception could happened dn want report error replica not found count throw new io exception cannot obtain block length locatedblock synchronized get file length return located blocks null returns datanode stream currently reading datanode info get current datanode return current node returns block containing target position synchronized extended block get current block current located block null return current located block get block return collection blocks already located synchronized list located block get all blocks throws io exception return get block range get file length get block specified position fetch namenode cached synchronized located block get block at offset assert located blocks null located blocks null located block blk check offset offset offset get file length else offset located blocks get file length else update current position update position return blk fetch block namenode cache synchronized fetch block at offset throws io exception target block idx located blocks find block offset target block idx block cached fetch blocks located blocks new blocks new blocks dfs client call get
585	hdfs\src\java\org\apache\hadoop\hdfs\DFSOutputStream.java	heartbeat	package org apache hadoop hdfs dfs output stream creates files stream bytes the client application writes data cached internally stream data broken packets packet typically k size a packet comprises chunks each chunk typically bytes associated checksum when client application fills current packet enqueued data queue the data streamer thread picks packets data queue sends first datanode pipeline moves data queue ack queue the response processor receives acks datanodes when successful ack packet received datanodes response processor removes corresponding packet ack queue in case error outstanding packets moved ack queue a new pipeline setup eliminating bad datanode original pipeline the data streamer starts sending packets data queue dfs output stream extends fs output summer implements syncable dfs client dfs client max packets packet k total mb socket closed accessed different threads different locks volatile boolean closed false string src block size data checksum checksum data queue ack queue protected data queue lock linked list packet data queue new linked list packet linked list packet ack queue new linked list packet packet current packet null data streamer streamer current seqno last queued seqno last acked seqno bytes cur block bytes writen current block packet size write packet size including header chunks per packet volatile io exception last exception null artificial slowdown last flush offset offset flush invoked persist blocks namenode atomic boolean persist blocks new atomic boolean false volatile boolean append chunk false appending existing partial block initial file size time file open progressable progress short block replication replication factor file packet seqno sequencenumber buffer block offset in block offset block boolean last packet in block last packet block num chunks number chunks currently packet max chunks max chunks packet buffer accumulating packet checksum data byte buffer buffer wraps buf one two may non null byte buf buf pointed like follows c checksum data d payload data hhhhhccccc dddddddddddddddd checksum pos data start data pos checksum start checksum start data start data pos checksum pos heart beat seqno l create heartbeat packet packet create new packet packet pkt size chunks per pkt offset in block write data byte inarray len write checksum byte inarray len returns byte buffer contains one full packet including header byte buffer get buffer get packet last byte offset block get last byte offset block check packet heart beat packet boolean heartbeat packet string string the data streamer responsible sending data packets datanodes pipeline it retrieves new blockid block locations namenode starts streaming packets pipeline datanodes every packet sequence number associated when packets block sent acks received data streamer closes current block data streamer extends daemon volatile boolean streamer closed false extended block block length number bytes acked token block token identifier access token data output stream block stream data input stream block reply stream response processor response null volatile datanode info nodes null list targets current block array list datanode info excluded nodes new array list datanode info volatile boolean error false volatile error index block construction stage stage block construction stage bytes sent number bytes sent nodes used pipeline failed list datanode info failed new array list
586	hdfs\src\java\org\apache\hadoop\hdfs\DFSUtil.java	unrelated	package org apache hadoop hdfs dfs util thread local random random new thread local random protected random initial value random get random return random get compartor sorting data node info based decommissioned states decommissioned nodes moved end array sorting compartor comparator datanode info decom comparator new comparator datanode info whether pathname valid currently prohibits relative paths names contain boolean valid name string src path must absolute src starts with path separator check string tokenizer tokens new string tokenizer src path separator tokens more tokens return true utility facilitate junit test error simulation error simulator boolean simulation null error simulation events initialize error simulation event number of events boolean get error simulation index set error simulation index clear error simulation index converts byte array using utf encoding string bytes string byte bytes try catch unsupported encoding exception e return null converts byte array using utf encoding byte bytes string str try catch unsupported encoding exception e return null given list path components returns path utf string string byte array string byte path components path components length path components length path components length try catch unsupported encoding exception ex return null splits array bytes array arrays bytes byte separator byte bytes byte array byte bytes byte separator return bytes byte array bytes bytes length separator splits first len bytes bytes array arrays bytes byte separator byte bytes byte array byte bytes assert len bytes length splits len count splits omit multiple separators last one len last len last bytes last separator splits bytes separator splits byte result new byte splits start index next index index build splits index splits return result convert located blocks block locations block location located blocks locations located blocks blocks blocks null nr blocks blocks located block count block location blk locations new block location nr blocks nr blocks idx located block blk blocks get located blocks return blk locations returns collection nameservice ids configuration collection string get name service ids configuration conf return conf get string collection dfs federation nameservices given list keys order preference returns value key given order configuration string get conf value string default value string key suffix string value null string key keys value null return value returns list inet socket address given set keys list inet socket address get addresses configuration conf collection string nameservice ids get name service ids conf list inet socket address isas new array list inet socket address configuration single namenode nameservice ids null nameservice ids empty else return isas returns list inet socket address corresponding backup node rpc addresses configuration list inet socket address get backup node addresses list inet socket address address list get addresses conf address list null return address list returns list inet socket addresses corresponding secondary namenode http addresses configuration list inet socket address get secondary name node addresses list inet socket address address list get addresses conf null address list null return address list returns list inet socket addresses corresponding namenodes configuration note used datanodes get list namenode addresses talk returns namenode address specifically configured datanodes using service ports found if regular rpc address
587	hdfs\src\java\org\apache\hadoop\hdfs\DistributedFileSystem.java	unrelated	package org apache hadoop hdfs implementation file system dfs system this object way end user code interacts hadoop distributed file system distributed file system extends file system path working dir uri uri dfs client dfs boolean verify checksum true hdfs configuration init distributed file system distributed file system inet socket address namenode configuration conf throws io exception initialize name node get uri namenode conf uri get uri return uri initialize uri uri configuration conf throws io exception super initialize uri conf set conf conf string host uri get host host null inet socket address namenode name node get address uri get authority dfs new dfs client namenode conf statistics uri uri create fs constants hdfs uri scheme uri get authority working dir get home directory permit paths explicitly specify default port protected check path path path uri uri get uri uri uri path uri string authority uri get authority uri get scheme null super check path path normalize paths explicitly specify default port path make qualified path path uri uri get uri uri uri path uri string authority uri get authority uri get scheme null return super make qualified path path get working directory return working dir get default block size return dfs get default block size short get default replication return dfs get default replication path make absolute path f f absolute else set working directory path dir string result make absolute dir uri get path dfs util valid name result working dir make absolute dir inherit doc path get home directory return make qualified new path user dfs ugi get short user name string get path name path file check path file string result make absolute file uri get path dfs util valid name result return result block location get file block locations file status file start file null return get file block locations file get path start len block location get file block locations path p statistics increment read ops return dfs get block locations get path name p start len set verify checksum boolean verify checksum verify checksum verify checksum start lease recovery file boolean recover lease path f throws io exception return dfs recover lease get path name f fs data input stream open path f buffer size throws io exception statistics increment read ops return new dfs client dfs data input stream this optional operation yet supported fs data output stream append path f buffer size statistics increment write ops dfs output stream op dfs append get path name f buffer size progress return new fs data output stream op statistics op get initial len fs data output stream create path f fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception statistics increment write ops return new fs data output stream dfs create get path name f permission protected fs data output stream primitive create path f fs permission absolute permission enum set create flag flag buffer size short replication block size progressable progress bytes per checksum throws io exception statistics increment read ops return new fs data output
588	hdfs\src\java\org\apache\hadoop\hdfs\HdfsConfiguration.java	heartbeat	package org apache hadoop hdfs adds deprecated keys configuration hdfs configuration extends configuration add deprecated keys adds default resources configuration add default resource hdfs default xml configuration add default resource hdfs site xml hdfs configuration super hdfs configuration boolean load defaults super load defaults hdfs configuration configuration conf super conf this method invoked hdfs configuration loaded already previously loaded upon loading initializer block executed add deprecated keys add default resources it safe method called multiple times initializer block get invoked this replaces previously dangerous practice calling configuration add default resource hdfs default xml directly without loading hdfs configuration first thereby skipping key deprecation init deprecate string old key string new key configuration add deprecation old key new string new key add deprecated keys deprecate dfs backup address dfs config keys dfs namenode backup address key deprecate dfs backup http address dfs config keys dfs namenode backup http address key deprecate dfs balance bandwidth per sec dfs config keys dfs datanode balance bandwidthpersec key deprecate dfs data dir dfs config keys dfs datanode data dir key deprecate dfs http address dfs config keys dfs namenode http address key deprecate dfs https address dfs config keys dfs namenode https address key deprecate dfs max objects dfs config keys dfs namenode max objects key deprecate dfs name dir dfs config keys dfs namenode name dir key deprecate dfs name dir restore dfs config keys dfs namenode name dir restore key deprecate dfs name edits dir dfs config keys dfs namenode edits dir key deprecate dfs read prefetch size dfs config keys dfs client read prefetch size key deprecate dfs safemode extension dfs config keys dfs namenode safemode extension key deprecate dfs safemode threshold pct dfs config keys dfs namenode safemode threshold pct key deprecate dfs secondary http address dfs config keys dfs namenode secondary http address key deprecate dfs socket timeout dfs config keys dfs client socket timeout key deprecate fs checkpoint dir dfs config keys dfs namenode checkpoint dir key deprecate fs checkpoint edits dir dfs config keys dfs namenode checkpoint edits dir key deprecate fs checkpoint period dfs config keys dfs namenode checkpoint period key deprecate fs checkpoint size dfs config keys dfs namenode checkpoint size key deprecate dfs upgrade permission dfs config keys dfs namenode upgrade permission key deprecate heartbeat recheck interval dfs config keys dfs namenode heartbeat recheck interval key deprecate storage id dfs config keys dfs datanode storageid key deprecate dfs https client keystore resource dfs config keys dfs client https keystore resource key deprecate dfs https need client auth dfs config keys dfs client https need auth key deprecate slave host name dfs config keys dfs datanode host name key deprecate session id dfs config keys dfs metrics session id key deprecate dfs access time precision dfs config keys dfs namenode accesstime precision key deprecate dfs replication consider load dfs config keys dfs namenode replication considerload key deprecate dfs replication interval dfs config keys dfs namenode replication interval key deprecate dfs replication min dfs config keys dfs namenode replication min key deprecate dfs replication pending timeout sec dfs config
589	hdfs\src\java\org\apache\hadoop\hdfs\HDFSPolicyProvider.java	unrelated	package org apache hadoop hdfs link policy provider hdfs protocols hdfs policy provider extends policy provider service hdfs services service get services
590	hdfs\src\java\org\apache\hadoop\hdfs\HftpFileSystem.java	unrelated	package org apache hadoop hdfs an implementation protocol accessing filesystems http the following implementation provides limited read filesystem http hftp file system extends file system http url connection set follow redirects true string nn http url uri hdfs uri protected inet socket address nn addr protected user group information ugi string hftp timezone utc string hftp date format yyyy mm dd t hh mm ss z token delegation token identifier delegation token string hftp service name key hdfs service host simple date format get date format simple date format df new simple date format hftp date format df set time zone time zone get time zone hftp timezone return df protected thread local simple date format df new thread local simple date format protected simple date format initial value renewer thread renewer new renewer thread renewer start protected get default port return dfs config keys dfs https port default string get canonical service name return security util build dt service name hdfs uri get default port string build uri string schema string host port string builder sb new string builder schema return sb append host append append port string initialize uri name configuration conf throws io exception super initialize name conf set conf conf ugi user group information get current user nn addr net utils create socket addr name string case open connection hftp different cluster need know cluster https port set assume cluster port url port conf get int dfs hftp https port url port nn http url log debug using url get dt nn http url one uses rpc port different default one one specify setvice name delegation token otherwise hostname rpc port string key hftp file system hftp service name key security util build dt service name name dfs config keys dfs https port default log debug enabled string nn service name conf get key nn port name node default port nn service name null get real port try catch uri syntax exception ue user group information security enabled synchronized token get delegation token string renewer throws io exception try catch interrupted exception e uri get uri try catch uri syntax exception e return url pointing given path namenode url get namenode file url path p throws io exception return get namenode url data p uri get path return url pointing given path namenode url get namenode url string path string query throws io exception try catch uri syntax exception e ugi parameter http connection string get ugi parameter string builder ugi paramenter new string builder ugi get short user name string g ugi get group names return ugi paramenter string void throw io exception from connection code connection get response code string connection get response message throw null ioe open http connection namenode read file data metadata protected http url connection open connection string path string query query update query query url url get namenode url path query http url connection connection http url connection url open connection try catch io exception ioe return connection protected string update query string query throws io exception string token string null
591	hdfs\src\java\org\apache\hadoop\hdfs\HsftpFileSystem.java	unrelated	package org apache hadoop hdfs an implementation protocol accessing filesystems https the following implementation provides limited read filesystem https hsftp file system extends hftp file system mm seconds per day volatile exp warn days initialize uri name configuration conf throws io exception super initialize name conf setup ssl conf exp warn days conf get int ssl expiration warn days set ssl resources setup ssl configuration conf throws io exception configuration ssl conf new hdfs configuration false ssl conf add resource conf get dfs config keys dfs client https keystore resource key file input stream fis null try catch exception e finally protected http url connection open connection string path string query try catch uri syntax exception e uri get uri try catch uri syntax exception e dummy hostname verifier used bypass hostname checking protected dummy hostname verifier implements hostname verifier boolean verify string hostname ssl session session dummy trustmanager used trust server certificates protected dummy trust manager implements x trust manager check client trusted x certificate chain string auth type check server trusted x certificate chain string auth type x certificate get accepted issuers
592	hdfs\src\java\org\apache\hadoop\hdfs\LeaseRenewer.java	unrelated	package org apache hadoop hdfs p used link dfs client renewing file written leases namenode when file opened write create append namenode stores file lease recording identity writer the writer e dfs client required renew lease periodically when lease renewed expires namenode considers writer failed may either let another writer obtain lease close file p p this also provides following functionality ul li it maintains map namenode user pairs lease renewers the link lease renewer instance used renewing lease link dfs client namenode user li li each renewer maintains list link dfs client periodically leases clients renewed a client removed list client closed li li a thread per namenode per user used link lease renewer renew leases li ul p lease renewer log log log factory get log lease renewer lease renewer grace default l lease renewer sleep default l get link lease renewer instance lease renewer get instance string authority factory string clien name postfix dfs util get random next int the time milliseconds map became empty empty time long max value a fixed lease renewal time period milliseconds renewal fs constants lease softlimit period a daemon renewing lease daemon daemon null only daemon current id run current id grace period sleep period factory key factorykey a list clients corresponding renewer list dfs client dfsclients new array list dfs client string instantiation trace lease renewer factory key factorykey synchronized get renewal time string get client name string id add client synchronized add client dfs client dfsc synchronized boolean clients running synchronized get sleep period set grace period adjust sleep period accordingly synchronized set grace sleep period grace period is daemon running synchronized boolean running used tests synchronized string get daemon name is empty period longer grace period synchronized boolean renewer expired synchronized put string src dfs output stream close file close file string src dfs client dfsc close given client synchronized close client dfs client dfsc interrupt and join throws interrupted exception renew throws io exception run id throws interrupted exception string string get names clients synchronized string clients string
593	hdfs\src\java\org\apache\hadoop\hdfs\SocketCache.java	unrelated	package org apache hadoop hdfs a cache sockets socket cache log log log factory get log socket cache linked list multimap socket address socket multimap capacity create socket cache given capacity socket cache capacity get cached socket given address synchronized socket get socket address remote give unused socket cache synchronized put socket sock synchronized size evict oldest entry cache synchronized evict oldest empty cache close sockets synchronized clear protected finalize
594	hdfs\src\java\org\apache\hadoop\hdfs\protocol\AlreadyBeingCreatedException.java	unrelated	package org apache hadoop hdfs protocol the exception happens ask create file already created closed yet already being created exception extends io exception serial version uid x ad l already being created exception string msg
595	hdfs\src\java\org\apache\hadoop\hdfs\protocol\Block.java	unrelated	package org apache hadoop hdfs protocol a block hadoop fs primitive identified block implements writable comparable block string block file prefix blk string metadata extension meta register ctor pattern block file pattern pattern pattern meta file pattern pattern boolean block filename file f filename id string name boolean meta filename string name get generation stamp name metafile name get generation stamp string meta file get block id name metafile name get block id string meta file block id num bytes generation stamp block block blkid len generation stamp block blkid block block blk find blockid given filename block file f len genstamp set blkid len gen stamp get block id set block id bid string get block name get num bytes set num bytes len get generation stamp set generation stamp stamp string string writable write data output throws io exception read fields data input throws io exception write helper data output throws io exception read helper data input throws io exception write identifier part block write id data output throws io exception read identifier part block read id data input throws io exception compare to block b boolean equals object hash code
596	hdfs\src\java\org\apache\hadoop\hdfs\protocol\BlockListAsLongs.java	unrelated	package org apache hadoop hdfs protocol this provides accessing list blocks implemented this useful block report rather send block reports block send the structure array follows length finalized replica list length construction replica list followed finalized replica list replica represented longs one block id one block length one generation stamp followed invalid replica represented three followed construction replica list replica represented longs three block id length generation stamp forth replica state block list as longs implements iterable block longs per finalized block longs per uc block number longs header header size index block id block index block list block list as longs list extends block finalized block list as longs block list as longs block list get block list as longs block report iterator implements iterator block iterator block iterator block report iterator get block report iterator get number of blocks get number of finalized replicas get number of uc replicas block id index block length index block generation stamp index replica state block replica state index get block id index get block len index get block gen stamp index t extends block set block index t b set delimiting block finalized szie
597	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ClientDatanodeProtocol.java	pooling	package org apache hadoop hdfs protocol an client datanode protocol block recovery client datanode protocol extends versioned protocol log log log factory get log client datanode protocol added delete block pool method version id l return visible length replica get replica visible length extended block b throws io exception refresh list federated namenodes updated configuration adds new namenodes stops deleted namenodes refresh namenodes throws io exception delete block pool directory if force false deleted empty otherwise deleted along contents e contain block files otherwise deleted along contents delete block pool string bpid boolean force throws io exception
598	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ClientProtocol.java	pooling	package org apache hadoop hdfs protocol client protocol used user code via link org apache hadoop hdfs distributed file system communicate name node user code manipulate directory namespace well open close file streams etc client protocol extends versioned protocol version id l file contents located blocks get block locations string src fs server defaults get server defaults throws io exception create string src fs permission masked string client name located block append string src string client name boolean set replication string src short replication set permission string src fs permission permission set owner string src string username string groupname abandon block extended block b string src string holder located block add block string src string client name located block get additional datanode string src extended block blk boolean complete string src string client name extended block last report bad blocks located block blocks throws io exception namespace management boolean rename string src string dst concat string trg string srcs rename string src string dst options rename options boolean delete string src boolean delete string src boolean recursive boolean mkdirs string src fs permission masked boolean create parent directory listing get listing string src system issues management renew lease string client name throws access control exception boolean recover lease string src string client name throws io exception get stats capacity idx get stats used idx get stats remaining idx get stats under replicated idx get stats corrupt blocks idx get stats missing blocks idx get stats throws io exception datanode info get datanode report fs constants datanode report type type get preferred block size string filename boolean set safe mode fs constants safe mode action action save namespace throws access control exception io exception boolean restore failed storage string arg throws access control exception refresh nodes throws io exception finalize upgrade throws io exception upgrade status report distributed upgrade progress upgrade action action corrupt file blocks meta save string filename throws io exception hdfs file status get file info string src throws access control exception hdfs file status get file link info string src content summary get content summary string path set quota string path namespace quota diskspace quota fsync string src string client set times string src mtime atime create symlink string target string link fs permission dir perm string get link target string path throws access control exception located block update block for pipeline extended block block update pipeline string client name extended block old block token delegation token identifier get delegation token text renewer renew delegation token token delegation token identifier token cancel delegation token token delegation token identifier token
599	hdfs\src\java\org\apache\hadoop\hdfs\protocol\CorruptFileBlocks.java	unrelated	package org apache hadoop hdfs protocol contains list paths corresponding corrupt files cookie used iterative calls name node list corrupt file blocks corrupt file blocks implements writable used hash code prime string files string cookie corrupt file blocks corrupt file blocks string files string cookie string get files string get cookie inherit doc read fields data input throws io exception inherit doc write data output throws io exception inherit doc boolean equals object obj inherit doc hash code
600	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DatanodeID.java	unrelated	package org apache hadoop hdfs protocol datanode id composed data node name hostname port number data storage id currently represents datanode id implements writable comparable datanode id datanode id empty array string name hostname port number string storage id unique per cluster storage id protected info port port infoserver running ipc port port ipc server running equivalent datanode id datanode id equivalent datanode id node name datanode id string node name node name datanode id copy constructor datanode id datanode id create datanode id datanode id string node name string storage id set name string name set info port info port set ipc port ipc port string get name string get storage id get info port get ipc port sets data storage id set storage id string storage id string get host get port boolean equals object hash code string string update fields new registration request comes note update storage id update reg info datanode id node reg comparable basis compare string name host port number compare to datanode id writable inherit doc write data output throws io exception inherit doc read fields data input throws io exception
601	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DatanodeInfo.java	pooling	package org apache hadoop hdfs protocol datanode info represents status data node this object used communication datanode protocol client protocol datanode info extends datanode id implements node protected capacity protected dfs used protected remaining protected block pool used protected last update protected xceiver count protected string location network topology default rack host name supplied datanode registration name namenode uses datanode ip address name protected string host name null administrative states datanode enum admin states protected admin states admin state datanode info datanode info datanode info datanode info datanode id node id protected datanode info datanode id node id string location string host name the raw capacity get capacity return capacity the used space data node get dfs used return dfs used the used space block pool data node get block pool used return block pool used the used space data node get non dfs used the used space data node percentage present capacity get dfs used percent the raw free space get remaining return remaining used space block pool percentage present capacity get block pool used percent the remaining space percentage configured capacity get remaining percent the time information accurate get last update return last update number active connections get xceiver count return xceiver count sets raw capacity set capacity capacity sets used space datanode set dfs used dfs used sets raw free space set remaining remaining sets block pool used space set block pool used bp used sets time information accurate set last update last update sets number active connections set xceiver count xceiver count rack name synchronized string get network location return location sets rack name synchronized set network location string location string get host name set host name string host a formatted reporting status data node string get datanode report a formatted printing status data node string dump datanode start decommissioning node old state start decommission stop decommissioning node old state stop decommission returns true node process decommissioned boolean decommission in progress returns true node decommissioned boolean decommissioned sets admin state indicate decommission complete set decommissioned retrieves admin state node admin states get admin state sets admin state node protected set admin state admin states new state transient level level tree node resides transient node parent parent return node parent node get parent return parent set parent node parent parent parent return node level tree e g root tree returns children return get level return level set level level level level writable register ctor inherit doc write data output throws io exception inherit doc read fields data input throws io exception read datanode info datanode info read data input throws io exception hash code boolean equals object obj
602	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DirectoryListing.java	unrelated	package org apache hadoop hdfs protocol this defines partial listing directory support iterative directory listing directory listing implements writable register ctor hdfs file status partial listing remaining entries directory listing directory listing hdfs file status partial listing hdfs file status get partial listing get remaining entries boolean more byte get last name writable read fields data input throws io exception write data output throws io exception
603	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DSQuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol ds quota exceeded exception extends quota exceeded exception protected serial version uid l ds quota exceeded exception ds quota exceeded exception string msg ds quota exceeded exception quota count string get message
604	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ExtendedBlock.java	pooling	package org apache hadoop hdfs protocol identifies block uniquely across block pools extended block implements writable string pool id block block register ctor extended block extended block extended block b extended block string pool id block id extended block string pool id block b extended block string pool id blkid len write data output throws io exception read fields data input throws io exception write identifier part block write id data output throws io exception read identifier part block read id data input throws io exception string get block pool id returns block file name block string get block name get num bytes get block id get generation stamp set block id bid set generation stamp gen stamp set num bytes len set string pool id block blk block get local block extended block b block get local block boolean equals object hash code string string
605	hdfs\src\java\org\apache\hadoop\hdfs\protocol\FSConstants.java	heartbeat	package org apache hadoop hdfs protocol some handy constants fs constants min blocks for write long indicates leave current quota unchanged quota dont set long max value quota reset l timeouts constants heartbeat interval blockreport interval blockreport initial delay lease softlimit period lease hardlimit period lease softlimit period lease recover period ms we need limit length depth path filesystem hadoop currently set maximum length k characters maximum depth k max path length max path depth buffer size new hdfs configuration get int io file buffer size used writing header etc small buffer size math min buffer size todo mb media style com conf injected default block size default bytes per checksum default write packet size short default replication factor default file buffer size default data socket size size of integer integer size byte size safe mode actions enum safe mode action safemode leave safemode enter safemode get type datanode report enum datanode report type all live dead distributed upgrade actions get upgrade status get detailed upgrade status proceed upgrade stuck matter status enum upgrade action uri scheme hdfs namenode ur is string hdfs uri scheme hdfs please see link layout version adding new layout version layout version
606	hdfs\src\java\org\apache\hadoop\hdfs\protocol\FSLimitException.java	unrelated	package org apache hadoop hdfs protocol abstract deriving exceptions related filesystem constraints fs limit exception extends quota exceeded exception protected serial version uid l protected fs limit exception protected fs limit exception string msg protected fs limit exception quota count path component too long exception extends fs limit exception max directory items exceeded exception extends fs limit exception
607	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsFileStatus.java	unrelated	package org apache hadoop hdfs protocol interface represents wire information file hdfs file status implements writable register ctor byte path local name inode encoded java utf byte symlink symlink target encoded java utf null length boolean isdir short block replication blocksize modification time access time fs permission permission string owner string group byte empty name new byte default constructor hdfs file status constructor hdfs file status length boolean isdir block replication get length file bytes get len is directory boolean dir is symbolic link boolean symlink get block size file get block size get replication factor file short get replication get modification time file get modification time get access time file get access time get fs permission associated file fs permission get permission get owner file string get owner get group associated file string get group check local name empty boolean empty local name get representation local name string get local name get java utf representation local name byte get local name in bytes get representation full path name string get full name string parent get full path path get full path path parent get representation symlink string get symlink writable write data output throws io exception read fields data input throws io exception
608	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsLocatedFileStatus.java	unrelated	package org apache hadoop hdfs protocol interface represents wire information including block locations file hdfs located file status extends hdfs file status located blocks locations default constructor hdfs located file status constructor hdfs located file status length boolean isdir located blocks get block locations writable write data output throws io exception read fields data input throws io exception
609	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsProtoUtil.java	pooling	package org apache hadoop hdfs protocol utilities converting protocol buffers used hdfs wire protocol well generic utilities useful dealing protocol buffers hdfs proto util block token hdfs protos block token identifier proto proto token block token token block token identifier proto hdfs protos block token identifier proto proto extended block hdfs protos extended block proto proto extended block block extended block proto hdfs protos extended block proto proto datanode id hdfs protos datanode id proto proto datanode id proto hdfs protos datanode id proto id proto datanode info hdfs protos datanode info proto proto datanode info dni datanode info proto hdfs protos datanode info proto dni proto array list extends hdfs protos datanode info proto protos datanode info protos input stream vint prefixed input stream input throws io exception
610	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LayoutVersion.java	unrelated	package org apache hadoop hdfs protocol this tracks changes layout version hdfs layout version changed following reasons ol li the layout namenode datanode stores information disk changes li li a new operation code added editlog li li modification format record content record editlog fsimage li ol br b how update layout version br b when change requires new layout version please add entry link feature short enum name new layout version description change please see link feature details br layout version enum feature build layout version corresponding feature matrix map integer enum set feature map static initialization init map special init lv feature f string get string boolean supports feature f lv get current layout version
611	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LocatedBlock.java	unrelated	package org apache hadoop hdfs protocol a located block pair block datanode info objects it tells find block located block implements writable register ctor extended block b offset offset first byte block file datanode info locs corrupt flag true replicas block corrupt else false if block corrupt replicas filtered locations part object boolean corrupt token block token identifier block token new token block token identifier located block located block string bpid block b datanode info locs located block extended block b datanode info locs located block extended block b datanode info locs start offset located block extended block b datanode info locs start offset token block token identifier get block token set block token token block token identifier token extended block get block datanode info get locations get start offset get block size set start offset value set corrupt boolean corrupt boolean corrupt writable write data output throws io exception read fields data input throws io exception read located block located block read data input throws io exception inherit doc string string
612	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LocatedBlocks.java	unrelated	package org apache hadoop hdfs protocol collection blocks locations file length located blocks implements writable file length list located block blocks array blocks prioritized locations boolean construction located block last located block null boolean last block complete false located blocks constructor located blocks flength boolean under constuction get located blocks list located block get located blocks get last located block located block get last located block is last block completed boolean last block complete get located block located block get index get number located blocks located block count get file length return ture file construction located blocks constructed false otherwise boolean under construction find block containing specified offset find block offset insert range block idx list located block new blocks get insert index bin search result writable register ctor write data output throws io exception read fields data input throws io exception inherit doc string string
613	hdfs\src\java\org\apache\hadoop\hdfs\protocol\NSQuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol ns quota exceeded exception extends quota exceeded exception protected serial version uid l ns quota exceeded exception ns quota exceeded exception string msg ns quota exceeded exception quota count string get message
614	hdfs\src\java\org\apache\hadoop\hdfs\protocol\QuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol this exception thrown modification hdfs results violation directory quota a directory quota might namespace quota limit number files directories diskspace quota limit space taken file directory tree br br the message exception specifies directory quota violated actual quotas specific message generated corresponding exception ds quota exceeded exception ns quota exceeded exception quota exceeded exception extends io exception protected serial version uid l protected string path name null protected quota quota protected count actual value protected quota exceeded exception protected quota exceeded exception string msg protected quota exceeded exception quota count set path name string path string get message
615	hdfs\src\java\org\apache\hadoop\hdfs\protocol\RecoveryInProgressException.java	unrelated	package org apache hadoop hdfs protocol exception indicating replica already recovery recovery in progress exception extends io exception serial version uid l recovery in progress exception string msg
616	hdfs\src\java\org\apache\hadoop\hdfs\protocol\UnregisteredNodeException.java	unrelated	package org apache hadoop hdfs protocol this exception thrown node previously registered trying access name node unregistered node exception extends io exception serial version uid l unregistered node exception node registration node reg the exception thrown different data node claims storage id existing one unregistered node exception datanode id node id datanode info stored node
617	hdfs\src\java\org\apache\hadoop\hdfs\protocol\UnresolvedPathException.java	unrelated	package org apache hadoop hdfs protocol thrown symbolic link encountered path unresolved path exception extends unresolved link exception serial version uid l string original path the original path containing link string link target the target link string remaining path the path part following link used remote exception instantiate unresolved path exception unresolved path exception string msg unresolved path exception string original path string remaining path path get unresolved path throws io exception path get resolved path throws io exception string get message
618	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\BlockConstructionStage.java	unrelated	package org apache hadoop hdfs protocol datatransfer block construction stage enum block construction stage the enumerates always listed regular stage followed recovery stage changing order make get recovery stage working pipeline set block append pipeline setup append pipeline set failed pipeline setup append recovery pipeline setup append recovery data streaming data streaming pipeline setup failed data streaming recovery pipeline setup streaming recovery close block pipeline pipeline close recover failed pipeline close pipeline close recovery pipeline set block creation pipeline setup create transfer rbw adding datanodes transfer rbw transfer finalized adding datanodes transfer finalized byte recovery bit byte get recovery stage stage block construction stage get recovery stage
619	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\DataTransferProtocol.java	unrelated	package org apache hadoop hdfs protocol datatransfer transfer data datanode using streaming protocol data transfer protocol log log log factory get log data transfer protocol version data transfers clients datanodes this change serialization datanode info protocol changes it obvious version declare methods data transfer protocol data transfer version read block read block extended block blk write block datanode pipeline write block extended block blk transfer block another datanode the block stage must either link block construction stage transfer rbw link block construction stage transfer finalized transfer block extended block blk receive block source datanode notifies namenode remove copy original datanode note source datanode original datanode different it used balancing purpose replace block extended block blk copy block it used balancing purpose copy block extended block blk get block checksum md crc block checksum extended block blk
620	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\DataTransferProtoUtil.java	unrelated	package org apache hadoop hdfs protocol datatransfer static utilities dealing protocol buffers used data transfer protocol data transfer proto util block construction stage proto op write block proto block construction stage proto client operation header proto build client header extended block blk base header proto build base header extended block blk
621	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Op.java	unrelated	package org apache hadoop hdfs protocol datatransfer operation enum op write block byte read block byte read metadata byte replace block byte copy block byte block checksum byte transfer block byte the code operation byte code op byte code first code values code return object represented code op value of byte code read op read data input throws io exception write write data output throws io exception
622	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\PacketHeader.java	unrelated	package org apache hadoop hdfs protocol datatransfer header data packet goes read write pipelines packet header header size packet proto size pkt header len packet len packet header proto proto packet header packet header packet len offset in block seqno get data len boolean last packet in block get seqno get offset in block get packet len string string read fields byte buffer buf throws io exception read fields data input stream throws io exception write header buffer this requires pkt header len bytes available put in buffer byte buffer buf write data output stream throws io exception perform sanity check packet returning true sane sequence number larger boolean sanity check last seq no boolean equals object hash code
623	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\PipelineAck.java	unrelated	package org apache hadoop hdfs protocol datatransfer pipeline acknowledgment pipeline ack pipeline ack proto proto unkown seqno default constructor pipeline ack constructor pipeline ack seqno status replies get sequence number get seqno get number replies short get num of replies get ith reply status get reply check ack contains error status boolean success writable read fields input stream throws io exception write output stream throws io exception string string
624	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Receiver.java	unrelated	package org apache hadoop hdfs protocol datatransfer receiver receiver implements data transfer protocol protected data input stream create receiver data transfer protocol socket protected receiver data input stream read op it also checks protocol version protected op read op throws io exception process op corresponding method protected process op op op throws io exception receive op read block op read block throws io exception receive op write block op write block data input stream throws io exception receive link op transfer block op transfer block data input stream throws io exception receive op replace block op replace block data input stream throws io exception receive op copy block op copy block data input stream throws io exception receive op block checksum op block checksum data input stream throws io exception
625	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\ReplaceDatanodeOnFailure.java	unrelated	package org apache hadoop hdfs protocol datatransfer the setting replace datanode failure feature enum replace datanode on failure the feature disabled entire site disable never add new datanode never default policy let r replication number let n number existing datanodes add new datanode r either floor r n r n block hflushed appended default always add new datanode existing datanode removed always check feature enabled check enabled is policy satisfied boolean satisfy get setting configuration replace datanode on failure get configuration conf write setting configuration write configuration conf
626	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Sender.java	unrelated	package org apache hadoop hdfs protocol datatransfer sender sender implements data transfer protocol data output stream create sender data transfer protocol output stream sender data output stream initialize operation op data output op op send data output stream op opcode read block extended block blk write block extended block blk transfer block extended block blk replace block extended block blk copy block extended block blk block checksum extended block blk
627	hdfs\src\java\org\apache\hadoop\hdfs\protocol\proto\DataTransferProtos.java	unrelated	package org apache hadoop hdfs protocol proto data transfer protos data transfer protos register all extensions enum status success error error checksum error invalid error exists error access token checksum ok success value error value error checksum value error invalid value error exists value error access token value checksum ok value get number return value status value of value com google protobuf internal enum lite map status com google protobuf internal enum lite map status com google protobuf descriptors enum value descriptor com google protobuf descriptors enum descriptor com google protobuf descriptors enum descriptor status values status value of index value status index value base header proto or builder required extended block proto block boolean block org apache hadoop hdfs protocol proto hdfs protos extended block proto get block org apache hadoop hdfs protocol proto hdfs protos extended block proto or builder get block or builder optional block token identifier proto token boolean token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto get token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto or builder get token or builder base header proto extends use base header proto new builder construct base header proto builder builder base header proto boolean init base header proto default instance base header proto get default instance base header proto get default instance for type com google protobuf descriptors descriptor protected com google protobuf generated message field accessor table bit field required extended block proto block block field number org apache hadoop hdfs protocol proto hdfs protos extended block proto block boolean block org apache hadoop hdfs protocol proto hdfs protos extended block proto get block org apache hadoop hdfs protocol proto hdfs protos extended block proto or builder get block or builder optional block token identifier proto token token field number org apache hadoop hdfs protocol proto hdfs protos block token identifier proto token boolean token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto get token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto or builder get token or builder init fields byte memoized is initialized boolean initialized write to com google protobuf coded output stream output memoized serialized size get serialized size serial version uid l protected java lang object write replace boolean equals java lang object obj hash code org apache hadoop hdfs protocol proto data transfer protos base header proto parse from org apache hadoop hdfs protocol proto data transfer protos base header proto parse from org apache hadoop hdfs protocol proto data transfer protos base header proto parse from byte data org apache hadoop hdfs protocol proto data transfer protos base header proto parse from org apache hadoop hdfs protocol proto data transfer protos base header proto parse from java io input stream input org apache hadoop hdfs protocol proto data transfer protos base header proto parse from org apache hadoop hdfs protocol proto data transfer protos base header proto parse delimited from java io input stream input org apache hadoop hdfs protocol proto data transfer protos base
628	hdfs\src\java\org\apache\hadoop\hdfs\protocol\proto\HdfsProtos.java	pooling	package org apache hadoop hdfs protocol proto hdfs protos hdfs protos register all extensions extended block proto or builder required pool id boolean pool id string get pool id required uint block id boolean block id get block id required uint num bytes boolean num bytes get num bytes required uint generation stamp boolean generation stamp get generation stamp extended block proto extends use extended block proto new builder construct extended block proto builder builder extended block proto boolean init extended block proto default instance extended block proto get default instance extended block proto get default instance for type com google protobuf descriptors descriptor protected com google protobuf generated message field accessor table bit field required pool id poolid field number java lang object pool id boolean pool id string get pool id com google protobuf byte string get pool id bytes required uint block id blockid field number block id boolean block id get block id required uint num bytes numbytes field number num bytes boolean num bytes get num bytes required uint generation stamp generationstamp field number generation stamp boolean generation stamp get generation stamp init fields byte memoized is initialized boolean initialized write to com google protobuf coded output stream output memoized serialized size get serialized size serial version uid l protected java lang object write replace boolean equals java lang object obj hash code org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from byte data org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from java io input stream input org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse delimited from java io input stream input org apache hadoop hdfs protocol proto hdfs protos extended block proto parse delimited from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from org apache hadoop hdfs protocol proto hdfs protos extended block proto parse from builder new builder return builder create builder new builder for type return new builder builder new builder org apache hadoop hdfs protocol proto hdfs protos extended block proto prototype builder builder return new builder protected builder new builder for type builder extends throw new null pointer exception bit field x block token identifier proto or builder required bytes identifier boolean identifier com google protobuf byte string get identifier required bytes password boolean password com google protobuf byte string get password required kind boolean kind string get kind required service boolean service string get service block token identifier proto extends use block token identifier proto new builder construct block token identifier proto builder builder block token identifier proto boolean init block token identifier proto default instance block token identifier proto get default instance block token identifier proto get default instance for
629	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockKey.java	unrelated	package org apache hadoop hdfs security token block key used generating verifying block tokens block key extends delegation key block key block key key id expiry date secret key key
630	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockPoolTokenSecretManager.java	pooling	package org apache hadoop hdfs security token block manages link block token secret manager per block pool routes requests given block pool id corresponding link block token secret manager block pool token secret manager extends map string block token secret manager map add block pool id corresponding link block token secret manager map synchronized add block pool string bpid synchronized block token secret manager get string bpid return empty block token identifer block token identifier create identifier byte create password block token identifier identifier byte retrieve password block token identifier identifier see link block token secret manager check access block token identifier string extended block access mode check access block token identifier id string user id see link block token secret manager check access token string extended block access mode check access token block token identifier token see link block token secret manager set keys exported block keys set keys string bpid exported block keys exported keys see link block token secret manager generate token extended block enum set token block token identifier generate token extended block b
631	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenIdentifier.java	pooling	package org apache hadoop hdfs security token block block token identifier extends token identifier text kind name new text hdfs block token expiry date key id string user id string block pool id block id enum set access mode modes byte cache block token identifier block token identifier string user id string bpid block id text get kind user group information get user get expiry date set expiry date expiry date get key id set key id key id string get user id string get block pool id get block id enum set access mode get access modes string string boolean equal object object b inherit doc boolean equals object obj inherit doc hash code read fields data input throws io exception write data output throws io exception byte get bytes
632	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenSecretManager.java	pooling	package org apache hadoop hdfs security token block block token secret manager instantiated modes master mode slave mode master generate new block keys export block keys slaves slaves import use block keys received master both master slave generate verify block tokens typically master mode used nn slave mode used dn block token secret manager extends log log log factory token block token identifier dummy token new token block token identifier boolean master key update interval interval nn updates block keys it set enough live dn balancer sync ed block keys nn least interval key update interval volatile token lifetime serial no new secure random next int block key current key block key next key map integer block key keys enum access mode constructor block token secret manager boolean master key update interval initialize block keys synchronized generate keys export block keys used master mode synchronized exported block keys export keys synchronized remove expired keys set block keys used slave mode synchronized set keys exported block keys exported keys update block keys used master mode synchronized update keys throws io exception generate block token current user token block token identifier generate token extended block block generate block token specified user token block token identifier generate token string user id check access allowed user id checked null this method check token password correct it used token password already verified e g rpc layer check access block token identifier id string user id check access allowed user id checked null check access token block token identifier token string user id boolean expired expiry date check token expired unit test return true token expired false otherwise boolean token expired token block token identifier token set token lifetime set token lifetime token lifetime create empty block token identifier block token identifier create identifier create new password secret given block token identifier block token identifier protected byte create password block token identifier identifier look token password secret given block token identifier block token identifier look byte retrieve password block token identifier identifier
633	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenSelector.java	unrelated	package org apache hadoop hdfs security token block a block token selector hdfs block token selector implements token selector block token identifier token block token identifier select token text service
634	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\ExportedBlockKeys.java	unrelated	package org apache hadoop hdfs security token block object passing block keys exported block keys implements writable exported block keys dummy keys new exported block keys boolean block token enabled key update interval token lifetime block key current key block key keys exported block keys exported block keys boolean block token enabled key update interval boolean block token enabled get key update interval get token lifetime block key get current key block key get all keys writable register ctor write data output throws io exception read fields data input throws io exception
635	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\InvalidBlockTokenException.java	unrelated	package org apache hadoop hdfs security token block access token verification failed invalid block token exception extends io exception serial version uid l invalid block token exception invalid block token exception string msg
636	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenIdentifier.java	unrelated	package org apache hadoop hdfs security token delegation a delegation token identifier specific hdfs delegation token identifier text hdfs delegation kind new text hdfs delegation token create empty delegation token identifier reading delegation token identifier create new delegation token identifier delegation token identifier text owner text renewer text real user text get kind string string string stringify token token token throws io exception
637	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenSecretManager.java	unrelated	package org apache hadoop hdfs security token delegation a hdfs specific delegation token secret manager the secret manager responsible generating accepting password token delegation token secret manager log log log factory fs namesystem namesystem create secret manager secret keys tokens expired tokens delegation token secret manager delegation key update interval delegation token identifier create identifier returns expiry time token given identifier synchronized get token expiry time load secret manager state fsimage synchronized load secret manager state data input stream store current state secret manager persistence synchronized save secret manager state data output stream this method intended used reading edit logs fsimage synchronized add persisted delegation token add master key list keys synchronized update persisted master key delegation key key update token cache renewal record edit logs synchronized update persisted token renewal update token cache cancel record edit logs synchronized update persisted token cancellation returns number delegation keys currently stored synchronized get number of keys private helper methods save delegation keys tokens fsimage synchronized save current tokens data output stream save current state keys synchronized save all keys data output stream private helper methods load delegation tokens fsimage synchronized load current tokens data input stream private helper method load delegation keys fsimage synchronized load all keys data input stream throws io exception call namesystem update editlogs new master key protected log update master key delegation key key
638	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenSelector.java	unrelated	package org apache hadoop hdfs security token delegation a delegation token specialized hdfs delegation token selector delegation token selector
639	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\Balancer.java	heartbeat	package org apache hadoop hdfs server balancer p the balancer tool balances disk space usage hdfs cluster datanodes become full new empty nodes join cluster the tool deployed application program run cluster administrator live hdfs cluster applications adding deleting files p synopsis pre to start bin start balancer sh threshold threshold example bin start balancer sh start balancer default threshold bin start balancer sh threshold start balancer threshold to stop bin stop balancer sh pre p description p the threshold parameter fraction range default value the threshold sets target whether cluster balanced a cluster balanced datanode utilization node ratio used space node total capacity node differs utilization ratio used space cluster total capacity cluster threshold value the smaller threshold balanced cluster become it takes time run balancer small threshold values also small threshold cluster may able reach balanced state applications write delete files concurrently p the tool moves blocks highly utilized datanodes poorly utilized datanodes iteratively in iteration datanode moves receives lesser g bytes threshold fraction capacity each iteration runs minutes at end iteration balancer obtains updated datanodes information namenode p a system property limits balancer use bandwidth defined default configuration file pre property name dfs balance bandwidth per sec name value value description specifies maximum bandwidth datanode utilize balancing purpose term number bytes per second description property pre p this property determines maximum speed block moved one datanode another the default value mb the higher bandwidth faster cluster reach balanced state greater competition application processes if administrator changes value property configuration file change observed hdfs next restarted p monitering balancer progress p after balancer started output file name balancer progress recorded printed screen the administrator monitor running balancer reading output file the output shows balancer status iteration iteration in iteration prints starting time iteration number total number bytes moved previous iterations total number bytes left move order cluster balanced number bytes moved iteration normally bytes already moved increasing bytes left to move decreasing p running multiple instances balancer hdfs cluster prohibited tool p the balancer automatically exits following five conditions satisfied ol li the cluster balanced li no block moved li no block moved five consecutive iterations li an io exception occurs communicating namenode li another balancer running ol p upon exit balancer returns exit code prints one following messages output file corresponding exit reasons ol li the cluster balanced exiting li no block moved exiting li no block moved iterations exiting li received io exception failure reason exiting li another balancer running exiting ol p the administrator interrupt execution balancer time running command stop balancer sh machine balancer running balancer log log log factory get log balancer max blocks size to fetch l gb win width l hour the maximum number concurrent blocks moves balancing purpose datanode max num concurrent moves name node connector nnc balancing policy policy threshold data node lists collection source utilized datanodes collection source avg utilized datanodes collection balancer datanode avg utilized datanodes collection balancer datanode utilized datanodes collection source sources collection balancer datanode targets map block balancer block global block list moved blocks moved blocks
640	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\BalancingPolicy.java	pooling	package org apache hadoop hdfs server balancer balancing policy since datanode may contain multiple block pools link pool implies link node not way around balancing policy total capacity total used space avg utilization reset total capacity l total used space l avg utilization get policy name string get name accumulate used space capacity accumulate spaces datanode info init avg utilization avg utilization total used space total capacity get avg utilization return avg utilization return utilization datanode get utilization datanode info string string return balancing policy get simple name get link balancing policy instances balancing policy parse string balancing policy balancing policy node instance balancing policy p throw new illegal argument exception cannot parse cluster balanced node balanced node extends balancing policy node instance new node node string get name accumulate spaces datanode info get utilization datanode info cluster balanced pool node balanced pool extends balancing policy pool instance new pool pool string get name accumulate spaces datanode info get utilization datanode info
641	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\NameNodeConnector.java	pooling	package org apache hadoop hdfs server balancer the provides utilities link balancer access name node name node connector log log balancer log path balancer id path new path system balancer id inet socket address namenode address string blockpool id namenode protocol namenode client protocol client file system fs output stream boolean block token enabled boolean run key updater interval block token secret manager block token secret manager daemon keyupdaterthread access key updater thread name node connector inet socket address namenode address configuration conf get access token block token block token identifier get access token extended block eb the idea making sure one balancer output stream check and mark running balancer throws io exception close connection close string string build namenode protocol connection namenode namenode protocol create namenode inet socket address address block key updater implements runnable
642	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockInfo.java	unrelated	package org apache hadoop hdfs server blockmanagement internal block metadata block info extends block implements light weight g set linked element i node file inode for implementing link light weight g set linked element light weight g set linked element next linked element object triplets block info replication block info block blk replication protected block info block info i node file get i node set i node i node file inode datanode descriptor get datanode index block info get previous index block info get next index set datanode index datanode descriptor node set previous index block info set next index block info get capacity ensure capacity num num nodes boolean add node datanode descriptor node boolean remove node datanode descriptor node find datanode datanode descriptor dn block info list insert block info head datanode descriptor dn block info list remove block info head datanode descriptor dn boolean list is consistent datanode descriptor dn block uc state get block uc state boolean complete block info under construction convert to block under construction hash code boolean equals object obj light weight g set linked element get next set next light weight g set linked element next
643	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockInfoUnderConstruction.java	unrelated	package org apache hadoop hdfs server blockmanagement represents block currently constructed br this usually last block file opened write append block info under construction extends block info block state see link block uc state block uc state block uc state block replicas assigned block allocated this defines pipeline order list replica under construction replicas a data node responsible block recovery primary node index the new generation stamp block recovery succeeds also used recovery id identify right recovery abandoned recoveries appear block recovery id replica under construction contains information replicas construction the gs length state replica reported data node it guaranteed expected data nodes actually corresponding replicas replica under construction extends block datanode descriptor expected location replica state state replica under construction block block expected block replica location assigned block allocated this defines pipeline order it guaranteed expected data node actually replica datanode descriptor get expected location get replica state reported data node replica state get state set replica state set state replica state is data node replica belongs alive boolean alive hash code boolean equals object obj inherit doc string string create block set state link block uc state under construction block info under construction block blk replication blk replication block uc state under construction null create block currently constructed block info under construction block blk replication super blk replication assert get block uc state block uc state complete block uc state state set expected locations targets convert construction block complete block generation stamp length committed client least minimal number replicas reported data nodes block info convert to complete block throws io exception assert get block uc state block uc state complete get block uc state block uc state committed return new block info set expected locations set expected locations datanode descriptor targets num locations targets null targets length replicas new array list replica under construction num locations num locations create array expected replica locations assigned choose targets datanode descriptor get expected locations num locations replicas null replicas size datanode descriptor locations new datanode descriptor num locations num locations return locations get number expected locations get num expected locations return replicas null replicas size return state block construction block uc state get block uc state return block uc state set block uc state block uc state block uc state get block recovery id get block recovery id return block recovery id commit block length generation stamp reported client set block state link block uc state committed commit block block block throws io exception get block id block get block id block uc state block uc state committed set get block id block get num bytes block get generation stamp initialize lease recovery block find first alive data node starting previous primary make primary initialize block recovery recovery id set block uc state block uc state under recovery block recovery id recovery id replicas size previous primary node index replicas size add replica if not present datanode descriptor dn replica under construction r replicas replicas add new replica under construction block dn r state block info under construction participates maps way block info
644	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockManager.java	unrelated	package org apache hadoop hdfs server blockmanagement keeps information related blocks stored hadoop cluster this helper link fs namesystem requires several methods called lock held link fs namesystem block manager log log log factory get log block manager default load factor map default map load factor f fs namesystem namesystem volatile pending replication blocks count l volatile corrupt replica blocks count l volatile replicated blocks count l volatile scheduled replication blocks count l volatile excess blocks count l volatile pending deletion blocks count l used metrics get pending replication blocks count used metrics get under replicated blocks count used metrics get corrupt replica blocks count used metrics get scheduled replication blocks count used metrics get pending deletion blocks count used metrics get excess blocks count replication recheck interval often namenode checks new replication work replication recheck interval blocks map blocks map datanode manager datanode manager replication thread daemon replication thread new daemon new replication monitor store blocks datanodedescriptor map corrupt replicas corrupt replicas map corrupt replicas new corrupt replicas map keeps collection every named machine containing blocks recently invalidated thought live machine question mapping storage id array list block map string collection block recent invalidate sets keeps tree set every named node each treeset contains list blocks extra location we eventually remove extras mapping storage id tree set block map string collection block excess replicate map store set blocks need replicated times we also store pending replication orders under replicated blocks needed replications new under replicated blocks pending replication blocks pending replications the maximum number replicas allowed block max replication the maximum number outgoing replication streams max replication streams minimum copies needed else write disallowed min replication default number replicas default replication the maximum number entries returned get corrupt inodes max corrupt files returned variable enable check enough racks boolean check for enough racks last block index used replication work repl index block replicas placement block placement policy blockplacement block manager fs namesystem fsn configuration conf throws io exception activate configuration conf close datanode manager get datanode manager block placement policy get block placement policy set block placement policy set block placement policy block placement policy newpolicy meta save print writer boolean check min replication block block commit block i node file under construction file i node commit or complete last block i node file under construction file i node block info complete block i node file file i node blk index throws io exception block info complete block i node file file i node block info block throws io exception located block convert last block to under construction list string get valid locations block block list located block get block locations block info blocks offset located block get block location block info blk pos blocks with locations get blocks with locations datanode id datanode remove datanode remove datanode datanode descriptor node remove from invalidates string storage id block block boolean belongs to invalidates string storage id block block add to invalidates block b datanode info dn boolean log add to invalidates block b datanode info dn add to invalidates block b
645	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicy.java	unrelated	package org apache hadoop hdfs server blockmanagement this used choosing desired number targets placing block replicas block placement policy log log log factory get log block placement policy not enough replicas exception extends exception serial version uid l not enough replicas exception string msg choose num of replicas data nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path same link choose target string datanode descriptor list boolean hash map return chosen nodes equal false datanode descriptor choose target string src path return choose target src path num of replicas writer chosen nodes false choose num of replicas data nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path choose num of replicas data nodes writer if return many the base implemenatation extracts pathname file specified src inode could costly operation depending file system implementation concrete implementations method avoid overhead sorted pipeline datanode descriptor choose target fs inode info src inode return choose target src inode get full path name num of replicas writer verify block replicated least min racks different racks min racks rack system block replicated verify block placement string src path decide whether deleting specified replica block still makes block conform configured block placement policy datanode descriptor choose replica to delete fs inode info src inode used setup block placement policy object this defined implementations block placement policy protected initialize configuration conf fs cluster stats stats get instance configured block placement policy based value configuration paramater dfs block replicator classname block placement policy get instance configuration conf class extends block placement policy replicator class block placement policy replicator block placement policy reflection utils new instance replicator initialize conf stats cluster map return replicator choose num of replicas nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path return choose target src path num of replicas writer choose num of replicas nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path return choose target src path num of replicas writer
646	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicyDefault.java	unrelated	package org apache hadoop hdfs server blockmanagement the responsible choosing desired number targets placing block replicas the replica placement strategy writer datanode st replica placed local machine otherwise random datanode the nd replica placed datanode different rack the rd replica placed datanode different node rack second replica block placement policy default extends block placement policy boolean consider load network topology cluster map fs cluster stats stats string enable debug logging for information please enable block placement policy default configuration conf fs cluster stats stats block placement policy default inherit doc initialize configuration conf fs cluster stats stats thread local string builder thread local builder inherit doc datanode descriptor choose target string src path inherit doc datanode descriptor choose target string src path inherit doc datanode descriptor choose target fs inode info src inode this implementation datanode descriptor choose target num of replicas choose num of replicas data nodes datanode descriptor choose target num of replicas choose local machine target local machine available choose node rack datanode descriptor choose local node choose one node rack local machine node available choose one node rack second replica still node available choose random node cluster datanode descriptor choose local rack choose num of replicas nodes racks local machine not enough nodes available choose remaining ones local rack choose remote rack num of replicas randomly choose one target nodes datanode descriptor choose random randomly choose num of replicas targets nodes choose random num of replicas judge node good target return true node enough space much load rack many nodes boolean good target datanode descriptor node boolean good target datanode descriptor node return pipeline nodes the pipeline formed finding shortest path starts writer traverses nodes this basically traveling salesman problem datanode descriptor get pipeline inherit doc verify block placement string src path inherit doc datanode descriptor choose replica to delete fs inode info inode
647	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlocksMap.java	unrelated	package org apache hadoop hdfs server blockmanagement this maintains map block metadata block metadata currently includes i node belongs datanodes store block blocks map node iterator implements iterator datanode descriptor constant link light weight g set capacity capacity g set block block info blocks blocks map load factor compute capacity close i node file get i node block b block info add i node block info b i node file node remove block block block returns block object exists map block info get stored block block b iterator datanode descriptor node iterator block b iterator datanode descriptor node iterator block info stored block counts number containing nodes better using iterator num nodes block b boolean remove node block b datanode descriptor node size iterable block info get blocks boolean contains block block boolean contains block block datanode descriptor datanode get capacity hash map stores blocks get capacity block info replace block block info new block
648	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\CorruptReplicasMap.java	unrelated	package org apache hadoop hdfs server blockmanagement stores information corrupt blocks file system a block considered corrupt replicas corrupt while reporting replicas block hide corrupt copies these copies removed block found expected number good replicas mapping block tree set datanode descriptor corrupt replicas map sorted map block collection datanode descriptor corrupt replicas map mark block belonging datanode corrupt add to corrupt replicas map block blk datanode descriptor dn remove block corrupt blocks map remove from corrupt replicas map block blk remove block given datanode corrupt block map boolean remove from corrupt replicas map block blk datanode descriptor datanode get nodes corrupt replicas block collection datanode descriptor get nodes block blk check replica belonging datanode corrupt boolean replica corrupt block blk datanode descriptor node num corrupt replicas block blk size return range corrupt replica block ids up num expected blocks blocks starting next block starting block id returned fewer num expected blocks blocks unavailable if starting block id null num expected blocks blocks returned beginning if starting block id cannot found null returned num expected blocks beginning get corrupt replica block ids num expected blocks
649	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DatanodeDescriptor.java	unrelated	package org apache hadoop hdfs server blockmanagement datanode descriptor tracks stats given data node available storage capacity last update time etc maintains set blocks stored datanode this data structure internal namenode it sent wire client datanodes neither stored persistently fs image datanode descriptor extends datanode info stores status decommissioning if node decommissioning use object anything decommissioning status decommissioning status new decommissioning status block targets pair block target pair block block datanode descriptor targets block target pair block block datanode descriptor targets a block target pair queue block queue e queue e blockq new linked list e size queue synchronized size return blockq size enqueue synchronized boolean offer e e dequeue synchronized list e poll num blocks returns tt true tt queue contains specified element boolean contains e e volatile block info block list null num blocks alive heartbeats contains this optimization contains takes o n time arraylist boolean alive false boolean need key update false a queue blocks replicated datanode block queue block target pair replicate blocks new block queue block target pair a queue blocks recovered datanode block queue block info under construction recover blocks a set blocks invalidated datanode set block invalidate blocks new tree set block variables maintaining number blocks scheduled written datanode this count approximate might slightly bigger case errors e g datanode report error occurs writing block curr approx blocks scheduled prev approx blocks scheduled last blocks scheduled roll time blocks scheduled roll interval min volume failures when set true node list allowed communicate namenode boolean disallowed false default constructor datanode descriptor datanode descriptor constructor datanode descriptor datanode id node id node id l l l l datanode descriptor constructor datanode descriptor datanode id node id node id network location null datanode descriptor constructor datanode descriptor datanode id node id node id network location host name l l l l datanode descriptor constructor datanode descriptor datanode id node id super node id update heartbeat capacity dfs used remaining bpused xceiver count datanode descriptor constructor datanode descriptor datanode id node id super node id network location host name update heartbeat capacity dfs used remaining bpused xceiver count add datanode block add block head list blocks belonging data node boolean add block block info b b add node add head data node list block list b list insert block list num blocks return true remove block list blocks belonging data node remove datanode block boolean remove block block info b block list b list remove block list b remove node else move block head list blocks belonging data node move block to head block info b block list b list remove block list block list b list insert block list replace specified old block new one data node descriptor block info replace block block info old block block info new block boolean done remove block old block assert done old block belong data node replacing done add block new block assert done new block belong data node replacing return new block reset blocks capacity remaining block pool used dfs used xceiver count block list null invalidate blocks clear volume
650	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DatanodeManager.java	heartbeat	package org apache hadoop hdfs server blockmanagement manage datanodes decommission activities datanode manager log log log factory get log datanode manager fs namesystem namesystem stores datanode block map p done storing set link datanode descriptor objects sorted storage id in order keep storage map consistent tracks storages ever registered namenode a descriptor corresponding specific storage id ul li added map new storage id li li updated new datanode started replacement old one storage id li li removed existing datanode restarted serve different storage id li ul br the list link datanode descriptor map checkpointed namespace image file only link datanode info part persistent list blocks restored datanode block reports p mapping storage id datanode descriptor navigable map string datanode descriptor datanode map cluster network topology network topology networktopology new network topology host names datanode descriptors mapping host nodes map host datanode map new host nodes map dns to switch mapping dns to switch mapping read exclude files hosts file reader hosts reader the period wait datanode heartbeat heartbeat expire interval ask datanode many blocks delete block invalidate limit datanode manager fs namesystem namesystem configuration conf daemon decommissionthread null activate configuration conf close network topology get network topology sort located blocks distance target host sort located blocks string targethost cyclic iteration string datanode descriptor get datanode cyclic iteration datanode descriptor get datanode by host string host get datanode descriptor given corresponding storage id datanode descriptor get datanode string storage id get data node storage id datanode descriptor get datanode datanode id node id prints information datanodes datanode dump print writer remove dead datanode remove dead datanode datanode id node id is datanode dead boolean datanode dead datanode descriptor node add datanode add datanode datanode descriptor node physically remove node datanode map wipe datanode datanode id node throws io exception resolve node network location resolve network location datanode descriptor node boolean hosts list datanode id node string ip addr boolean excluded hosts list datanode id node string ip addr remove already decommissioned data node neither exclude hosts lists list live dead nodes this used display already decommssioned data node operators the operation procedure making already decommissioned data node displayed following ol li host must hosts list hosts list must empty li li host decommissioned remaining hosts list added exclude hosts list name node updated new information issuing dfsadmin refresh nodes command li li host removed hosts exclude hosts lists name node updated new informationby issuing dfsamin refresh nodes command li ol array list live dead nodes remove decom node from list list datanode descriptor node list check given node datanode id ip address exclude list if ip address null check based upon given datanode id if ip address null ip address refers host given datanode id refers datanode id refers boolean check in list datanode id node decommission node exclude list check decommissioning datanode descriptor node reg string ip addr generate new storage id note collisions still possible somebody try bring data storage different cluster string new storage id register datanode datanode registration node reg reread exclude files refresh hosts reader configuration conf throws
651	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DecommissionManager.java	unrelated	package org apache hadoop hdfs server blockmanagement manage node decommissioning decommission manager log log log factory get log decommission manager fs namesystem fsnamesystem block manager block manager decommission manager fs namesystem namesystem fsnamesystem namesystem block manager fsnamesystem get block manager periodically check decommission status monitor implements runnable recheck interval often namenode checks node finished decommission recheck interval the number decommission nodes check interval num nodes per check firstkey initialized anything string firstkey monitor recheck interval in second num nodes per check check decommission status num nodes per check nodes every recheck interval milliseconds run check
652	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\Host2NodesMap.java	unrelated	package org apache hadoop hdfs server blockmanagement a map host names datanode descriptors host nodes map hash map string datanode descriptor map read write lock hostmap lock new reentrant read write lock check node already map boolean contains datanode descriptor node add node map return true node added false otherwise boolean add datanode descriptor node remove node map return true node removed false otherwise boolean remove datanode descriptor node get data node host datanode descriptor get datanode by host string host find data node name datanode descriptor get datanode by name string name
653	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\NumberReplicas.java	unrelated	package org apache hadoop hdfs server blockmanagement a immutable object stores number live replicas number decommissined replicas number replicas live replicas decommissioned replicas corrupt replicas excess replicas number replicas number replicas live decommissioned corrupt excess initialize live decommissioned corrupt excess live replicas decommissioned replicas corrupt replicas excess replicas
654	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\PendingReplicationBlocks.java	unrelated	package org apache hadoop hdfs server blockmanagement pending replication blocks bookkeeping blocks getting replicated it following record blocks getting replicated instant coarse grain timer track age replication request thread periodically identifies replication requests never made pending replication blocks log log block manager log map block pending block info pending replications array list block timed out items daemon timer thread null volatile boolean fs running true it might take anywhere minutes request timed timeout default recheck interval pending replication blocks timeout period timeout period pending replications new hash map block pending block info timed out items new array list block start timer thread new daemon new pending replication monitor timer thread start add block list pending replications add block block num replicas synchronized pending replications one replication request block finished decrement number pending replication requests block remove block block synchronized pending replications the total number blocks undergoing replication size return pending replications size how many copies block pending replication get num replicas block block synchronized pending replications return returns list blocks timed replication requests returns null blocks timed block get timed out blocks synchronized timed out items an object contains information block replicated it records timestamp system started replicating recent copy block it also records number replication requests progress pending block info time stamp num replicas in progress pending block info num replicas get time stamp set time stamp increment replicas increment decrement replicas get num replicas a periodic thread scans blocks never finished replication request pending replication monitor implements runnable run iterate items detect timed items pending replication check shuts pending replication monitor thread waits thread exit stop fs running false timer thread null return timer thread interrupt try catch interrupted exception ie iterate items print meta save print writer synchronized pending replications
655	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\UnderReplicatedBlocks.java	unrelated	package org apache hadoop hdfs server blockmanagement class keeping track replication blocks blocks replication priority priority indicating highest blocks one replicas highest under replicated blocks implements iterable block level queue with corrupt blocks list tree set block priority queues new array list tree set block constructor under replicated blocks level empty queues clear level return total number replication blocks synchronized size size level return size return number replication blocks excluding corrupt blocks synchronized get under replicated block count size queue with corrupt blocks return size return number corrupt blocks synchronized get corrupt block size return priority queues get queue with corrupt blocks size check block needed replication queue synchronized boolean contains block block tree set block set priority queues return false return priority block get priority block block assert cur replicas negative replicas cur replicas expected replicas else cur replicas else cur replicas else cur replicas expected replicas else add block replication queue according priority synchronized boolean add assert cur replicas negative replicas pri level get priority block cur replicas decomissioned replicas pri level level priority queues get pri level add block return false remove block replication queue synchronized boolean remove block block pri level get priority block old replicas return remove block pri level remove block replication queue given priority boolean remove block block pri level pri level pri level level else return false update priority level block synchronized update block block cur replicas old replicas cur replicas cur replicas delta old expected replicas cur expected replicas expected replicas delta cur pri get priority block cur replicas decommissioned replicas cur expected replicas old pri get priority block old replicas decommissioned replicas old expected replicas name node state change log debug enabled old pri level old pri cur pri cur pri level priority queues get cur pri add block returns iterator blocks given priority queue synchronized block iterator iterator level return new block iterator level return iterator replication blocks synchronized block iterator iterator return new block iterator block iterator implements iterator block level boolean iterator for level false list iterator block iterators new array list iterator block block iterator block iterator update block next boolean next remove get priority
656	hdfs\src\java\org\apache\hadoop\hdfs\server\common\GenerationStamp.java	unrelated	package org apache hadoop hdfs server common a generation stamp hadoop fs primitive identified generation stamp implements comparable generation stamp the first valid generation stamp first valid stamp l generation stamp blocks pre date introduction generation stamp grandfather generation stamp volatile genstamp create new instance initialized first valid stamp generation stamp create new instance initialized specified value generation stamp stamp returns current generation stamp get stamp sets current generation stamp set stamp stamp first increments counter returns stamp synchronized next stamp compare to generation stamp boolean equals object hash code
657	hdfs\src\java\org\apache\hadoop\hdfs\server\common\HdfsConstants.java	unrelated	package org apache hadoop hdfs server common some handy internal hdfs constants hdfs constants type node enum node type startup options enum startup option timeouts communicating data node streaming writes reads read timeout read timeout extension write timeout write timeout extension write pipeline dn keepalive timeout defines name node role enum namenode role block replica states go constructed enum replica state states block go construction enum block uc state string namenode lease holder hdfs name node namenode lease recheck interval
658	hdfs\src\java\org\apache\hadoop\hdfs\server\common\InconsistentFSStateException.java	unrelated	package org apache hadoop hdfs server common the exception thrown file system state inconsistent recoverable inconsistent fs state exception extends io exception serial version uid l inconsistent fs state exception file dir string descr inconsistent fs state exception file dir string descr throwable ex string get file path file dir
659	hdfs\src\java\org\apache\hadoop\hdfs\server\common\IncorrectVersionException.java	unrelated	package org apache hadoop hdfs server common the exception thrown external version match current version application incorrect version exception extends io exception serial version uid l incorrect version exception version reported string what incorrect version exception version reported incorrect version exception string version reported
660	hdfs\src\java\org\apache\hadoop\hdfs\server\common\JspHelper.java	authenticate	package org apache hadoop hdfs server common jsp helper string current conf current conf string web ugi property name dfs config keys dfs web ugi key string delegation parameter name delegation string namenode address nnaddr string set delegation delegation parameter name log log log factory get log jsp helper private constructor preventing creating jsp helper object jsp helper data structure count number blocks datanodes node record extends datanode info frequency node record node record datanode info info count boolean equals object obj hash code compare two records based frequency node record comparator implements comparator node record compare node record node record datanode info best node located blocks blks throws io exception hash map datanode info node record map located block block blks get located blocks node record nodes map values array new node record map size arrays sort nodes new node record comparator return best node nodes false datanode info best node located block blk throws io exception datanode info nodes blk get locations return best node nodes true datanode info best node datanode info nodes boolean random throws io exception tree set datanode info dead nodes new tree set datanode info datanode info chosen node null failures socket null index nodes null nodes length null close return chosen node stream block in ascii inet socket address addr string pool id chunk size to view return socket new socket connect addr hdfs constants read timeout set so timeout hdfs constants read timeout byte buf new byte amt to read read offset retries amt to read block reader null close print html quoting quote html chars new string buf add table header jsp writer throws io exception print table border print tbody add table row jsp writer string columns throws io exception print tr columns length print tr add table row jsp writer string columns row throws io exception print tr columns length print tr add table footer jsp writer throws io exception print tbody table sort node list array list datanode descriptor nodes node comapare implements comparator datanode descriptor collections sort nodes new node comapare field order print path with links string dir jsp writer try catch unsupported encoding exception ex print goto form jsp writer print form action browse directory jsp method get name goto print goto print input name dir type text width id dir value file print input name go type submit value go print input name namenode info port type hidden user group information security enabled print input name namenode address type hidden print form create title jsp writer file null file start math max file length start print title hdfs file title convert string chunk size view chunk size to view string default value n null integer parse int return n n default value return table containing version information string get version table return div id dfstable table validate filename otherwise return validated filename string validate path string p return p null p length validate value otherwise return validated long object long validate long string value return value null null long parse long value validate url otherwise
661	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Storage.java	unrelated	package org apache hadoop hdfs server common storage information file p local storage information stored separate file version it contains type node storage layout version namespace id fs state creation time p local storage reside multiple directories each directory contain version file others during startup hadoop servers name node data nodes read local storage information p the servers hold lock storage directory run nodes able startup sharing storage the locks released servers stop normally abnormally storage extends storage info log log log factory get log storage get name constants last layout version support upgrades last pre upgrade layout version corresponds hadoop last upgradable layout version protected string last upgradable hadoop version hadoop removed last upgradable lv goes beyond upgrade code uses constant also removed pre generationstamp layout version layout versions release layout versions string storage file lock use lock protected string storage file version version string storage dir current current string storage dir previous previous string storage tmp removed removed tmp string storage tmp previous previous tmp string storage tmp finalized finalized tmp string storage tmp last ckpt lastcheckpoint tmp string storage previous ckpt previous checkpoint enum storage state non existent not formatted complete upgrade recover upgrade complete finalize complete rollback recover rollback complete checkpoint recover checkpoint normal an denote storage directory type implementations define type storage directory implementing storage dir type storage dir type get storage dir type boolean of type storage dir type type protected node type storage type type node using storage protected list storage directory storage dirs new array list storage directory dir iterator implements iterator storage directory storage dir type dir type prev index remove next index next dir iterator storage dir type dir type boolean next storage directory next remove return default iterator this iterator returns entries storage dirs iterator storage directory dir iterator return dir iterator null return iterator based storage directory type this iterator selects entries storage dirs type dir type returns via iterator iterator storage directory dir iterator storage dir type dir type return new dir iterator dir type generate storage list debug line string list storage directories string builder buf new string builder storage directory sd storage dirs return buf string one storage directories storage directory file root root directory boolean use lock flag enable storage lock storage dir type dir type storage dir type file lock lock storage lock storage directory file dir storage directory file dir storage dir type dir type constructor disables locking storage directory file dir storage dir type dir type boolean use lock get root directory storage file get root get storage directory type storage dir type get storage dir type read file storage storage throws io exception clear create storage directory p removes contents current directory creates empty directory this fully format storage directory it cannot write version file since written last storage type dependent files written derived storage responsible setting specific storage values writing version file disk clear directory throws io exception directory code current contains latest files defining file system meta data file get current dir file code version contains following fields ol li
662	hdfs\src\java\org\apache\hadoop\hdfs\server\common\StorageInfo.java	unrelated	package org apache hadoop hdfs server common common storage information todo namespace id computed hash address port storage info implements writable layout version layout version storage data namespace id id file system string cluster id id cluster c time creation time file system state storage info storage info layout v ns id string cid c t storage info storage info get layout version return layout version get namespace id return namespace id string get cluster id return cluster id get c time return c time set storage info storage info writable write data output throws io exception read fields data input throws io exception string string
663	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Upgradeable.java	unrelated	package org apache hadoop hdfs server common common distributed upgrade objects each upgrade object corresponds layout version latest version upgraded using object that components whose layout version greater equal one returned link get version must upgraded object upgradeable extends comparable upgradeable get layout version upgrade object get version get type software component object upgrading hdfs constants node type get type description upgrade object displaying string get description upgrade status determines percentage work done total amount required upgrade means upgrade completed any value means complete the return value provide least values e g short get upgrade status prepare upgrade e g initialize upgrade data structures set status returns upgrade command used broadcasting cluster components e g name node informs data nodes must perform distributed upgrade upgrade command start upgrade throws io exception complete upgrade e g cleanup upgrade data structures write metadata disk returns upgrade command used broadcasting cluster components e g data nodes inform name node completed upgrade data nodes still upgrading upgrade command complete upgrade throws io exception get status report upgrade false otherwise upgrade status report get upgrade status report boolean details throws io exception
664	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeManager.java	unrelated	package org apache hadoop hdfs server common generic upgrade manager link broadcast command command upgrade manager protected sorted set upgradeable current upgrades null protected boolean upgrade state false true upgrade progress protected upgrade version protected upgrade command broadcast command null synchronized upgrade command get broadcast command synchronized boolean get upgrade state synchronized get upgrade version synchronized set upgrade state boolean u state u version sorted set upgradeable get distributed upgrades throws io exception synchronized short get upgrade status synchronized boolean initialize upgrade throws io exception synchronized boolean upgrade completed hdfs constants node type get type boolean start upgrade throws io exception complete upgrade throws io exception
665	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeObject.java	unrelated	package org apache hadoop hdfs server common abstract upgrade object contains default implementation common methods link upgradeable upgrade object implements upgradeable protected short status short get upgrade status string get description upgrade status report get upgrade status report boolean details compare to upgradeable boolean equals object hash code
666	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeObjectCollection.java	unrelated	package org apache hadoop hdfs server common collection upgrade objects upgrade objects registered used upgrade object collection initialize registered distributed upgrade objects register upgrade new upgrade object uo signature implements comparable uo signature version hdfs constants node type type string name uo signature upgradeable uo get version hdfs constants node type get type string get class name upgradeable instantiate throws io exception compare to uo signature boolean equals object static collection upgrade objects sorted version layout versions negative therefore newer versions go first sorted set uo signature upgrade table initialize upgrade table new tree set uo signature register upgrade upgradeable uo registered distributed upgrade objects upgrade table add new uo signature uo sorted set upgradeable get distributed upgrades version from assert fs constants layout version version from incorrect version sorted set upgradeable upgrade objects new tree set upgradeable uo signature sig upgrade table upgrade objects size return upgrade objects
667	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeStatusReport.java	unrelated	package org apache hadoop hdfs server common base upgrade upgrade status overload specific status fields need reported describes status current upgrade upgrade status report implements writable protected version protected short upgrade status protected boolean finalized upgrade status report upgrade status report version short status boolean finalized get version short get upgrade status boolean finalized string get status text boolean details string string writable register ctor write data output throws io exception read fields data input throws io exception
668	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Util.java	unrelated	package org apache hadoop hdfs server common util log log log factory get log util get name current system time interprets passed uri in case error assumes specified file uri as uri string throws io exception converts passed file uri uri file as uri file f throws io exception converts collection strings collection ur is collection uri collection as ur is
669	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockMetadataHeader.java	unrelated	package org apache hadoop hdfs server datanode block metadata header manages metadata data blocks datanodes this related block related functionality namenode the biggest part data block metadata crc block block metadata header short metadata version fs dataset metadata version header includes everything except checksum version two bytes following data checksum occupies bytes short version data checksum checksum null block metadata header short version data checksum checksum short get version data checksum get checksum this reads fields till beginning checksum block metadata header read header data input stream throws io exception reads header top metadata file returns header block metadata header read header file file throws io exception version already read block metadata header read header short version data input stream this writes fields till beginning checksum write header data output stream writes fields till beginning checksum write header data output stream data checksum checksum returns size header get header size
670	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockPoolSliceScanner.java	unrelated	package org apache hadoop hdfs server datanode performs two types scanning li gets block files data directories reconciles difference blocks disk memory link fs dataset li li scans data directories block files block pool verifies files corrupt li this keeps track blocks last verification times currently modify metadata block block pool slice scanner log log log factory get log block pool slice scanner max scan rate mb per sec min scan rate mb per sec default scan period hours l three weeks string block pool id string date format string yyyy mm dd hh mm ss sss string verification log file dncp block verification log verfication log limit num blocks scan period default scan period hours data node datanode fs dataset dataset sorted set tree set block scan info block info set hash map block block scan info block map processed blocks keeps track blocks scanned since last run hash map long integer processed blocks total scans total scan errors total transient errors total blocks scanned in last run used test current period start system current time millis bytes left bytes scan period total bytes to scan log file handler verification log data transfer throttler throttler null enum scan type verification scan scanned part periodic verfication none block scan info implements comparable block scan info block block last scan time scan type last scan type scan type none boolean last scan ok true block scan info block block hash code boolean equals object get last scan time compare to block scan info block pool slice scanner data node datanode fs dataset dataset configuration conf datanode datanode dataset dataset block pool id bpid scan period conf get int dfs config keys dfs datanode scan period hours key scan period scan period log info periodic block verification scan initialized interval scan period string get block pool id return block pool id synchronized boolean initialized return throttler null update bytes to scan len last scan time len could negative block deleted total bytes to scan len last scan time current period start should change throttler bandwidth every time bytes left changes really required synchronized add block info block scan info info boolean added block info set add info block map put info block info added synchronized del block info block scan info info boolean exists block info set remove info block map remove info block exists update block map given log entry synchronized update block info log entry e block scan info info block map get new block e block id e gen stamp info null e verification time init throws io exception get list blocks arrange random order list block arr dataset get finalized blocks block pool id collections shuffle arr block info set new tree set block scan info block map new hash map block block scan info scan time block block arr pick first directory existing scanner log otherwise pick first directory file dir null list fs volume volumes dataset volumes get volumes fs dataset fs volume vol dataset volumes get volumes dir null try catch io exception e synchronized synchronized get
671	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockPoolSliceStorage.java	pooling	package org apache hadoop hdfs server datanode manages storage set block pool slices share particular block pool id data node this supports following functionality ol li formatting new block pool storage li li recovering storage state consistent state possible li li taking snapshot block pool upgrade li li rolling back block pool previous snapshot li li finalizing block storage deletion snapshot li ul block pool slice storage extends storage pattern block pool path pattern pattern string blockpool id id blockpool block pool slice storage storage info storage info string bpid block pool slice storage namespace id string bp id c time recover transition read data node datanode namespace info ns info format file dn cur dir namespace info ns info throws io exception format storage directory bp sdir namespace info ns info throws io exception protected set properties from fields properties props storage directory sd validate set block pool id set block pool id file storage string bpid protected set fields from properties properties props storage directory sd transition data node datanode storage directory sd upgrade storage directory bp sd namespace info ns info throws io exception cleanup detach dir file detach dir throws io exception rollback storage directory bp sd namespace info ns info finalize file dn cur dir throws io exception link all blocks file dir file dir throws io exception verify distributed upgrade progress upgrade manager datanode um string get data node storage root string bp root string string file get bp root string bp id file dn cur dir boolean pre upgradable layout storage directory sd throws io exception
672	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockReceiver.java	pooling	package org apache hadoop hdfs server datanode a receives block writes disk meanwhile may copies another site if throttler provided streaming throttling also supported block receiver implements closeable fs constants log log data node log log client trace log data node client trace log data input stream null data read data checksum checksum chunks block read output stream null block file local disk output stream cout null output stream cehcksum file data output stream checksum out null crc file local disk bytes per checksum checksum size byte buffer buf contains one full packet buf read amount valid data buf max packet read len protected string addr protected string addr string mirror addr data output stream mirror out daemon responder null data transfer throttler throttler fs dataset block write streams streams datanode info src data node null checksum partial crc null data node datanode volatile boolean mirror error the client name it empty datanode client string clientname boolean client boolean datanode block receive extended block block replica write replica in pipeline interface replica info pipeline stage block construction stage stage boolean transfer block receiver extended block block data input stream try catch replica already exists exception bae catch replica not found exception bne catch io exception ioe return datanode object data node get data node return datanode close files close throws io exception io exception ioe null close checksum file try catch io exception e finally close block file try catch io exception e finally disk check ioe null flush block data metadata files disk flush throws io exception checksum out null null while writing mirror out failure write mirror affect datanode unless caused interruption handle mirror out error io exception ioe throws io exception string bpid block get block pool id log info datanode get dn registration for bp bpid thread interrupted shut thread interrupted else encounter error writing mirror verify multiple crc chunks verify chunks byte data buf data off len datanode protocol nn datanode get bp namenode block get block pool id len makes sure buf position zero without modifying buf remaining it moves data position needs changed shift buf data buf read buf limit shift remaining data buf front buf position reads upto read byte buf buf limit increments limit throws io exception read succeed read to buf read throws io exception read n read read buf array buf limit read n read buf read buf limit n read buf limit buf read return n read reads least one packet returns packet length buf position points start packet buf limit point end packet there could data next packet buf br br it tries read full packet single read call consecutive packets usually length read next packet throws io exception this dances around buf little bit mainly read full packet single read accept arbitarary size next packet time buf null see data left buffer buf read buf limit buf remaining size of integer we mostly full packet least enough buf mark payload len buf get int buf reset check corrupt values pkt len mb upper limit ok payload len payload
673	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockSender.java	pooling	package org apache hadoop hdfs server datanode reads block disk sends recipient block sender implements java io closeable fs constants log log data node log log client trace log data node client trace log extended block block block read replica read replica replica the visible length replica replica visible length boolean bit system get property sun arch data model equals input stream block in data stream block in position updated using transfer to data input stream checksum in checksum datastream data checksum checksum checksum stream offset starting position read end offset ending position bytes per checksum chunk size checksum size checksum size boolean corrupt checksum ok need verify checksum boolean chunk offset ok need send chunk offset seqno sequence number packet boolean transfer to allowed true set entire requested byte range sent client boolean sent entire byte range boolean verify checksum true check verified reading data transfer throttler throttler string client trace fmt format client trace log message minimum buffer used sending data clients used transfer to enabled kb large it could larger sure much improvement min buffer with transferto volatile chunk checksum last chunk checksum null block sender extended block block start offset length block start offset length corrupt checksum ok chunk offset ok block sender extended block block start offset length throws io exception try block block synchronized datanode data replica datanode data get replica block get block pool id replica null replica visible length replica get visible length min end offset start offset length write progress chunk checksum chunk checksum null replica instanceof replica being written replica get bytes on disk min end offset current bytes on disk replica get bytes on disk current bytes on disk min end offset replica in pipeline rip replica in pipeline replica chunk checksum rip get last checksum and data len replica get generation stamp block get generation stamp throw new io exception replica visible length throw new io exception the replica readable block data node log debug enabled data node log debug block block replica replica chunk offset ok chunk offset ok corrupt checksum ok corrupt checksum ok verify checksum verify checksum transfer to fully fails bit platforms block sizes gb use normal transfer cases transfer to allowed datanode transfer to allowed bit length integer max value client trace fmt client trace fmt corrupt checksum ok datanode data meta file exists block checksum in new data input stream new buffered input stream datanode data read handle common header for version block metadata header header block metadata header read header checksum in short version header get version version fs dataset metadata version checksum header get checksum else log warn could find metadata file block this decides buffer size use buffer size checksum data checksum new data checksum data checksum checksum null if bytes per checksum large metadata file mostly corrupted for truncate bytes perchecksum block length bytes per checksum checksum get bytes per checksum bytes per checksum bytes per checksum replica visible length checksum data checksum new data checksum checksum get checksum type bytes per checksum checksum get bytes per checksum checksum size
674	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockVolumeChoosingPolicy.java	unrelated	package org apache hadoop hdfs server datanode block volume choosing policy allows data node specify policy used choosing volume block request block volume choosing policy returns specific fs volume applying suitable choice algorithm place given block given list fs volumes block size sought storage policies maintain state must thread safe fs volume choose volume list fs volume volumes block size
675	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ChunkChecksum.java	unrelated	package org apache hadoop hdfs server datanode holder holds checksum bytes length block checksum bytes end ex length checksum bytes bytes checksum applies last chunk bytes chunk checksum data length null available byte checksum chunk checksum data length byte checksum get data length byte get checksum
676	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataBlockScanner.java	pooling	package org apache hadoop hdfs server datanode data block scanner manages block scanning block pools for block pool link block pool slice scanner created runs separate thread scan blocks block pool when link bp offer service becomes alive dies block pool scanner map updated data block scanner implements runnable log log log factory get log data block scanner data node datanode fs dataset dataset configuration conf tree map string block pool slice scanner block pool scanner map thread block scanner thread null data block scanner data node datanode fs dataset dataset configuration conf run wait least one block pool wait for init string bpid block pool slice scanner get next bp scanner string current bp id synchronized get block pool set size synchronized block pool slice scanner get bp scanner string bpid synchronized string get bp id list add block extended block block synchronized boolean initialized string bpid synchronized print block report string builder buffer delete block string pool id block delete delete blocks string pool id block delete synchronized shutdown synchronized add block pool string block pool id synchronized remove block pool string block pool id this method used testing get blocks scanned in last run string bpid throws io exception start servlet extends http servlet
677	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataNode.java	heartbeat	package org apache hadoop hdfs server datanode data node program stores set blocks dfs deployment a single deployment one many data nodes each data node communicates regularly single name node it also communicates client code data nodes time time data nodes store series named blocks the data node allows client code read blocks write new block data the data node may also response instructions name node delete blocks copy blocks data nodes the data node maintains one critical table block stream bytes block size less this info stored local disk the data node reports table contents name node upon startup every often afterwards data nodes spend lives endless loop asking name node something a name node cannot connect data node directly name node simply returns values functions invoked data node data nodes maintain open server socket client code data nodes read write data the host port server reported name node sends information clients data nodes might interested data node extends configured log log log factory get log data node string dn clienttrace format log client trace log inet socket address create socket addr string target block pool manager volatile boolean run true block pool manager block pool manager volatile fs dataset interface data null string cluster id null string empty del hint atomic integer xmits in progress new atomic integer daemon data xceiver server null thread group thread group null block report interval boolean reset block report time true initial block report delay blockreport initial delay l heart beat interval boolean heartbeats disabled for tests false data storage storage null http server info server null data node metrics metrics inet socket address self addr volatile string host name host name datanode string dn thread name socket timeout socket write timeout boolean transfer to allowed true write packet size boolean block token enabled block pool token secret manager block pool token secret manager boolean sync on close data block scanner block scanner null directory scanner directory scanner null activated plug ins list service plugin plugins for inter data node protocol server ipc server secure resources secure resources null abstract list file data dirs configuration conf data node configuration conf data node configuration conf synchronized set cluster id string cid throws io exception string get host name configuration config init config configuration conf start info server configuration conf throws io exception start plugins configuration conf init ipc server configuration conf throws io exception initialize datanode periodic scanners link data block scanner link directory scanner they report results per blockpool basis scanning per volume basis minimize competition disk iops parameters periodic scanners init periodic scanners configuration conf shutdown periodic scanners synchronized init data block scanner configuration conf shutdown data block scanner synchronized init directory scanner configuration conf synchronized shutdown directory scanner init data xceiver configuration conf throws io exception calls specific bp protected notify namenode received block extended block block string del hint report bad blocks extended block block throws io exception used testing set heartbeats disabled for tests bp offer service implements runnable start data node configuration conf bp offer service get all bp
678	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DatanodeJspHelper.java	pooling	package org apache hadoop hdfs server datanode datanode jsp helper dfs client get dfs client user group information user simple date format ls date format get default chunk size get default chunk size configuration conf generate directory structure jsp writer generate file details jsp writer generate file chunks jsp writer http servlet request req generate file chunks for tail jsp writer http servlet request req get dfs client namenode corresponding bpid datanode dfs client get dfs client http servlet request request
679	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataNodeMXBean.java	unrelated	package org apache hadoop hdfs server datanode this jmx management data node information data node mx bean gets version hadoop string get version gets rpc port string get rpc port gets http port string get http port gets namenode ip addresses string get namenode addresses gets information volume datanode please see implementation format returned information string get volume info gets cluster id string get cluster id
680	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataStorage.java	pooling	package org apache hadoop hdfs server datanode data storage information file p data storage extends storage constants string block subdir prefix subdir string block file prefix blk string copy file prefix dncp string storage dir detached detach string storage dir rbw rbw string storage dir finalized finalized string storage dir tmp tmp pattern pre genstamp meta file pattern access variable guarded string storage id flag ensure initialzing storage occurs boolean initilized false block pool storage map block pool id block pool storage map string block pool slice storage bp storage map data storage storage info get bp storage string bpid data storage storage info storage info string strg id synchronized string get storage id synchronized set storage id string new storage id synchronized create storage id datanode port analyze storage directories recover previous transitions required perform fs state transition necessary depending namespace info read storage info br this method synchronized multiple dn threads only first dn thread dn level storage dir recover transition read synchronized recover transition read data node datanode recover transition read specific block pool recover transition read data node datanode string bp id namespace info ns info create physical directory block pools data node list data directories configuration instance use make block pool data dir collection file data dirs format storage directory sd namespace info ns info throws io exception set cluster id storage id storage type c time data storage version file protected set properties from fields properties props read cluster id storage id storage type c time data storage version file verify protected set fields from properties properties props storage directory sd boolean pre upgradable layout storage directory sd throws io exception analize whether transition fs state required perform necessary rollback previous lv layout version prev c time namenode c time upgrade lv layout version c time namenode c time regular startup lv layout version c time namenode c time transition data node datanode upgrade move current storage backup directory hardlink blocks new current directory upgrade pre later release e g ul li if sd previous exists delete li li rename sd current sd previous tmp li li create new sd current bpid current directory li ul li hard links block files created sd previous tmp sd current bpid current li li saves new version file sd current bpid current directory li ul li rename sd previous tmp sd previous li ul there one namenode cluster first time upgrade upgrade storage directory sd namespace info ns info throws io exception cleanup detach dir if directory empty report error otherwise remove directory cleanup detach dir file detach dir throws io exception rolling back snapshot previous directory moving current directory rollback procedure br if previous directory exists ol li rename current removed tmp li li rename previous current li li remove removed tmp li ol do nothing previous directory exist rollback storage directory sd finalize procedure deletes existing snapshot ol li rename previous finalized tmp directory li li fully delete finalized tmp directory li ol do nothing previous directory exist finalize storage directory sd throws io exception finalize upgrade
681	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataXceiver.java	pooling	package org apache hadoop hdfs server datanode thread processing incoming outgoing data stream data xceiver extends receiver implements runnable fs constants log log data node log log client trace log data node client trace log socket boolean local local connection string remote address address remote side string local address local address daemon data node datanode data xceiver server data xceiver server socket keepalive timeout op start time start time receiving op data xceiver socket data node datanode update current thread name contain current status use receiver started thread e outside constructor update current thread name string status return datanode object data node get data node return datanode read write data data xceiver server run read block extended block block write block extended block block transfer block extended block blk block checksum extended block block copy block extended block block replace block extended block block elapsed utility function sending response send response socket status status write response status status output stream throws io exception check access data output stream boolean reply
682	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataXceiverServer.java	unrelated	package org apache hadoop hdfs server datanode server used receiving sending block data this created listen requests clients data nodes this small server use hadoop ipc mechanism data xceiver server implements runnable fs constants log log data node log server socket ss data node datanode record sockets opened data transfer map socket socket child sockets collections synchronized map maximal number concurrent xceivers per node enforcing limit required order avoid data node running memory max xceiver count dfs config keys dfs datanode max receiver threads default a manager make sure cluster balancing take much resources it limits number block moves balancing total amount bandwidth use block balance throttler extends data transfer throttler num threads constructor block balance throttler bandwidth check block move start return true thread quota exceeded counter incremented false otherwise synchronized boolean acquire mark move completed the thread counter decremented synchronized release block balance throttler balance throttler we need estimate block size check disk partition enough space for set default block size set server side configuration ideal default block size client size configuration a better solution header estimated block size e either actual block size default block size estimate block size data xceiver server server socket ss configuration conf ss ss datanode datanode max xceiver count estimate block size set parameter cluster balancing balance throttler new block balance throttler run datanode run try catch io exception ie kill assert datanode run false try catch io exception ie close sockets accepted earlier synchronized child sockets
683	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DirectoryScanner.java	pooling	package org apache hadoop hdfs server datanode periodically scans data directories block block metadata files reconciles differences block information maintained link fs dataset directory scanner implements runnable log log log factory get log directory scanner data node datanode fs dataset dataset executor service report compile thread pool scheduled executor service master thread scan period msecs volatile boolean run false boolean retain diffs false scan info per block pool diffs new scan info per block pool map string stats stats new hash map string stats allow retaining diffs unit test analysis set retain diffs boolean b retain diffs b stats tracked reporting testing per blockpool stats string bpid total blocks missing meta file missing block file missing memory blocks mismatch blocks stats string bpid string string scan info per block pool extends serial version uid l scan info per block pool super scan info per block pool sz super sz merges scan info per block pool one add all scan info per block pool convert linked list values scan info per block pool map sorted arrays return new map arrays per blockpool map string scan info sorted arrays tracks files information related block disk missing file indicated setting corresponding member null scan info implements comparable scan info block id file meta file file block file fs volume volume scan info block id scan info block id file block file file meta file fs volume vol file get meta file file get block file get block id fs volume get volume compare to scan info b boolean equals object hash code get gen stamp directory scanner data node dn fs dataset dataset configuration conf datanode dn dataset dataset interval conf get int dfs config keys dfs datanode directoryscan interval key scan period msecs interval l msec threads report compile thread pool executors new fixed thread pool threads master thread new scheduled thread pool executor start run true offset dfs util get random next int scan period msecs l l msec first scan time system current time millis offset log info periodic directory tree verification scan starting master thread schedule at fixed rate offset scan period msecs unit test boolean get run status return run clear diffs clear stats clear main program loop directory scanner runs reconcile periodically master thread run try catch exception e catch error er shutdown run else run false master thread null master thread shutdown report compile thread pool null report compile thread pool shutdown retain diffs clear reconcile differences disk memory blocks reconcile scan entry string linked list scan info entry diffs entry set retain diffs clear scan differences disk memory blocks scan finalized blocks lists disk memory scan clear map string scan info disk report get disk report hold fs dataset lock prevent changes block map synchronized dataset end synchronized block found disk in memory block missing match block disk add difference linked list scan info diff record stats record missing meta file info get meta file null stats record missing block file info get block file null diff record add info block found disk add difference linked list
684	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FinalizedReplica.java	unrelated	package org apache hadoop hdfs server datanode this describes replica finalized finalized replica extends replica info boolean unlinked copy write done block finalized replica block id len gen stamp finalized replica block block fs volume vol file dir finalized replica finalized replica replica state get state boolean unlinked set unlinked get visible length get bytes on disk boolean equals object hash code string string
685	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDataset.java	pooling	package org apache hadoop hdfs server datanode fs dataset manages set data blocks each block unique name extent disk fs dataset implements fs constants fs dataset interface a node type built tree reflecting hierarchy blocks local disk fs dir file dir num blocks fs dir children last child idx fs dir file dir file add block block b file src throws io exception file add block block b file src boolean create ok get volume map string bpid replicas map volume map fs volume volume throws io exception recover temp unlinked block throws io exception check dir tree throws disk error exception clear path file f boolean clear path file f string dir names idx string string a block pool slice represents portion block pool stored volume taken together block pool slices sharing block pool id across cluster represent single block pool block pool slice string bpid fs volume volume volume block pool belongs file current dir storage directory current bpid current fs dir finalized dir directory store finalized replica file rbw dir directory store rbw replica file tmp dir directory store temporary replica todo federation scalability issue thread per du needed du dfs usage block pool slice string bpid fs volume volume file bp dir configuration conf file get directory file get current dir file get finalized dir file get rbw dir dec dfs used value get dfs used throws io exception file create tmp file block b throws io exception file create rbw file block b throws io exception file add block block b file f throws io exception check dirs throws disk error exception get volume map replicas map volume map throws io exception add to replicas map replicas map volume map file dir validate integrity file block file gen stamp clear path file f string string shutdown fs volume map string block pool slice map new hash map string block pool slice file current dir storage directory current df usage reserved fs volume file current dir configuration conf throws io exception return storage directory corresponding volume file get dir file get current dir file get rbw dir string bpid throws io exception dec dfs used string bpid value get dfs used throws io exception get block pool used string bpid throws io exception get capacity throws io exception get available throws io exception get reserved string get mount throws io exception block pool slice get block pool slice string bpid throws io exception string get block pool list file create tmp file string bpid block b throws io exception file create rbw file string bpid block b throws io exception file add block string bpid block b file f throws io exception check dirs throws disk error exception get volume map replicas map volume map throws io exception get volume map string bpid replicas map volume map throws io exception add to replicas map string bpid replicas map volume map clear path string bpid file f throws io exception string string shutdown add block pool string bpid configuration conf shutdown block pool string bpid boolean bp dir empty
686	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDatasetAsyncDiskService.java	pooling	package org apache hadoop hdfs server datanode this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion block files fs dataset we want create new thread deletion request want deletions heartbeat thread since deletion slow want use single thread pool inefficient volume async disk service solution this used inside fs dataset in future extract async disk service put common the fs dataset specific logic reside fs dataset async disk service log log log factory get log fs dataset async disk service thread pool core pool size core threads per volume thread pool maximum pool size maximum threads per volume thread pool keep alive time threads core pool size threads keep alive seconds thread group thread group new thread group async disk service hash map file thread pool executor executors fs dataset async disk service file volumes synchronized execute file root runnable task synchronized shutdown delete async fs dataset fs volume volume string bpid file block file a task deleting block file associated meta file well replica file delete task implements runnable
687	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDatasetInterface.java	pooling	package org apache hadoop hdfs server datanode this underlying storage stores blocks data node examples fs dataset stores blocks dirs simulated fs dataset simulates data fs dataset interface extends fs dataset m bean returns length metadata file specified block get meta data length extended block b throws io exception this provides input stream length metadata block meta data input stream extends filter input stream meta data input stream input stream stream len length get length returns meta data block b input stream length meta data input stream get meta data input stream extended block b does meta file exist block boolean meta file exists extended block b throws io exception returns specified block disk length excluding metadata get length extended block b throws io exception get reference replica meta info replicas map to called methods synchronized link fs dataset replica get replica string bpid block id string get replica string string bpid block id block get stored block string bpid blkid returns input stream read contents specified block input stream get block input stream extended block b throws io exception returns input stream specified offset specified block starting offset input stream get block input stream extended block b seek offset returns input stream specified offset specified block the block still tmp directory finalized starting offset block input streams get tmp input streams extended block b blkoff block write streams this contains input streams data checksum block block input streams implements closeable input stream data in input stream checksum in block input streams input stream data in input stream checksum in inherit doc close creates temporary replica returns meta information replica replica in pipeline interface create temporary extended block b throws io exception creates rbw replica returns meta info replica replica in pipeline interface create rbw extended block b throws io exception recovers rbw replica returns meta info replica replica in pipeline interface recover rbw extended block b throws io exception covert temporary replica rbw replica in pipeline interface convert temporary to rbw append finalized replica returns meta info replica replica in pipeline interface append extended block b recover failed append finalized replica returns meta info replica replica in pipeline interface recover append extended block b recover failed pipeline close it bumps replica generation stamp finalize rbw replica recover close extended block b finalizes block previously opened writing using write to block the block size parameter b must match amount data written finalize block extended block b throws io exception unfinalizes block previously opened writing using write to block the temporary file associated block deleted unfinalize block extended block b throws io exception returns block report full list blocks stored block pool block list as longs get block report string bpid is block valid boolean valid block extended block b is block valid rbw boolean valid rbw extended block b invalidates specified blocks invalidate string bpid block invalid blks throws io exception check data directories healthy check data dir throws disk error exception stringifies name storage string string shutdown fs dataset shutdown sets file pointer checksum stream last checksum overwritten adjust crc channel
688	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\Replica.java	unrelated	package org apache hadoop hdfs server datanode this represents block replicas stored data node replica get block id get block id get generation stamp get generation stamp get replica state replica state get state get number bytes received get num bytes get number bytes written disk get bytes on disk get number bytes visible readers get visible length
689	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaAlreadyExistsException.java	unrelated	package org apache hadoop hdfs server datanode exception indicating target block already exists set recovered overwritten replica already exists exception extends io exception serial version uid l replica already exists exception replica already exists exception string msg
690	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaBeingWritten.java	unrelated	package org apache hadoop hdfs server datanode this represents replicas written those replicas created pipeline initiated dfs client replica being written extends replica in pipeline constructor zero length replica replica being written block id gen stamp constructor replica being written block block constructor replica being written block id len gen stamp copy constructor replica being written replica being written get visible length replica state get state boolean equals object hash code
691	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInfo.java	unrelated	package org apache hadoop hdfs server datanode this used datanodes maintain meta data replicas it provides general meta information replica replica info extends block implements replica fs volume volume volume replica belongs file dir directory block meta files belong replica info block id gen stamp fs volume vol file dir replica info block block fs volume vol file dir replica info block id len gen stamp replica info replica info string get meta file name file get block file file get meta file fs volume get volume set volume fs volume vol file get dir set dir file dir boolean unlinked set unlinked unlink file file file block b throws io exception boolean unlink block num links throws io exception set newer generation stamp new gs throws io exception string string
692	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInPipeline.java	unrelated	package org apache hadoop hdfs server datanode this defines replica pipeline includes persistent replica written dfs client temporary replica replicated source datanode copied balancing purpose the base implements temporary replica replica in pipeline extends replica info bytes acked bytes on disk byte last checksum thread writer replica in pipeline block block replica in pipeline block id len gen stamp replica in pipeline replica in pipeline get visible length replica state get state get bytes acked set bytes acked bytes acked get bytes on disk synchronized set last checksum and data len data length byte last checksum synchronized chunk checksum get last checksum and data len set writer thread writer boolean equals object stop writer throws io exception hash code block write streams create streams boolean create string string
693	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInPipelineInterface.java	unrelated	package org apache hadoop hdfs server datanode this defines replica pipeline written replica in pipeline interface extends replica set number bytes received set num bytes bytes received get number bytes acked get bytes acked set number bytes acked set bytes acked bytes acked store checksum last chunk along data length set last checksum and data len data length byte last checksum gets last chunk checksum length block corresponding checksum chunk checksum get last checksum and data len create output streams writing replica one block file one crc file block write streams create streams boolean create
694	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaNotFoundException.java	unrelated	package org apache hadoop hdfs server datanode exception indicating data node replica matches target block replica not found exception extends io exception serial version uid l string non rbw replica cannot recover non rbw replica string unfinalized replica string unfinalized and nonrbw replica string non existent replica string unexpected gs replica replica not found exception replica not found exception extended block b replica not found exception string msg
695	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicasMap.java	pooling	package org apache hadoop hdfs server datanode maintains replicas map replicas map object using synchronized object mutex map block pool id another map block id replica info map string map long replica info map new hash map string map long replica info replicas map object mutex mutex null mutex mutex string get block pool list synchronized mutex check block pool string bpid bpid null check block block b b null get meta information replica matches block id generation stamp replica info get string bpid block block check block pool bpid check block block replica info replica info get bpid block get block id replica info null return null get meta information replica matches block id replica info get string bpid block id check block pool bpid synchronized mutex add replica meta information map replica info add string bpid replica info replica info check block pool bpid check block replica info synchronized mutex remove replica meta information map matches input block id generation stamp replica info remove string bpid block block check block pool bpid check block block synchronized mutex return null remove replica meta information map present replica info remove string bpid block id check block pool bpid synchronized mutex return null get size map given block pool size string bpid map long replica info null synchronized mutex get collection replicas given block pool this method b synchronized b it needs synchronized externally using mutex getting replicas values map iterating mutex accessed using link get mutext method collection replica info replicas string bpid map long replica info null map get bpid return null values null init block pool string bpid check block pool bpid synchronized mutex clean up block pool string bpid check block pool bpid synchronized mutex give access mutex used synchronizing replicas map object get mutext return mutex
696	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaUnderRecovery.java	unrelated	package org apache hadoop hdfs server datanode this represents replicas block recovery it recovery id equal generation stamp replica bumped recovery the recovery id used handle multiple concurrent block recoveries a recovery higher recovery id preempts recoveries lower id replica under recovery extends replica info replica info original original replica needs recovered recovery id recovery id also generation stamp replica under recovery replica info replica recovery id replica under recovery replica under recovery get recovery id set recovery id recovery id replica info get original replica replica state get orignal replica state boolean unlinked set unlinked replica state get state get visible length get bytes on disk set block id block id set generation stamp gs set num bytes num bytes set dir file dir set volume fs volume vol boolean equals object hash code string string replica recovery info create info
697	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaWaitingToBeRecovered.java	unrelated	package org apache hadoop hdfs server datanode this represents replica waiting recovered after datanode restart replica rbw directory loaded replica waiting recovered a replica waiting recovered provision read participates pipeline recovery it become outdated client continues write recovered result lease recovery replica waiting to be recovered extends replica info boolean unlinked copy write done block replica waiting to be recovered block id len gen stamp replica waiting to be recovered block block fs volume vol file dir replica waiting to be recovered replica waiting to be recovered replica state get state boolean unlinked set unlinked get visible length get bytes on disk boolean equals object hash code string string
698	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\RoundRobinVolumesPolicy.java	unrelated	package org apache hadoop hdfs server datanode round robin volumes policy implements block volume choosing policy cur volume synchronized fs volume choose volume list fs volume volumes block size
699	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\SecureDataNodeStarter.java	authenticate	package org apache hadoop hdfs server datanode utility start datanode secure cluster first obtaining privileged resources main startup handing datanode secure data node starter implements daemon secure resources string args secure resources resources init daemon context context throws exception start throws exception
700	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\UpgradeManagerDatanode.java	unrelated	package org apache hadoop hdfs server datanode upgrade manager data nodes distributed upgrades data node performed separate thread the upgrade starts data node receives start upgrade command namenode at point manager finds respective upgrade object starts daemon order perform upgrade defined object upgrade manager datanode extends upgrade manager data node data node null daemon upgrade daemon null string bpid null upgrade manager datanode data node data node string bpid hdfs constants node type get type synchronized initialize upgrade namespace info ns info throws io exception start distributed upgrade instantiates distributed upgrade objects synchronized boolean start upgrade throws io exception synchronized process upgrade command upgrade command command synchronized complete upgrade throws io exception synchronized shutdown upgrade
701	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\UpgradeObjectDatanode.java	pooling	package org apache hadoop hdfs server datanode base data node upgrade objects data node upgrades run separate threads upgrade object datanode extends upgrade object implements runnable data node data node null string bpid null hdfs constants node type get type protected data node get datanode protected datanode protocol get namenode throws io exception set datanode data node data node string bpid upgrade throws io exception boolean pre upgrade action namespace info ns info throws io exception run upgrade command complete upgrade throws io exception
702	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\metrics\DataNodeMetrics.java	heartbeat	package org apache hadoop hdfs server datanode metrics this maintaining various data node statistics publishing metrics interfaces this also registers jmx m bean rpc p this number metrics variables publicly accessible variables objects methods update values example p link blocks read inc data node metrics metrics registry registry new metrics registry datanode string name data node metrics string name string session id data node metrics create configuration conf string dn name string name return name add heartbeat latency add block report latency incr blocks replicated delta incr blocks written incr blocks removed delta incr bytes written delta incr block verification failures incr blocks verified add read block op latency add write block op latency add replace block op latency add copy block op latency add block checksum op latency incr bytes read delta incr blocks read shutdown incr writes from client boolean local incr reads from client boolean local incr volume failures
703	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\metrics\FSDatasetMBean.java	pooling	package org apache hadoop hdfs server datanode metrics this interface defines methods get status fs dataset data node it also used publishing via jmx hence follow jmx naming convention note used metrics dynamic m bean base implement fs dataset m bean stable published p data node runtime statistic info report another m bean fs dataset m bean returns total space bytes used block pool get block pool used string bpid throws io exception returns total space bytes used dfs datanode get dfs used throws io exception returns total capacity bytes storage used unused get capacity throws io exception returns amount free storage space bytes get remaining throws io exception returns storage id underlying storage string get storage info returns number failed volumes datanode get num failed volumes
704	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\BackupImage.java	pooling	package org apache hadoop hdfs server namenode extension fs image backup node this handles setup journaling spool backup namenode backup image extends fs image names journal spool directory spool file string storage jspool dir jspool string storage jspool file backup input stream loading edits memory edit log backup input stream backup input stream is journal spooling progress volatile j spool state js state enum j spool state backup image recover create read collection uri image dirs synchronized reset throws io exception load checkpoint checkpoint signature sig throws io exception save checkpoint throws io exception fs directory get fs directory root lock file get j spool dir storage directory sd file get j spool file storage directory sd synchronized journal length byte data throws io exception synchronized wait spool end synchronized start journal spool namenode registration nn reg throws io exception synchronized set checkpoint time length byte data throws io exception converge journal spool throws io exception
705	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\BackupNode.java	unrelated	package org apache hadoop hdfs server namenode backup node p backup node play two roles ol li link namenode role checkpoint node periodically creates checkpoints downloads image edits active node merges uploads new image back active li li link namenode role backup node keeps namespace sync active node periodically creates checkpoints simply saving namespace image local disk li ol backup node extends name node string bn address name key dfs config keys dfs namenode backup address key string bn address default dfs config keys dfs namenode backup address default string bn http address name key dfs config keys dfs namenode backup http address key string bn http address default dfs config keys dfs namenode backup http address default string bn service rpc address key dfs config keys dfs namenode backup service rpc address key name node proxy namenode protocol namenode name node rpc address string nn rpc address name node http address string nn http address checkpoint manager checkpointer checkpoint manager cluster id backup node belongs string cluster id block pool id peer namenode backup node string block pool id backup node configuration conf namenode role role throws io exception super conf role common name node methods implementation backup node protected inet socket address get rpc server address configuration conf throws io exception string addr conf get bn address name key bn address default return net utils create socket addr addr protected inet socket address get service rpc server address configuration conf throws io exception string addr conf get bn service rpc address key addr null addr empty return net utils create socket addr addr protected set rpc server address configuration conf conf set bn address name key get host port string rpc address protected set rpc service server address configuration conf conf set bn service rpc address key get host port string service rpc address protected inet socket address get http server address configuration conf assert rpc address null rpc address calculated first string addr conf get bn http address name key bn http address default return net utils create socket addr addr protected set http server address configuration conf conf set bn http address name key get host port string get http address protected load namesystem configuration conf throws io exception backup image bn image new backup image namesystem new fs namesystem conf bn image bn image recover create read fs namesystem get namespace dirs conf protected initialize configuration conf throws io exception trash disabled backup name node turned back ever becomes active conf set long common configuration keys fs trash interval key namespace info ns info handshake conf super initialize conf backup node never lease recovery therefore lease hard limit never expire namesystem lease manager set lease period cluster id ns info get cluster id block pool id ns info get block pool id register active name node register with ns info checkpoint daemon start rpc server started run checkpoint daemon conf stop checkpoint manager null namenode null get registration null stop rpc client rpc stop proxy namenode namenode null stop checkpoint manager checkpoint manager null stop name node
706	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\CancelDelegationTokenServlet.java	authenticate	package org apache hadoop hdfs server namenode cancel delegation tokens http use hftp cancel delegation token servlet extends dfs servlet log log log factory get log cancel delegation token servlet string path spec cancel delegation token string token token protected get http servlet request req http servlet response resp
707	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\Checkpointer.java	unrelated	package org apache hadoop hdfs server namenode the checkpointer responsible supporting periodic checkpoints hdfs metadata the checkpointer daemon periodically wakes determined schedule specified configuration triggers periodic checkpoint goes back sleep the start checkpoint triggered one two factors time size edits file checkpointer extends daemon log log backup node backup node volatile boolean run checkpoint period seconds checkpoint size size mb current edit log string info bind address backup image get fs image namenode protocol get namenode create connection primary namenode checkpointer configuration conf backup node bn node throws io exception initialize checkpoint initialize configuration conf throws io exception shut checkpointer shutdown the main work loop run get journal size throws io exception download code fsimage code code edits code files remote name node download checkpoint checkpoint signature sig throws io exception copy new image remote name node upload checkpoint checkpoint signature sig throws io exception create new checkpoint checkpoint throws io exception
708	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\CheckpointSignature.java	unrelated	package org apache hadoop hdfs server namenode a unique signature intended identify checkpoint transactions checkpoint signature extends storage info string field separator edits time l checkpoint time l md hash image digest null string blockpool id checkpoint signature checkpoint signature fs image fs image checkpoint signature string str get md image digest md hash get image digest get cluster id checkpoint signature string get cluster id get block pool id checkpoint signature string get blockpool id set block pool id checkpoint signature set blockpool id string blockpool id string string validate storage info fs image si throws io exception comparable compare to checkpoint signature boolean equals object hash code writable write data output throws io exception read fields data input throws io exception
709	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ClusterJspHelper.java	pooling	package org apache hadoop hdfs server namenode this generates data needed displayed cluster web console cluster jsp helper log log log factory get log cluster jsp helper string overall status overall status string dead dead string jmx qry cluster status generate cluster health report decommission status generate decommissioning report get decommission node cluster state update unknown status map string map string string status map get datanode http port configuration conf namenode mx bean helper cluster status namenode status enum decommission states decommission status xml item block xml outputter doc string key string value xml item block with link xml outputter doc string value create namenode exception msg xml outputter doc create general exception xml outputter doc string read output url url throws io exception string query mbean string http address configuration conf json node get property string props string propertyname throws io exception
710	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ContentSummaryServlet.java	unrelated	package org apache hadoop hdfs server namenode servlets file checksum content summary servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request
711	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\DfsServlet.java	unrelated	package org apache hadoop hdfs server namenode a base servlets dfs dfs servlet extends http servlet for java io serializable serial version uid l log log log factory get log dfs servlet get canonical name write object xml format protected write xml exception except string path xml outputter doc protected client protocol create name node proxy throws io exception create uri redirecting request datanode protected uri create redirect uri string servletpath get filename request protected string get filename http servlet request request protected user group information get ugi http servlet request request
712	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogBackupInputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log input stream used updates hdfs meta data state backup node org apache hadoop hdfs server protocol namenode registration byte edit log backup input stream extends edit log input stream string address sender address byte buffer input stream inner data input stream byte buffer input stream extends byte array input stream edit log backup input stream string name throws io exception string get name journal type get type available throws io exception read throws io exception read byte b len throws io exception close throws io exception length throws io exception data input stream get data input stream set bytes byte new bytes throws io exception clear throws io exception
713	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogBackupOutputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log output stream streams edits backup node org apache hadoop hdfs server protocol namenode registration byte edit log backup output stream extends edit log output stream default buffer size namenode protocol backup node rpc proxy backup node namenode registration bn registration backup node registration namenode registration nn registration active node registration edits double buffer buf data output buffer serialized output sent backup node edit log backup output stream namenode registration bn reg backup node throws io exception string get name journal type get type write fs edit log op op throws io exception write raw byte bytes offset length throws io exception there persistent storage just clear buffers create throws io exception close throws io exception set ready to flush throws io exception protected flush and sync throws io exception there persistent storage therefore length p length used check large enough start checkpoint this criteria used backup streams length throws io exception send ja throws io exception get backup node registration namenode registration get registration verify backup node alive boolean alive
714	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogFileInputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log input stream reads edits local file edit log file input stream extends edit log input stream file file file input stream f stream edit log file input stream file name throws io exception string get name journal type get type available throws io exception read throws io exception read byte b len throws io exception close throws io exception length throws io exception
715	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogFileOutputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log output stream stores edits local file edit log file output stream extends edit log output stream edits file header size bytes integer size byte size file file file output stream fp file stream storing edit logs file channel fc channel file stream sync edits double buffer buf byte buffer fill byte buffer allocate direct preallocation mb edit log file output stream file name size throws io exception string get name journal type get type inherit doc write fs edit log op op throws io exception inherit doc write raw byte bytes offset length throws io exception create throws io exception close throws io exception set ready to flush throws io exception protected flush and sync throws io exception boolean force sync length throws io exception allocate big chunk data preallocate throws io exception boolean operation supported byte op file get file set file channel for testing file channel fc file channel get file channel for testing
716	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogInputStream.java	unrelated	package org apache hadoop hdfs server namenode a generic support reading edits log data persistent storage it stream bytes storage exactly written link edit log output stream edit log input stream extends input stream implements journal stream inherit doc available throws io exception inherit doc read throws io exception inherit doc read byte b len throws io exception inherit doc close throws io exception length throws io exception data input stream get data input stream
717	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogOutputStream.java	unrelated	package org apache hadoop hdfs server namenode a generic support journaling edits logs persistent storage edit log output stream implements journal stream statistics counters num sync number sync disk total time sync total time sync edit log output stream throws io exception write fs edit log op op throws io exception write raw byte bytes offset length create throws io exception inherit doc close throws io exception set ready to flush throws io exception protected flush and sync throws io exception flush throws io exception length throws io exception boolean force sync boolean operation supported byte op get total sync time get num sync string string
718	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditsDoubleBuffer.java	unrelated	package org apache hadoop hdfs server namenode a buffer edits new edits written first buffer second available flushed each time buffer flushed two internal buffers swapped this allows edits progress concurrently flushes without allocating new buffers time edits double buffer data output buffer buf current current buffer writing data output buffer buf ready buffer ready flushing init buffer size writer writer edits double buffer default buffer size write op fs edit log op op throws io exception write raw byte bytes offset length throws io exception close throws io exception set ready to flush writes content ready buffer given output stream resets does swap buffers flush to output stream throws io exception boolean force sync data output buffer get current buf boolean flushed count buffered bytes
719	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FileChecksumServlets.java	unrelated	package org apache hadoop hdfs server namenode servlets file checksum file checksum servlets redirect file checksum queries appropriate datanode redirect servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request http servlet response response get file checksum get servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request http servlet response response
720	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FileDataServlet.java	unrelated	package org apache hadoop hdfs server namenode redirect queries hosted filesystem appropriate datanode file data servlet extends dfs servlet for java io serializable serial version uid l create redirection uri protected uri create uri string parent hdfs file status user group information ugi select datanode service request currently looks first five blocks file selecting datanode randomly represented datanode id pick src datanode located blocks blks hdfs file status service get request described request code get http nn port data path http get http servlet request request
721	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FsckServlet.java	unrelated	package org apache hadoop hdfs server namenode this used namesystem web server fsck namenode fsck servlet extends dfs servlet java io serializable serial version uid l handle fsck request get http servlet request request http servlet response response
722	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSClusterStats.java	unrelated	package org apache hadoop hdfs server namenode this used retrieving load related statistics cluster fs cluster stats indication total load cluster writes currently occuring cluster get total load
723	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSDirectory.java	unrelated	package org apache hadoop hdfs server namenode fs directory stores filesystem directory state it handles writing loading values disk logging changes go it keeps filename blockset mapping always current logged disk fs directory implements closeable i node directory with quota root dir fs image fs image volatile boolean ready false unknown disk space max component length max dir items ls limit max list limit lock protect directory block map reentrant read write lock dir lock condition cond utility methods acquire release read lock write lock read lock read unlock write lock write unlock boolean write lock boolean read lock caches frequently used file names used link i node reuse byte objects reduce heap usage name cache byte array name cache access existing dfs name directory fs directory fs namesystem ns configuration conf throws io exception fs directory fs image fs image fs namesystem ns configuration conf fs namesystem get fs namesystem block manager get block manager load fs image collection uri data dirs exposed unit tests protected set ready boolean flag incr deleted file count count shutdown filestore close throws io exception block object ready used wait for ready add given filename fs i node file under construction add file string path i node unprotected add file string path i node directory add to parent byte src i node directory parent i node add block file returns reference added block block info add block string path throws quota exceeded exception persist block list inode persist blocks string path i node file under construction file close file close file string path i node file file remove block file boolean remove block string path i node file under construction file node boolean rename to string src string dst rename to string src string dst options rename options change path name boolean unprotected rename to string src string dst timestamp rename src dst see link distributed file system rename path path options rename details related rename semantics exceptions boolean unprotected rename to string src string dst timestamp set file replication block set replication string src short replication old replication block unprotected set replication string src get blocksize file get preferred block size string filename throws unresolved link exception boolean exists string src throws unresolved link exception set permission string src fs permission permission unprotected set permission string src fs permission permissions set owner string src string username string groupname unprotected set owner string src string username string groupname concat blocks srcs trg delete srcs files concat string target string srcs concat blocks srcs trg delete srcs files must also called edit logs note update quota needed concat unprotected concat string target string srcs timestamp delete target directory collect blocks boolean delete string src list block collected blocks return directory empty boolean dir empty string src throws unresolved link exception boolean empty delete path name space update count ancestor directory quota br note this used link fs edit log br unprotected delete string src mtime delete path name space update count ancestor directory quota unprotected delete string src list block collected blocks replaces specified inode specified
724	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLog.java	unrelated	package org apache hadoop hdfs server namenode fs edit log maintains log namespace modifications fs edit log implements nn storage listener string no journal streams warning warning log log log factory get log fs edit log volatile size output flush buffer array list edit log output stream edit streams null monotonically increasing counter represents transaction ids txid stores last synced transaction id synctxid time printing statistics log file last print time sync currently running volatile boolean sync running automatic sync scheduled volatile boolean auto sync scheduled false statistics counters num transactions number transactions num transactions batched in sync total time transactions total time transactions name node metrics metrics nn storage storage thread local checksum local checksum new thread local checksum protected checksum initial value get thread local checksum checksum get checksum return local checksum get transaction id txid transaction id value stores current transaction id thread thread local transaction id transaction id new thread local transaction id protected synchronized transaction id initial value fs edit log nn storage storage sync running false storage storage storage register listener metrics name node get name node metrics last print time file get edit file storage directory sd return storage get edit file sd file get edit new file storage directory sd return storage get edit new file sd get num edits dirs return storage get num storage dirs name node dir type edits synchronized get num edit streams return edit streams null edit streams size return currently active edit streams this used unit tests array list edit log output stream get edit streams return edit streams boolean open return get num edit streams create empty edit log files initialize output stream logging synchronized open throws io exception num transactions total time transactions num transactions batched in sync edit streams null array list storage directory al null iterator storage directory al null if error every storage dir one removed list storage directories storage get num storage dirs name node dir type edits synchronized add new edit log stream file e file throws io exception edit log output stream e stream new edit log file output stream e file edit streams add e stream synchronized create edit log file file name throws io exception wait for sync to finish edit log output stream e stream new edit log file output stream name e stream create e stream close shutdown file store synchronized close wait for sync to finish edit streams null edit streams empty print statistics true num transactions total time transactions num transactions batched in sync array list edit log output stream error streams null iterator edit log output stream get output stream iterator null next disable and report error on streams error streams edit streams clear close remove edit log stream synchronized remove stream index edit log output stream e stream edit streams get index try catch exception e edit streams remove index the specified streams io errors close remove synchronized disable and report error on streams list edit log output stream error streams error streams null error streams size array list storage directory error
725	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogLoader.java	unrelated	package org apache hadoop hdfs server namenode fs edit log loader fs namesystem fs namesys fs edit log loader fs namesystem fs namesys fs namesys fs namesys load edit log apply changes memory structure this apply edits writing disk along load fs edits edit log input stream edits throws io exception start time num edits load fs edits edits true fs image log info edits file edits get name return num edits read header fsedit log read log version data input stream throws io exception log version read log file version could missing mark if edits log greater g available method return negative numbers avoid call available boolean available true try catch eof exception e available assert log version storage last upgradable layout version return log version load fs edits edit log input stream edits boolean close on exit throws io exception buffered input stream bin new buffered input stream edits data input stream new data input stream bin num edits log version try finally log version fs constants layout version version return num edits load edit records log version data input stream fs directory fs dir fs namesys dir num edits num op add num op close num op delete fs namesys write lock fs dir write lock keep track file offsets last several opcodes this handy manually recovering corrupted edits files position tracking input stream tracker new position tracking input stream new data input stream tracker recent opcode offsets new arrays fill recent opcode offsets try catch throwable finally fs image log debug enabled return num edits throw appropriate exception upgrade editlog loading could fail due opcode conflicts check upgrade failure log version io exception ex version version conflicting opcodes later releases the editlog must emptied restarting namenode proceeding upgrade storage layout version log version else stream wrapper keeps track current file position position tracking input stream extends filter input stream cur pos mark pos position tracking input stream input stream read throws io exception read byte data throws io exception read byte data offset length throws io exception mark limit reset throws io exception get pos
726	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogOp.java	unrelated	package org apache hadoop hdfs server namenode helper reading ops input stream all ops derive fs edit log op instantiated reader read op fs edit log op fs edit log op codes op code thread local enum map fs edit log op codes fs edit log op op instances new thread local enum map fs edit log op codes fs edit log op constructor edit log op edit log ops cannot constructed directly reader read op fs edit log op fs edit log op codes op code op code op code read fields data input stream log version write fields data output stream add close op extends fs edit log op length string path short replication mtime atime block size block blocks permission status permissions string client name string client machine datanode descriptor data node descriptors unused add close op fs edit log op codes op code t extends add close op t set path string path t extends add close op t set replication short replication t extends add close op t set modification time mtime t extends add close op t set access time atime t extends add close op t set block size block size t extends add close op t set blocks block blocks t extends add close op t set permission status permission status permissions t extends add close op t set client name string client name t extends add close op t set client machine string client machine write fields data output stream throws io exception read fields data input stream log version this method defined compatibility reason datanode descriptor read datanode descriptor array data input block read blocks add op extends add close op add op add op get instance close op extends add close op close op close op get instance set replication op extends fs edit log op string path short replication set replication op set replication op get instance set replication op set path string path set replication op set replication short replication write fields data output stream throws io exception read fields data input stream log version concat delete op extends fs edit log op length string trg string srcs timestamp concat delete op concat delete op get instance concat delete op set target string trg concat delete op set sources string srcs concat delete op set timestamp timestamp write fields data output stream throws io exception read fields data input stream log version rename old op extends fs edit log op length string src string dst timestamp rename old op rename old op get instance rename old op set source string src rename old op set destination string dst rename old op set timestamp timestamp write fields data output stream throws io exception read fields data input stream log version delete op extends fs edit log op length string path timestamp delete op delete op get instance delete op set path string path delete op set timestamp timestamp write fields data output stream throws io exception read fields data input stream log version mkdir op extends fs edit log op
727	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogOpCodes.java	unrelated	package org apache hadoop hdfs server namenode op codes edits file enum fs edit log op codes last op code file op invalid byte op add byte op rename old byte deprecated operation op delete byte op mkdir byte op set replication byte op set permissions byte op set owner byte op close byte op set genstamp byte op set ns quota byte obsolete op clear ns quota byte obsolete op times byte set atime mtime op set quota byte op rename byte filecontext rename op concat delete byte concat files op symlink byte op get delegation token byte op renew delegation token byte op cancel delegation token byte op update master key byte op reassign lease byte must namenode protocol ja jspool start op jspool start byte must namenode protocol ja checkpoint time op checkpoint time byte byte op code constructor fs edit log op codes byte op code return byte value enum byte get op code map byte fs edit log op codes byte to enum converts byte fs edit log op codes enum value fs edit log op codes byte byte op code
728	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImage.java	pooling	package org apache hadoop hdfs server namenode fs image handles checkpointing logging namespace edits fs image implements nn storage listener closeable protected log log log factory get log fs image get name simple date format date form checkpoint states enum checkpoint states start rolled edits upload start upload done protected fs namesystem namesystem null protected fs edit log edit log null boolean upgrade finalized false protected md hash new image digest null protected nn storage storage null ur is importing image checkpoint in default case ur is represent directories collection uri checkpoint dirs collection uri checkpoint edits dirs configuration conf can fs image rolled volatile protected checkpoint states ckpt state fs image checkpoint states start fs image fs namesystem null constructor fs image configuration conf throws io exception conf conf todo many constructors mess conf get boolean dfs config keys dfs namenode name dir restore key set checkpoint directories fs image get checkpoint dirs conf null fs image fs namesystem ns conf new configuration storage new nn storage conf ns null storage register listener edit log new fs edit log storage set fs namesystem ns fs image collection uri fs dirs collection uri fs edits dirs storage set storage directories fs dirs fs edits dirs fs image storage info storage info string bpid storage new nn storage storage info bpid represents image image edit file fs image uri image dir throws io exception array list uri dirs new array list uri array list uri edits dirs new array list uri dirs add image dir edits dirs add image dir storage set storage directories dirs edits dirs protected fs namesystem get fs namesystem return namesystem set fs namesystem fs namesystem ns namesystem ns ns null set checkpoint directories collection uri dirs checkpoint dirs dirs checkpoint edits dirs edits dirs analyze storage directories recover previous transitions required perform fs state transition necessary depending namespace info read storage info boolean recover transition read collection uri data dirs assert start opt startup option format none data dirs exist data dirs size edits dirs size start opt startup option import start opt startup option import storage set storage directories data dirs edits dirs for data directory calculate state check whether consistent transitioning map storage directory storage state data dir states boolean formatted false iterator storage directory storage dir iterator next formatted start opt startup option rollback layout version storage get layout version layout version storage last pre upgrade layout version start opt startup option upgrade storage process startup options for upgrade start opt layout version check whether distributed upgrade required continued storage verify distributed upgrade progress start opt format unformatted dirs storage set checkpoint time l iterator storage directory storage dir iterator next do transitions switch start opt case upgrade case import case rollback case regular boolean need to save load fs image assert edit log null edit log must initialized edit log open return need to save upgrade throws io exception storage get distributed upgrade state upgrade allowed previous fs states directories iterator storage directory storage dir iterator next load latest image load fs image do
729	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageCompression.java	unrelated	package org apache hadoop hdfs server namenode simple container handles support compressed fsimage files fs image compression codec use save load image null image compressed compression codec image codec fs image compression fs image compression compression codec codec fs image compression create noop compression fs image compression create compression configuration conf fs image compression create compression configuration conf fs image compression read compression header data input stream unwrap input stream input stream throws io exception data output stream write header and wrap stream output stream os throws io exception string string
730	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageFormat.java	unrelated	package org apache hadoop hdfs server namenode contains inner reading writing disk format fs images fs image format log log fs image log static fs image format a one shot responsible loading image the load function called getter methods may used retrieve information image loaded loading successful loader configuration conf namesystem loader working fs namesystem namesystem set true file loaded using loader boolean loaded false the image version loaded file img version the namespace id loaded file img namespace id the md sum loaded file md hash img digest loader configuration conf fs namesystem namesystem get loaded image version md hash get loaded image md get loaded namespace id check loaded check not loaded load file cur file update root node attributes update root attr i node root ns quota root get ns quota ds quota root get ds quota fs directory fs dir namesystem dir ns quota ds quota fs dir root dir set modification time root get modification time fs dir root dir set permission status root get permission status load fsimage files assuming local names stored load local name i nodes num files data input stream throws io exception load children directory load directory data input stream throws io exception load fsimage files assuming full path names stored load full name i nodes num files byte path components byte parent path fs directory fs dir namesystem dir i node directory parent i node fs dir root dir num files load inode fsimage except name i node load i node data input stream throws io exception modification time atime block size short replication read short replication namesystem adjust replication replication modification time read long layout version supports feature file access time img version img version num blocks read int block info blocks null older versions blocklist size indicates directory img version num blocks older versions hdfs store block size inode if file one block use size first block blocksize otherwise use default block size img version block size get quota node directory ns quota l load datanodes data input stream throws io exception load files under construction data input stream throws io exception load secret manager state data input stream throws io exception read num files data input stream throws io exception boolean root byte path boolean parent byte path byte parent string get parent string path byte get parent byte path a one shot responsible writing image file the write function called getter functions may used retrieve information file written saver set true image written boolean saved false the md checksum file written md hash saved digest byte path separator dfs util bytes path separator check saved check not saved md hash get saved digest save file new file save image byte buffer current dir name
731	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageSerialization.java	unrelated	package org apache hadoop hdfs server namenode static utility functions serializing various pieces data correct format fs image file some members currently benefit offline image viewer located outside package these members made package protected oiv refactored fs image serialization static fs image serialization in order reduce allocation reuse objects however methods thread safe since image saving multithreaded need keep objects thread local thread local tl data tl data new thread local tl data protected tl data initial value simple container threadlocal data tl data deprecated utf u str new deprecated utf fs permission file perm new fs permission short helper function reads i node under construction input stream i node file under construction read i node under construction byte name read bytes short block replication read short modification time read long preferred block size read long num blocks read int block info blocks new block info num blocks block blk new block num blocks last block under construction num blocks permission status perm permission status read string client name read string string client machine read string these locations used num locs read int datanode descriptor locations new datanode descriptor num locs num locs return new i node file under construction name helper function writes i node under construction input stream write i node under construction data output stream write string path write short cons get replication write long cons get modification time write long cons get preferred block size nr blocks cons get blocks length write int nr blocks nr blocks cons get permission status write write string cons get client name write string cons get client machine write int store locations last block save one inode attributes image save i node image i node node byte name node get local name bytes write short name length write name fs permission file perm tl data get file perm node directory else node link else this reverted package image loader code moved package this method called code string read string data input stream throws io exception deprecated utf ustr tl data get u str ustr read fields return ustr string string read string empty as null data input stream throws io exception string read string return empty null write string string str data output stream throws io exception deprecated utf ustr tl data get u str ustr set str ustr write same comments apply method read string byte read bytes data input stream throws io exception deprecated utf ustr tl data get u str ustr read fields len ustr get length byte bytes new byte len system arraycopy ustr get bytes bytes len return bytes reading path image converting byte directly saves us array copy conversions string path component byte read path components data input stream deprecated utf ustr tl data get u str ustr read fields return dfs util bytes byte array ustr get bytes datanode image used store persistent information datanodes fs image datanode image implements writable datanode descriptor node new datanode descriptor skip one data input throws io exception writable public method serializes information datanode stored fs image write data
732	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSInodeInfo.java	unrelated	package org apache hadoop hdfs server namenode this used used pluggable block placement policy expose characteristics inode fs inode info representation inode string get full path name
733	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSNamesystem.java	authenticate	package org apache hadoop hdfs server namenode fs namesystem actual bookkeeping work data node it tracks several important tables valid fsname blocklist kept disk logged set valid blocks inverted block machinelist kept memory rebuilt dynamically reports machine blocklist inverted lru cache updated heartbeat machines fs namesystem implements fs constants fs namesystem m bean fs cluster stats name node mx bean log log log factory get log fs namesystem thread local string builder audit buffer new thread local string builder log audit event user group information ugi string builder sb audit buffer get sb set length sb append ugi append ugi append sb append ip append addr append sb append cmd append cmd append sb append src append src append sb append dst append dst append null stat else audit log info sb logger audit events noting successful fs namesystem operations emits fs namesystem audit info each event causes set tab separated code key value code pairs written following properties code ugi lt ugi rpc gt ip lt remote ip gt cmd lt command gt src lt src path gt dst lt dst path optional gt perm lt permissions optional gt code log audit log log factory get log default max corrupt fileblocks returned block deletion increment boolean permission enabled user group information fs owner string supergroup permission status default permission fs namesystem metrics counter variables capacity total l capacity used l capacity remaining l block pool used l total load boolean block token enabled block token secret manager block token secret manager block key update interval block token lifetime scan interval configurable delegation token remover scan interval time unit milliseconds convert time unit hours delegation token secret manager dt secret manager stores correct file name hierarchy fs directory dir block manager block manager block pool id used namenode string block pool id stores subset datanode map containing nodes considered alive the heartbeat monitor periodically checks dated entries removes list array list datanode descriptor heartbeats new array list datanode descriptor lease manager lease manager new lease manager threaded object checks see getting heartbeats clients daemon hbthread null heartbeat monitor thread daemon lmthread null lease monitor thread daemon smmthread null safe mode monitor thread daemon nnrmthread null namenode resource monitor thread volatile boolean resources available false volatile boolean fs running true system start heartbeat recheck interval often namenode checks expired datanodes heartbeat recheck interval resource recheck interval often namenode checks disk space availability resource recheck interval the actual resource checker instance name node resource checker nn resource checker fs server defaults server defaults allow appending hdfs files boolean support appends true replace datanode on failure dtp replace datanode on failure volatile safe mode info safe mode safe mode information max fs objects maximum number fs objects the global generation stamp file system generation stamp generation stamp new generation stamp precision access times access time precision lock protect fs namesystem reentrant read write lock fs lock fs namesystem constructor fs namesystem configuration conf throws io exception try catch io exception e initialize fs namesystem initialize configuration conf fs image fs image resource recheck interval nn
734	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSPermissionChecker.java	unrelated	package org apache hadoop hdfs server namenode perform permission checking link fs namesystem fs permission checker log log log factory get log user group information user group information ugi string user set string groups new hash set string boolean super fs permission checker string fs owner string supergroup throws access control exception try ugi user group information get current user catch io exception e throw new access control exception e groups add all arrays list ugi get group names user ugi get short user name super user equals fs owner groups contains supergroup check callers group contains required values boolean contains group string group return groups contains group verify caller required permission this result exception caller allowed access resource check superuser privilege user group information owner fs permission checker checker new fs permission checker owner get short user name supergroup checker super throw new access control exception access denied user check whether current user permissions access path traverse always checked parent path means parent directory path ancestor path means last closest existing ancestor directory path note parent path exists parent path ancestor path for example suppose path foo bar baz no matter baz file directory parent path foo bar if bar exists ancestor path also foo bar if bar exist foo exists ancestor path foo further foo bar exist ancestor path access required path sub directories if path directory effect check permission string path i node directory root boolean check owner fs action ancestor access fs action parent access fs action access fs action sub access throws access control exception unresolved link exception log debug enabled log debug access check check parent access null file exists check sb resolve symlinks check performed link target i node inodes root get existing path i nodes path true ancestor index inodes length ancestor index inodes ancestor index null check traverse inodes ancestor index parent access null parent access implies fs action write ancestor access null inodes length parent access null inodes length access null sub access null check owner check owner i node inode throws access control exception inode null user equals inode get user name return throw new access control exception permission denied check traverse i node inodes last throws access control exception j j last j check inodes j fs action execute check sub access i node inode fs action access throws access control exception inode null inode directory return stack i node directory directories new stack i node directory directories push i node directory inode directories empty i node directory directories pop check access i node child get children check i node inodes fs action access throws access control exception check inodes null access check i node inode fs action access throws access control exception inode null return fs permission mode inode get fs permission user equals inode get user name user mode get user action implies access return else groups contains inode get group name group mode get group action implies access return else mode get other action implies access return throw new access control exception permission denied user user check
735	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\GetDelegationTokenServlet.java	unrelated	package org apache hadoop hdfs server namenode serve delegation tokens http use hftp get delegation token servlet extends dfs servlet log log log factory get log get delegation token servlet string path spec get delegation token string renewer renewer protected get http servlet request req http servlet response resp
736	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\GetImageServlet.java	unrelated	package org apache hadoop hdfs server namenode this used namesystem jetty retrieve file typically used secondary name node retrieve image edit file periodic checkpointing get image servlet extends http servlet serial version uid l log log log factory get log get image servlet get http servlet request request data transfer throttler get throttler configuration conf protected boolean valid requestor string remote user configuration conf
737	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INode.java	unrelated	package org apache hadoop hdfs server namenode we keep memory representation file block hierarchy this base i node containing common fields file directory inodes i node implements comparable byte fs inode info protected byte name protected i node directory parent protected modification time protected access time simple wrapper two counters dir counts only updated update permission status other codes modify permission enum permission status format protected i node i node permission status permissions time atime protected i node string name permission status permissions copy constructor i node i node boolean root set link permission status protected set permission status permission status ps get link permission status protected permission status get permission status synchronized update permission status get user name string get user name set user protected set user string user get group name string get group name set group protected set group string group get link fs permission fs permission get fs permission protected short get fs permission short set link fs permission link i node protected set permission fs permission permission boolean directory collect subtree blocks and clear list block v compute link content summary content summary compute content summary compute content summary summary get ns quota get ds quota boolean quota set dir counts space consumed in tree dir counts counts string get local name string get local parent dir byte get local name bytes set local name string name set local name byte name inherit doc string get full path name inherit doc string string i node directory get parent get modification time set modification time modtime set modification time force modtime get access time set access time atime boolean under construction boolean link byte get path components string path convert strings byte arrays path components byte get path components string strings string get path names string path string construct path byte components start boolean remove node comparable compare to byte boolean equals object hash code methods compare bytes byte byte i node new i node permission status permissions
738	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeDirectory.java	unrelated	package org apache hadoop hdfs server namenode directory i node i node directory extends i node protected default files per directory string root name list i node children i node directory string name permission status permissions i node directory permission status permissions time constructor i node directory byte local name permission status permissions time copy constructor i node directory i node directory check whether directory boolean directory i node remove child i node node replace child name new child new child replace child i node new child i node get child string name i node get child i node byte name return i node last component components null last component exist i node get node byte components boolean resolve link this external i node get node string path boolean resolve link retrieve existing i nodes path if existing big enough store path components existing non existing existing i nodes stored starting root i node existing existing big enough store path components last existing non existing i nodes stored existing existing length refers i node component an unresolved path exception always thrown intermediate path component refers symbolic link if path component refers symbolic link unresolved path exception thrown resolve link true p example br given path c c c c c exists resulting following path components c c c p code get existing path i nodes c c code fill array c br code get existing path i nodes c c c code fill array null p code get existing path i nodes c c code fill array c c br code get existing path i nodes c c c code fill array c null p code get existing path i nodes c c code fill array root i node c c null br code get existing path i nodes c c c code fill array root i node c c null thrown path refers symbolic link get existing path i nodes byte components i node existing retrieve existing i nodes along given path the first i node always exist i node thrown path refers symbolic link appear following path root i node deepest i nodes the array size number expected components path non existing components filled null i node get existing path i nodes string path boolean resolve link given child name return index next child next child byte name add child inode directory needed replaying addition parent already proper mod time node otherwise t extends i node t add child t node boolean inherit permission equivalent add node path new node false t extends i node t add node string path t new node add new i node file tree find parent insert directory t extends i node t add node string path t new node boolean inherit permission add new inode parent specified optimized version add node parent null null already exists directory i node directory add to parent byte localname i node directory get parent byte path components throws file not found exception unresolved link exception add new inode optimized version add node null already exists directory i node
739	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeDirectoryWithQuota.java	unrelated	package org apache hadoop hdfs server namenode directory i node quota restriction i node directory with quota extends i node directory ns quota name space quota ns count ds quota disk space quota diskspace convert existing directory inode one given quota i node directory with quota ns quota ds quota i node directory throws quota exceeded exception constructor quota verification i node directory with quota constructor quota verification i node directory with quota string name permission status permissions get directory namespace quota get ns quota get directory diskspace quota get ds quota set directory quota set quota new ns quota new ds quota dir counts space consumed in tree dir counts counts get number names subtree rooted directory num items in tree diskspace consumed update size tree update num items in tree ns delta ds delta update size tree unprotected update num items in tree ns delta ds delta set space consumed namespace diskspace verify namespace count disk space satisfies quota restriction verify quota ns delta ds delta throws quota exceeded exception
740	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeFile.java	unrelated	package org apache hadoop hdfs server namenode i node closed file i node file extends i node fs permission umask fs permission create immutable short number bits block size short blockbits header mask bit representation format bits replication bits preferred block size headermask xffff l blockbits protected header protected block info blocks null i node file permission status permissions protected i node file protected i node file permission status permissions block info blklist set link fs permission link i node file since file link fs action execute action ignored protected set permission fs permission permission boolean directory get block replication file short get replication set replication short replication get preferred block size file get preferred block size set preferred block size preferred blk size get file blocks block info get blocks append array blocks blocks append blocks i node file inodes total added blocks add block block list add block block info newblock set file block set block idx block info blk collect subtree blocks and clear list block v inherit doc compute content summary summary compute file size may may block info under construction compute file size boolean includes block info under construction dir counts space consumed in tree dir counts counts diskspace consumed diskspace consumed block blk arr return penultimate allocated block file block info get penultimate block get last block file make sure right type t extends block info t get last block throws io exception num blocks
741	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeFileUnderConstruction.java	unrelated	package org apache hadoop hdfs server namenode i node file written i node file under construction extends i node file string client name lease holder string client machine datanode descriptor client node client cluster node i node file under construction permission status permissions i node file under construction byte name string get client name set client name string client name string get client machine datanode descriptor get client node is inode constructed boolean under construction converts i node file under construction i node file use modification time access time i node file convert to inode file remove block block list this block last one list remove last block block oldblock throws io exception convert last block file construction block set locations block info under construction set last block block info last block throws io exception
742	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeSymlink.java	unrelated	package org apache hadoop hdfs server namenode an i node representing symbolic link i node symlink extends i node byte symlink the target uri i node symlink string value mod time atime boolean link set link value string value string get link value byte get symlink dir counts space consumed in tree dir counts counts collect subtree blocks and clear list block v compute content summary summary boolean directory
743	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\JournalStream.java	unrelated	package org apache hadoop hdfs server namenode a generic journal input output streams journal stream type underlying persistent storage type stream based upon ul li link journal type file streams edits local file see link fs edit log edit log file output stream link fs edit log edit log file input stream li li link journal type backup streams edits backup node see link edit log backup output stream link edit log backup input stream li ul enum journal type get stream name string get name get type stream determines underlying persistent storage type journal type get type
744	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\LeaseExpiredException.java	unrelated	package org apache hadoop hdfs server namenode the lease used create file expired lease expired exception extends io exception serial version uid l lease expired exception string msg
745	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\LeaseManager.java	unrelated	package org apache hadoop hdfs server namenode lease manager lease housekeeping writing files this also provides useful methods lease recovery lease recovery algorithm namenode retrieves lease information for file f lease consider last block b f get datanodes contains b assign one datanodes primary datanode p p obtains new generation stamp namenode p gets block info datanode p computes minimum block length p updates datanodes valid generation stamp new generation stamp minimum block length p acknowledges namenode update results namenode updates block info namenode removes f lease removes lease files removed namenode commit changes edit log lease manager log log log factory get log lease manager fs namesystem fsnamesystem soft limit fs constants lease softlimit period hard limit fs constants lease hardlimit period used handling lock leases mapping lease holder lease sorted map string lease leases new tree map string lease set lease sorted set lease sorted leases new tree set lease map path names leases it protected sorted leases lock the map stores pathnames lexicographical order sorted map string lease sorted leases by path new tree map string lease lease manager fs namesystem fsnamesystem fsnamesystem fsnamesystem lease get lease string holder sorted set lease get sorted leases return sorted leases lease get lease by path string src return sorted leases by path get src synchronized count lease return sorted leases size synchronized count path synchronized lease add lease string holder string src synchronized remove lease lease lease string src synchronized remove lease string holder string src synchronized lease reassign lease lease lease string src string new holder synchronized string find path i node file under construction pending file synchronized renew lease string holder synchronized renew lease lease lease lease implements comparable lease synchronized change lease string src string dst synchronized remove lease with prefix path string prefix list map entry string lease find lease with prefix path set lease period soft limit hard limit monitor implements runnable check leases beginning oldest synchronized check leases inherit doc synchronized string string
746	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ListPathsServlet.java	unrelated	package org apache hadoop hdfs server namenode obtain meta information filesystem list paths servlet extends dfs servlet for java io serializable serial version uid l thread local simple date format df write node output node information includes path modification permission owner group for files also includes size replication block size write info path fullpath hdfs file status build map query setting values defaults protected map string string build root http servlet request request service get request described request code get http nn port list paths path option option http where option default recursive quot quot filter quot quot exclude quot crc quot response a flat list files directories following format code listing path recursive yes filter time yyyy mm dd hh mm ss utc version directory path modified yyyy mm dd hh mm ss file path modified yyyy mm dd t hh mm ss z accesstime yyyy mm dd t hh mm ss z blocksize replication size listing get http servlet request request http servlet response response
747	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameCache.java	unrelated	package org apache hadoop hdfs server namenode caches frequently used names facilitate reuse example byte representation file name link i node this used initially adding file names cache tracks number times name used transient map it promotes name used code use threshold cache one names added link initialized called finish initialization the transient map use count tracked discarded cache ready use p this must synchronized externally name cache k use count log log log factory get log name cache get name indicates initialization progress boolean initialized false names used code use threshold added cache use threshold times cache look successful lookups cached names hash map k k cache new hash map k k names number occurrences tracked initialization map k use count transient map new hash map k use count name cache use threshold k put k name get lookup count size initialized promote frequently used name cache promote k name
748	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNode.java	heartbeat	package org apache hadoop hdfs server namenode name node serves directory namespace manager inode table hadoop dfs there single name node running dfs deployment well except second backup failover name node using federated name nodes the name node controls two critical tables filename blocksequence namespace block machinelist inodes the first table stored disk precious the second table rebuilt every time name node comes name node refers well name node server the fs namesystem actually performs filesystem management the majority name node concerned exposing ipc http server outside world plus configuration management name node implements link org apache hadoop hdfs protocol client protocol allows clients ask dfs services link org apache hadoop hdfs protocol client protocol designed direct use authors dfs client code end users instead use link org apache hadoop fs file system name node also implements link org apache hadoop hdfs server protocol datanode protocol used data nodes actually store dfs data blocks these methods invoked repeatedly automatically data nodes dfs deployment name node also implements link org apache hadoop hdfs server protocol namenode protocol used secondary namenodes rebalancing processes get partial name node state example partial blocks map etc name node implements namenode protocols fs constants string nameservice specific keys get protocol version string protocol protocol signature get protocol signature string protocol default port log log log factory get log name node get name log state change log log factory get log org apache hadoop hdfs state change protected fs namesystem namesystem protected namenode role role rpc server package protected use tests server server rpc server hdfs services communication protected server service rpc server rpc server address protected inet socket address rpc address null rpc server dn address protected inet socket address service rpc address null http server protected name node http server http server thread emptier used testing purposes protected boolean stop requested false registration information name node protected namenode registration node registration is service level authorization enabled boolean service auth enabled false activated plug ins list service plugin plugins format new filesystem destroys filesystem may already format configuration conf throws io exception name node metrics metrics return link fs namesystem object fs namesystem get namesystem init metrics configuration conf namenode role role name node metrics get name node metrics inet socket address get address string address set service address configuration conf inet socket address get service address configuration conf inet socket address get address configuration conf inet socket address get address uri filesystem uri uri get uri inet socket address namenode string get host port string inet socket address addr common name node methods implementation active name node role namenode role get role boolean role namenode role protected inet socket address get service rpc server address configuration conf protected inet socket address get rpc server address configuration conf throws io exception protected set rpc service server address configuration conf protected set rpc server address configuration conf protected inet socket address get http server address configuration conf protected set http server address configuration conf protected load namesystem configuration conf throws io exception namenode registration get registration namenode registration set
749	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NamenodeFsck.java	pooling	package org apache hadoop hdfs server namenode this provides rudimentary checking dfs volumes errors sub optimal conditions p the tool scans files directories starting indicated root path the following abnormal conditions detected handled p ul li files blocks completely missing datanodes br in case tool perform one following actions ul li none link fixing none li li move corrupted files lost found directory dfs link fixing move remaining data blocks saved block chains representing longest consecutive series valid blocks li li delete corrupted files link fixing delete li ul li li detect files replicated replicated blocks li ul additionally tool collects detailed overall dfs statistics optionally print detailed statistics block locations replication factors file namenode fsck log log log factory get log name node get name return marking fsck status string corrupt status corrupt string healthy status healthy string nonexistent status exist string failure status failed don attempt fixing fixing none move corrupted files lost found fixing move delete corrupted files fixing delete name node namenode network topology networktopology total datanodes short min replication inet address remote address string lost found null boolean lf inited false boolean lf inited ok false boolean show files false boolean show open files false boolean show blocks false boolean show locations false boolean show racks false boolean show corrupt file blocks false fixing fixing none string path we return back n files corrupt list files returned ordered block id allow continuation support pass last block previous call string start block after null configuration conf print writer namenode fsck configuration conf name node namenode fsck list corrupt file blocks throws io exception check string parent hdfs file status file result res throws io exception lost found move string parent hdfs file status file located blocks blocks copy block dfs client dfs located block lblock datanode info best node dfs client dfs datanode info nodes lost found init dfs client dfs result
750	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeHttpServer.java	unrelated	package org apache hadoop hdfs server namenode encapsulates http server started name node name node http server http server http server configuration conf name node nn log log name node log inet socket address http address inet socket address bind address string namenode address attribute key name node address string fsimage attribute key name system image protected string namenode attribute key name node name node http server string get default server principal throws io exception start throws io exception stop throws exception inet socket address get http address setup servlets http server http server fs image get fs image from context servlet context context name node get name node from context servlet context context configuration get conf from context servlet context context inet socket address get name node address from context
751	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NamenodeJspHelper.java	pooling	package org apache hadoop hdfs server namenode namenode jsp helper string get safe mode text fs namesystem fsn fsn in safe mode return safe mode on em fsn get safe mode tip em br returns security mode cluster namenode string get security mode text user group information security enabled else string get inode limit text fs namesystem fsn inodes fsn dir total inodes blocks fsn get blocks total maxobjects fsn get max objects memory mx bean mem management factory get memory mx bean memory usage heap mem get heap memory usage total memory heap get used max memory heap get max commited memory heap get committed memory usage non heap mem get non heap memory usage total non heap non heap get used max non heap non heap get max commited non heap non heap get committed used total memory commited memory used non heap total non heap commited non heap string str inodes files directories blocks blocks maxobjects str br str heap memory used string utils byte desc total memory str non heap memory used string utils byte desc total non heap return str string get upgrade status text fs namesystem fsn string status text try catch io exception e return status text return table containing version information string get version table fs namesystem fsn return div id dfstable table generate warning text corrupt files string get corrupt files warning fs namesystem fsn missing blocks fsn get missing blocks count missing blocks return health jsp row num col num string sorter field null string sorter order null string row txt string col txt string col txt string title counter reset generate conf report jsp writer name node nn generate health report jsp writer name node nn string get delegation token name node nn token delegation token identifier token ugi return token null null token encode to url string network topology get network topology name node namenode return namenode get namesystem get block manager get datanode manager datanode descriptor get random datanode name node namenode return datanode descriptor get network topology namenode choose random redirect to random data node servlet context context name node nn name node http server get name node from context context configuration conf configuration context datanode id datanode get random datanode nn user group information ugi jsp helper get ugi context request conf string token string get delegation token nn request conf ugi user defined get delegation token stringify string redirect location string node to redirect redirect port datanode null else string addr name node get host port string nn get name node address string fqdn inet address get by name node to redirect get canonical host name redirect location http fqdn redirect port resp send redirect redirect location node list jsp row num disk bytes string disk byte str gb string sorter field null string sorter order null string nodes live string row txt counter reset string node header str string name generate node data header jsp writer datanode descriptor generate decommissioning node data jsp writer datanode descriptor generate node data jsp writer datanode descriptor string suffix
752	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeMXBean.java	pooling	package org apache hadoop hdfs server namenode this jmx management namenode information name node mx bean gets version hadoop string get version gets used space data nodes get used gets total non used raw bytes get free gets total raw bytes including non dfs used space get total gets safemode status string get safemode checks upgrade finalized boolean upgrade finalized gets total used space data nodes non dfs purposes storing temporary files local file system get non dfs used space gets total used space data nodes percentage total capacity get percent used gets total remaining space data nodes percentage total capacity get percent remaining get total space used block pools namenode get block pool used space get total space used block pool percentage total capacity get percent block pool used gets total numbers blocks cluster get total blocks gets total number files cluster get total files gets total number missing blocks cluster get number of missing blocks gets number threads get threads gets live node information cluster string get live nodes gets dead node information cluster string get dead nodes gets decommissioning node information cluster string get decom nodes gets cluster id string get cluster id gets block pool id string get block pool id
753	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeResourceChecker.java	unrelated	package org apache hadoop hdfs server namenode name node resource checker provides method code available disk space code return true name node disk space available volumes configured checked volumes containing file system name edits dirs added default arbitrary extra volumes may configured well name node resource checker log log log factory get log name node resource checker get name space bytes reserved per volume du reserved configuration conf map string df volumes create name node resource checker check name dirs edits dirs set code conf code name node resource checker configuration conf throws io exception add passed directories list volumes check the directories whose volumes checked available space add dirs to check collection uri directories to check return true disk space available configured volumes volumes false otherwise boolean available disk space return set directories low space collection string get volumes low on space throws io exception
754	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NNStorage.java	pooling	package org apache hadoop hdfs server namenode nn storage responsible management storage directories used name node nn storage extends storage implements closeable log log log factory get log nn storage get name string message digest property image md digest the filenames used storing images enum name node file implementation storage dir type specific namenode storage a storage directory could type image stores fsimage type edits stores edits type image and edits stores fsimage edits enum name node dir type implements storage dir type interface implemented make use storage directories they notified storage directory causing errors becoming available formatted this allows implementors take specific action storage directory occurs nn storage listener list nn storage listener listeners upgrade manager upgrade manager null protected md hash image digest null protected string blockpool id id block pool flag controls try restore failed storages boolean restore failed storage false object restoration lock new object boolean disable pre upgradable layout check false checkpoint time l the age image list failed thus removed storages protected list storage directory removed storage dirs construct nn storage nn storage configuration conf construct nn storage nn storage storage info storage info string bpid boolean pre upgradable layout storage directory sd throws io exception close throws io exception set flag whether attempt made restore failed storage directories next available oppurtuinity set restore failed storage boolean val boolean get restore failed storage see removed storages writable returned service if save namespace set method called save namespace attempt restore removed storage list storage directory get removed storage dirs set storage directories used nn storage close called ensure previous storage directories freed synchronized due initialization storage dirs removed storage dirs synchronized set storage directories collection uri fs name dirs add name dirs appropriate name node dir type checks consistency uri particular scheme specified supported concrete implementation check scheme consistency uri u throws io exception retrieve current directories type image collection uri get image directories throws io exception retrieve current directories type edits collection uri get edits directories throws io exception return number storage directories given type get num storage dirs name node dir type dir type return list locations used specific purpose e image edit log storage collection uri get directories name node dir type dir type determine checkpoint time specified storage directory read checkpoint time storage directory sd throws io exception write last checkpoint time separate file write checkpoint time storage directory sd throws io exception record new checkpoint time order distinguish healthy directories removed ones if error writing new checkpoint time corresponding storage directory removed list increment checkpoint time the age namespace state p reflects latest time image saved modified every save checkpoint persisted version file get checkpoint time set checkpoint time this method persist checkpoint time storage immediately set checkpoint time new cp t set current checkpoint time writes new checkpoint time available storage directories set checkpoint time in storage new cp t return name image file uploaded periodic checkpointing file get fs image name checkpoint return name image file file get fs image name file get fs edit name throws io exception file
755	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NotReplicatedYetException.java	unrelated	package org apache hadoop hdfs server namenode the file finished written enough datanodes yet not replicated yet exception extends io exception serial version uid l not replicated yet exception string msg
756	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\RenewDelegationTokenServlet.java	unrelated	package org apache hadoop hdfs server namenode renew delegation tokens http use hftp renew delegation token servlet extends dfs servlet log log log factory get log renew delegation token servlet string path spec renew delegation token string token token protected get http servlet request req http servlet response resp
757	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SafeModeException.java	unrelated	package org apache hadoop hdfs server namenode this exception thrown name node safe mode client cannot modified namespace safe mode safe mode exception extends io exception serial version uid l safe mode exception safe mode exception string text fs namesystem safe mode info mode
758	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SecondaryNameNode.java	pooling	package org apache hadoop hdfs server namenode the secondary name node helper primary name node the secondary responsible supporting periodic checkpoints hdfs metadata the current design allows one secondary name node per hd fs cluster the secondary name node daemon periodically wakes determined schedule specified configuration triggers periodic checkpoint goes back sleep the secondary name node uses client protocol talk primary name node secondary name node implements runnable hdfs configuration init log log log factory get log secondary name node get name starttime system current time millis volatile last checkpoint time string fs name checkpoint storage checkpoint image namenode protocol namenode configuration conf inet socket address name node addr volatile boolean run http server info server info port image port string info bind address fs namesystem namesystem collection uri checkpoint dirs collection uri checkpoint edits dirs checkpoint period seconds checkpoint size size bytes current edit log inherit doc string string return get class get simple name status fs image get fs image return checkpoint image create connection primary namenode secondary name node configuration conf throws io exception try catch io exception e inet socket address get http address configuration conf return net utils create socket addr conf get initialize secondary name node initialize configuration conf throws io exception inet socket address info soc addr get http address conf info bind address info soc addr get host name user group information set configuration conf user group information security enabled initiate java vm metrics jvm metrics create secondary name node create connection namenode run true name node addr name node get service address conf true conf conf namenode initialize checkpoint directories fs name get info server checkpoint dirs fs image get checkpoint dirs conf checkpoint edits dirs fs image get checkpoint edits dirs conf checkpoint image new checkpoint storage conf checkpoint image recover create checkpoint dirs checkpoint edits dirs initialize scheduling parameters configuration checkpoint period conf get long dfs namenode checkpoint period key checkpoint size conf get long dfs namenode checkpoint size key initialize webserver uploading files kerberized ssl servers must run host principal user group information http ugi try catch interrupted exception e log info web server init done the web server port ephemeral ensure correct info info port info server get port user group information security enabled conf set dfs namenode secondary http address key info bind address info port log info secondary web server info bind address info port log info secondary image servlet info bind address image port log warn checkpoint period checkpoint period secs log warn log size trigger checkpoint size bytes shut instance datanode returns shutdown complete shutdown run false try catch exception e try catch io exception e run user group information security enabled else the main work loop work poll namenode every minutes find size pending edit log period minutes checkpoint period period run download code fsimage code code edits code files name node boolean download checkpoint files checkpoint signature sig try inet socket address get name node address return name node addr copy new fsimage name node put fs image checkpoint signature sig throws io
759	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SerialNumberManager.java	unrelated	package org apache hadoop hdfs server namenode manage name serial number maps users groups serial number manager this instance link serial number manager serial number manager instance new serial number manager serial number map string usermap new serial number map string serial number map string groupmap new serial number map string serial number manager get user serial number string u return usermap get u get group serial number string g return groupmap get g string get user n return usermap get n string get group n return groupmap get n get user serial number null get group serial number null serial number map t max next serial number return max map t integer new hash map t integer map integer t new hash map integer t synchronized get t synchronized t get inherit doc string string
760	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\StreamFile.java	unrelated	package org apache hadoop hdfs server namenode stream file extends dfs servlet java io serializable serial version uid l string content length content length return dfs client use make given http request protected dfs client get dfs client http servlet request request get http servlet request request http servlet response response send partial content response given range if satisfiable ranges multiple ranges requested unsupported respond range satisfiable send partial data fs input stream copy count bytes given offset one stream another copy from offset fs input stream output stream offset
761	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\TransferFsImage.java	unrelated	package org apache hadoop hdfs server namenode this provides fetching specified file name node transfer fs image implements fs constants string content length content length boolean get image boolean get edit boolean put image remoteport string machine name checkpoint signature token md hash new checksum null transfer fs image map string string pmap boolean get edit boolean get image boolean put image checkpoint signature get token md hash get new checksum string get info server throws io exception get file server output stream outstream file localfile md hash get file client string fs name string id file local path
762	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UnsupportedActionException.java	unrelated	package org apache hadoop hdfs server namenode this exception thrown operation supported unsupported action exception extends io exception java io serializable serial version uid l unsupported action exception string action
763	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UpgradeManagerNamenode.java	unrelated	package org apache hadoop hdfs server namenode upgrade manager name nodes distributed upgrades name node starts safe mode conditions met name node exit at point name node enters manual safe mode remain upgrade completed after name nodes processes upgrade commands data nodes updates status upgrade manager namenode extends upgrade manager hdfs constants node type get type fs namesystem namesystem upgrade manager namenode fs namesystem namesystem start distributed upgrade instantiates distributed upgrade objects synchronized boolean start upgrade throws io exception synchronized upgrade command process upgrade command upgrade command command synchronized complete upgrade throws io exception synchronized upgrade status report distributed upgrade progress
764	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UpgradeObjectNamenode.java	unrelated	package org apache hadoop hdfs server namenode base name node upgrade objects data node upgrades run separate threads upgrade object namenode extends upgrade object upgrade command process upgrade command upgrade command command hdfs constants node type get type upgrade command start upgrade throws io exception force proceed throws io exception
765	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\metrics\FSNamesystemMBean.java	unrelated	package org apache hadoop hdfs server namenode metrics this interface defines methods get status fs namesystem name node it also used publishing via jmx hence follow jmx naming convention note used metrics dynamic m bean base implement name node state m bean stable published p name node runtime activity statistic info reported fs namesystem m bean the state file system safemode operational string get fs state number allocated blocks system get blocks total total storage capacity get capacity total free unused storage capacity get capacity remaining used storage capacity get capacity used total number files directories get files total blocks pending replicated get pending replication blocks blocks replicated get under replicated blocks blocks scheduled replication get scheduled replication blocks total load fs namesystem get total load number live data nodes get num live data nodes number dead data nodes get num dead data nodes
766	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\metrics\NameNodeMetrics.java	unrelated	package org apache hadoop hdfs server namenode metrics this maintaining various name node activity statistics publishing metrics interfaces name node metrics metrics registry registry new metrics registry namenode mutable counter long files deleted mutable counter long transactions batched in sync name node metrics string process name string session id name node metrics create configuration conf namenode role r shutdown incr get block locations incr files created incr create file ops incr files appended incr add block ops incr get additional datanode ops incr files renamed incr files deleted delta incr delete file ops incr get listing ops incr files in get listing ops delta incr file info ops incr create symlink ops incr get link target ops add transaction latency incr transactions batched in sync add sync elapsed set fs image load time elapsed add block report latency set safe mode time elapsed
767	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlockCommand.java	pooling	package org apache hadoop hdfs server protocol a block command instruction datanode regarding blocks control it tells data node either invalidate set indicated blocks copy set indicated blocks another data node block command extends datanode command string pool id block blocks datanode info targets block command create block command transferring blocks another datanode block command action string pool id datanode info empty target create block command given action block command action string pool id block blocks string get block pool id block get blocks datanode info get targets writable register ctor write data output throws io exception read fields data input throws io exception
768	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlockRecoveryCommand.java	unrelated	package org apache hadoop hdfs server protocol block recovery command instruction data node recover specified blocks the data node receives command treats primary data node recover process block recovery identified recovery id also new generation stamp block recovery succeeds block recovery command extends datanode command collection recovering block recovering blocks this block locations recovered new generation stamp block successful recovery the new generation stamp block also plays role recovery id recovering block extends located block new generation stamp create empty recovering block recovering block create recovering block recovering block extended block b datanode info locs new gs return new generation stamp block also plays role recovery id get new generation stamp writable register ctor write data output throws io exception read fields data input throws io exception create empty block recovery command block recovery command create block recovery command specified capacity recovering blocks block recovery command capacity super datanode protocol dna recoverblock recovering blocks new array list recovering block capacity return list recovering blocks collection recovering block get recovering blocks return recovering blocks add recovering block command add recovering block block recovering blocks add block writable register ctor writable factories set factory write data output throws io exception super write write int recovering blocks size recovering block block recovering blocks read fields data input throws io exception super read fields num blocks read int recovering blocks new array list recovering block num blocks num blocks
769	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlocksWithLocations.java	unrelated	package org apache hadoop hdfs server protocol a implement array block locations it provide efficient customized serialization deserialization methods stead using default array de serialization provided rpc blocks with locations implements writable a keep track block locations block with locations implements writable block block string datanode i ds default constructor block with locations constructor block with locations block b string datanodes get block block get block get block locations string get datanodes deserialization method read fields data input throws io exception serialization method write data output throws io exception block with locations blocks default constructor blocks with locations constructor one parameter blocks with locations block with locations blocks blocks blocks getter block with locations get blocks return blocks serialization method write data output throws io exception writable utils write v int blocks length blocks length deserialization method read fields data input throws io exception len writable utils read v int blocks new block with locations len len
770	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\CheckpointCommand.java	unrelated	package org apache hadoop hdfs server protocol checkpoint command p returned backup node name node reply link namenode protocol start checkpoint namenode registration request br contains ul li link checkpoint signature identifying particular checkpoint li li indicator whether backup image discarded starting checkpoint li li indicator whether image transfered back name node upon completion checkpoint li ul checkpoint command extends namenode command checkpoint signature c sig boolean image obsolete boolean need to return image checkpoint command checkpoint command checkpoint signature sig checkpoint signature used ensure nodes talking checkpoint checkpoint signature get signature indicates whether current backup image obsolete therefore need discarded boolean image obsolete indicates whether new checkpoint image needs transfered back name node checkpoint done boolean need to return image writable write data output throws io exception read fields data input throws io exception
771	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeCommand.java	pooling	package org apache hadoop hdfs server protocol base data node command issued name node notify data nodes done declare subclasses avro denormalized representation datanode command extends server command register extends datanode command finalize extends datanode command register ctor datanode command register new register datanode command datanode command action
772	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeProtocol.java	heartbeat	package org apache hadoop hdfs server protocol protocol dfs datanode uses communicate name node it used upload current load information block reports the way name node communicate data node returning values functions datanode protocol extends versioned protocol add block pool id block version id l error code notify disk error still valid volumes dn invalid block fatal disk error valid volumes left dn determines actions data node perform receiving datanode command dna unknown unknown action dna transfer transfer blocks another datanode dna invalidate invalidate blocks dna shutdown shutdown node dna register register dna finalize finalize previous upgrade dna recoverblock request block recovery dna accesskeyupdate update access key register datanode new storage id datanode one registration id communication datanode registration register datanode datanode registration registration send heartbeat tells name node data node still alive well includes status info it also gives name node chance return array datanode command objects a datanode command tells data node invalidate local block copy data nodes etc datanode command send heartbeat datanode registration registration block report tells name node locally stored blocks the name node returns array blocks become obsolete deleted this function meant upload locally stored blocks it invoked upon startup infrequently afterwards each block represented longs this done instead block reduce memory used block reports datanode command block report datanode registration registration block received allows data node tell name node recently received block data hint pereferred replica deleted excessive blocks for example whenever client code writes new block another data node copies block data node call block received block received datanode registration registration error report tells name node something gone awry useful debugging error report datanode registration registration namespace info version request throws io exception this general way send command name node distributed upgrade process the generosity variety upgrade commands unpredictable the reply name node also received form upgrade command upgrade command process upgrade command upgrade command comm throws io exception link org apache hadoop hdfs protocol client protocol report bad blocks located block report bad blocks located block blocks throws io exception commit block synchronization lease recovery commit block synchronization extended block block
773	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol datanode registration contains information name node needs identify verify data node contacts name node this information sent data node communication request datanode registration extends datanode id implements writable node registration register ctor storage info storage info exported block keys exported keys datanode registration datanode registration string node name set storage info storage info storage get version string get registration id string get address string string writable inherit doc write data output throws io exception inherit doc read fields data input throws io exception boolean equals object hash code
774	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DisallowedDatanodeException.java	unrelated	package org apache hadoop hdfs server protocol this exception thrown datanode tries register communicate namenode appear list included nodes specifically excluded disallowed datanode exception extends io exception java io serializable serial version uid l disallowed datanode exception datanode id node id
775	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\InterDatanodeProtocol.java	pooling	package org apache hadoop hdfs server protocol an inter datanode protocol updating generation stamp inter datanode protocol extends versioned protocol log log log factory get log inter datanode protocol add block pool id block version id l initialize replica recovery null data node replica replica recovery info init replica recovery recovering block r block throws io exception update replica new generation stamp length extended block update replica under recovery extended block old block
776	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\KeyUpdateCommand.java	unrelated	package org apache hadoop hdfs server protocol key update command extends datanode command exported block keys keys key update command key update command exported block keys keys exported block keys get exported keys writable register ctor write data output throws io exception read fields data input throws io exception
777	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeCommand.java	unrelated	package org apache hadoop hdfs server protocol base name node command issued name node notify name nodes done namenode command extends server command namenode command namenode command action
778	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeProtocol.java	unrelated	package org apache hadoop hdfs server protocol protocol secondary name node uses communicate name node it used get part name node state namenode protocol extends versioned protocol compared previous version following changes introduced only latest change reflected the log historical changes retrieved svn added one parameter roll fs image changed definition checkpoint signature version id l error codes passed error report notify fatal journal action codes see journal byte ja is alive check whether journal alive byte ja journal journal byte ja jspool start fs edit log op codes op jspool start byte ja checkpoint time fs edit log op codes op checkpoint time act unknown unknown action act shutdown shutdown node act checkpoint checkpoint get list blocks belonging code datanode code whose total size equals code size code blocks with locations get blocks datanode info datanode size throws io exception get current block keys exported block keys get block keys throws io exception get size current edit log bytes see link org apache hadoop hdfs server namenode secondary name node get edit log size throws io exception closes current edit log opens new one the call fails file system safe mode see link org apache hadoop hdfs server namenode secondary name node checkpoint signature roll edit log throws io exception rolls fs image log it removes old fs image copies new image fs image removes old edits renames edits new edits the call fails four files missing see link org apache hadoop hdfs server namenode secondary name node roll fs image checkpoint signature sig throws io exception request name node version storage information name node namespace info version request throws io exception report active name node error occurred subordinate node depending error code active node may decide unregister reporting node error report namenode registration registration register subordinate name node like backup node node registered namenode registration register namenode registration registration throws io exception a request active name node start checkpoint the name node decide whether admit reject the name node also decides done backup node image checkpoint namenode command start checkpoint namenode registration registration throws io exception a request active name node finalize previously started checkpoint end checkpoint namenode registration registration get size active name node journal edit log bytes journal size namenode registration registration throws io exception journal edit records this message sent active name node backup node via code edit log backup output stream order synchronize meta data changes backup namespace image journal namenode registration registration
779	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeProtocols.java	unrelated	package org apache hadoop hdfs server protocol the full set rpc methods implemented namenode namenode protocols extends client protocol
780	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol information sent subordinate name node active name node registration process namenode registration extends storage info implements node registration string rpc address rpc address node string http address http address node namenode role role node role checkpoint time l age image namenode registration namenode registration string address string get address string get registration id get version string string get name node role namenode role get role boolean role namenode role get age image get checkpoint time writable write data output throws io exception read fields data input throws io exception
781	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamespaceInfo.java	pooling	package org apache hadoop hdfs server protocol namespace info returned name node reply data node handshake namespace info extends storage info string build version distributed upgrade version string block pool id id block pool namespace info namespace info ns id string cluster id string bp id string get build version get distributed upgrade version string get block pool id writable register ctor write data output throws io exception read fields data input throws io exception string string
782	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol generic specifying information need sent name node registration process node registration string get address string get registration id get version string string
783	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\ReplicaRecoveryInfo.java	unrelated	package org apache hadoop hdfs server protocol replica recovery information replica recovery info extends block replica state original state replica recovery info replica recovery info block id disk len gs replica state r state replica state get original replica state boolean equals object hash code writable register ctor read fields data input throws io exception write data output throws io exception
784	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\ServerCommand.java	unrelated	package org apache hadoop hdfs server protocol base server command issued name node notify servers done commands defined actions defined respective protocols server command implements writable action server command server command action get action writable write data output throws io exception read fields data input throws io exception
785	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\UpgradeCommand.java	unrelated	package org apache hadoop hdfs server protocol this generic distributed upgrade command during upgrade cluster components send upgrade commands order obtain share information it supposed upgrade defines specific upgrade command deriving the upgrade command contains version upgrade verified receiving side current status upgrade upgrade command extends datanode command uc action unknown datanode protocol dna unknown uc action report status report upgrade status uc action start upgrade start upgrade version short upgrade status upgrade command upgrade command action version short status get version short get current status writable register ctor write data output throws io exception read fields data input throws io exception
786	hdfs\src\java\org\apache\hadoop\hdfs\tools\DelegationTokenFetcher.java	unrelated	package org apache hadoop hdfs tools fetch delegation token current namenode store specified file delegation token fetcher log log string webservice webservice string renewer renewer string cancel cancel string renew renew string print print print usage print stream err throws io exception collection token read tokens path file configuration conf command line main string args throws exception credentials get d tfrom remote string nn addr renew delegation token renew delegation token string nn addr throws io exception parse message extract name exception message io exception get exception from response http url connection con cancel delegation token cancel delegation token string nn addr throws io exception
787	hdfs\src\java\org\apache\hadoop\hdfs\tools\DFSAdmin.java	pooling	package org apache hadoop hdfs tools this provides dfs administrative access dfs admin extends fs shell dfs admin command extends command a supports command clear quota clear quota command extends dfs admin command a supports command set quota set quota command extends dfs admin command a supports command clear space quota clear space quota command extends dfs admin command a supports command set quota set space quota command extends dfs admin command dfs admin dfs admin configuration conf protected distributed file system get dfs throws io exception report throws io exception set safe mode string argv idx throws io exception save namespace throws io exception restore faile storage string arg throws io exception refresh nodes throws io exception print help string cmd finalize upgrade throws io exception upgrade progress string argv idx throws io exception meta save string argv idx throws io exception print topology throws io exception user group information get ugi throws io exception refresh service acl throws io exception refresh user to groups mappings throws io exception refresh super user groups configuration throws io exception print usage string cmd run string argv throws exception client datanode protocol get data node proxy string datanode delete block pool string argv throws io exception refresh namenodes string argv throws io exception main string argv throws exception
788	hdfs\src\java\org\apache\hadoop\hdfs\tools\DFSck.java	unrelated	package org apache hadoop hdfs tools this provides rudimentary checking dfs volumes errors sub optimal conditions p the tool scans files directories starting indicated root path the following abnormal conditions detected handled p ul li files blocks completely missing datanodes br in case tool perform one following actions ul li none link org apache hadoop hdfs server namenode namenode fsck fixing none li li move corrupted files lost found directory dfs link org apache hadoop hdfs server namenode namenode fsck fixing move remaining data blocks saved block chains representing longest consecutive series valid blocks li li delete corrupted files link org apache hadoop hdfs server namenode namenode fsck fixing delete li ul li li detect files replicated replicated blocks li ul additionally tool collects detailed overall dfs statistics optionally print detailed statistics block locations replication factors file the tool also provides option filter open files scan df sck extends configured implements tool user group information ugi print stream df sck configuration conf throws io exception df sck configuration conf print stream throws io exception print usage run string args throws io exception integer list corrupt file blocks string dir string base url string get current namenode address work string args throws io exception main string args throws exception
789	hdfs\src\java\org\apache\hadoop\hdfs\tools\GetConf.java	unrelated	package org apache hadoop hdfs tools tool getting configuration information configuration file adding options ul li if adding simple option get value corresponding key configuration use regular link get conf command handler see link get conf command exclude file example li li if adding option return value key add subclass link get conf command handler set link get conf command see link get conf command namenode example ul get conf extends configured implements tool string description hdfs getconf utility enum command string usage command handler name nodes command handler extends command handler backup nodes command handler extends command handler secondary name nodes command handler extends command handler nn rpc addresses command handler extends command handler print stream stream printing command output print stream err stream printing error get conf configuration conf get conf configuration conf print stream print stream err print error string message print out string message print list list inet socket address list print usage work string args run string args throws exception main string args throws exception
790	hdfs\src\java\org\apache\hadoop\hdfs\tools\GetGroups.java	unrelated	package org apache hadoop hdfs tools hdfs implementation tool getting groups given user belongs get groups extends get groups base get groups configuration conf get groups configuration conf print stream protected inet socket address get protocol address configuration conf main string argv throws exception
791	hdfs\src\java\org\apache\hadoop\hdfs\tools\HDFSConcat.java	unrelated	package org apache hadoop hdfs tools hdfs concat string def uri hdfs localhost main string args throws io exception
792	hdfs\src\java\org\apache\hadoop\hdfs\tools\JMXGet.java	unrelated	package org apache hadoop hdfs tools tool get data name node data node using m beans currently following m beans available hadoop domain hadoop service name node name fs namesystem state hadoop service name node name name node activity dynamic hadoop service name node name rpc activity for port dynamic hadoop service data node name rpc activity for port dynamic hadoop name service data node fs dataset state undefined storage id hadoop service data node name data node activity undefined storage id dynamic implementation note logging sent system err since command line tool jmx get string format n array list object name hadoop object names m bean server connection mbsc string service name node port server localhost string local vm url null jmx get set service string service set port string port set server string server set local vm url string url print attributes values print all values throws exception get single value key string get value string key throws exception initializes m bean server init throws exception print jmx get usage information print usage options opts err string msg parse args command line parse args options opts string args throws illegal argument exception main main string args
793	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\BinaryEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer binary edits visitor implements binary edits visitor binary edits visitor extends edits visitor data output stream create processor writes given file reads using given tokenizer binary edits visitor string filename tokenizer tokenizer create processor writes given file reads using given tokenizer may also print screen binary edits visitor string filename start visitor initialization start throws io exception finish visitor finish throws io exception finish visitor indicate error finish abnormally throws io exception close output stream prevent writing close throws io exception visit enclosing element element elements visit enclosing element tokenizer token value throws io exception end eclosing element leave enclosing element throws io exception visit token tokenizer token visit tokenizer token value throws io exception
794	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\BinaryTokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer reads tokens binary file binary tokenizer implements tokenizer data input stream binary tokenizer constructor binary tokenizer string filename throws file not found exception binary tokenizer constructor binary tokenizer data input stream throws io exception token read token throws io exception
795	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsElement.java	unrelated	package org apache hadoop hdfs tools offline edits viewer structural elements edit log may encountered within file edits visitor able process elements enum edits element edits edits version record opcode data
796	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsLoader.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an edits loader read hadoop edit log file walk structure using supplied edits visitor each implementation edits loader designed rapidly process edits log file as minor changes made one layout version another acceptable tweak one implementation read next however layout version changes enough would make processor slow difficult read another processor created this allows processor quickly read edits log without getting bogged dealing significant differences layout versions edits loader loads edits file load edits throws io exception can processor handle specified version edit log file boolean load version version factory obtaining version edits log loader read particular edits log format loader factory java support methods interfaces necessitates factory create edits log loader point one might need add later edits loader get loader edits visitor v
797	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsLoaderCurrent.java	unrelated	package org apache hadoop hdfs tools offline edits viewer edits loader current processes hadoop edit logs files walks using provided edits visitor calling visitor element enumerated edits loader current implements edits loader supported versions edits visitor v edits version constructor edits loader current edits visitor visitor checks edits loader load given version edits boolean load version version visit op invalid visit op invalid throws io exception visit op add visit op add throws io exception visit op close visit op close throws io exception visit op add op close almost visit op add op close fs edit log op codes edits op code visit op rename old visit op rename old throws io exception visit op delete visit op delete throws io exception visit op mkdir visit op mkdir throws io exception visit op set replication visit op set replication throws io exception visit op set permissions visit op set permissions throws io exception visit op set owner visit op set owner throws io exception visit op set genstamp visit op set genstamp throws io exception visit op times visit op times throws io exception visit op set quota visit op set quota throws io exception visit op rename visit op rename throws io exception visit op concat delete visit op concat delete throws io exception visit op symlink visit op symlink throws io exception visit op get delegation token visit op get delegation token throws io exception visit op renew delegation token visit op renew delegation token visit op cancel delegation token visit op cancel delegation token visit op update master key visit op update master key visit op reassign lease visit op code fs edit log op codes edits op code loads edits file uses visitor process elements load edits throws io exception
798	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an implementation edits visitor traverse structure hadoop edits log respond structures within file edits visitor tokenizer tokenizer edits visitor tokenizer tokenizer begin visiting edits log structure opportunity perform initialization necessary implementing visitor start throws io exception finish visiting edits log structure opportunity perform clean necessary implementing visitor finish throws io exception finish visiting edits log structure error occurred processing opportunity perform clean necessary implementing visitor finish abnormally throws io exception visit non enclosing element edits log specified value tokenizer token visit tokenizer token value throws io exception convenience shortcut method parse specific token type byte token visit byte edits element e throws io exception convenience shortcut method parse specific token type short token visit short edits element e throws io exception convenience shortcut method parse specific token type int token visit int edits element e throws io exception convenience shortcut method parse specific token type v int token visit v int edits element e throws io exception convenience shortcut method parse specific token type long token visit long edits element e throws io exception convenience shortcut method parse specific token type v long token visit v long edits element e throws io exception convenience shortcut method parse specific token type string utf token visit string utf edits element e throws io exception convenience shortcut method parse specific token type string text token visit string text edits element e throws io exception convenience shortcut method parse specific token type blob token visit blob edits element e length throws io exception convenience shortcut method parse specific token type bytes writable token visit bytes writable edits element e throws io exception convenience shortcut method parse specific token type empty token visit empty edits element e throws io exception begin visiting element encloses another element beginning list blocks comprise file visit enclosing element tokenizer token value convenience shortcut method virutally always uses empty token visit enclosing element edits element e throws io exception leave current enclosing element called instance end processing blocks compromise file leave enclosing element throws io exception
799	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsVisitorFactory.java	unrelated	package org apache hadoop hdfs tools offline edits viewer edits visitor factory different implementations edits visitor edits visitor factory factory function creates edits visitor object edits visitor get edits visitor string filename
800	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\OfflineEditsViewer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer this implements offline edits viewer tool used view edit logs offline edits viewer extends configured implements tool edits loader edits loader string default processor xml set edits loader edits loader edits loader go edits visitor visitor throws io exception print help options build options run string argv throws exception main string argv throws exception
801	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\StatisticsEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer statistics edits visitor implements text version edits visitor aggregates counts op codes processed statistics edits visitor extends edits visitor boolean print to screen false boolean ok to write false file writer fw map fs edit log op codes long op code count create processor writes file named statistics edits visitor string filename tokenizer tokenizer create processor writes file named may may also output screen specified statistics edits visitor string filename start visitor initialization start throws io exception non javadoc finish throws io exception non javadoc finish abnormally throws io exception close output stream prevent writing close throws io exception visit enclosing element element elements visit enclosing element tokenizer token value throws io exception end eclosing element leave enclosing element throws io exception visit token calculate statistics tokenizer token visit tokenizer token value throws io exception write parameter output file possibly screen protected write string write throws io exception increment op code counter increment op code count fs edit log op codes op code get statistics map fs edit log op codes long get statistics get statistics format suitable printing string get statistics string
802	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\TextEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer text edits visitor implements text version edits visitor text edits visitor extends edits visitor boolean print to screen false boolean ok to write false file writer fw create processor writes file named text edits visitor string filename tokenizer tokenizer create processor writes file named may may also output screen specified text edits visitor string filename non javadoc finish throws io exception non javadoc finish abnormally throws io exception close output stream prevent writing close throws io exception write parameter output file possibly screen protected write string write throws io exception
803	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\Tokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer hides details different input formats tokenizer abstract token derive tokens needed types token edits element e constructor token edits element e e e edits element accessor edits element get edits element return e creates token string string throws io exception creates token binary stream binary data input stream throws io exception converts token string string writes token value binary format binary data output stream throws io exception byte token extends token byte value byte token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception short token extends token short value short token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception int token extends token value int token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception v int token extends token value v int token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception long token extends token value long token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception v long token extends token value v long token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception string utf token extends token string value string utf token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception string text token extends token string value string text token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception blob token extends token byte value null blob token edits element e length string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception bytes writable token extends token bytes writable value new bytes writable bytes writable token edits element e super e string string throws io exception binary data input stream throws io exception string string binary data output stream throws io exception empty token extends token empty token edits element e super e string string throws io exception binary data input stream throws io exception string string return binary data output stream throws io exception read token note write function writing handled visitor individual string binary functions given token implementations note works token gets parameter returns token done called pipe like pattern token f f f f creates instance token token read token throws io exception
804	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\TokenizerFactory.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer factory different implementations tokenizer tokenizer factory factory function creates tokenizer object input format set based filename xml xml otherwise binary tokenizer get tokenizer string filename throws io exception
805	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\XmlEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an xml edits visitor walks edit log structure writes equivalent xml document contains edit log components xml edits visitor extends text edits visitor linked list edits element tag q depth counter depth counter new depth counter create processor writes file named may may also output screen specified xml edits visitor string filename tokenizer tokenizer create processor writes file named may may also output screen specified xml edits visitor string filename start visitor initialization start throws io exception finish visitor finish throws io exception finish error finish abnormally throws io exception visit token tokenizer token visit tokenizer token value throws io exception visit enclosing element element cntains elements visit enclosing element tokenizer token value throws io exception leave enclosing element leave enclosing element throws io exception write xml tag write tag string tag string value throws io exception prepared values print indents likely use string indents prints leading spaces based depth level print indents throws io exception
806	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\XmlTokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer reads tokens xml file xml tokenizer implements tokenizer file input stream null xml stream reader xml tokenizer constructor xml tokenizer string filename throws io exception get next element value checks element name wanted name string get next elements value string wanted name throws io exception token read token throws io exception
807	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\DelimitedImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer a delimited image visitor generates text representation fsimage element separated delimiter all elements common inodes inodes construction included when processing fsimage layout version element access time output file column value value included individual block information file currently included the default delimiter tab unlikely value included inode path text metadata the delimiter value via constructor delimited image visitor extends text writer image visitor string default delimiter linked list image element elem q new linked list image element file size elements fsimage interested tracking collection image element elements to track values elements elements to track abstract map image element string elements string delimiter delimited image visitor string filename throws io exception delimited image visitor string output file boolean print to screen delimited image visitor string output file boolean print to screen reset values elements tracking order handle next file reset leave enclosing element throws io exception iterate elements tracking value recorded write write line throws io exception visit image element element string value throws io exception visit enclosing element image element element throws io exception visit enclosing element image element element image element key start throws io exception nothing
808	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\DepthCounter.java	unrelated	package org apache hadoop hdfs tools offline image viewer utility tracking descent structure visitor image visitor edits visitor etc depth counter depth inc level depth dec level depth depth get level return depth
809	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\FileDistributionVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer file size distribution visitor description this tool analyzing file sizes namespace image in order run tool one define range integers tt max size tt specifying tt max size tt tt step tt the range integers divided segments size tt step tt tt sub sub sub n sub max size tt visitor calculates many files system fall segment tt sub sub sub sub tt note files larger tt max size tt always fall last segment input ul li tt filename tt specifies location image file li li tt max size tt determines range tt max size tt files sizes considered visitor li li tt step tt range divided segments size step li ul output the output file formatted tab separated two column table size num files where size represents start segment num files number files form image size falls segment file distribution visitor extends text writer image visitor linked list image element elem s new linked list image element max size default x l tb interval default x mb distribution max size step total files total directories total blocks total space max file size file context current boolean inode false file directory information file context string path file size num blocks replication file distribution visitor string filename super filename false max size max size max size default max size step step interval default step num intervals max size step num intervals integer max value distribution new num intervals total files total directories total blocks total space max file size start throws io exception finish throws io exception write distribution output file write size num files n distribution length system println total files total files system println total directories total directories system println total blocks total blocks system println total space total space system println max file size max file size super finish leave enclosing element throws io exception image element elem elem s pop elem image element inode inode false current num blocks total files total blocks current num blocks total space current file size current replication max file size current file size high current file size max size else distribution high total files visit image element element string value throws io exception inode visit enclosing element image element element throws io exception elem s push element element image element inode visit enclosing element image element element elem s push element element image element inode else element image element blocks
810	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageLoader.java	unrelated	package org apache hadoop hdfs tools offline image viewer an image loader accept data input stream hadoop fs image file walk structure using supplied image visitor each implementation image loader designed rapidly process image file as minor changes made one layout version another acceptable tweak one implementation read next however layout version changes enough would make processor slow difficult read another processor created this allows processor quickly read image without getting bogged dealing significant differences layout versions image loader load image data input stream image visitor v can processor handle specified version fs image file boolean load version version factory obtaining version image loader read particular image format loader factory java support methods interfaces necessitates factory find image loader capable interpreting specified layout version number if none return null image loader get loader version
811	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageLoaderCurrent.java	unrelated	package org apache hadoop hdfs tools offline image viewer image loader current processes hadoop fs image files walks using provided image visitor calling visitor element enumerated the difference v v utilization stickybit therefore viewer reader either format versions fsimage layout changes image version namepsace id num files generation stamp i nodes count num files i node path string replication short modification time date access time added block size num blocks blocks count num blocks block block id num bytes generation stamp namespace quota diskspace quota added permissions username string groupname string octal perms short string modified symlink string added num i nodes under construction i nodes under construction count num i nodes under construction i node under construction path bytes replication short modification time date preferred block size num blocks blocks block block id num bytes generation stamp permissions username string groupname string octal perms short string client name string client machine string num locations datanode descriptors count num locations loaded memory short still file enum current delegation key id num delegation keys delegation keys count num delegation keys delegation key length vint delegation key bytes delegation token sequence number num delegation tokens delegation tokens count num delegation tokens delegation token identifier owner string renewer string real user string issue date vlong max date vlong sequence number vint master key id vint expiry time image loader current implements image loader protected date format date format versions image version non javadoc boolean load version version non javadoc load image data input stream image visitor v process delegation token related section fsimage process delegation tokens data input stream image visitor v process i nodes construction section fsimage process i nodes uc data input stream image visitor v process blocks section fsimage process blocks data input stream image visitor v extract i node permissions stored fsimage file process permission data input stream image visitor v process i node records stored fsimage process i nodes data input stream image visitor v process image full path name process local name i nodes data input stream image visitor v process directory data input stream image visitor v process full name i nodes data input stream image visitor v process i node data input stream image visitor v helper method format dates processing string format date date
812	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer an implementation image visitor traverse structure hadoop fsimage respond structures within file image visitor structural elements fs image may encountered within file image visitors able handle processing elements enum image element begin visiting fsimage structure opportunity perform initialization necessary implementing visitor start throws io exception finish visiting fsimage structure opportunity perform clean necessary implementing visitor finish throws io exception finish visiting fsimage structure error occurred processing opportunity perform clean necessary implementing visitor finish abnormally throws io exception visit non enclosing element fsimage specified value visit image element element string value throws io exception convenience methods automatically convert numeric value types strings visit image element element value throws io exception visit image element element value throws io exception begin visiting element encloses another element beginning list blocks comprise file visit enclosing element image element element begin visiting element encloses another element beginning list blocks comprise file also provide additional key value element number items within element visit enclosing element image element element convenience methods automatically convert value types strings visit enclosing element image element element visit enclosing element image element element leave current enclosing element called instance end processing blocks compromise file leave enclosing element throws io exception
813	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\IndentedImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer indented image visitor walks fs image displays structure using indenting organize sections within image file indented image visitor extends text writer image visitor indented image visitor string filename throws io exception indented image visitor string filename boolean print to screen throws io exception depth counter dc new depth counter track leading spacing start throws io exception finish throws io exception super finish finish abnormally throws io exception leave enclosing element throws io exception visit image element element string value throws io exception visit enclosing element image element element throws io exception print element along associated key value pair brackets visit enclosing element image element element print appropriate number spaces current level fs images potentially millions lines caching significantly speed output string indents print indents throws io exception
814	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\LsImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer ls image visitor displays blocks namespace format similar output ls lsr entries marked directories permissions listed replication username groupname along size modification date full path note a significant difference output lsr command image visitor cannot sort file entries listed order stored within fsimage file therefore output cannot directly compared output lsr command ls image visitor extends text writer image visitor linked list image element elem q new linked list image element num blocks string perms replication string username string group filesize string mod time string path string link target boolean inode false string builder sb new string builder formatter formatter new formatter sb ls image visitor string filename throws io exception ls image visitor string filename boolean print to screen throws io exception new line width repl width user width group width size width mod string ls str width repl width user print line throws io exception start throws io exception finish throws io exception finish abnormally throws io exception leave enclosing element throws io exception maintain state location within image tree record values needed display inode ls style format visit image element element string value throws io exception visit enclosing element image element element throws io exception visit enclosing element image element element
815	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\NameDistributionVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer file name distribution visitor p it analyzes file names fsimage prints following information li number unique file names li li number file names corresponding number range files use names li li heap saved file name objects reused li name distribution visitor extends text writer image visitor hash map string integer counts new hash map string integer name distribution visitor string filename boolean print to screen finish throws io exception visit image element element string value throws io exception leave enclosing element throws io exception start throws io exception visit enclosing element image element element throws io exception visit enclosing element image element element image element key
816	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\OfflineImageViewer.java	unrelated	package org apache hadoop hdfs tools offline image viewer offline image viewer dump contents hadoop image file xml console main entry point utility either via command line programatically offline image viewer string usage boolean skip blocks string input file image visitor processor offline image viewer string input file image visitor processor process image file go throws io exception check fsimage datainputstream version number the datainput stream returned point passed method effect datainputstream read pointer find image version data input stream throws io exception build command line options descriptions options build options entry point command line driven operation user may specify options start fsimage viewer command line program process image file exit cleanly error encountered inform user exit main string args throws io exception print application usage instructions print usage
817	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\TextWriterImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer text writer image processor mixes ability image visitor implementations easily write output text file implementing sure call super methods constructors finish finish abnormally methods order underlying file may opened closed correctly note add newlines text written file enabled screen this implementing responsibility text writer image visitor extends image visitor boolean print to screen false boolean ok to write false file writer fw text writer image visitor string filename throws io exception text writer image visitor string filename boolean print to screen non javadoc finish throws io exception non javadoc finish abnormally throws io exception close throws io exception protected write string write throws io exception
818	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\XmlImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer an xml image visitor walks fsimage structure writes equivalent xml document contains fsimage components xml image visitor extends text writer image visitor linked list image element tag q xml image visitor string filename throws io exception xml image visitor string filename boolean print to screen finish throws io exception finish abnormally throws io exception leave enclosing element throws io exception start throws io exception visit image element element string value throws io exception visit enclosing element image element element throws io exception visit enclosing element image element element write tag string tag string value throws io exception
819	hdfs\src\java\org\apache\hadoop\hdfs\util\ByteArray.java	unrelated	package org apache hadoop hdfs util wrapper byte use byte key hash map byte array hash cache hash code byte bytes byte array byte bytes byte get bytes hash code boolean equals object
820	hdfs\src\java\org\apache\hadoop\hdfs\util\ByteBufferOutputStream.java	unrelated	package org apache hadoop hdfs util output stream writes link byte buffer byte buffer output stream extends output stream byte buffer buf byte buffer output stream byte buffer buf write b throws io exception write byte b len throws io exception
821	hdfs\src\java\org\apache\hadoop\hdfs\util\DataTransferThrottler.java	unrelated	package org apache hadoop hdfs util throttle data transfers this thread safe it shared multiple threads the parameter bandwidth per sec specifies total bandwidth shared threads data transfer throttler period period bw imposed period extension max period bw accumulates bytes per period total number bytes sent period cur period start current period starting time cur reserve remaining bytes sent period bytes already used constructor data transfer throttler bandwidth per sec data transfer throttler period bandwidth per sec synchronized get bandwidth synchronized set bandwidth bytes per second given num of bytes sent received since last time throttle called synchronized throttle num of bytes
822	hdfs\src\java\org\apache\hadoop\hdfs\util\ExactSizeInputStream.java	unrelated	package org apache hadoop hdfs util an input stream implementations reads input stream expects exact number bytes any attempts read past specified number bytes return end stream reached if end underlying stream reached prior specified number bytes eof exception thrown exact size input stream extends filter input stream remaining construct input stream read num bytes bytes if eof occurs underlying stream num bytes bytes read eof exception thrown exact size input stream input stream num bytes available throws io exception read throws io exception read byte b len skip n throws io exception boolean mark supported mark readlimit
823	hdfs\src\java\org\apache\hadoop\hdfs\util\GSet.java	unrelated	package org apache hadoop hdfs util a link g set set supports link get object operation the link get object operation uses key lookup element null element supported g set k e extends k extends iterable e size boolean contains k key e get k key e put e element e remove k key
824	hdfs\src\java\org\apache\hadoop\hdfs\util\GSetByHashMap.java	unrelated	package org apache hadoop hdfs util a link g set implementation link hash map g set by hash map k e extends k implements g set k e hash map k e g set by hash map initial capacity load factor size boolean contains k k e get k k e put e element e remove k k iterator e iterator
825	hdfs\src\java\org\apache\hadoop\hdfs\util\LightWeightGSet.java	unrelated	package org apache hadoop hdfs util a low memory footprint link g set implementation uses array storing elements linked lists collision resolution no rehash performed therefore internal array never resized this support null element this thread safe subclass k implementing link linked element light weight g set k e extends k implements g set k e linked element log log log factory get log g set max array length prevent overflow problem min array length linked element entries a mask computing array index hash value element hash mask the size set entry array size modification version fail fast volatile modification light weight g set recommended length compute actual length actual array length recommended size get index k key e convert linked element e e get k key boolean contains k key e put e element e remove index k key e remove k key iterator e iterator string string print detailed information object print details print stream set iterator implements iterator e
826	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\CombinerJobCreator.java	unrelated	package org apache hadoop mapreduce combiner job creator job create job string args throws exception
827	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\GenericMRLoadJobCreator.java	unrelated	package org apache hadoop mapreduce generic mr load job creator extends generic mr load generator job create job string argv boolean mapoutput compressed
828	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\GridMixRunner.java	unrelated	package org apache hadoop mapreduce grid mix runner num of large jobs per class num of medium jobs per class num of small jobs per class num of reducers for small job num of reducers for medium job num of reducers for large job string grid mix data gridmix data string varcompseq grid mix data web simulation block compressed string fixcompseq grid mix data monster query block compressed string varinfltext grid mix data sort uncompressed string gridmixconfig gridmix config xml configuration config init config file system fs init fs job control gridmix num of jobs enum size small small name medium medium name large large name string str string path num jobs num reducers size string str string path num jobs num reducers string default path string base default num jobs default num reducers string string enum grid mix job streamsort stream sort add job num reducers boolean mapoutput compressed javasort java sort add job num reducers boolean mapoutput compressed webdatascan webdata scan add job num reducers boolean mapoutput compressed combiner combiner add job num reducers boolean mapoutput compressed monsterquery monster query add job num reducers boolean mapoutput compressed webdatasort webdata sort add job num reducers boolean mapoutput compressed string name grid mix job string name string get name add job num reducers boolean map comp grid mix runner throws io exception gridmix new job control grid mix null config null fs file system init fs try catch exception e return null configuration init config configuration conf new configuration string config file system getenv gridmixconfig config file null try catch exception e return conf get ints configuration conf string name default v string vals conf get strings name string value of default v results new vals length vals length return results string get input dirs for string job type string default indir string input file config get strings job type default indir string buffer indir buffer new string buffer input file length return indir buffer substring indir buffer length clear dir string dir try catch io exception ex boolean select total selected index selected selected total step total selected effective total total total selected return index effective total index step string add ts suffix string date date calendar get instance get time string ts string value of date get time return ts add jobs grid mix job job size size throws io exception string prefix string format jobs job get name size num jobs get ints config prefix num of jobs num reduces get ints config prefix num of reduces num jobs length num reduces length num mapoutput compressed config get int num output compressed config get int total jobs n job num jobs current index num jobs length add all jobs grid mix job job throws io exception size size enum set of size addjobs throws io exception grid mix job jobtype enum set of grid mix job system println total simple stats min value max value average value medium value n simple stats data task execution stats tree map string simple stats stats compute stats string name data task execution
829	mapreduce\src\contrib\block_forensics\src\java\org\apache\hadoop\blockforensics\BlockSearch.java	unrelated	package org apache hadoop blockforensics block search mapred job designed search input appearances strings the syntax bin hadoop jar jar location hdfs input path hdfs output dir all arguments required this tool designed used search one block ids log files used general text search assuming search strings contain tokens it assumes one search appear per line block search extends configured implements tool map extends mapper long writable text text text text block id text new text text val text new text list string block ids null protected setup context context map long writable key text value context context reduce extends reducer text text text text text val new text reduce text key iterator text values context context throws io exception interrupted exception run string args throws exception args length configuration conf get conf conf set block ids args job job new job conf job set combiner class reduce job set jar by class block search job set job name block search job set mapper class map job set output key class text job set output value class text job set reducer class reduce file input format set input paths job new path args file output format set output path job new path args return job wait for completion true main string args throws exception res tool runner run new configuration new block search args system exit res
830	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\AbstractQueue.java	scheduler	package org apache hadoop mapred parent hierarchy queues all queues extend p even though queue extend categories queues define p container queue composite queues job queue leaf level queues p typically container queue consists job queue all scheduling context data container queue cummulative children p job queue consists actual job list e running job waiting job etc p this done make sure job related data one place queues higher level typically cummulative data organization children level abstract queue log log log factory get log abstract queue protected queue scheduling context qsc protected abstract queue parent protected abstract queue abstract queue parent queue scheduling context qsc update map cluster capacity reduce cluster capacity queue scheduling context get queue scheduling context set queue scheduling context queue scheduling context qsc string get name protected abstract queue get parent protected set parent abstract queue queue list abstract queue get descendent job queues list abstract queue get descendant container queues sort comparator queue comparator list abstract queue get children add child abstract queue queue distribute un configured capacity string string abstract queue comparator implements comparator abstract queue boolean equals object hash code validate and copy queue contexts abstract queue source queue
831	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\CapacitySchedulerConf.java	scheduler	package org apache hadoop mapred class providing access capacity scheduler configuration default values queue configuration capacity scheduler configuration includes settings link job initialization poller default values queue configuration these read file link capacity scheduler conf scheduler conf file classpath the main queue configuration defined file link queue manager queue conf file name classpath p this also provides ap is get set configuration queues capacity scheduler conf log log log factory get log capacity scheduler conf string capacity property capacity string supports priority property supports priority string maximum initialized jobs per user property string minimum user limit percent property default file name capacity scheduler configuration read string scheduler conf file capacity scheduler xml default ulimit minimum boolean default support priority string queue conf property name prefix map string properties queue properties string default percentage of pmem in vmem property string upper limit on task pmem property string max capacity property maximum capacity initialization thread polling interval max initialization worker threads configuration rm conf default max jobs per users to initialize capacity scheduler conf capacity scheduler conf path config file initialize defaults set properties string queue name properties properties get capacity string queue string get property string queue string property get max capacity string queue boolean priority supported string queue get minimum user limit percent string queue string full property name string queue get max jobs per user to initialize string queue get sleep interval get max worker threads configuration get cs conf get float string value string default value get int string value string default value
832	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\CapacityTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler implements requirements hadoop provides hod less way share large clusters this scheduler provides following features support queues job submitted queue queues assigned fraction capacity grid capacity sense certain capacity resources disposal all jobs submitted queues org access capacity org free resources allocated queue beyond capacity queues optionally support job priorities disabled default within queue jobs higher priority access queue resources jobs lower priority however job running preempted higher priority job in order prevent one users monopolizing resources queue enforces limit percentage resources allocated user given time competition capacity task scheduler extends task scheduler quick way get qsc object given queue name map string queue scheduling context queue info map new hash map string queue scheduling context root level queue it cluster capacity disposal queues declared users would children queue cs would handle root abstract queue root null this captures scheduling information want display log scheduling display info string queue name capacity task scheduler scheduler scheduling display info string queue name capacity task scheduler scheduler string string encapsulates result task lookup task lookup result enum look up status constant task lookup result objects should accessed directly task lookup result no task lookup result task lookup result mem failed lookup result look up status look up status task task call constructor directly use factory methods task lookup result task look up status u status task lookup result get task found result task task lookup result get no task found result task lookup result get mem failed result task get task look up status get look up status this handles scheduling algorithms the algos map reduce tasks there may slight variations later case make base derived map reduce task scheduling mgr task scheduler object protected capacity task scheduler scheduler protected task type type null task obtain new task task tracker status task tracker get cluster capacity task scheduling context get tsc to check job speculative task particular tracker boolean speculative task job in progress job comparator sort queues for maps need sort queue scheduling context map tsc for reducers use reduce tsc so need separate comparators queue comparator subclass map reduce comparators map queue comparator extends queue comparator reduce queue comparator extends queue comparator comparator instances protected map queue comparator map comparator protected reduce queue comparator reduce comparator comparator use protected queue comparator queue comparator returns queues sorted according queue comparator mainly testing purposes string get ordered queues return ordered list link job queue wrapped link abstract queue ordering according link queue comparator to reflect true ordering job queues complete hierarchy sorted link abstract queue ordered according needs level hierarchy leaf level link job queue returned sorted needs list abstract queue get ordered job queues task scheduling mgr capacity task scheduler sched ceil result dividing two integers this utility method neither code code code b code negative divide and ceil b boolean user over limit job in progress j log debug enabled this central scheduling method it tries get task jobs single queue always return task lookup result object don return null task lookup result get
833	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\ContainerQueue.java	scheduler	package org apache hadoop mapred composite queue hierarchy container queue extends abstract queue list immediate children container queue duplicate childrens allowed list abstract queue children container queue abstract queue parent queue scheduling context qsc update map cluster capacity reduce cluster capacity update children context sort comparator queue comparator list abstract queue get descendent job queues list abstract queue get descendant container queues list abstract queue get children add child abstract queue queue distribute un configured capacity
834	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobInitializationPoller.java	scheduler	package org apache hadoop mapred this asynchronously initializes jobs submitted map reduce cluster running link capacity task scheduler p the comprises main poller thread set worker threads together initialize jobs the poller thread periodically looks jobs submitted scheduler selects set initialized it passes worker threads initializing each worker thread configured look jobs submitted fixed set queues it initializes jobs round robin manner selecting first job order queue ready initialized p p an initialized job occupies memory resources job tracker hence poller limits number jobs initialized given time configured limit the limit specified per user per queue p p however since job needs initialized scheduler select tasks run tries keep backlog jobs initialized scheduler need wait let empty slots go waste the core logic poller pick right jobs good potential run next scheduler to picks jobs submitted across users across queues account guaranteed capacities user limits it also always initializes high priority jobs whenever need initialized even means going limit initialized jobs p job initialization poller extends thread log log log factory max additional users to init job queues manager job queue manager sleep interval pool size job initialization thread extends thread queue info hash map string queue info job queues hash map job id job in progress initialized jobs volatile boolean running task tracker manager ttm hash map string job initialization thread threads to queue map job initialization poller init set string queues setup job initializer configuration set string queues get max users to init capacity scheduler conf scheduler conf refresh queue info capacity scheduler conf scheduler conf run select jobs to initialize print jobs array list job in progress jobs to initialize job initialization thread create job initialization thread assign threads to queues array list job in progress get jobs to initialize string queue clean up initialized jobs list boolean scheduled job in progress job terminate job in progress get initializing job string queue set job id get initialized job list hash map string job initialization thread get threads to queue map get sleep interval
835	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobQueue.java	scheduler	package org apache hadoop mapred job queue extends abstract queue log log log factory get log job queue job queue abstract queue parent queue scheduling context qsc if queue supports priorities jobs must sorted priorities start times technically insertion time if queue support priorities jobs sorted based start time comparator job scheduling info this involves updating q c structure update map cluster capacity reduce cluster capacity update stats on running job probably many plus may need go list compute num slots occupied by user if expensive keep list running jobs per user then need consider first jobs per user jobqueue schedulinginfo initial length string get job queue sched info map job scheduling info job in progress map job scheduling info job in progress comparator job scheduling info collection job in progress get waiting jobs collection job in progress get running jobs add running job job in progress job job in progress remove running job job in progress remove waiting job add waiting job job in progress job get waiting job count called job added synchronized job added job in progress job throws io exception setup link capacity task scheduler specific information prior job initialization p to do currently method uses capacity task scheduler based variables need shift pre initialize job job in progress job called job completes synchronized job completed job in progress job this used reposition job queue a job get repositioned change job priority job start time reorder jobs list abstract queue get descendent job queues list abstract queue get descendant container queues job updated job change event event list abstract queue get children dont anything sort leaf level queue sort comparator queue comparator update scheduler job state changed job state changed job status change event event method removes jobs running waiting job queue job queue manager job completed add child abstract queue queue distribute un configured capacity
836	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobQueuesManager.java	scheduler	package org apache hadoop mapred a link job in progress listener maintains jobs managed one queues job queues manager extends job in progress listener maintain hashmap queue names queue info map string job queue job queues log log log factory get log job queues manager job queues manager add given queue map queue name job queues add queue job queue queue job added job in progress job throws io exception note job removed job completes e job upated job removed job in progress job job updated job change event event comparator job scheduling info get comparator string queue job queue get job queue job in progress jip job queue get job queue string name set string get job queue names
837	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\MemoryMatcher.java	scheduler	package org apache hadoop mapred memory matcher log log log factory get log memory matcher mem size for map slot on jt job conf disabled memory limit mem size for reduce slot on jt job conf disabled memory limit limit max mem for map tasks job conf disabled memory limit limit max mem for reduce tasks job conf disabled memory limit memory matcher find memory already used running tasks residing given task tracker null memory cannot computed reason synchronized long get mem reserved for tasks check tt enough memory run task specified job boolean matches memory requirements job in progress job task type task type boolean scheduling based on mem enabled initialize memory related conf configuration conf get mem size for map slot get mem size for reduce slot get limit max mem for map slot get limit max mem for reduce slot
838	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\QueueHierarchyBuilder.java	scheduler	package org apache hadoop mapred hierarchy builder capacity scheduler queue hierarchy builder log log log factory get log queue hierarchy builder queue hierarchy builder create new link abstract queue hierarchy set new queue properties passed link capacity scheduler conf abstract queue create hierarchy list job queue info root children string total capacity overflown msg recursively create complete abstract queues hierarchy parent root hierarchy children immediate children parent may turn parent child queues any job queue info children used create job queue abstract queues hierarchy every abstract queue used create container queue p while creating hierarchy make sure level total capacity children level cross properties set the new queue properties set key names obtained expanding queue names reflect whole hierarchy create hierarchy abstract queue parent create new link queue scheduling context given props also set properties passed scheduler configuration object queue scheduling context load context properties props create link abstract queue empty link queue scheduling context can used root queue create link abstract queue hierarchies abstract queue create root abstract queue
839	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\QueueSchedulingContext.java	scheduler	package org apache hadoop mapred keeping track scheduling information queues p we need maintain scheduling information relevant queue name capacity etc along information specific kind task map reduce num running tasks pending tasks etc p this scheduling information used decide allocate tasks redistribute capacity etc p a queue scheduling context qsc object represents scheduling information queue queue scheduling context max capacity stretch set config mapred capacity scheduler queue queue name maximum capacity maximum capacity stretch defines limit beyond sub queue cannot use capacity parent queue queue scheduling context string get queue name set queue name string queue name get map capacity set map capacity map capacity get reduce capacity set reduce capacity reduce capacity get capacity percent set capacity percent capacity percent map string integer get num jobs by user set num jobs by user map string integer num jobs by user get ul min set ul min ul min task scheduling context get map tsc set map tsc task scheduling context map tsc task scheduling context get reduce tsc set reduce tsc task scheduling context reduce tsc boolean supports priorities set supports priorities boolean supports priorities get num of waiting jobs set num of waiting jobs num of waiting jobs get max capacity percent set max capacity percent max capacity percent update context map cluster capacity reduce cluster capacity
840	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\TaskDataView.java	scheduler	package org apache hadoop mapred task view job returns running pending information job job in progress factory method provides map reduce data view based type task data view get running tasks job in progress job get pending tasks job in progress job get slots per task job in progress job task scheduling context get tsi queue scheduling context qsi get num reserved task trackers job in progress job get slots occupied job in progress job boolean sufficient reserved task trackers job in progress job task data view map task data view task data view reduce task data view task data view get task data view task type type map task data view extends task data view reduce task data view extends task data view
841	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\TaskSchedulingContext.java	scheduler	package org apache hadoop mapred keeping track scheduling information queues p maintain information specific kind task map reduce num running tasks pending tasks etc p this scheduling information used decide allocate tasks redistribute capacity etc p a task scheduling context tsi object represents scheduling information particular kind task map reduce p task scheduling context actual capacity depends many slots available cluster given time capacity number running tasks num running tasks number slots occupied running tasks num slots occupied actual capacity stretch depends many slots available cluster given time max capacity user need keep track number slots occupied running tasks map string integer num slots occupied by user reset variables associated tasks reset task vars returns capacity queue slots get capacity mutator method capacity set capacity capacity return information tasks string string get num running tasks set num running tasks num running tasks get num slots occupied set num slots occupied num slots occupied map string integer get num slots occupied by user set num slots occupied by user get max capacity set max capacity max capacity update task scheduling context tc update no of slots occupied by user map string integer nou
842	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleDataJoinMapper.java	unrelated	package org apache hadoop contrib utils join this subclass data join mapper base used demonstrate functionality inner join data sources tab separated text files based first column sample data join mapper extends data join mapper base protected text generate input tag string input file protected text generate group key tagged map output record protected tagged map output generate tagged map output object value
843	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleDataJoinReducer.java	unrelated	package org apache hadoop contrib utils join this subclass data join reducer base used demonstrate functionality inner join data sources tab separated text files based first column sample data join reducer extends data join reducer base protected tagged map output combine object tags object values
844	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleTaggedMapOutput.java	unrelated	package org apache hadoop contrib utils join this subclass tagged map output used demonstrate functionality inner join data sources tab separated text files based first column sample tagged map output extends tagged map output text data sample tagged map output sample tagged map output text data writable get data write data output throws io exception read fields data input throws io exception
845	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop contrib utils join this provides implementation resetable iterator the implementation based array list array list backed iterator implements resetable iterator iterator iter array list object data array list backed iterator array list backed iterator array list object data add object item boolean next object next remove reset close throws io exception
846	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinJob.java	unrelated	package org apache hadoop contrib utils join this implements main function creating map reduce job join data different sources to create sucn job user must implement mapper extends data join mapper base reducer extends data join reducer base data join job class get class by name string name job conf create data join job string args throws io exception boolean run job job conf job throws io exception main string args
847	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinMapperBase.java	unrelated	package org apache hadoop contrib utils join this serves base mapper data join job this expects subclasses implement methods following functionalities compute source tag input values compute map output value object compute map output key object the source tag used reducer determine source table sql terminology value comes computing map output value object amounts performing projecting filtering work sql statement select clauses computing map output key amounts choosing join key this provides appropriate plugin points user defined subclasses implement appropriate logic data join mapper base extends job base protected string input file null protected job conf job null protected text input tag null protected reporter reporter null configure job conf job protected text generate input tag string input file protected tagged map output generate tagged map output object value protected text generate group key tagged map output record map object key object value close throws io exception reduce object arg iterator arg
848	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinReducerBase.java	unrelated	package org apache hadoop contrib utils join this serves base reducer data join job the reduce function first group values according input tags compute cross product groups for tuple cross product calls following method expected implemented subclass protected tagged map output combine object tags object values the method expected produce one output value array records different sources the user code also perform filtering it return null decides records meet certain conditions data join reducer base extends job base protected reporter reporter null max num of values per group protected largest num of values protected num of values protected collected protected job conf job close throws io exception configure job conf job protected resetable iterator create resetable iterator sorted map object resetable iterator regroup object key reduce object key iterator values protected collect object key tagged map output record join and collect object tags resetable iterator values join and collect object tags resetable iterator values text source tags field new text source tags text num of values field new text num of values protected tagged map output combine object tags object values map object arg object arg output collector arg
849	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\JobBase.java	unrelated	package org apache hadoop contrib utils join a common base implementing statics collecting mechanisms commonly used typical map reduce job job base implements mapper reducer log log log factory get log datajoin job sorted map object long counters null sorted map object double counters null set given counter given value counter name value counter protected set long value object name value set given counter given value counter name value counter protected set double value object name value counter name protected long get long value object name counter name protected double get double value object name increment given counter given incremental value if counter exist one created value counter name incremental value protected long add long value object name inc increment given counter given incremental value if counter exist one created value counter name incremental value protected double add double value object name inc log counters protected report log counters protected string get report initializes new instance link job conf configuration configure job conf job
850	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\ResetableIterator.java	unrelated	package org apache hadoop contrib utils join this defines iterator help reducer group input source tags once values grouped reducer receive cross product values different groups resetable iterator extends iterator reset add object item close throws io exception
851	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\TaggedMapOutput.java	unrelated	package org apache hadoop contrib utils join this serves base values flow mappers reducers data join job typically job mappers compute source tag input record based attributes based file name input file this tag used reducers group values given key according source tags tagged map output implements writable protected text tag tagged map output text get tag set tag text tag writable get data tagged map output clone job conf job
852	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\AllocationStore.java	unrelated	package org apache hadoop mapred abstract implementing persistent store allocation information allocation store map string budget queue queue cache new hash map string budget queue init configuration conf load save get budget string queue get spending string queue synchronized add budget string queue budget synchronized add queue string queue string get queue info string queue synchronized remove queue string queue synchronized set spending string queue spending synchronized set usage string queue used pending gets queue status budget spending usage collection budget queue get queues
853	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\BudgetQueue.java	unrelated	package org apache hadoop mapred class hold accounting info queue remaining budget spending rate whether queue usage budget queue string name volatile budget volatile spending volatile used volatile pending deduct budget budget queue string name budget spending thread safe addition budget synchronized add budget new budget
854	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\DynamicPriorityScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler provides following features the purpose scheduler allow users increase decrease queue priorities continuosly meet requirements current workloads the scheduler aware current demand makes expensive boost priority peak usage times thus users move workload low usage times rewarded discounts priorities boosted within limited quota all users given quota budget deducted periodically configurable accounting intervals how much budget deducted determined per user spending rate may modified time directly user the cluster slots share allocated particular user computed users spending rate sum spending rates accounting period this scheduler designed meta scheduler top existing map reduce schedulers responsible enforcing shares computed dynamic scheduler cluster dynamic priority scheduler extends task scheduler this periodically checks spending rates queues updates queue capacity shares budgets allocations extends timer task implements queue allocator map string queue allocation allocation configuration conf hash map string string queue info new hash map string string total spending set string info queues queue manager queue manager allocation store store allocations configuration conf queue manager queue manager add budget string queue budget add queue string queue synchronized get price remove queue string queue set spending string queue spending string get info string queue string get queue infos synchronized update allocation calculates shares proportion spending rates sets appropriate configuration parameter schedulers read synchronized set shares synchronized set allocation map string queue allocation shares inherit doc synchronized map string queue allocation get allocation inherit doc synchronized set usage string queue used pending used expose queue info job tracker web ui synchronized string get queue info string queue run allocation interval calculate new shares based updated budgets spending rates run merges queue info underlying map reduce scheduler dynamic scheduler displayed job tracker web ui queue info string queue object info allocations allocations queue info string queue object info allocations allocations string string actual scheduler picks jobs run e g priority scheduler protected queue task scheduler scheduler timer timer new timer true protected allocations allocations log log log factory get log dynamic priority scheduler used testing discrete time set timer timer timer timer timer start throws io exception configuration conf get conf queue manager queue manager task tracker manager get queue manager allocations new allocations conf queue manager scheduler reflection utils new instance scheduler set allocator allocations scheduler set conf conf scheduler set task tracker manager task tracker manager scheduler start interval conf get long priority scheduler options dynamic scheduler alloc interval timer schedule at fixed rate allocations interval interval string queue queue manager get leaf queue names task tracker manager instanceof job tracker terminate throws io exception scheduler terminate list task assign tasks task tracker task tracker return scheduler assign tasks task tracker collection job in progress get jobs string queue name return scheduler get jobs queue name
855	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\DynamicPriorityServlet.java	scheduler	package org apache hadoop mapred servlet controlling queue allocations installed job tracker url scheduler link dynamic priority scheduler use operations supported br price br time br info queue query requires user admin privilege br infos requires admin privilege br add budget budget add queue queue change requires admin privilege br set spending spending set queue queue change requires user admin privilege br add queue queue add requires admin privilege br remove queue queue remove requires admin privilege br dynamic priority servlet extends http servlet dynamic priority scheduler scheduler job tracker job tracker priority authorization auth init throws servlet exception protected post http servlet request req http servlet response resp check admin role string query throws io exception check user role http servlet request request get http servlet request request
856	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\FileAllocationStore.java	scheduler	package org apache hadoop mapred implements persistent storage queue budget spending information file file allocation store extends allocation store log log log factory get log file allocation store string file name boolean loaded false inherit doc init configuration conf inherit doc save close closeable closeable inherit doc load
857	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\PriorityAuthorization.java	scheduler	package org apache hadoop mapred this implements symmetric key hmac sha signature based authorization users admins priority authorization user admin no access hash map string user acl acl new hash map string user acl last successful reload start time system current time millis string acl file log log log factory get log priority authorization boolean debug log debug enabled string hmac sha algorithm hmac sha init configuration conf string hmac string data string key user acl reload acl load acl boolean replay string timestamp string signature user acl user acl authorize string data string signature string user string timestamp
858	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\PriorityScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler provides following features allows continuous enforcement user controlled dynamic queue shares preempts tasks exceeding queue shares instantaneously new jobs arrive work conserving tracks queue usage charge jobs pending running authorizes queue submissions based symmetric key hmac sha signatures priority scheduler extends queue task scheduler init thread extends thread job in progress job init thread job in progress job run job listener extends job in progress listener job added job in progress job job removed job in progress job job updated job change event event comparator task in progress task comparator new comparator task in progress compare task in progress task in progress comparator kill queue queue comparator new comparator kill queue compare kill queue kill queue queue jobs string name linked list job in progress jobs new linked list job in progress queue jobs string name queue quota quota map used reduce used map pending reduce pending mappers reducers string name queue quota string name queue allocator allocator log log log factory get log priority scheduler boolean map true boolean reduce false boolean fill true boolean no fill false job listener job listener new job listener boolean debug log debug enabled boolean sort tasks true last kill kill interval priority authorization auth new priority authorization linked list job in progress job queue new linked list job in progress hash map string queue jobs queue jobs new hash map string queue jobs start throws io exception task tracker manager add job in progress listener job listener sort tasks conf get boolean mapred priority scheduler sort tasks true kill interval conf get long mapred priority scheduler kill interval auth init conf terminate throws io exception set allocator queue allocator allocator allocator allocator boolean assign map red task job in progress job string queue get queue job queue quota quota queue quota get queue quota null quota quota fill task null map else null return false map string queue quota get queue quota max map tasks max reduce tasks debug max tasks map max map tasks max reduce tasks map string queue allocation shares allocator get allocation map string queue quota quota map new hash map string queue quota queue allocation share shares values return quota map schedule jobs available slots boolean map boolean fill available slots count tasks to kill map string queue quota queue quota boolean map kill tasks queue quota quota queue quota values return kill tasks protected mark idle map string queue quota queue quota queue quota quota queue quota values synchronized assign map red tasks list task assigned tasks task offset assigned tasks size max tasks map task tracker get max map slots count tasks map task tracker count map tasks available slots max tasks count tasks map capacity reduce capacity cluster status status task tracker manager get cluster status status null map string queue quota queue quota get queue quota map capacity debug schedule jobs available slots map no fill task tracker num trackers available slots assigned tasks size task offset schedule jobs available slots map fill task tracker num
859	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueAllocation.java	scheduler	package org apache hadoop mapred class hold queue share info communicated scheduler queue share manager queue allocation string name share queue allocation string name share gets queue share get share gets queue name string get name
860	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueAllocator.java	scheduler	package org apache hadoop mapred this intended allowing schedulers communicate queue share management implementation schedulers periodically poll obtain latest queue allocations queue allocator used schedulers obtain queue allocations periodically map string queue allocation get allocation used schedulers push queue usage info accounting purposes set usage string queue used pending
861	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueTaskScheduler.java	scheduler	package org apache hadoop mapred this allows scheduler retrieve periodic queue allocation info queue share manager queue task scheduler extends task scheduler set allocator queue allocator allocator
862	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\Activator.java	unrelated	package org apache hadoop eclipse the activator controls plug life cycle activator extends abstract ui plugin string plugin id org apache hadoop eclipse activator plugin activator start bundle context context throws exception stop bundle context context throws exception activator get default
863	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\ErrorMessageDialog.java	unrelated	package org apache hadoop eclipse error dialog helper error message dialog display string title string message display exception e
864	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\HadoopPerspectiveFactory.java	unrelated	package org apache hadoop eclipse creates links new map reduce based wizards views map reduce perspective hadoop perspective factory implements i perspective factory create initial layout i page layout layout
865	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\ImageLibrary.java	pooling	package org apache hadoop eclipse icons manager image library bundle bundle activator get default get bundle singleton instance volatile image library instance null i shared images shared images where resources icons images available bundle string resource dir resources public access image descriptors image descriptor get string name public access images image get image string name singleton access image library get instance map registered resources image descriptor image map string image descriptor desc map map string image image map new hash map string image image library constructor put image definitions image library accessor images image descriptor get image descriptor by name string name accessor images image get image by name string name access platform shared images image descriptor get shared by name string name load register new image if image resource exist fails load default error resource supplied boolean new image string name string filename register image workspace shared image pool if image resource exist fails load default error resource supplied boolean new shared image string name string shared name register image workspace shared image pool if image resource exist fails load default error resource supplied boolean new plugin image string name string plugin id
866	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\MapReduceNature.java	unrelated	package org apache hadoop eclipse class configure deconfigure eclipse project map reduce project nature map reduce nature implements i project nature string id org apache hadoop eclipse nature i project project logger log logger get logger map reduce nature get name configures eclipse project map reduce project adding hadoop libraries project classpath configure throws core exception deconfigure project map reduce status currently unimplemented deconfigure throws core exception returns project project nature applies i project get project sets project nature applies used instantiating project nature runtime set project i project project
867	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewDriverWizard.java	unrelated	package org apache hadoop eclipse wizard creating new driver runs map reduce job new driver wizard extends new element wizard implements i new wizard new driver wizard page page run i progress monitor monitor new driver wizard init i workbench workbench i structured selection selection boolean perform finish protected finish page i progress monitor monitor i java element get created element
868	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewDriverWizardPage.java	unrelated	package org apache hadoop eclipse pre fills new map reduce driver template new driver wizard page extends new type wizard page button create map method text reducer text text mapper text boolean show container selector new driver wizard page new driver wizard page boolean show container selector set selection i structured selection selection create type i progress monitor monitor throws core exception protected create type members i type new type imports manager imports create control composite parent protected handle field changed string field name validate create mapper controls composite composite create reducer controls composite composite text create browse class control composite composite
869	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewMapperWizard.java	unrelated	package org apache hadoop eclipse wizard creating new mapper runs map portion map reduce job the pre filled template new mapper wizard extends new element wizard implements i new wizard page page new mapper wizard run i progress monitor monitor init i workbench workbench i structured selection selection page extends new type wizard page boolean perform finish protected finish page i progress monitor monitor i java element get created element
870	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewMapReduceProjectWizard.java	unrelated	package org apache hadoop eclipse wizard creating new map reduce project new map reduce project wizard extends wizard implements i workbench wizard i executable extension logger log hadoop first page first page new java project wizard page java page new driver wizard page new driver page i configuration element config new map reduce project wizard set window title new map reduce project wizard init i workbench workbench i structured selection selection boolean finish return first page page complete java page page complete first page generate driver get selection new driver page page complete i wizard page get next page i wizard page page page first page first page generate driver get selection return new driver page generate mapper checked second page new driver page else i wizard page answer super get next page page answer new driver page else answer java page else i wizard page get previous page i wizard page page page new driver page else hadoop first page extends wizard new project creation page hadoop first page link open preferences button workspace hadoop button project hadoop text location button browse string path string current path button generate driver create control composite parent boolean page complete boolean validate hadoop location update hadoop dir label from preferences widget default selected selection event e widget selected selection event e add pages first page new hadoop first page add page first page add page new java project wizard second page first page first page new hadoop first page java page new driver page new new driver wizard page false new driver page set page complete false ensure finish button initially disabled add page first page add page java page add page new driver page boolean perform finish try catch invocation target exception e catch interrupted exception e return true set initialization data i configuration element config config config
871	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewReducerWizard.java	unrelated	package org apache hadoop eclipse wizard creating new reducer runs reduce portion map reduce job the pre filled template new reducer wizard extends new element wizard implements page page new reducer wizard run i progress monitor monitor init i workbench workbench i structured selection selection page extends new type wizard page boolean perform finish protected finish page i progress monitor monitor i java element get created element
872	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\PropertyTester.java	unrelated	package org apache hadoop eclipse class help debugging properties property tester extends logger log logger get logger property tester get name property tester boolean test object receiver string property object args
873	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\DFSActionImpl.java	unrelated	package org apache hadoop eclipse actions actual implementation dfs actions dfs action impl implements i object action delegate i selection selection i workbench part target part set active part i action action i workbench part target part target part target part run i action action ignore non structured selections selection instanceof i structured selection operate dfs asynchronously prevent blocking main ui i structured selection ss i structured selection selection string action id action get action definition id display get default async exec new runnable create new sub folder existing directory mkdir i structured selection selection list dfs folder folders filter selection dfs folder selection folders size implement import action upload files current machine hdfs upload files to dfs i structured selection selection ask user files upload file dialog dialog dialog set text select local files upload dialog open list file files new array list file string fname dialog get file names todo enable upload command selection exactly one folder list dfs folder folders filter selection dfs folder selection folders size implement import action upload directory current machine hdfs upload directory to dfs i structured selection selection ask user local directory upload directory dialog dialog dialog set text select local file directory upload string dir name dialog open file dir new file dir name list file files new array list file files add dir todo enable upload command selection exactly one folder list dfs folder folders folders size upload to dfs dfs folder folder list file files platform ui get workbench get progress service busy cursor while reconnect i structured selection selection dfs location location filter selection dfs location selection disconnect i structured selection selection selection size object first selection get first element first instanceof dfs locations root dfs locations root root dfs locations root first root disconnect root refresh implements download action hdfs current machine download from dfs i structured selection selection ask user put downloaded files directory dialog dialog dialog set text copy local directory dialog set message copy selected files directories string directory dialog open directory null file dir new file directory dir exists dir directory list dfs path paths filter selection dfs path selection platform ui get workbench get progress service busy cursor while open selected dfs path editor window open i structured selection selection throws io exception dfs file file filter selection dfs file selection refresh i structured selection selection dfs path path filter selection dfs path selection delete i structured selection selection list dfs path list filter selection dfs path selection list empty string buffer msg new string buffer msg append are sure want delete dfs path path list message dialog open confirm null confirm delete dfs msg selection changed i action action i selection selection selection selection extract list t structured selection t list t filter selection class t clazz list t list new array list t object obj selection list return list compute upload work file file file directory else file file else adapter allow viewing dfs file editor window dfs file editor input extends platform object implements i storage editor input dfs file file
874	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\EditLocationAction.java	unrelated	package org apache hadoop eclipse actions editing server properties action edit location action extends action server view server view edit location action server view server view run
875	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\NewLocationAction.java	unrelated	package org apache hadoop eclipse actions action corresponding creating new map reduce server new location action extends action new location action run
876	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\OpenNewMRClassWizardAction.java	unrelated	package org apache hadoop eclipse actions action open new map reduce class open new mr class wizard action extends action implements i cheat sheet action logger log logger get logger open new mr class wizard action run string params i cheat sheet manager manager params null params length i new wizard get wizard string type name type name equals mapper else type name equals reducer else type name equals driver else
877	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\OpenNewMRProjectAction.java	unrelated	package org apache hadoop eclipse actions action open new map reduce project open new mr project action extends action run
878	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\ActionProvider.java	unrelated	package org apache hadoop eclipse dfs allows user delete refresh items dfs tree action provider extends common action provider i common action extension site site action provider init i common action extension site site action provider site null super init site action provider site site fill action bars i action bars action bars action bars set global action handler action factory delete get id action bars set global action handler action factory refresh get id site null site get structured viewer get selection instanceof i structured selection action bars update action bars fill context menu i menu manager menu actions multiple selections menu append to group i common menu constants group edit new dfs action menu append to group i common menu constants group open new dfs action menu append to group i common menu constants group new new dfs action site null i selection isel site get structured viewer get selection isel instanceof i structured selection actions single selections i structured selection issel i structured selection isel issel size object element issel get first element element instanceof dfs file else element instanceof dfs folder else element instanceof dfs location else element instanceof dfs locations root representation action dfs entry browser dfs action extends action string id string title dfs actions action dfs action string id string title dfs action dfs actions action string get text image descriptor get image descriptor string get action definition id run boolean enabled
879	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSActions.java	unrelated	package org apache hadoop eclipse dfs enum dfs actions delete delete refresh refresh download download dfs open string title string id string prefix dfs browser action dfs actions get by id string def dfs actions string title
880	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSContent.java	unrelated	package org apache hadoop eclipse dfs interface define content entities dfs browser dfs content boolean children dfs content get children refresh
881	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSContentProvider.java	unrelated	package org apache hadoop eclipse dfs handles viewing dfs locations p the content handled provider tree tt br dfs locations root br hadoop server br dfs folder br dfs file br dfs folder br br hadoop server tt the code block blocking operations need done asynchronously freeze ui dfs content provider implements i tree content provider the viewer displays tree content viewer viewer structured viewer sviewer map hadoop server dfs content root folders constructor load resources icons dfs content provider dfs locations root locations root new dfs locations root i tree content provider implementation object get children object parent object test object parent element object get parent object element boolean children object element i structure content provider implementation object get elements object input element i label provider implementation image get image object element string get text object element i base label provider implementation add listener i label provider listener listener remove listener i label provider listener listener boolean label property object element string property i content provider implementation dispose input changed viewer viewer object old input object new input miscellaneous ask viewer content refresh refresh ask viewer refresh single element refresh dfs content content viewer get viewer
882	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSFile.java	unrelated	package org apache hadoop eclipse dfs file handling methods dfs dfs file extends dfs path implements dfs content protected length protected short replication constructor upload file distributed file system dfs file dfs path parent path path file file super parent path upload monitor file dfs file dfs path parent path path super parent path try catch io exception e download view contents file input stream open throws io exception return get dfs open path download file local file system this creates download status monitor download to local file file file platform ui get workbench get progress service busy cursor while download to local directory i progress monitor monitor file dir file dfs path new file get path string file destination new file dir dfs path get name destination exists try catch exception e provides detailed file tt lt filename gt lt size gt r lt replication gt tt string detailed string string units b kb mb gb tb unit length unit units length return string format f r super string string string return path string download dfs file local file use given monitor report status operation download to local file i progress monitor monitor file file task size monitor set task name download file path buffered output stream ostream null data input stream istream null try catch exception e finally upload local file file distributed file system upload i progress monitor monitor file file task size monitor set task name upload file path buffered input stream istream null data output stream ostream null try catch exception e finally refresh get parent refresh compute download work return length creates adapter file open editor i storage get i storage return new i storage adapter i storage adapter open file editor i storage adapter extends platform object implements i storage input stream get contents throws core exception i path get full path string get name boolean read only implementation dfs content dfs content get children return null boolean children return false
883	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSFolder.java	unrelated	package org apache hadoop eclipse dfs local representation folder dfs the constructor creates empty representation folder spawn thread fill dfs folder extends dfs path implements dfs content logger log logger get logger dfs folder get name dfs content children protected dfs folder dfs content provider provider hadoop server location dfs folder dfs path parent path path protected load dfs folder children throws io exception upload given file directory dfs folder upload i progress monitor monitor file file download to local directory i progress monitor monitor file dir compute download work create new sub directory directory mkdir string folder name implementation dfs content boolean children dfs content get children refresh string string
884	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSLocation.java	unrelated	package org apache hadoop eclipse dfs dfs content representation hdfs location dfs location implements dfs content dfs content provider provider hadoop server location dfs content root folder null dfs location dfs content provider provider hadoop server server string string implementation dfs content dfs content get children boolean children refresh actions refresh location using new connection reconnect
885	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSLocationsRoot.java	unrelated	package org apache hadoop eclipse dfs representation root element containing dfs servers this content registers observer hadoop servers update servers updated dfs locations root implements dfs content i hadoop server listener dfs content provider provider map hadoop server dfs location map register listeners track dfs locations updates dfs locations root dfs content provider provider implementation i hadoop server listener synchronized server changed hadoop server location recompute map hadoop locations synchronized reload locations string string implementation dfs content synchronized dfs content get children boolean children refresh actions disconnect
886	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSMessage.java	unrelated	package org apache hadoop eclipse dfs dfs content displays message dfs message implements dfs content string message dfs message string message string string implementation dfs content dfs content get children boolean children refresh
887	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSPath.java	unrelated	package org apache hadoop eclipse dfs dfs path handling dfs dfs path implements dfs content protected dfs content provider provider protected hadoop server location distributed file system dfs null protected path path protected dfs path parent for debugging purpose logger log logger get logger dfs path get name create path representation given location given viewer dfs path dfs content provider provider hadoop server location create sub path representation given parent path protected dfs path dfs path parent path path protected dispose string string does recursive delete remote directory tree node delete dfs path get parent refresh refresh ui element content refresh copy dfs path given local directory download to local directory i progress monitor monitor path get path gets connection dfs distributed file system get dfs throws io exception compute download work
888	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\HadoopApplicationLaunchShortcut.java	unrelated	package org apache hadoop eclipse launch add shortcut run hadoop run menu hadoop application launch shortcut extends java application launch shortcut logger log action delegate delegate new run on hadoop action delegate hadoop application launch shortcut protected i launch configuration find launch configuration i type type find existing create launch configuration standard way i launch configuration conf conf null conf super create configuration type i launch configuration working copy conf wc try catch core exception e update selected configuration specific hadoop location target i resource resource type get resource resource instanceof i file run on hadoop wizard wizard wizard dialog dialog dialog create dialog set block on open true dialog open wizard dialog ok try catch core exception e return conf wc was used run run on hadoop wizard inside provide progress monitor dialog extends wizard dialog dialog shell parent shell i wizard new wizard create
889	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\LocalMapReduceLaunchTabGroup.java	unrelated	package org apache hadoop eclipse launch handler local map reduce job launches todo jz may needed almost always deploy remote server locally locally may able exec scripts without going java local map reduce launch tab group extends abstract launch configuration tab group local map reduce launch tab group todo auto generated constructor stub create tabs i launch configuration dialog dialog string mode set tabs new i launch configuration tab new map reduce launch tab map reduce launch tab extends abstract launch configuration tab text combiner class text reducer class text mapper class boolean save boolean valid i launch configuration launch config create control composite parent create row composite parent composite panel string get name initialize from i launch configuration configuration perform apply i launch configuration working copy configuration set defaults i launch configuration working copy configuration
890	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\MutexRule.java	unrelated	package org apache hadoop eclipse launch mutex rule implements i scheduling rule string id mutex rule string id boolean contains i scheduling rule rule boolean conflicting i scheduling rule rule
891	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\StartHadoopLaunchTabGroup.java	unrelated	package org apache hadoop eclipse launch create tab group dialog window starting hadoop job start hadoop launch tab group extends start hadoop launch tab group todo jz consider appropriate tabs case create tabs i launch configuration dialog dialog string mode
892	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\MapReducePreferencePage.java	unrelated	package org apache hadoop eclipse preferences this represents preference page contributed preferences dialog by sub classing tt field editor preference page tt use field support built j face allows us create page small knows save restore apply p this page used modify preferences they stored preference store belongs main plug that way preferences accessed directly via preference store map reduce preference page extends field editor preference page map reduce preference page create field editors init i workbench workbench
893	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\PreferenceConstants.java	unrelated	package org apache hadoop eclipse preferences constant definitions plug preferences preference constants string p path path preference string p boolean boolean preference string p choice choice preference string p string preference
894	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\PreferenceInitializer.java	unrelated	package org apache hadoop eclipse preferences class used initialize default preference values preference initializer extends abstract preference initializer initialize default preferences
895	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\ConfProp.java	unrelated	package org apache hadoop eclipse server enum conf prop property name hadoop location name pi location name true location name new hadoop location property name master host name job tracker pi job tracker host true jobtracker host localhost property name dfs master host name name node pi name node host true namenode host localhost property name installation directory master node pi install dir true install dir dir hadoop version user name use hadoop operations pi user name true user name system get property user name property name socks proxy activation pi socks proxy enable true socks proxy enable property name socks proxy host pi socks proxy host true socks proxy host host property name socks proxy port pi socks proxy port true socks proxy port tcp port number name node pi name node port true namenode port tcp port number job tracker pi job tracker port true jobtracker port are map reduce distributed fs masters hosted machine pi colocate masters true masters colocate yes property name naming job tracker uri this property related link pi master host name job tracker uri false mapreduce jobtracker address localhost property name naming default file system uri fs default uri false fs default name hdfs localhost property name default socket factory socket factory default false hadoop rpc socket factory default property name socks server uri socks server false hadoop socks server host map property name conf prop map string conf prop map synchronized register property string name conf prop get by name string prop name string name string def val conf prop boolean internal string name string def val string get configuration conf set configuration conf string value
896	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopJob.java	unrelated	package org apache hadoop eclipse server representation map reduce running job given location hadoop job enum representation job state enum job state location job runs hadoop server location unique identifier job job id job id status representation running job this actually contains reference job client its methods might block running job running last polled status job status status last polled counters counters counters job configuration job conf job conf null boolean completed false boolean successful false boolean killed false total maps total reduces completed maps completed reduces map progress reduce progress constructor hadoop job representation hadoop job hadoop server location job id id running job running try locate load job conf file job get details job number maps reduces load job file hash code boolean equals object obj get running status job see link job status job state get state job id get job id hadoop server get location boolean completed string get job name string get job file return tracking url job string get tracking url returns representation job status string get status update job status according given job status update job status status print job counters debugging purpose print counters kill job kill print job status debugging purpose display
897	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopPathPage.java	unrelated	package org apache hadoop eclipse server hadoop path page implements i editor part i editor input get editor input i editor site get editor site init i editor site site i editor input input add property listener i property listener listener create part control composite parent dispose i workbench part site get site string get title image get title image string get title tool tip remove property listener i property listener listener set focus object get adapter class adapter save i progress monitor monitor save as boolean dirty boolean save as allowed boolean save on close needed
898	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopServer.java	unrelated	package org apache hadoop eclipse server representation hadoop location meaning master node name node job tracker p this create ssh connection anymore tunneling must setup outside eclipse using putty tt ssh d lt port gt lt host gt tt p em todo em li disable updater location becomes unreachable fails tool li stop updater location disposal removal hadoop server protected status observation delay location status updater extends job logger log logger get logger hadoop server get name configuration conf set i job listener job listeners new hash set i job listener transient map job id hadoop job running jobs location status updater status updater state status transient transient string state hadoop server hadoop server file file throws parser configuration exception hadoop server hadoop server existing add job listener i job listener dispose collection hadoop job get jobs purge job hadoop job job configuration get configuration string get conf prop conf prop prop string get conf prop string prop name string get location name string get master host name string get state load hadoop server existing boolean load from xml file file throws parser configuration exception set conf prop conf prop prop string prop value set conf prop string prop name string prop value set location name string new name store settings to file file file throws io exception string string add plugin config default properties synchronized start status updater file system get dfs throws io exception job client get job client throws io exception protected fire jar publish done jar module jar protected fire jar publish start jar module jar protected fire job added hadoop job job protected fire job removed hadoop job job protected fire job changed hadoop job job
899	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\IJobListener.java	unrelated	package org apache hadoop eclipse server interface updating adding jobs map reduce server view i job listener job changed hadoop job job job added hadoop job job job removed hadoop job job publish start jar module jar publish done jar module jar
900	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\JarModule.java	unrelated	package org apache hadoop eclipse server methods interacting jar file containing mapper reducer driver map reduce job jar module implements i runnable with progress logger log logger get logger jar module get name i resource resource file jar file jar module i resource resource resource resource string get name return resource get project get name resource get name creates jar file containing given resource java main associated resources run i progress monitor monitor log fine build jar jar package data jarrer new jar package data jarrer set export java files true jarrer set export class files true jarrer set export output folders true jarrer set overwrite true try catch exception e allow retrieval resulting jar file file get jar file return jar file static way create jar package given resource showing progress bar file create jar package i resource resource jar module jar module new jar module resource try catch exception e file jar file jar module get jar file jar file null return jar file
901	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\HadoopLocationWizard.java	unrelated	package org apache hadoop eclipse servers wizard editing settings hadoop location the wizard contains tabs general tunneling advanced it edits parameters location member either new location copy existing registered location hadoop location wizard extends wizard page image circle the location effectively edited wizard this location copy new one hadoop server location the original location edited wizard null create new instance hadoop server original new hadoop location wizard hadoop location wizard super hadoop server new hadoop location null original null location new hadoop server location set location name constructor edit parameters existing hadoop server hadoop location wizard hadoop server server super create new hadoop location edit hadoop location null original server location new hadoop server server performs actions appropriate response user pressed finish button refuse finishing permitted hadoop server perform finish try catch exception e validates current hadoop location settings look hadoop installation directory test location set message not implemented yet i message provider warning location complete finish button available host name specified boolean page complete set message define location hadoop infrastructure return true create wizard create control composite parent set title define hadoop location set description define location hadoop infrastructure composite panel new composite parent swt fill grid layout glayout new grid layout false panel set layout glayout tab mediator mediator new tab mediator panel set control panel mediator folder tab listener notify change conf prop prop string prop value mediator pattern keep tabs synchronized location state tab mediator tab folder folder set tab listener tabs new hash set tab listener tab mediator composite parent access current configuration settings string get string prop name string get conf prop prop implements change notifications tab update location state tabs notify change tab listener source conf prop prop change notifications properties name a property might reflected conf prop enum if notification forwarded conf prop notify change method if processed notify change tab listener source string prop name string prop value broadcast property change registered tabs if tab identified source change tab notified fire change tab listener source conf prop prop string value create swt text component given link conf prop text configuration property text create conf text modify listener listener composite parent text text new text parent swt single swt border grid data data new grid data grid data fill horizontal text set layout data data text set data prop prop text set text location get conf prop prop text add modify listener listener return text create swt checked button component given link conf prop boolean configuration property button create conf check button selection listener listener button button new button parent swt check button set text text button set data prop prop button set selection location get conf prop prop equals ignore case yes button add selection listener listener return button create editor entry given configuration property the editor couple label text text create conf label text modify listener listener label label new label parent swt none label text null label set text label text return create conf text listener parent prop create editor entry given configuration name text create conf name editor modify listener
902	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\HadoopServerSelectionListContentProvider.java	unrelated	package org apache hadoop eclipse servers provider enables selection predefined hadoop server hadoop server selection list content provider implements dispose input changed viewer viewer object old input object new input image get column image object element column index string get column text object element column index add listener i label provider listener listener boolean label property object element string property remove listener i label provider listener listener object get elements object input element
903	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\IHadoopServerListener.java	unrelated	package org apache hadoop eclipse servers interface monitoring server changes i hadoop server listener server changed hadoop server location type
904	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\RunOnHadoopWizard.java	unrelated	package org apache hadoop eclipse servers wizard publishing job hadoop server run on hadoop wizard extends wizard main wizard page main page hadoop location wizard create new page the file resource containing main run hadoop location i file resource the launch configuration update i launch configuration working copy conf i progress monitor progress monitor run on hadoop wizard i file resource resource resource conf conf set force previous and next buttons true set needs progress monitor true set window title run hadoop this wizard contains pages li first one lets user choose already existing location li second one allows user create new location case already exist add pages add page main page new main wizard page add page create new page new hadoop location wizard performs actions appropriate response user pressed finish button refuse finishing permitted boolean perform finish create new location get existing one hadoop server location null main page create new get selection else main page table get selection length location null get base directory plug storing configurations ja rs file base dir activator get default get state location file package job jar file jar file jar module create jar package resource jar file null generate temporary hadoop configuration directory add classpath launch configuration file conf dir try catch io exception ioe prepare hadoop configuration job conf conf new job conf location get configuration conf set jar jar file get absolute path write disk file try catch io exception ioe setup launch path list string path try catch core exception e location run resource resource progress monitor return true refresh buttons get container update buttons allows finish existing server selected new server location defined boolean finish main page null return false this main page wizard it allows user either choose already existing location indicate wants create new location main wizard page extends wizard page button create new table table text arguments text button choose existing main wizard page boolean flip to next page create control composite parent returns whether page state allows wizard finish boolean finish set progress monitor i progress monitor progress monitor progress monitor progress monitor
905	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\ServerRegistry.java	unrelated	package org apache hadoop eclipse servers register hadoop locations each location corresponds hadoop link configuration stored xml file workspace plug configuration directory p tt lt workspace dir gt metadata plugins org apache hadoop eclipse locations xml tt server registry server registry instance new server registry server added server removed server state changed file base dir file save dir new file base dir locations server registry map string hadoop server servers set i hadoop server listener listeners server registry get instance synchronized collection hadoop server get servers load available locations workspace configuration directory synchronized load synchronized store dispose synchronized hadoop server get server string location hadoop server map listeners add listener i hadoop server listener remove listener i hadoop server listener fire listeners hadoop server location kind synchronized remove server hadoop server server synchronized add server hadoop server server update one hadoop location synchronized update server string original name
906	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\view\servers\ServerView.java	unrelated	package org apache hadoop eclipse view servers map reduce locations view displays available hadoop locations jobs running finished locations server view extends view part implements i tree content provider i table label provider i job listener i hadoop server listener deletion action delete hadoop location kill running job remove finished job entry delete action extends action delete action run this object root content content provider object content root new object i action delete action new delete action i action edit server action new edit location action i action new location action new new location action tree viewer viewer server view init i view site site throws part init exception super init site dispose server registry get instance remove listener creates columns view create part control composite parent tree main main set header visible true main set lines visible false main set layout data new grid data grid data fill both tree column server col new tree column main swt single server col set text location server col set width server col set resizable true tree column location col new tree column main swt single location col set text master node location col set width location col set resizable true tree column state col new tree column main swt single state col set text state state col set width state col set resizable true tree column status col new tree column main swt single status col set text status status col set width status col set resizable true viewer new tree viewer main viewer set content provider viewer set label provider viewer set input content root care get view site set selection provider viewer get view site get action bars set global action handler get view site get action bars get tool bar manager add edit server action get view site get action bars get tool bar manager add new location action create actions create context menu actions create actions add item action new action add run add item add item action set image descriptor image library get server view location new delete item action new action delete run delete item delete item action set image descriptor get image descriptor delete gif select all action new action select all run select all add selection listener viewer add selection changed listener new i selection changed listener add item system printf add item n update action enablement i structured selection sel i structured selection viewer get selection delete item action set enabled sel size contextual menu create context menu create menu manager menu manager menu mgr new menu manager menu mgr set remove all when shown true menu mgr add menu listener new i menu listener create menu menu menu menu mgr create context menu viewer get control viewer get control set menu menu register menu extension get site register context menu menu mgr viewer fill context menu i menu manager mgr mgr add new location action mgr add edit server action mgr add delete action mgr add new group marker i workbench action constants mb additions mgr add delete item action mgr add new
907	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\AllocationConfigurationException.java	pooling	package org apache hadoop mapred thrown allocation file link pool manager malformed allocation configuration exception extends exception serial version uid l allocation configuration exception string message
908	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\CapBasedLoadManager.java	scheduler	package org apache hadoop mapred a link load manager use link fair scheduler allocates tasks evenly across nodes per node maximum using default load management algorithm hadoop cap based load manager extends load manager max diff f set conf configuration conf determine many tasks given type want run task tracker this cap chosen based many tasks type outstanding total cluster used capacity tasks spread uniformly across nodes rather clumped whichever machines sent heartbeats earliest get cap total runnable tasks local max tasks total slots boolean assign map task tracker status tracker boolean assign reduce task tracker status tracker boolean launch task task tracker status tracker
909	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\DefaultTaskSelector.java	scheduler	package org apache hadoop mapred a link task selector implementation wraps around default link job in progress obtain new map task task tracker status link job in progress obtain new reduce task task tracker status methods link job in progress using default hadoop locality speculative threshold algorithms default task selector extends task selector needed speculative maps job in progress job needed speculative reduces job in progress job task obtain new map task task tracker status task tracker job in progress job task obtain new reduce task task tracker status task tracker job in progress job
910	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairScheduler.java	pooling	package org apache hadoop mapred a link task scheduler implements fair sharing fair scheduler extends task scheduler log log log factory get log how often fair shares calculated protected update interval how often dump scheduler state event log protected dump interval how often tasks preempted must longer couple heartbeats give task kill commands chance act protected preemption interval used iterate map reduce task types task type map and reduce new task type task type map task type reduce maximum locality delay auto computing locality delays max autocomputed locality delay protected pool manager pool mgr protected load manager load mgr protected task selector task selector protected weight adjuster weight adjuster can null weight adjuster protected map job in progress job info infos per job scheduling variables new hash map job in progress job info protected last update time time last updated infos protected last preemption update time time last updated preemption vars protected boolean initialized are initialized protected volatile boolean running are running protected boolean assign multiple simultaneously assign map reduce protected map assign cap max maps launch per heartbeat protected reduce assign cap max reduces launch per heartbeat protected node locality delay time wait node locality protected rack locality delay time wait rack locality protected boolean auto compute locality delay false compute locality delay protected boolean size based weight give larger weights larger jobs protected boolean wait for maps before launching reduces true protected boolean preemption enabled protected boolean log preemption only log tasks killed clock clock job listener job listener job initializer job initializer boolean mock mode used unit tests disables background updates fair scheduler event log event log protected last dump time time last dumped state log protected last heartbeat time time last ran assign tasks last preempt check time time last ran preempt tasks if necessary a holding per job scheduler variables these always contain values variables last update used along time delta update map reduce deficits new update job info boolean runnable false can job run given user pool limits does job need initialized volatile boolean needs initializing true job schedulable map schedulable job schedulable reduce schedulable variables used delay scheduling locality level last map locality level locality level last map launched time waited for local map time waiting local map since last map boolean skipped at last heartbeat was job skipped previous assign tasks job info job schedulable map sched job schedulable reduce sched fair scheduler new clock false constructor used tests change clock disable updates protected fair scheduler clock clock boolean mock mode clock clock mock mode mock mode job listener new job listener start try catch exception e log info successfully configured fair scheduler metrics updater metrics updater responsible pushing hadoop metrics returns load manager object used fair share scheduler load manager get load manager return load mgr register metrics fair scheduler start thread update periodically init metrics metrics context context metrics util get context fairscheduler metrics updater new metrics updater context register updater metrics updater terminate throws io exception event log null running false job initializer terminate job listener null event log null metrics
911	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairSchedulerEventLog.java	scheduler	package org apache hadoop mapred event log used fair scheduler machine readable debug info this uses log j rolling file appender write log uses custom tab separated event format form pre date event type param param pre various event types used fair scheduler the purpose logging format enable tools parse history log easily read internal scheduler variables rather trying make log human readable the fair scheduler also logs human readable messages job tracker main log constructing creates disabled log it must initialized using link fair scheduler event log init configuration string begin writing file fair scheduler event log log log log factory get log set true logging disabled due error boolean log disabled true string log dir string log file log j appender used write log file daily rolling file appender appender boolean init configuration conf string jobtracker hostname synchronized log string event type object params shutdown boolean enabled
912	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairSchedulerServlet.java	scheduler	package org apache hadoop mapred servlet displaying fair scheduler information installed job tracker url scheduler link fair scheduler use the main features viewing job task count fair share admin controls change job priorities pools ui there also advanced view debugging turned going job tracker url scheduler advanced fair scheduler servlet extends http servlet serial version uid l date format date format fair scheduler scheduler job tracker job tracker last id used generate unique element i ds init throws servlet exception protected post http servlet request req http servlet response resp get http servlet request request http servlet response response print view pools given output writer show pools print writer boolean advanced view print view running jobs given output writer show jobs print writer boolean advanced view generate html select control given list choices given option selected when selection changed take user code submit url code the code submit url code made option selected first occurrence substring code lt choice gt code replaced option chosen string generate select iterable string choices obtained initialized jobs collection job in progress get inited jobs
913	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FifoJobComparator.java	scheduler	package org apache hadoop mapred order link job in progress objects priority submit time default scheduler hadoop fifo job comparator implements comparator job in progress compare job in progress j job in progress j
914	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\JobSchedulable.java	scheduler	package org apache hadoop mapred job schedulable extends schedulable fair scheduler scheduler job in progress job task type task type demand job schedulable fair scheduler scheduler job in progress job task type get task type string get name job in progress get job update demand boolean runnable get demand redistribute share job priority get priority get running tasks get start time get weight get min share task assign task task tracker status tts current time protected string get metrics context name update metrics
915	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\LoadManager.java	scheduler	package org apache hadoop mapred a pluggable object manages load link task tracker telling link task scheduler launch new tasks load manager implements configurable protected configuration conf protected task tracker manager task tracker manager protected fair scheduler event log scheduling log configuration get conf set conf configuration conf synchronized set task tracker manager set event log fair scheduler event log scheduling log lifecycle method allow load manager start work separate threads start throws io exception lifecycle method allow load manager stop work terminate throws io exception can given link task tracker run another map task this method may check whether specified tracker enough resources run another map task boolean assign map task tracker status tracker can given link task tracker run another reduce task this method may check whether specified tracker enough resources run another reduce task boolean assign reduce task tracker status tracker can given link task tracker run another new task given job this method provided use load managers take account jobs individual resource needs placing tasks boolean launch task task tracker status tracker
916	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\LocalityLevel.java	scheduler	package org apache hadoop mapred represents level data locality job fair scheduler allowed launch tasks by default jobs allowed launch non data local tasks waited small number seconds find slot node data if job waited allowed launch rack local tasks well nodes may task input data share rack node finally wait jobs allowed launch tasks anywhere cluster this enum defines three levels node rack any allowing tasks launched node a map task level obtained job link task job in progress task task tracker status in addition locality level possible get level cap pass link job in progress obtain new map task task tracker status ensure tasks level lower launched link cache level cap method enum locality level node rack any locality level task job in progress job task map task obtain job in progress cache level cap pass link job in progress obtain new map task task tracker status ensure tasks locality level lower launched cache level cap
917	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\NewJobWeightBooster.java	scheduler	package org apache hadoop mapred a link weight adjuster implementation gives weight boost new jobs certain amount time default x weight boost seconds this used make shorter jobs finish faster emulating shortest job first scheduling starving jobs new job weight booster extends configured implements weight adjuster default factor default duration factor duration set conf configuration conf adjust weight job in progress job task type task type
918	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\Pool.java	pooling	package org apache hadoop mapred a schedulable pool jobs pool name default pool jobs pool parameter go string default pool name default pool name string name jobs specific pool children pools jobs collection job in progress jobs new array list job in progress scheduling mode jobs inside pool fair fifo scheduling mode scheduling mode pool schedulable map schedulable pool schedulable reduce schedulable pool fair scheduler scheduler string name collection job in progress get jobs add job job in progress job remove job job in progress job string get name scheduling mode get scheduling mode set scheduling mode scheduling mode scheduling mode boolean default pool pool schedulable get map schedulable pool schedulable get reduce schedulable pool schedulable get schedulable task type type update metrics
919	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolManager.java	pooling	package org apache hadoop mapred maintains list pools well scheduling parameters pool guaranteed share allocations fair scheduler config file pool manager log log log factory get log time wait checks allocation file alloc reload interval time wait allocation modified reloading done prevent loading file fully written alloc reload wait string explicit pool property mapred fairscheduler pool fair scheduler scheduler map reduce minimum allocations pool map string integer map allocs new hash map string integer map string integer reduce allocs new hash map string integer if set cap number map reduce tasks pool map string integer pool max maps new hash map string integer map string integer pool max reduces new hash map string integer sharing weights pool map string double pool weights new hash map string double max concurrent running jobs pool user addition users max specified use user max jobs default map string integer pool max jobs new hash map string integer map string integer user max jobs new hash map string integer user max jobs default integer max value pool max jobs default integer max value min share preemption timeout pool seconds if job pool waits without receiving guaranteed share allowed preempt jobs tasks map string long min share preemption timeouts default min share preemption timeout pools set explicitly default min share preemption timeout long max value preemption timeout jobs fair share seconds if job remains half fair share allowed preempt tasks fair share preemption timeout long max value scheduling mode default scheduling mode scheduling mode fair object alloc file path xml file containing allocations this string pool name property jobconf property use determining map string pool pools new hash map string pool last reload attempt last time tried reload pools file last successful reload last time successfully reloaded pools boolean last reload attempt failed false pool manager fair scheduler scheduler initialize throws io exception sax exception get pool name creating necessary synchronized pool get pool string name get pool given job pool get pool job in progress job reload allocations file loaded reload allocs if necessary updates allocation list allocation config file this file expected following whitespace separated format code pool name map alloc reduce alloc pool name map alloc reduce alloc code blank lines lines starting ignored reload allocs throws io exception parser configuration exception does pool incompatible max min allocation set link task type map link task type reduce pool name boolean inverted min max task type type string pool scheduling mode parse scheduling mode string text get allocation particular pool get allocation string pool task type task type get maximum map reduce slots given pool get max slots string pool name task type task type add job appropriate pool synchronized add job job in progress job remove job synchronized remove job job in progress job change pool particular job synchronized set pool job in progress job string pool get collection pools synchronized collection pool get pools get pool name job in progress configuration this uses value mapred fairscheduler pool specified otherwise value property named mapred fairscheduler poolnameproperty specified otherwise neither specified uses user name property jobconf
920	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolSchedulable.java	pooling	package org apache hadoop mapred pool schedulable extends schedulable log log log factory get log fair scheduler scheduler pool pool task type task type pool manager pool mgr list job schedulable job scheds new linked list job schedulable demand variables used preemption last time at min share last time at half fair share pool schedulable fair scheduler scheduler pool pool task type type add job job in progress job remove job job in progress job update demand asking jobs pool update update demand distribute pool fair share among jobs redistribute share get demand get min share get weight job priority get priority get running tasks get start time task assign task task tracker status tts current time string get name pool get pool task type get task type collection job schedulable get job schedulables get last time at min share set last time at min share last time at min share get last time at half fair share set last time at half fair share last time at half fair share protected string get metrics context name update metrics
921	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\Schedulable.java	pooling	package org apache hadoop mapred a schedulable represents entity launch tasks job pool it provides common algorithms fair sharing applied within pool across pools there currently two types schedulables job schedulables represent single job pool schedulables allocate among jobs pool separate sets schedulables used maps reduces each pool map schedulable reduce schedulable job a schedulable responsible three roles it launch tasks assign task it provides information job pool scheduler including demand maximum number tasks required number currently running tasks minimum share pools job pool weight fair sharing start time priority fifo it assigned fair share use fair scheduling schedulable also contains two methods performing scheduling computations update demand called periodically compute demand various jobs pools may expensive e g jobs must iterate tasks count failed tasks tasks speculated etc redistribute share called demands updated schedulable fair share set parent let distribute share among schedulables within e g pools want perform fair sharing among jobs schedulable fair share assigned schedulable fair share protected metrics record metrics name job pool used debugging well breaking ties scheduling order deterministically string get name task type get task type maximum number tasks required schedulable this defined number currently running tasks number unlaunched tasks tasks either yet launched need speculated get demand number tasks schedulable currently running get running tasks minimum share slots assigned schedulable get min share job pool weight fair sharing get weight job priority jobs fifo pools meaningless pool schedulables job priority get priority start time jobs fifo pools meaningless pool schedulables get start time refresh schedulable demand children update demand distribute fair share assigned schedulable among children used pools internal scheduler fair sharing redistribute share obtain task given task tracker null schedulable tasks launch moment wish launch task task tracker e g waiting task tracker local data in addition job skipped search waiting task tracker local data method expected add tt visited tt collection passed scheduler properly mark skipped heartbeat please see link fair scheduler get allowed locality level job in progress details delay scheduling waiting trackers local data considered search job assign task assign task task tracker status tts current time assign fair share schedulable set fair share fair share get fair share assigned schedulable get fair share return name metrics context schedulable protected string get metrics context name set metrics context protected init metrics cleanup metrics protected set metric values metrics record metrics update metrics convenient string implementation debugging string string
922	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\SchedulingAlgorithms.java	scheduler	package org apache hadoop mapred utility containing scheduling algorithms used fair scheduler scheduling algorithms log log log factory get log fifo comparator implements comparator schedulable fair share comparator implements comparator schedulable compute fair shares iterations compute fair shares slots used with weight to slot ratio w ratio compute share schedulable sched w ratio
923	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\SchedulingMode.java	pooling	package org apache hadoop mapred internal scheduling modes pools enum scheduling mode fair fifo
924	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\TaskSelector.java	scheduler	package org apache hadoop mapred a pluggable object selecting tasks run link job in progress given link task tracker use link task scheduler the code task selector code charge managing locality speculative execution for latter purpose must also provide counts many tasks speculative job needs launch scheduler take account calculations task selector implements configurable protected configuration conf protected task tracker manager task tracker manager configuration get conf set conf configuration conf synchronized set task tracker manager lifecycle method allow task selector start work separate threads start throws io exception lifecycle method allow task selector stop work terminate throws io exception how many speculative map tasks given job want launch needed speculative maps job in progress job how many speculative reduce tasks given job want launch needed speculative reduces job in progress job choose map task run given job given task tracker map launched job task tracker task obtain new map task task tracker status task tracker choose reduce task run given job given task tracker reduce launched job task tracker task obtain new reduce task task tracker status task tracker
925	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\WeightAdjuster.java	scheduler	package org apache hadoop mapred a pluggable object altering weights jobs fair scheduler used example link new job weight booster give higher weight new jobs short jobs finish faster may implement link configurable access configuration parameters weight adjuster adjust weight job in progress job task type task type
926	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\AvgRecordFactory.java	unrelated	package org apache hadoop mapred gridmix given byte record targets emit roughly equal sized records satisfying contract avg record factory extends record factory percentage record key data string gridmix key frc gridmix key fraction string gridmix missing rec size target bytes target records step avgrec key len acc bytes l acc records l unspilled bytes min spilled bytes avg record factory target bytes target records avg record factory target bytes target records boolean next gridmix key key gridmix record val throws io exception get progress throws io exception close throws io exception
927	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ClusterSummarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes hadoop cluster used link gridmix run statistics reported ul li total number active trackers cluster li li total number blacklisted trackers cluster li li max map task capacity cluster li li max reduce task capacity cluster li ul apart statistics link job tracker link file system addresses also recorded summary cluster summarizer implements stat listener cluster stats log log log factory get log cluster summarizer num blacklisted trackers num active trackers max map tasks max reduce tasks string job tracker info summarizer na string namenode info summarizer na update cluster stats item summarizes cluster used link gridmix run string string start configuration conf getters protected get num blacklisted trackers protected get num active trackers protected get max map tasks protected get max reduce tasks protected string get job tracker info protected string get namenode info
928	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\CompressionEmulationUtil.java	pooling	package org apache hadoop mapred gridmix this utility compression related modules compression emulation util log log log factory get log compression emulation util string compression emulation enable string input decompression emulation enable string gridmix map input compression ratio string gridmix map output compression ratio string gridmix reduce output compression ratio default compression ratio f compression ratio lookup table compression lookup table random text data mapper extends mapper null writable long writable text text configure job job throws io exception interrupted exception compression ratio lookup table setup data generator config configuration conf random text data generator get random text data generator ratio publishes compression related data statistics following statistics data statistics publish compressed data statistics path input dir throws io exception set compression emulation enabled configuration conf boolean val boolean compression emulation enabled configuration conf set input compression emulation enabled configuration conf boolean input compression emulation enabled configuration conf set map input compression emulation ratio configuration conf get map input compression emulation ratio configuration conf set map output compression emulation ratio configuration conf get map output compression emulation ratio configuration conf set reduce output compression emulation ratio configuration conf get reduce output compression emulation ratio configuration conf standardize compression ratio ratio input stream get possibly decompressed input stream path file throws io exception output stream get possibly compressed output stream path file throws io exception configure compression emulation configuration source
929	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\DistributedCacheEmulator.java	unrelated	package org apache hadoop mapred gridmix emulation distributed cache usage gridmix br emulation distributed cache load gridmix put load task trackers affects execution time tasks localization distributed cache files task trackers br gridmix creates distributed cache files simulated jobs launching map reduce job link generate dist cache data advance e launching simulated jobs br the distributed cache file paths used original cluster mapped unique file names simulated cluster br all hdfs based distributed cache files generated gridmix distributed cache files but gridmix makes sure load incurred due localization distributed cache files original cluster also faithfully simulated gridmix emulates load due distributed cache files mapping distributed cache files different users original cluster different distributed cache files simulated cluster br the configuration properties like link mr job config cache files link mr job config cache file visibilities link mr job config cache files sizes link mr job config cache file timestamps obtained trace used decide li file size distributed cache file generated li whether distributed cache file already seen trace file li whether distributed cache file considered br br gridmix configures generated files distributed cache files simulated jobs distributed cache emulator log log avg bytes per map l mb if least distributed cache file missing expected distributed cache dir gridmix cannot proceed emulation distributed cache load missing dist cache files error path dist cache path map simulated cluster distributed cache file paths file sizes unique distributed cache files entered map distributed cache files considered file paths visibilities timestamps map string long dist cache files new hash map string long configuration property whether gridmix emulate distributed cache usage default value true string gridmix emulate distributedcache whether emulate distributed cache usage boolean emulate distributed cache true whether generate distributed cache data boolean generate dist cache data false configuration conf gridmix configuration pseudo local file system local fs based distributed cache files created gridmix file system pseudo local fs null cache directory distributed cache emulator configuration conf path io path this called method distributed cache emulator br checks emulation distributed cache load needed feasible sets flags generate dist cache data emulate distributed cache appropriate values br gridmix emulate distributed cache load ol li specific gridmix job type need emulation distributed cache load or li trace coming stream instead file or li distributed cache dir distributed cache data generated gridmix local file system or li execute permission ascendant directories lt io path gt till root this emulation distributed cache load distributed cache files created lt io path distributed cache gt considered hadoop distributed cache files li creation pseudo local file system fails ol br for generation distributed cache data also disabled stream stdin init string trace in job creator job creator boolean generate boolean emulate dist cache load boolean generate dist cache data path get distributed cache dir create distributed cache directories also create file contains list distributed cache files used distributed cache files simulated jobs setup generate dist cache data job story producer jsp create distributed cache directory distributed cache files created map reduce job link generate dist cache data job name create dist cache directory throws
930	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\EchoUserResolver.java	unrelated	package org apache hadoop mapred gridmix echos ugi offered echo user resolver implements user resolver log log log factory get log gridmix echo user resolver synchronized boolean set target users uri userdesc configuration conf throws io exception synchronized user group information get target ugi inherit doc br br since link echo user resolver simply returns user name passed argument need target list users boolean needs target users list
931	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ExecutionSummarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes link gridmix run statistics reported ul li total number jobs input trace li li trace signature li li total number jobs processed input trace li li total number jobs submitted li li total number successful failed jobs li li total number map reduce tasks launched li li gridmix start end time li li total time gridmix run data generation simulation li li gridmix configuration e job type submission type resolver li ul execution summarizer implements stat listener job stats log log log factory get log execution summarizer fast date format util fast date format get instance num jobs in input trace total successful jobs total failed jobs total map tasks launched total reduce tasks launched total simulation time total runtime string command line args start time end time simulation start time string input trace location string input trace signature string job submission policy string resolver data statistics data stats string expected data size basic constructor initialized runtime arguments execution summarizer string args default constructor execution summarizer start configuration conf process job state job stats stats throws exception process job tasks job stats stats throws exception process job stats stats update job stats item generates signature trace file based filename modification time file length owner protected string get trace signature string input throws io exception finalize job factory factory string input path data size throws io exception summarizes current link gridmix run string string gets stringified version data statistics string stringify data statistics data statistics stats getters protected string get expected data size protected string get user resolver protected string get input data statistics protected string get input trace signature protected string get input trace location protected get num jobs in trace protected get num successful jobs protected get num failed jobs protected get num submitted jobs protected get num map tasks launched protected get num reduce tasks launched protected get start time protected get end time protected get init time protected get simulation start time protected get simulation time protected get runtime protected string get command line args string protected string get job submission policy
932	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\FilePool.java	pooling	package org apache hadoop mapred gridmix class caching pool input data used synthetic jobs simulating read traffic file pool log log log factory get log file pool the minimum file size added pool default mi b string gridmix min file gridmix min file size the maximum size files added pool defualts ti b string gridmix max total gridmix max total scan node root path path file system fs configuration conf read write lock update lock initialize filepool path provided populate cache file pool configuration conf path input throws io exception root null conf conf path input fs path get file system conf update lock new reentrant read write lock gather collection files least large min size get input files min size collection file status files update lock read lock lock try finally re generate cache input file status objects refresh throws io exception update lock write lock lock try finally get set locations given file block location locations for file status stat start len todo cache return fs get file block locations stat start len node protected random rand new random total size files directories current node get size return set files whose cumulative size least tt target size tt todo clearly size criterion e g refresh generated data without including running task output tolerance permission issues etc select files target size collection file status files files current directory node leaf desc extends node size array list file status curdir leaf desc array list file status curdir size get size select files target size collection file status files a subdirectory current node inner desc extends node size dist node subdir comparator node node comparator inner desc file system fs file status dir min file filter filter get size select files target size collection file status files filter enforcing min file max total parameters scan min file filter total scan min file size min file filter min file size total scan boolean done boolean accept file status stat
933	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\FileQueue.java	unrelated	package org apache hadoop mapred gridmix given link org apache hadoop mapreduce lib input combine file split circularly read input source file queue extends input stream idx curlen l input stream input byte z new byte path paths lengths startoffset configuration conf file queue combine file split split configuration conf protected next source throws io exception read throws io exception read byte b throws io exception read byte b len throws io exception close throws io exception
934	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GenerateData.java	unrelated	package org apache hadoop mapred gridmix todo replace form gridmix job generate data extends gridmix job total bytes write string gridmix gen bytes gridmix gen bytes maximum size per file written string gridmix gen chunk gridmix gen bytes per file size writes output file string gridmix val bytes gendata val bytes status reporting interval megabytes string gridmix gen interval gendata interval mb blocksize generated data string gridmix gen blocksize gridmix gen blocksize replication generated data string gridmix gen replication gridmix gen replicas string job name gridmix generate input data generate data configuration conf path outdir genbytes super conf l job name job get configuration set long gridmix gen bytes genbytes file output format set output path job outdir represents input data characteristics data statistics data size num files boolean data compressed data statistics data size num files boolean compressed get data size get num files boolean data compressed publish data statistics data statistics publish data statistics path input dir gen bytes throws io exception compression emulation util compression emulation enabled conf else data statistics publish plain data statistics configuration conf throws io exception file system fs input dir get file system conf obtain input data file statuses data size file count remote iterator located file status iter fs list files input dir true path filter filter new utils output file utils output files filter iter next publish plain data statistics log info total size input data log info total number input data files file count return new data statistics data size file count false job call throws io exception interrupted exception user group information ugi user group information get login user ugi as new privileged exception action job return job protected boolean emulate compression return false gen data mapper bytes writable val random r new random protected setup context context map null writable key long writable value context context gen data format extends input format null writable long writable list input split get splits job context job ctxt throws io exception record reader null writable long writable create record reader gen split extends input split implements writable bytes n loc string locations gen split gen split bytes string locations gen split bytes n loc string locations get length string get locations read fields data input throws io exception write data output throws io exception raw bytes output format record writer null writable bytes writable get record writer chunk writer extends record writer null writable bytes writable
935	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GenerateDistCacheData.java	unrelated	package org apache hadoop mapred gridmix gridmix job generates distributed cache files link generate dist cache data expects list distributed cache files generated input this list expected stored sequence file filename expected configured using code gridmix distcache file list this input file contains list distributed cache files sizes for record e file size file path input file file specific file size specific path created generate dist cache data extends gridmix job number distributed cache files created gridmix string gridmix distcache file count total number bytes written distributed cache files gridmix e sum sizes unique distributed cache files created gridmix string gridmix distcache byte count the special file created used gridmix contains list unique distributed cache files created sizes string gridmix distcache file list string job name gridmix generate distcache data generate dist cache data configuration conf throws io exception super conf l job name job call throws io exception interrupted exception user group information ugi user group information get login user ugi as new privileged exception action job return job protected boolean emulate compression return false gen dc data mapper bytes writable val random r new random file system fs protected setup context context create one distributed cache file needed file size key distributed cache file size value distributed cache file path map long writable key bytes writable value context context input format generate dist cache data input generate dist cache data special file sequence file format contains list distributed cache files generated along file sizes gen dc data format split special file contains list distributed cache file paths file sizes split corresponds approximately amount distributed cache data generated consider num task trackers num map slots per tracker number maps job lot data generated list input split get splits job context job ctxt throws io exception returns reader split distributed cache file list record reader long writable bytes writable create record reader
936	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Gridmix.java	pooling	package org apache hadoop mapred gridmix driver gridmix benchmark gridmix accepts timestamped stream trace job task descriptions for job trace client submit corresponding synthetic job target cluster rate original trace the intent provide benchmark configured extended closely match measured resource profile actual production loads gridmix extends configured implements tool log log log factory get log gridmix string gridmix out dir gridmix output directory string gridmix sub thr gridmix client submit threads string gridmix que dep string gridmix sub mul gridmix submit multiplier string gridmix usr rsv gridmix user resolve string original job name string original job id gridmix job original job id distributed cache emulator dist cache emulator submit data structures job factory factory job submitter submitter job monitor monitor statistics statistics summarizer summarizer shutdown hook shutdown sdh new shutdown gridmix string args gridmix get input data directory gridmix input directory io path input path get gridmix input data path path io path protected write input data genbytes path input dir protected write dist cache data configuration conf launch input dist cache data generation job wait completion launch gridmix job gridmix job job protected job story producer create job story producer string trace in get gridmix job submission policy protected gridmix job submission policy get job submission policy start threads configuration conf string trace in path io path protected job monitor create job monitor statistics stats throws io exception protected job submitter create job submitter job monitor monitor threads protected job factory create job factory job submitter submitter string trace in user resolver user resolver user resolver get current user resolver run string argv throws io exception interrupted exception run job configuration conf string argv start configuration conf string trace in path io path genbytes setup emulation configuration conf string trace in setup dist cache emulation configuration conf string trace in shutdown extends thread main string argv throws exception t string get enum values enum extends t e string get job types string get submission policies protected print usage print stream component t
937	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixJob.java	pooling	package org apache hadoop mapred gridmix synthetic job generated trace description gridmix job implements callable job delayed gridmix job name format gridmix digit sequence number string job name prefix gridmix log log log factory get log gridmix job thread local formatter name format new thread local formatter protected seq protected path outdir protected job job protected job story jobdesc protected user group information ugi protected submission time nanos concurrent hash map integer list input split desc cache new concurrent hash map integer list input split protected string gridmix job seq gridmix job seq protected string gridmix use queue in trace protected string gridmix default queue configuration key enable disable high ram feature emulation string gridmix highram emulation enable gridmix highram emulation enable configuration key enable disable task jvm options string gridmix task jvm options enable gridmix task jvm options enable pattern max heap pattern pattern compile xmx k km mg gt t set job queue job job string queue queue null gridmix job configuration conf submission millis ugi ugi jobdesc jobdesc seq seq string builder name format get set length job name prefix length try catch interrupted exception e submission time nanos time unit nanoseconds convert outdir new path root seq protected configure task jvm options configuration original job conf get heap related java opts used original job set simulated job set task task heap options configure task jvm max heap options original job conf simulated job conf set map task heap options configure task jvm max heap options original job conf simulated job conf set reduce task heap options configure task jvm max heap options original job conf simulated job conf configures task max heap options using specified key configure task jvm max heap options configuration src conf string src heap opts src conf get key src heap opts null extract max heap opts string java options string opt java options split scales desired job level configuration parameter this api makes sure ratio job level configuration parameter cluster level configuration parameter maintained simulated run hence values scaled original cluster configuration simulated cluster configuration higher emulation accuracy this kind scaling useful memory parameters scale config parameter configuration source conf simulated cluster default value original cluster default value original job value scale factor original job value original cluster default value simulated job value scale factor simulated cluster default value log debug enabled dest conf set long job value key simulated job value checks scaling original job memory parameter value valid boolean check memory upper limits string job key string limit key conf get limit key null return false check parameter scaling exceed cluster limits validate task memory limits configuration conf check memory upper limits job key sets high ram job properties simulated job configuration configure high ram properties configuration source conf set memory per map task scale config parameter source conf dest conf validate fail early validate task memory limits dest conf mr job config map memory mb set memory per reduce task scale config parameter source conf dest conf validate fail early validate task memory limits dest conf mr job config reduce
938	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixJobSubmissionPolicy.java	unrelated	package org apache hadoop mapred gridmix enum gridmix job submission policy replay replay stress stress serial serial string job submission policy string name polling interval gridmix job submission policy string name polling interval job factory create job factory get polling interval gridmix job submission policy get policy
939	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixKey.java	unrelated	package org apache hadoop mapred gridmix gridmix key extends gridmix record byte reduce spec byte data meta bytes byte type partition not serialized spec spec new spec gridmix key data l gridmix key byte type size seed super size seed type type setting type may change pcnt random bytes set size size get size switch type set size size switch type partition serialized get partition return partition set partition partition partition partition get reduce input records assert reduce spec get type return spec rec set reduce input records rec assert reduce spec get type orig size get size spec rec rec set size orig size get reduce output records assert reduce spec get type return spec rec set reduce output records rec assert reduce spec get type orig size get size spec rec rec set size orig size get reduce output bytes assert reduce spec get type return spec bytes set reduce output bytes b assert reduce spec get type orig size get size spec bytes b set size orig size get link resource usage metrics stored key resource usage metrics get reduce resource usage metrics assert reduce spec get type return spec metrics store link resource usage metrics key set reduce resource usage metrics resource usage metrics metrics assert reduce spec get type spec set resource usage specification metrics byte get type return type set type byte type throws io exception orig size get size switch type set size orig size set spec spec spec assert reduce spec get type orig size get size spec set spec set size orig size read fields data input throws io exception super read fields set type read byte reduce spec get type write data output throws io exception super write byte get type write byte reduce spec fixed bytes return super fixed bytes compare to gridmix record gridmix key gridmix key byte get type byte get type return super compare to note spec explicitly included changing spec may change size affect equality boolean equals object null get class get class return false hash code return super hash code get type spec implements writable rec rec bytes resource usage metrics metrics null size of resource usage metrics spec set spec sets link resource usage metrics link spec set resource usage specification resource usage metrics metrics get size read fields data input throws io exception write data output throws io exception comparator extends gridmix record comparator data input buffer di new data input buffer byte reset di get data comparator compare byte b byte b
940	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixRecord.java	unrelated	package org apache hadoop mapred gridmix gridmix record implements writable comparable gridmix record fixed bytes size seed data input buffer dib new data input buffer data output buffer dob new data output buffer long size byte size byte literal dob get data boolean compressible false compression ratio compression emulation util default compression ratio random text data generator rtg null gridmix record l gridmix record size seed seed seed set size internal size get size return size set size size set size internal size set compressibility boolean compressible ratio compressible compressible compression ratio ratio initialize random text data generator every grid mix record note random text data generator needed grid mix record configured generate compressible text data compressible set size internal size size math max size try catch io exception e set seed seed seed seed marsaglia next rand x x x x x return x x generate random text data compressed if record marked compressible via link file output format compress random data text data else link gridmix record write random data output invoked write random text data output size throws io exception tmp seed write long tmp size long size byte size todo should use size what data g string random word rtg get random word byte bytes random word get bytes utf random word size bytes length random word size pad remaining bytes write random data output size throws io exception tmp seed write long tmp size long size byte size long size byte size tmp next rand tmp read fields data input throws io exception size writable utils read v int payload size writable utils get v int size size payload long size byte size else v bytes skip bytes payload v bytes payload write data output throws io exception data bytes including vint encoding writable utils write v int size payload size writable utils get v int size size payload long size byte size else payload compare to gridmix record return compare seed seed fixed bytes min vint size return fixed bytes mask seed sd sz don use fixed bytes subclasses set intended random len sz fixed bytes else sz long size byte size fixed bytes return sd compare seed j seed j size size math max get size fixed bytes seed len math min size j size fixed bytes j seed mask seed j seed seed len seed mask seed seed seed len cmplen math min size j size cmplen byte size return size j size boolean equals object null get class get class return false hash code return seed get size comparator extends writable comparator comparator comparator class extends writable comparable sub compare byte b byte b
941	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixSplit.java	unrelated	package org apache hadoop mapred gridmix gridmix split extends combine file split id n spec maps reduces input records output bytes output records max memory reduce bytes new reduce records new spec reduces id mod reduce output bytes new reduce output records new gridmix split gridmix split combine file split cfsplit maps id get id get map count get input records get output bytes get output records get reduce bytes get reduce records write data output throws io exception read fields data input throws io exception
942	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\InputStriper.java	pooling	package org apache hadoop mapred gridmix given link file pool obtain set files capable satisfying full set splits iterate source fill request input striper log log log factory get log input striper idx current start file status current list file status files new array list file status configuration conf new configuration input striper file pool input dir map bytes dominating proportion input bytes combine file split split for file pool input dir bytes n locs long array array list long sigh comparator entry string double host rank
943	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\IntermediateRecordFactory.java	unrelated	package org apache hadoop mapred gridmix factory passing reduce specification last record intermediate record factory extends record factory gridmix key spec spec record factory factory partition target records boolean done false acc records l boundary passed intermediate record factory target bytes target records boundary passed intermediate record factory record factory factory partition boolean next gridmix key key gridmix record val throws io exception get progress throws io exception close throws io exception
944	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobCreator.java	unrelated	package org apache hadoop mapred gridmix enum job creator loadjob sleepjob string gridmix job type gridmix job type string sleepjob random locations create gridmix simulated job done gridmix job create gridmix job job creator get policy distributed cache load boolean emulate dist cache load distributed cache emulator dce this method called calling method job creator except emulate dist cache load especially emulate dist cache load returns true job type set dist cache emulator distributed cache emulator e
945	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobFactory.java	scheduler	package org apache hadoop mapred gridmix component reading job traces generated rumen each job trace assigned sequence number given submission time relative job preceded jobs enqueued job submitter provided construction job factory t implements gridmix component void stat listener t log log log factory get log job factory protected path scratch protected rate factor protected configuration conf protected thread r thread protected atomic integer sequence protected job submitter submitter protected count down latch start flag protected user resolver user resolver protected job creator job creator protected volatile io exception error null protected job story producer job producer protected reentrant lock lock new reentrant lock true protected num jobs in trace creating new instance start thread link org apache hadoop tools rumen zombie job producer job factory job submitter submitter input stream job trace submitter new zombie job producer job trace null scratch conf constructor permitting job story producer mocked protected job factory job submitter submitter job story producer job producer sequence new atomic integer scratch scratch rate factor conf get float gridmix gridmix sub mul f job producer job producer conf new configuration conf submitter submitter start flag start flag r thread create reader thread log debug enabled user resolver user resolver job creator job creator get policy conf job creator loadjob min task info extends task info min task info task info info get input bytes get input records get output bytes get output records get task memory protected filter job story implements job story protected job story job filter job story job story job job conf get job conf return job get job conf string get name return job get name job id get job id return job get job id string get user return job get user get submission time return job get submission time input split get input splits return job get input splits get number maps return job get number maps get number reduces return job get number reduces task info get task info task type task type task number task attempt info get task attempt info task type task type task number task attempt info get map task attempt info adjusted values get outcome string get queue name protected thread create reader thread gets next job trace bookkeeping job story get next job from trace throws io exception job story story job producer get next job story null return story protected job story get next job filtered throws io exception job story job get next job from trace job null return null job null new filter job story job obtain error caused thread exit unexpectedly io exception error return error add disabled add void ignored throw new unsupported operation exception get class get name start reader thread wait latch necessary start r thread start wait reader thread exhaust job trace join millis throws interrupted exception r thread join millis interrupt reader thread shutdown r thread interrupt interrupt reader thread this requires special consideration thread pending work queue abort currently special work r thread interrupt
946	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobMonitor.java	unrelated	package org apache hadoop mapred gridmix component accepting submitted running jobs responsible monitoring jobs success failure once job submitted polled status complete if job complete monitor thread returns immediately queue if monitor sleep duration job monitor implements gridmix component job log log log factory get log job monitor queue job jobs monitor thread thread blocking queue job running jobs poll delay millis statistics statistics boolean graceful false boolean shutdown false job monitor statistics statistics time unit seconds statistics create job monitor sleeps specified duration polling still running job job monitor poll delay time unit unit statistics statistics thread new monitor thread running jobs new linked blocking queue job jobs new linked list job poll delay millis time unit milliseconds convert poll delay unit statistics statistics add job polling queue add job job throws interrupted exception running jobs put job add submission failed job communicated back serial todo cleaner solution problem submission failed job job log info job submission failed notification job job get job id statistics add job temporary hook recording job success protected success job job log info job get job name job get job id success temporary hook recording job failure protected failure job job log info job get job name job get job id failure if shutdown jobs completed still running jobs may extracted component list job get remaining jobs thread alive synchronized jobs monitoring thread pulling running jobs component queue polled status monitor thread extends thread monitor thread check job success failure process job job throws io exception interrupted exception run start internal monitoring thread start thread start wait monitor halt assuming shutdown abort called note since submission may sporatic hang form shutdown requested join millis throws interrupted exception thread join millis drain submitted jobs queue stop monitoring thread upstream submitter assumed dead abort synchronized jobs thread interrupt when monitored jobs completed stop monitoring thread upstream submitter assumed dead shutdown synchronized jobs thread interrupt
947	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobSubmitter.java	pooling	package org apache hadoop mapred gridmix component accepting deserialized job traces computing split data submitting cluster deadline each job added upstream factory must submitted cluster deadline recorded once submitted jobs must added downstream component monitoring job submitter implements gridmix component gridmix job log log log factory get log job submitter semaphore sem statistics statistics file pool input dir job monitor monitor executor service sched volatile boolean shutdown false initialize submission component downstream monitor pool files split data may read see link gridmix gridmix sub thr see link gridmix gridmix que dep synthetic jobs job submitter job monitor monitor threads queue depth sem new semaphore queue depth sched new thread pool executor threads threads l input dir input dir monitor monitor statistics statistics runnable wrapping job submitted cluster submit task implements runnable gridmix job job submit task gridmix job job run enqueue job submitted per deadline associated add gridmix job job throws interrupted exception boolean add to queue shutdown add to queue re scan set input files splits derived refresh file pool throws io exception input dir refresh does nothing threadpool already initialized waiting work upstream factory start continue running queued jobs submitted cluster join millis throws interrupted exception shutdown sched await termination millis time unit milliseconds finish jobs pending submission accept new work shutdown complete pending tasks accept new tasks shutdown true sched shutdown discard pending work including precomputed work waiting submitted abort pending jobs clear shutdown true sched shutdown now
948	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\LoadJob.java	pooling	package org apache hadoop mapred gridmix synthetic job generated trace description load job extends gridmix job log log log factory get log load job load job configuration conf submission millis super conf submission millis jobdesc root ugi seq job call throws io exception interrupted exception ugi as return job protected boolean emulate compression return true this progress based resource usage matcher resource usage matcher runner extends thread resource usage matcher matcher progressive progress sleep time string sleep config default sleep time ms resource usage matcher runner task input output context context protected match throws exception run makes sure task tracker kill map reduce tasks emulating status reporter extends thread task attempt context context status reporter task attempt context context run load mapper extends mapper null writable gridmix record gridmix key gridmix record acc ratio array list record factory reduces random r new random gridmix key key new gridmix key gridmix record val new gridmix record resource usage matcher runner matcher null status reporter reporter null protected setup context ctxt throws io exception interrupted exception map null writable ignored gridmix record rec cleanup context context throws io exception interrupted exception load reducer extends reducer gridmix key gridmix record null writable gridmix record random r new random gridmix record val new gridmix record acc ratio record factory factory resource usage matcher runner matcher null status reporter reporter null protected setup context context throws io exception interrupted exception protected reduce gridmix key key iterable gridmix record values throws io exception interrupted exception protected cleanup context context throws io exception interrupted exception load record reader extends record reader null writable gridmix record record factory factory random r new random gridmix record val new gridmix record load record reader initialize input split generic split task attempt context ctxt throws io exception interrupted exception boolean next key value throws io exception get progress throws io exception null writable get current key gridmix record get current value close throws io exception load input format extends input format null writable gridmix record list input split get splits job context job ctxt throws io exception record reader null writable gridmix record create record reader build splits file pool input dir throws io exception map input bytes total l map output bytes total l map output records total l job story jobdesc get job desc null jobdesc maps jobdesc get number maps reds jobdesc get number reduces maps reduce record ratio new reds reduce byte ratio new reds reds input striper striper new input striper input dir map input bytes total list input split splits new array list input split maps push description id splits
949	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\LoadSplit.java	unrelated	package org apache hadoop mapred gridmix load split extends combine file split id n spec maps reduces input records output bytes output records max memory reduce bytes new reduce records new spec reduces id mod reduce output bytes new reduce output records new resource usage metrics map metrics resource usage metrics reduce metrics load split load split combine file split cfsplit maps id input bytes throws io exception get id get map count get input records get output bytes get output records get reduce bytes get reduce records resource usage metrics get map resource usage metrics resource usage metrics get reduce resource usage metrics write data output throws io exception read fields data input throws io exception
950	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Progressive.java	unrelated	package org apache hadoop mapred gridmix used track progress tasks progressive get progress
951	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\PseudoLocalFs.java	unrelated	package org apache hadoop mapred gridmix pseudo local file system generates random data file fly instead storing files disk so opening file multiple times give file content there directories file system root files root e all file ur is pseudo local file system format code pseudo lt name gt lt file size gt code name unique name lt file size gt number representing size file bytes pseudo local fs extends file system path home the creation time modification time files link pseudo local fs time system current time millis string home dir block size l mb default buffer size mb uri name uri create pseudo pseudo local fs new path home dir pseudo local fs path home super home home uri get uri return name path get home directory return home path get working directory return get home directory generates valid pseudo local file path given code file id code code file size code path generate file path string file id file size return new path file id file size creating pseudo local file nothing validating file path actual data file generated fly client tries open file reading fs data output stream create path path throws io exception try catch file not found exception e return null validate path provided expected format pseudo local file system based files validate file name format path path throws file not found exception path path make qualified boolean valid true file size path uri get scheme equals get uri get scheme else valid return file size fs data input stream open path path buffer size throws io exception file size validate file name format path input stream new random input stream file size buffer size return new fs data input stream fs data input stream open path path throws io exception return open path default buffer size file status get file status path path throws io exception file size validate file name format path return new file status file size false block size time path boolean exists path path try catch file not found exception e return true fs data output stream create path path fs permission permission return create path file status list status path path throws file not found exception return new file status get file status path input stream generates specified number random bytes random input stream extends input stream random r new random bytes writable val null position in val current position buffer val total size total number random bytes generated cur pos current position stream code buffer size code created if code buffer size code positive number default value mb used random input stream size buffer size read throws io exception read byte bytes throws io exception read byte bytes len throws io exception available read position byte buffer offset length read fully position byte buffer throws io exception read fully position byte buffer offset length get current position stream pseudo file get pos throws io exception seek pos throws io exception boolean seek to new source target pos throws io exception fs data output stream append path path buffer size
952	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RandomAlgorithms.java	pooling	package org apache hadoop mapred gridmix random algorithms random algorithms index mapper get pos swap b get size reset a sparse index mapping table useful want non destructively permute small fraction large array sparse index mapper implements index mapper map integer integer mapping new hash map integer integer size sparse index mapper size get pos swap b get size reset a dense index mapping table useful want non destructively permute large fraction array dense index mapper implements index mapper mapping dense index mapper size get pos swap b get size reset iteratively pick random numbers pool n each number picked selector index mapper mapping n random rand constructor the pool integers n percentage selected numbers this hint internal memory optimization random number generator selector n sel pcnt random rand select next random number next get remaining random number pool size get pool size reset selector reuse usage reset selecting random integers n select n random rand n selector selector new selector n n rand selected new return selected
953	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RandomTextDataGenerator.java	unrelated	package org apache hadoop mapred gridmix a random text generator the words simply sequences alphabets random text data generator log log log factory get log random text data generator configuration key random text data generator list size string gridmix datagen randomtext listsize configuration key random text data generator word size string gridmix datagen randomtext wordsize default random text data generator list size default list size default random text data generator word size default word size default random text data generator seed default seed l a list random words string words random random constructor link random text data generator default seed random text data generator size word size constructor link random text data generator random text data generator size long seed word size get configured random text data generator list size get random text data generator list size configuration conf set random text data generator list size set random text data generator list size configuration conf get configured random text data generator word size get random text data generator word size configuration conf set random text data generator word size set random text data generator word size configuration conf returns randomly selected word list random words string get random word this mainly testing list string get random words
954	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ReadRecordFactory.java	unrelated	package org apache hadoop mapred gridmix for every record consumed read key val bytes stream provided read record factory extends record factory size internal scratch buffer read internal stream string gridmix read buf size gridmix read buffer size byte buf input stream src record factory factory read record factory target bytes target records read record factory record factory factory input stream src boolean next gridmix key key gridmix record val throws io exception get progress throws io exception close throws io exception
955	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RecordFactory.java	unrelated	package org apache hadoop mapred gridmix interface producing records inputs outputs tasks record factory implements closeable transform given record perform operation boolean next gridmix key key gridmix record val estimate exhausted record capacity get progress throws io exception
956	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ReplayJobFactory.java	unrelated	package org apache hadoop mapred gridmix replay job factory extends job factory statistics cluster stats log log log factory get log replay job factory creating new instance start thread link org apache hadoop tools rumen zombie job producer replay job factory thread create reader thread update statistics cluster stats item replay reader thread extends thread start
957	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RoundRobinUserResolver.java	unrelated	package org apache hadoop mapred gridmix round robin user resolver implements user resolver log log log factory get log round robin user resolver uidx list user group information users collections empty list mapping user names original cluster ug is proxy users simulated cluster hash map string user group information usercache userlist assumes one user per line each line users list file form lt username gt group br group names ignored parsed list user group information parse user list uri user uri throws io exception synchronized boolean set target users uri userloc configuration conf throws io exception string build empty users error msg uri userloc synchronized user group information get target ugi inherit doc p link round robin user resolver needs map users trace provided list target users so user list needed boolean needs target users list
958	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SerialJobFactory.java	unrelated	package org apache hadoop mapred gridmix serial job factory extends job factory job stats log log log factory get log serial job factory condition job completed lock new condition creating new instance start thread link org apache hadoop tools rumen zombie job producer serial job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver resolver throws io exception super submitter job producer scratch conf start flag resolver thread create reader thread return new serial reader thread serial job factory serial reader thread extends thread serial reader thread string thread name serial in scenario method waits notification submitted job actually completed logic simple true wait till previousjob completed break submit new job previous job new job run serial once get notification stats collector job completion simply notify waiting thread update statistics job stats item simply notify case serial submissions we bothered submitted job completed lock lock try finally start reader thread wait latch necessary start log info starting serial submission r thread start
959	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SleepJob.java	pooling	package org apache hadoop mapred gridmix sleep job extends gridmix job log log log factory get log sleep job thread local random rand new thread local random string sleepjob maptask only gridmix sleep maptask boolean map tasks only fake locations string hosts selector selector interval report progress seconds string gridmix sleep interval gridmix sleep interval string gridmix sleep max map time gridmix sleep max map time string gridmix sleep max reduce time gridmix sleep max reduce time map max sleep time reduce max sleep time sleep job configuration conf submission millis job story jobdesc super conf submission millis jobdesc root ugi seq fake locations num locations hosts hosts selector fake locations new selector hosts length fake locations map tasks only conf get boolean sleepjob maptask only false map max sleep time conf get long gridmix sleep max map time long max value reduce max sleep time conf get long gridmix sleep max reduce time protected boolean emulate compression return false job call throws io exception interrupted exception class not found exception ugi as return job sleep mapper extends mapper long writable long writable gridmix key null writable map long writable key long writable value context context throws io exception interrupted exception cleanup context context throws io exception interrupted exception sleep reducer extends reducer gridmix key null writable null writable null writable duration l protected setup context context throws io exception interrupted exception protected cleanup context context throws io exception interrupted exception sleep input format extends input format long writable long writable list input split get splits job context job ctxt throws io exception record reader long writable long writable create record reader sleep split extends input split implements writable id n spec n maps sleep duration reduce durations new string locations new string sleep split sleep split write data output throws io exception read fields data input throws io exception get length get id get num maps get reduce durations string get locations task attempt info get successful attempt info task type type task task attempt info ret true ret get run state task status state succeeded return ret build splits file pool input dir throws io exception list input split splits new array list input split reds map tasks only jobdesc get number reduces maps jobdesc get number maps maps push description id splits
960	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Statistics.java	unrelated	package org apache hadoop mapred gridmix component collecting stats required components make decisions single thread collector tries collec stats each thread poll updates certain datastructure currently cluster stats components interested datastructure need register stats collector notifies listeners statistics implements component job log log log factory get log statistics stat collector statistics new stat collector job client cluster list cluster status listeners list stat listener cluster stats cluster statlisteners new copy on write array list stat listener cluster stats list job status listeners list stat listener job stats job stat listeners new copy on write array list stat listener job stats list jobids noof maps job map integer job stats job maps new concurrent hash map integer job stats completed jobs in current interval jt polling interval volatile boolean shutdown false max job completed in interval string max jobs completed in poll interval key gridmix max jobs completed poll interval reentrant lock lock new reentrant lock condition job completed lock new condition count down latch start flag statistics configuration conf polling interval count down latch start flag throws io exception interrupted exception jt polling interval polling interval max job completed in interval conf get int start flag start flag add job stats job job job story jobdesc seq gridmix job get job seq id job seq maps jobdesc null else job stats stats new job stats maps job job maps put seq stats used job monitor add completed job add job job this thread notified initially jobmonitor incase data generation ignore getting input generated statistics alive job stats stat job maps remove gridmix job get job seq id job stat null return completed jobs in current interval check reached maximum level job completions completed jobs in current interval max job completed in interval todo we types listeners if listeners increase move map kind model add cluster stats observers stat listener cluster stats listener cluster statlisteners add listener add job stats listeners stat listener job stats listener job stat listeners add listener attempt start service start statistics start stat collector extends thread stat collector run update and notify cluster stats listeners wait service completes it assumed either link shutdown link abort requested join millis throws interrupted exception statistics join millis shutdown shutdown true job maps clear cluster statlisteners clear job stat listeners clear statistics interrupt abort shutdown true job maps clear cluster statlisteners clear job stat listeners clear statistics interrupt class encapsulate job stats information current need information completed job todo in future need extend send information job stats of maps job job job stats of maps job job get no of maps returns job we use job get job id returns null xx use gridmix job get job seq id job instead job get job cluster stats cluster status status null cluster stats stats new cluster stats cluster stats cluster stats get cluster stats set cluster metric cluster status metrics cluster status get status get num running job collection job stats get running job stats
961	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\StatListener.java	unrelated	package org apache hadoop mapred gridmix stat listener stat listener t update t item
962	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\StressJobFactory.java	unrelated	package org apache hadoop mapred gridmix stress job factory extends job factory statistics cluster stats log log log factory get log stress job factory load status load status new load status condition cond underloaded lock new condition the minimum ratio pending running map tasks aka incomplete map tasks cluster map slot capacity us consider cluster overloaded for running maps count partially namely completed map counted map tasks calculation overload maptask mapslot ratio f string conf overload maptask mapslot ratio overload map task map slot ratio the minimum ratio pending running reduce tasks aka incomplete reduce tasks cluster reduce slot capacity us consider cluster overloaded for running reduces count partially namely completed reduce counted reduce tasks calculation overload reducetask reduceslot ratio f string conf overload reducetask reduceslot ratio gridmix throttle reduces task slot ratio overload reduce task reduce slot ratio the maximum share cluster mapslot capacity counted toward job incomplete map tasks overload calculation max mapslot share per job f string conf max mapslot share per job gridmix throttle maps max slot share per job max map slot share per job the maximum share cluster reduceslot capacity counted toward job incomplete reduce tasks overload calculation max reduceslot share per job f string conf max reduceslot share per job gridmix throttle reducess max slot share per job max reduce slot share per job the ratio maximum number pending running jobs number task trackers max job tracker ratio f string conf max job tracker ratio gridmix throttle jobs tracker ratio max job tracker ratio creating new instance start thread link org apache hadoop tools rumen zombie job producer stress job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver resolver throws io exception super overload map task map slot ratio conf get float overload reduce task reduce slot ratio conf get float max map slot share per job conf get float max reduce slot share per job conf get float max job tracker ratio conf get float thread create reader thread return new stress reader thread stress job factory worker thread responsible reading descriptions assigning sequence numbers normalizing time stress reader thread extends thread stress reader thread string name stress submits job stress mode jt overloaded wait if overloaded get number slots available keep submitting jobs till total jobs sufficient load jt that submit sigma maps job slots available run stress once get notification stats collector collect clustermetrics update current load status new load status jt update statistics cluster stats item lock lock try finally calc effective incomplete map tasks map slot capacity max eff incomplete map tasks math max f map slot capacity map progress adjusted math max math min map progress f f return math min max eff incomplete map tasks calc effective incomplete reduce tasks reduce slot capacity max eff incomplete reduce tasks math max f reduce slot capacity reduce progress adjusted return math min max eff incomplete reduce tasks we try use light weight mechanism determine cluster load check load and get slots to backfill cluster stats stats cluster status
963	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SubmitterUserResolver.java	unrelated	package org apache hadoop mapred gridmix resolves ug is submitting user submitter user resolver implements user resolver log log log factory get log submitter user resolver user group information ugi null submitter user resolver throws io exception synchronized boolean set target users uri userdesc configuration conf synchronized user group information get target ugi inherit doc p since link submitter user resolver returns user name running gridmix need target list users boolean needs target users list
964	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Summarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes various aspects link gridmix run summarizer execution summarizer execution summarizer cluster summarizer cluster summarizer protected string na n a summarizer summarizer string args execution summarizer get execution summarizer cluster summarizer get cluster summarizer start configuration conf this finalizes summarizer finalize job factory factory string path size throws io exception summarizes current link gridmix run cluster used string string
965	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\UserResolver.java	unrelated	package org apache hadoop mapred gridmix maps users trace set valid target users test cluster user resolver configure user map given uri configuration the resolver contract define resource interpreted default typically interpret uri link org apache hadoop fs path listing target users this method called link needs target users list returns true subclass contract target users boolean set target users uri userdesc configuration conf throws io exception map given ugi another per subclass contract user group information get target ugi user group information ugi indicates whether user resolver needs list target users provided user resolver boolean needs target users list
966	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\CumulativeCpuUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p a link resource usage emulator plugin emulates cumulative cpu usage performing certain cpu intensive operations performing cpu intensive operations essentially uses cpu every link resource usage emulator plugin configured feedback module e link resource calculator plugin monitor resource usage p p link cumulative cpu usage emulator plugin emulates cpu usage steps the frequency emulation configured via link cpu emulation progress interval cpu usage values matched via emulation interval boundaries p link cumulative cpu usage emulator plugin wrapper program managing cpu usage emulation feature it internally uses emulation algorithm called core described using link cpu usage emulator core performing actual emulation multiple calls core engine use amount cpu br p link cumulative cpu usage emulator plugin provides calibration feature via link initialize configuration resource usage metrics resource calculator plugin progressive calibrate plugin core underlying hardware as result calibration every call emulation engine core roughly use total usage value emulated this makes sure underlying hardware profiled use plugin accidently overuse cpu with unit emulation target value core engine roughly calls engine resulting roughly calls feedback resource usage monitor module excessive usage feedback module discouraged might result excess cpu usage resulting real cpu emulation p cumulative cpu usage emulator plugin implements resource usage emulator plugin protected cpu usage emulator core emulator core resource calculator plugin monitor progressive progress boolean enabled true emulation interval emulation interval target cpu usage last seen progress last seen cpu usage cpu usage configuration parameters string cpu emulation progress interval gridmix emulators resource usage cpu emulation interval default emulation frequency f times this core cpu usage emulation algorithm this core engine actually performs cpu intensive operations consume amount cpu multiple calls link compute help plugin emulate desired level cpu usage this core engine calibrated using link calibrate resource calculator plugin api suit underlying hardware better it also used optimize emulation cycle cpu usage emulator core performs computation use cpu compute allows core calibrate calibrate resource calculator plugin monitor this core engine emulate cpu usage the responsibility perform certain math intensive operations make sure desired value cpu used default cpu usage emulator implements cpu usage emulator core number times loop performing basic unit computation num iterations random random this fool jvm make think need value stored unit computation e link compute this prevent jvm optimizing code protected return value initialized link default cpu usage emulator default values note link default cpu usage emulator calibrated see link calibrate resource calculator plugin initialized using constructor default cpu usage emulator default cpu usage emulator num iterations this consume desired level cpu this api try use x percent target cumulative cpu usage currently x set compute perform unit computation the complete cpu emulation based multiple invocations unit computation module protected perform unit computation this calibrate algorithm single invocation link compute emulates roughly total desired resource usage value calibrate resource calculator plugin monitor cumulative cpu usage emulator plugin new default cpu usage emulator for testing cumulative cpu usage emulator plugin cpu usage emulator core core emulator core core note weighing function uses current progress in future might depend
967	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\ResourceUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p each resource emulated corresponding implementation implements link resource usage emulator plugin p br br link resource usage emulator plugin configured using link initialize configuration resource usage metrics resource calculator plugin progressive call every link resource usage emulator plugin also configured feedback module e link resource calculator plugin monitor current resource usage link resource usage metrics decides resource usage value emulate link progressive keeps track task progress p br br for configuring grid mix load use resource usage emulator see link resource usage matcher resource usage emulator plugin initialize configuration conf resource usage metrics metrics emulate throws io exception interrupted exception
968	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\ResourceUsageMatcher.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p this driver managing resource usage emulators link resource usage matcher expects comma separated list link resource usage emulator plugin implementations specified using link resource usage emulation plugins configuration parameter p p note order emulators invoked order configured resource usage matcher string resource usage emulation plugins list resource usage emulator plugin emulation plugins configure configuration conf resource calculator plugin monitor match resource usage throws exception
969	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\TotalHeapUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p a link resource usage emulator plugin emulates total heap usage loading jvm heap memory adding smaller chunks data heap essentially use heap space thus forcing jvm expand heap thus resulting increase heap usage p p link total heap usage emulator plugin emulates heap usage steps the frequency emulation configured via link heap emulation progress interval heap usage values matched via emulation specific interval boundaries p link total heap usage emulator plugin wrapper program managing heap usage emulation feature it internally uses emulation algorithm called core described using link heap usage emulator core performing actual emulation multiple calls core engine use amount heap total heap usage emulator plugin implements resource usage emulator plugin configuration parameters core engine emulate heap usage protected heap usage emulator core emulator core progress bar progressive progress decides plugin emulate heap usage boolean enabled true progress boundaries interval emulation done emulation interval target heap usage emulate target heap usage in mb the frequency based task progress memory emulation code run if value set emulation happen task progress the default value parameter link default emulation progress interval string heap emulation progress interval gridmix emulators resource usage heap emulation interval default value emulation interval default emulation progress interval f prev emulation progress f the minimum buffer reserved non emulation activities string min heap free ratio gridmix emulators resource usage heap min free ratio min free heap ratio default min free heap ratio f determines unit increase per call core engine load api this expressed percentage difference expected total heap usage current usage string heap load ratio gridmix emulators resource usage heap load ratio heap load ratio default heap load ratio f one mb defines core heap usage emulation algorithm this engine expected perform certain memory intensive operations consume amount heap link load load current heap increase heap usage specified value this core engine initialized using link initialize resource calculator plugin api suit underlying hardware better heap usage emulator core performs memory intensive operations use heap load size in mb initialize core initialize resource calculator plugin monitor reset resource usage reset this core engine emulate heap usage the responsibility perform certain memory intensive operations make sure desired value heap used default heap usage emulator implements heap usage emulator core store unit loads list protected array list object heap space new array list object increase heap usage current process given amount this done creating objects size mb load size in mb this initialize core check core emulate desired target underlying hardware initialize resource calculator plugin monitor clear references grid mix allocated special objects heap usage reduced reset total heap usage emulator plugin new default heap usage emulator for testing total heap usage emulator plugin heap usage emulator core core emulator core core protected get total heap usage in mb return runtime get runtime total memory one mb protected get max heap usage in mb return runtime get runtime max memory one mb emulate throws io exception interrupted exception enabled initialize configuration conf resource usage metrics metrics get target heap usage target heap usage in
970	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\HashingDistributionPolicy.java	unrelated	package org apache hadoop contrib index example choose shard insert delete based document id hashing do not use distribution policy number shards changes hashing distribution policy implements i distribution policy num shards non javadoc init shard shards non javadoc choose shard for insert document id key non javadoc choose shard for delete document id key
971	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\IdentityLocalAnalysis.java	unrelated	package org apache hadoop contrib index example identity local analysis maps inputs directly outputs identity local analysis implements non javadoc map document id key document and op value non javadoc configure job conf job non javadoc close throws io exception
972	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocInputFormat.java	unrelated	package org apache hadoop contrib index example an input format line doc plain text files line doc line doc input format extends non javadoc record reader document id line doc text and op get record reader
973	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocLocalAnalysis.java	unrelated	package org apache hadoop contrib index example convert line doc text and op document and op required i local analysis line doc local analysis implements string docid field name id string content field name content non javadoc map document id key line doc text and op value non javadoc configure job conf job non javadoc close throws io exception
974	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocRecordReader.java	unrelated	package org apache hadoop contrib index example a simple record reader line doc plain text files line doc each line follows document id space op space content eof op ins insert insert del delete delete u upd update update line doc record reader implements record reader document id line doc text and op char space char eol n start pos end buffered input stream byte array output stream buffer new byte array output stream provide bridge get bytes byte array output stream without creating new byte array text stuffer extends output stream text target write b write byte data offset len throws io exception text stuffer bridge new text stuffer constructor line doc record reader configuration job file split split start split get start end start split get length path file split get path open file seek start split file system fs file get file system job fs data input stream file in fs open split get path input stream file in boolean skip first line false start new buffered input stream skip first line skip first line establish start start start pos start end end non javadoc close throws io exception close non javadoc document id create key return new document id non javadoc line doc text and op create value return new line doc text and op non javadoc get pos throws io exception return pos non javadoc get progress throws io exception start end else non javadoc synchronized boolean next document id key line doc text and op value pos end key document id bytes first space read into key get text space read operation u ins del upd insert delete update text op text new text read into op text space string op str op text string document and op op op op str equals op str equals ins op str equals insert else op str equals op str equals del else op str equals u op str equals upd else value set op op op document and op op delete else boolean read into text text char delimiter throws io exception buffer reset bytes read read data buffer delimiter bytes read pos bytes read bridge target text buffer write to bridge return true read data input stream output stream char delimiter bytes true return bytes
975	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocTextAndOp.java	unrelated	package org apache hadoop contrib index example this represents operation the operation insert delete update if operation insert update new document form text specified line doc text and op implements writable document and op op op text doc line doc text and op set op document and op op op document and op op get op text get text non javadoc string string non javadoc write data output throws io exception non javadoc read fields data input throws io exception
976	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\RoundRobinDistributionPolicy.java	unrelated	package org apache hadoop contrib index example choose shard insert round robin fashion choose shards delete know stored round robin distribution policy implements i distribution policy num shards rr round robin implementation non javadoc init shard shards non javadoc choose shard for insert document id key non javadoc choose shard for delete document id key
977	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\FileSystemDirectory.java	unrelated	package org apache hadoop contrib index lucene this implements lucene directory top general file system currently support locking file system directory extends directory file system fs path directory io file buffer size file system directory file system fs path directory boolean create create throws io exception non javadoc string list throws io exception non javadoc boolean file exists string name throws io exception non javadoc file modified string name non javadoc touch file string name non javadoc file length string name throws io exception non javadoc delete file string name throws io exception non javadoc rename file string string throws io exception non javadoc index output create output string name throws io exception non javadoc index input open input string name throws io exception non javadoc index input open input string name buffer size throws io exception non javadoc lock make lock string name non javadoc close throws io exception non javadoc string string file system index input extends buffered index input file system index output extends buffered index output
978	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\LuceneIndexFileNameFilter.java	unrelated	package org apache hadoop contrib index lucene a wrapper convert index file name filter implements java io filename filter org apache hadoop fs path filter lucene index file name filter implements path filter lucene index file name filter singleton lucene index file name filter get filter index file name filter lucene filter lucene index file name filter non javadoc boolean accept path path
979	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\LuceneUtil.java	unrelated	package org apache hadoop contrib index lucene this copies methods lucene segment infos since lucene util index file names boolean segments file string name boolean segments gen file string name get current segment generation directory directory get current segment generation string files generation from segments file name string file name
980	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\MixedDeletionPolicy.java	unrelated	package org apache hadoop contrib index lucene for mixed directory use keep all deletion policy read directory keep init use keep only last commit deletion policy writable directory initially empty keep latest init mixed deletion policy implements index deletion policy keep all from init init list commits throws io exception commit list commits throws io exception
981	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\MixedDirectory.java	unrelated	package org apache hadoop contrib index lucene the initial version index stored read file system dir file system directory index files created newer versions written writable local fs dir lucene fs directory we use general file system directory writable dir well but use lucene fs directory currently lucene randome write file system directory supports sequential write note we may delete files read file system dir segment files uncommitted checkpoint for reason may create files writable dir already exist read dir logically overwrite ones read dir mixed directory extends directory directory read dir file system directory directory write dir lucene fs directory take advantage fact lucene fs directory file exists faster mixed directory file system read fs path read path file system write fs debugging mixed directory directory read dir directory write dir throws io exception string list throws io exception delete file string name throws io exception boolean file exists string name throws io exception file length string name throws io exception file modified string name throws io exception rename file string string throws io exception touch file string name throws io exception index output create output string name throws io exception index input open input string name throws io exception index input open input string name buffer size throws io exception close throws io exception string string
982	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\RAMDirectoryUtil.java	unrelated	package org apache hadoop contrib index lucene a utility writes index ram dir data output read data input index ram dir ram directory util buffer size ram output stream buffer size write ram files data output ram directory dir read ram files data input ram directory dir
983	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\ShardWriter.java	unrelated	package org apache hadoop contrib index lucene the initial version index stored perm dir index files created newer versions written temp dir local fs after successfully creating new version temp dir shard writer moves new files perm dir deletes temp dir close shard writer log log log factory get log shard writer file system fs file system local fs path perm path temp directory dir index writer writer max num segments num forms constructor shard writer file system fs shard shard string temp dir process intermediate form carrying lucene instance shard deletes inserts ram index form process intermediate form form throws io exception close shard writer optimize lucene instance shard closing necessary copy files created temp directory permanent directory closing close throws io exception non javadoc string string set parameters index update configuration iconf case previous reduce task fails restore generation original starting point deleting segments gen file segments n files whose generations greater starting generation rest unwanted files deleted unwanted segments n files deleted restore generation file system fs path perm start gen move files created temp dir perm dir delete temp dir local fs move from temp to perm throws io exception
984	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\main\UpdateIndex.java	unrelated	package org apache hadoop contrib index main a distributed index partitioned shards each shard corresponds lucene instance this contains main method uses map reduce job analyze documents update lucene instances parallel the main method update index requires following information updating shards input formatter this specifies format input documents analysis this defines analyzer use input the analyzer determines whether document inserted updated deleted for inserts updates analyzer also converts input document lucene document input paths this provides location updated documents e g hdfs files directories h base tables shard paths index path number shards either specify path shard specify index path shards sub directories index directory output path when update shard done message put number map tasks all information specified configuration file all first two also specified command line options check conf index config xml template configurable parameters note because parallel nature map reduce behaviour multiple inserts deletes updates document undefined update index log log log factory get log update index number format number format number format get instance print usage string cmd string get index path configuration conf get num shards configuration conf shard create shards string index path num shards main string argv
985	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\DocumentAndOp.java	unrelated	package org apache hadoop contrib index mapred this represents indexing operation the operation insert delete update if operation insert update new document must specified if operation delete update delete term must specified document and op implements writable op op op document doc term term document and op document and op op op document doc document and op op op term term document and op op op document doc term term set insert document doc set delete term term set update document doc term term op get op document get document term get term non javadoc string string non javadoc write data output throws io exception non javadoc read fields data input throws io exception
986	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\DocumentID.java	unrelated	package org apache hadoop contrib index mapred the represents document id type text document id implements writable comparable text doc id document id text get text non javadoc compare to object obj non javadoc hash code non javadoc string string non javadoc write data output throws io exception non javadoc read fields data input throws io exception
987	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IDistributionPolicy.java	unrelated	package org apache hadoop contrib index mapred a distribution policy decides given document document id one shard request sent request insert shard request sent request delete i distribution policy initialization it must called choose shard called init shard shards choose shard send insert request choose shard for insert document id key choose shard shards send delete request e g round robin distribution policy would send delete request shards represents shards choose shard for delete document id key
988	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IIndexUpdater.java	unrelated	package org apache hadoop contrib index mapred a implements index updater create map reduce job configuration run map reduce job analyze documents update lucene instances parallel i index updater run configuration conf path input paths path output path
989	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\ILocalAnalysis.java	unrelated	package org apache hadoop contrib index mapred application specific local analysis the output type must document id document and op i local analysis k extends writable comparable v extends writable extends mapper k v document id document and op
990	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateCombiner.java	unrelated	package org apache hadoop contrib index mapred this combiner combines multiple intermediate forms one intermediate form more specifically input intermediate forms single document ram index single delete term an output intermediate form contains multi document ram index multiple delete terms index update combiner extends map reduce base implements log log log factory get log index update combiner index update configuration iconf max size in bytes near max size in bytes non javadoc reduce shard key iterator intermediate form values intermediate form create form string message throws io exception close form intermediate form form string message non javadoc configure job conf job non javadoc close throws io exception
991	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateConfiguration.java	unrelated	package org apache hadoop contrib index mapred this provides getters setters number parameters most parameters related index update rest existing map reduce parameters index update configuration configuration conf index update configuration configuration conf configuration get configuration existing map reduce properties get io file buffer size return get int io file buffer size get io sort mb set io sort mb mb string get mapred temp dir properties index update class extends i distribution policy get distribution policy class set distribution policy class class extends analyzer get document analyzer class set document analyzer class class extends analyzer class class extends input format get index input format class set index input format class class extends input format class class extends i index updater get index updater class set index updater class class extends i index updater class class extends i local analysis get local analysis class set local analysis class class extends i local analysis class string get index shards set index shards string shards get index max field length set index max field length max field length get index max num segments set index max num segments max num segments boolean get index use compound file set index use compound file boolean use compound file get max ram size in bytes set max ram size in bytes b
992	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateMapper.java	unrelated	package org apache hadoop contrib index mapred this applies local analysis key value pair convert result docid operation pair shard intermediate form pair index update mapper k extends writable comparable v extends writable log log log factory get log index update mapper class extends writable comparable get map output key class class extends writable get map output value class index update configuration iconf analyzer analyzer shard shards i distribution policy distribution policy i local analysis k v local analysis document id tmp key document and op tmp value output collector document id document and op tmp collector map k key v value non javadoc configure job conf job non javadoc close throws io exception
993	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateOutputFormat.java	unrelated	package org apache hadoop contrib index mapred the record writer output format simply puts message output path shard update done index update output format extends file output format shard text non javadoc record writer shard text get record writer file system fs
994	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdatePartitioner.java	unrelated	package org apache hadoop contrib index mapred this partitioner puts values key case shard partition index update partitioner implements shard shards map shard integer map non javadoc get partition shard key intermediate form value num partitions non javadoc configure job conf job
995	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdater.java	unrelated	package org apache hadoop contrib index mapred an implementation index updater creates map reduce job configuration run map reduce job analyze documents update lucene instances parallel index updater implements i index updater log log log factory get log index updater index updater non javadoc run configuration conf path input paths path output path num map tasks shard shards throws io exception job conf job conf job client run job job conf job conf create job configuration conf path input paths path output path num map tasks shard shards throws io exception set starting generation shard reduce task fails new reduce task know start set shard generation conf shards iconf set sets properties conf index update configuration iconf new index update configuration conf shard set index shards iconf shards map task map output buffer uses job context io sort mb decide max buffer size max buffer size job context io sort mb here half en job context io sort mb use half memory build intermediate form index combiner iconf set io sort mb iconf get io sort mb create job configuration job conf job conf new job conf conf index updater job conf set job name get class get name provided application file input format set input paths job conf input paths file output format set output path job conf output path job conf set num map tasks num map tasks already set shards job conf set num reduce tasks shards length job conf set input format iconf get index input format class path inputs file input format get input paths job conf string builder buffer new string builder inputs string inputs length buffer append buffer append inputs string log info mapred input dir buffer string log info mapreduce output fileoutputformat outputdir log info mapreduce job maps job conf get num map tasks log info mapreduce job reduces job conf get num reduce tasks log info shards length shards iconf get index shards better create input format instance log info mapred input format set system job conf set map output key class index update mapper get map output key class job conf set map output value class index update mapper get map output value class job conf set output key class index update reducer get output key class job conf set output value class index update reducer get output value class job conf set mapper class index update mapper job conf set partitioner class index update partitioner job conf set combiner class index update combiner job conf set reducer class index update reducer job conf set output format index update output format return job conf set shard generation configuration conf shard shards throws io exception file system fs file system get conf shards length path path new path shards get directory generation fs exists path generation shards get generation
996	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateReducer.java	unrelated	package org apache hadoop contrib index mapred this reducer applies shard changes a new version shard created end reduce it important note new version shard derived scratch by leveraging lucene update algorithm new version lucene instance share many files possible previous version index update reducer extends map reduce base implements reducer shard intermediate form shard text log log log factory get log index update reducer text done new text done get reduce output key class extends writable comparable get output key class return shard get reduce output value class extends writable get output value class return text index update configuration iconf string mapred temp dir non javadoc reduce shard key iterator intermediate form values log info construct shard writer key file system fs file system get iconf get configuration string temp shard writer writer new shard writer fs key temp iconf update shard values next close shard reporter f reporter reporter new closeable close log info closed shard writer key writer writer output collect key done non javadoc configure job conf job iconf new index update configuration job mapred temp dir iconf get mapred temp dir mapred temp dir shard normalize path mapred temp dir non javadoc close throws io exception
997	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IntermediateForm.java	unrelated	package org apache hadoop contrib index mapred an intermediate form one parsed lucene documents delete terms it actually uses lucene file format format intermediate form using ram dir files note if process ever called close writer called otherwise need call close writer intermediate form implements writable index update configuration iconf null collection term delete list ram directory dir index writer writer num docs constructor intermediate form throws io exception configure using index update configuration configure index update configuration iconf get ram directory intermediate form directory get directory get iterator delete terms intermediate form iterator term delete term iterator this method used index update mapper process document operation current intermediate form process document and op doc analyzer analyzer throws io exception this method used index update combiner process intermediate form current intermediate form more specifically input intermediate forms single document ram index single delete term process intermediate form form throws io exception close lucene index writer associated intermediate form created do close ram directory in fact need close ram directory close writer throws io exception the total size files directory ram used index writer it memory used delete list total size in bytes throws io exception non javadoc string string index writer create writer throws io exception reset form throws io exception writable non javadoc write data output throws io exception non javadoc read fields data input throws io exception
998	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\Shard.java	unrelated	package org apache hadoop contrib index mapred this represents metadata shard version version number entire index directory directory shard resides generation lucene index generation version generation reserved future use note currently version number entire index used defaults shard implements writable comparable this method copied path string normalize path string path set index shards index update configuration conf shard get index shards index update configuration conf assume str formatted correctly shard shard create shard from string string str index shard version shards version index version number version string dir gen lucene generation shard shard version string dir gen shard shard shard get version string get directory get generation non javadoc string string writable non javadoc write data output throws io exception non javadoc read fields data input throws io exception comparable non javadoc compare to object compare to shard non javadoc boolean equals object non javadoc hash code
999	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\AllMapsCompletedTaskAction.java	unrelated	package org apache hadoop mapred this used notifying simulator task tracker running reduce task map tasks job done a simulator job tracker notifies simulator task tracker sending task tracker action response heartbeat represents directive start running user code reduce task we introduced extra push mechanism implement corresponding complicated pull part inter tracker protocol we use proper simulation events signaling hack heartbeat instead since job tracker emit events know recipient task tracker java object all maps completed task action extends task tracker action task attempt id reduce task proceed org apache hadoop mapreduce task attempt id task id all maps completed task action org apache hadoop mapreduce task attempt id get task id write data output throws io exception read fields data input throws io exception string string
1000	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\FakeConcurrentHashMap.java	unrelated	package org apache hadoop mapred a fake concurrent hash map implementation maintains insertion order entries traversing iterator the support deterministic replay mumak meant used concurrent hash map replacement multiple threads fake concurrent hash map k v extends concurrent hash map k v map k v map fake concurrent hash map v put if absent k key v value boolean remove object key object value v replace k key v value boolean replace k key v old value v new value clear boolean contains key object key boolean contains value object value set map entry k v entry set v get object key boolean empty set k key set v put k key v value put all map extends k extends v v remove object key size collection v values
1001	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\HeartbeatEvent.java	heartbeat	package org apache hadoop mapred this used link simulator task tracker signaling next hearbeat call job tracker due heartbeat event extends simulator event heartbeat event simulator event listener listener timestamp
1002	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\JobCompleteEvent.java	unrelated	package org apache hadoop mapred link job complete event created link simulator job tracker job completed link simulator job client picks event mark job completed when jobs completed simulation terminated job complete event extends simulator event simulator engine engine job status job status job complete event simulator job client jc timestamp simulator engine get engine job status get job status protected string real to string
1003	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\JobSubmissionEvent.java	unrelated	package org apache hadoop mapred link simulator event trigging submission job job tracker job submission event extends simulator event job story job job submission event simulator event listener listener timestamp job story get job protected string real to string
1004	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\LoadProbingEvent.java	unrelated	package org apache hadoop mapred link load probing event created link simulator job tracker link simulator job submission policy stress link simulator job client picks event would check whether system load stressed if would submit next job load probing event extends simulator event load probing event simulator job client jc timestamp
1005	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorClock.java	unrelated	package org apache hadoop mapred a clock mocked testing simulator clock extends clock current time simulator clock set time get time
1006	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorCSJobInitializationThread.java	scheduler	package org apache hadoop mapred simulator cs job initialization thread implements simulator event listener last called capacity task scheduler task scheduler job initialization poller job poller string queue sleep interval the log object send messages used debugging log log log factory get log simulator cs job initialization thread simulator cs job initialization thread task scheduler task scheduler list simulator event accept simulator event event throws io exception list simulator event init throws io exception
1007	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEngine.java	heartbeat	package org apache hadoop mapred link simulator engine main simulator to launch simulator user either run main directly two parameters input trace file corresponding topology file use script bin mumak sh trace json topology json trace file topology file produced rumen simulator engine extends configured implements tool list simulator event empty events new array list simulator event default number milliseconds required boot entire cluster default cluster startup duration protected simulator event queue queue new simulator event queue string trace file string topology file simulator job tracker jt simulator job client jc boolean shutdown false terminate time long max value current time the hash set storing simulated threads useful hash set simulator cs job initialization thread thread set the log object send messages used debugging log log log factory get log simulator engine master random seed start task trackers cluster story cluster job conf job conf get time property configuration conf string property name job conf create mumak conf init throws interrupted exception io exception init job conf job conf throws interrupted exception io exception start simulator threads cap sched throws io exception run throws io exception interrupted exception summary print stream main string args throws exception run string args throws exception parse parameters string args mark completed job job status job status timestamp shutdown get current time due hdfs node may appear job history logs numeric ips host names we remove parsed network topology feeding zombie cluster remove ip hosts logged network topology topology pattern ip pattern boolean ip address string hostname set static mapping logged network topology topology
1008	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEvent.java	unrelated	package org apache hadoop mapred link simulator event represents specific event mumak each link simulator event expected expiry time fired link simulator event listener handle link simulator event fired simulator event protected simulator event listener listener protected timestamp protected internal count protected simulator event simulator event listener listener timestamp listener listener timestamp timestamp get expected event expiry time get time stamp return timestamp get link simulator event listener handle link simulator event simulator event listener get listener return listener get internal counter link simulator event each link simulator event holds counter incremented every event order multiple events occur time get internal count return internal count set internal counter link simulator event set internal count count internal count count string string return get class get name real to string converts list fields values human readable format name override wanted new fields show string protected string real to string return timestamp timestamp listener listener
1009	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEventListener.java	unrelated	package org apache hadoop mapred interface entities handle events simulator event listener get initial events put event queue list simulator event init throws io exception process event generate events put event queue list simulator event accept simulator event event throws io exception
1010	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEventQueue.java	unrelated	package org apache hadoop mapred link simulator event queue maintains priority queue events scheduled future virtual time events happen virtual time order the link simulator event queue notion current time defined time stamp last event already handled an event inserted link simulator event queue time stamp must later current time simulator event queue list simulator event empty events new array list simulator event simulator event last event null event count priority queue simulator event events new priority queue simulator event get next earliest link simulator event handled this link simulator event smallest time stamp among link simulator event currently scheduled link simulator event queue simulator event get add single link simulator event link simulator event queue link simulator event convention collection add boolean add simulator event event adding link simulator event the container contains events added convention collection add all boolean add all collection extends simulator event events get current time queue it defined time stamp last event handled get current time get size currently scheduled events number events system major scaling factor simulator get size get total number events handled simulation this indicator large particular simulation run get event count
1011	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobCache.java	unrelated	package org apache hadoop mapred a link job id link job story mapping used link job client link job tracker job submission simulator job cache map job id job story submitted jobs new hash map job id job story put link job id link job story mapping put job id job id job story job get job identified link job id remove mapping job story get job id job id check job head queue without removing mapping job story peek job id job id
1012	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobClient.java	scheduler	package org apache hadoop mapred class simulates job client it main functionality submit jobs simulation engine shutdown simulation engine job producer runs jobs simulator job client implements simulator event listener protected job sketch info protected num maps protected num reduces job sketch info num maps num reduces client protocol job tracker job story producer job story producer simulator job submission policy submission policy load prob interval start load prob interval max load probing interval load prob interval start the minimum ratio pending running map tasks aka incomplete map tasks cluster map slot capacity us consider cluster overloaded for running maps count partially namely completed map counted map tasks calculation overlaod maptask mapslot ratio f keep track flight load probing event load probing event flight lpe null we handle simulator event queue thus cannot cancel events directly instead keep identity map identity set except jdk provide identity set skip events cancelled map load probing event boolean cancelled lpe new identity hash map load probing event boolean map job id job sketch info running jobs new linked hash map job id job sketch info boolean more jobs false job story next job constructor the job tracker submit job note link simulator job client interacts job tracker link client protocol simulator job client client protocol job tracker job tracker job tracker job story producer job story producer submission policy submission policy constructor the job tracker submit job note link simulator job client interacts job tracker link client protocol simulator job client client protocol job tracker job tracker job story producer simulator job submission policy replay list simulator event init throws io exception job story job job story producer get next job submission policy simulator job submission policy replay job submission event event new job submission event job submission policy simulator job submission policy stress else doing exponential back probing load probing could pretty expensive many pending jobs adjust load probing interval boolean overloaded overloaded else we try use light weight mechanism determine cluster load boolean overloaded throws io exception try catch interrupted exception e handles simulation event either job submission event job completion event list simulator event accept simulator event event throws io exception event instanceof job submission event else event instanceof job complete event else event instanceof load probing event else responds job submission event submitting job job tracker if serialize job submissions true postpones submission previous job finished instead list simulator event process job submission event submit job job status status null job story story submit event get job try catch interrupted exception e running jobs put status get job id new job sketch info story get number maps system println job status get job id submitted find next job submit next job job story producer get next job next job null else submission policy simulator job submission policy replay else submission policy simulator job submission policy stress return simulator engine empty events handles job completion event list simulator event process job complete event job status job status job complete event get job status system println job job status get job id completed
1013	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobInProgress.java	scheduler	package org apache hadoop mapred simulator job in progress extends job in progress log log log factory get log simulator job in progress job story contains information read cache job story job story task split meta info task split meta info simulator job in progress job id jobid string job submit dir job tracker jobtracker init tasks update information job story object synchronized init tasks throws io exception task split meta info create splits job story story throws io exception given map task attempt id returns task attempt info deconstructs map task attempt id looks job story parts task type id task id task attempt tasktracker task attempt synchronized task attempt info get map task attempt info get closest locality task tracker task tracker task split meta info split task attempt info get task attempt info task tracker task tracker given reduce task attempt id returns task attempt info deconstructs reduce task attempt id looks job story parts task type id task id task attempt tasktracker task attempt task attempt info get reduce task attempt info task tracker task tracker
1014	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobStory.java	unrelated	package org apache hadoop mapred this proxy job story zombie job customized submission time because simulation submission time totally produced simulator original submission time job trace ignored simulator job story implements job story job story job submission time simulator job story job story job time get submission time input split get input splits job conf get job conf task attempt info get map task attempt info adjusted task number string get name org apache hadoop mapreduce job id get job id get number maps get number reduces task attempt info get task attempt info task type task type task number task info get task info task type task type task number string get user pre job history constants values get outcome string get queue name
1015	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobStoryProducer.java	unrelated	package org apache hadoop mapred this creates link job story objects trace file rumen format it proxy link zombie job producer adjusts submission time aligned simulation time simulator job story producer implements job story producer zombie job producer producer first job start time relative time boolean first job true simulator job story producer path path zombie cluster cluster simulator job story producer path path zombie cluster cluster job story get next job filtered throws io exception job story get next job throws io exception close throws io exception
1016	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobSubmissionPolicy.java	unrelated	package org apache hadoop mapred job submission policies the set policies closed encapsulated link simulator job submission policy the handling submission policies embedded link simulator engine various events enum simulator job submission policy replay trace following job inter arrival rate faithfully replay ignore submission time keep submitting jobs cluster saturated stress submitting jobs sequentially serial string job submission policy mumak job submission policy simulator job submission policy get policy configuration conf
1017	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobTracker.java	heartbeat	package org apache hadoop mapred link simulator job tracker extends link job tracker it implements link inter tracker protocol protocols simulator job tracker extends job tracker a queue cleaning jobs memory the length queue always less constant specified jobs in mumak memory linked list job id cleanup queue the simulator clock maintains current simulation time always synchronized time maintained engine simulator clock clock null log log log factory get log simulator job tracker this constant used specify many jobs maintained memory mumak simulator jobs in mumak memory the simulator engine data structure engine drives simulator simulator engine engine null synchronized reset engine clock simulator engine engine simulator clock clock in addition standard job conf object constructor simulator job tracker requires start time simulation reference simulator engine object the clock job tracker set start time simulator job tracker job conf conf simulator clock clock starts job tracker given configuration given time it also starts job notifier thread initialize clock simulator job tracker start tracker job conf conf start time simulator engine engine throws io exception start simulator job tracker given configuration creating simulator engine pretty much used debugging simulator job tracker start tracker job conf conf start time throws io exception interrupted exception offer service throws interrupted exception io exception returns simulator clock object simulator job tracker clock get clock overriding get clean task reports function original job tracker since setup cleanup tasks synchronized task report get cleanup task reports job id jobid overriding since support queue acls queue acls info get queue acls for current user throws io exception overriding since simulate setup cleanup tasks synchronized task report get setup task reports job id jobid synchronized job status submit job throws io exception return simulator job object given job id simulator job in progress get simulator job job id jobid safely clean data structures end job success failure killed in addition performing tasks original finalize job also inform simulator engine completion job synchronized finalize job job in progress job the cleanup job method maintains queue clean queue when job finalized added cleanup queue jobs removed cleanup queue size maintained less specified jobs in mumak memory going added cleanup queue cleanup job job in progress job inter tracker protocol synchronized boolean process heartbeat task tracker status tracker status utility validate current simulation time validate and set clock new simulation time synchronized heartbeat response heartbeat task tracker status status the get map completion method intended inform task trackes change status reduce tasks shuffle reduce for reduce tasks task tracker shuffle phase get map completion tasks finds number finished maps job job in progress object if number equals number desired maps job adds all maps completed task action reduce task attempt the status task tracker list task tracker action get map completion tasks update task statuses task tracker status status
1018	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorLaunchTaskAction.java	unrelated	package org apache hadoop mapred this used augment link launch task action run time statistics task state successful failed simulator launch task action extends launch task action task attempt info task attempt info simulator launch task action task task get resource usage model task task attempt info get task attempt info string string
1019	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorTaskTracker.java	heartbeat	package org apache hadoop mapred explicitly use new api older mapred task attempt id deprecated this simulates link task tracker its main purpose call heartbeat simulated job tracker apropriately updated statuses tasks assigned the events emitted consumed heartbeat event task attempt completion event internal naming convention accept dispatches simulation events process event methods heartbeat dispactches task tracker actions handle action methods simulator task tracker implements simulator event listener default host name string default host name unknown default task tracker name string default tracker name default number map slots per task tracker default map slots default number reduce slots per task tracker default reduce slots default range heartbeat response perturbations milliseconds default heartbeat fuzz the name task tracker protected string task tracker name the name host task tracker running protected string host name the http port simulated task tracker reports jobtracker protected http port number map slots protected max map slots number reduce slots protected max reduce slots the job tracker task tracker slave protected inter tracker protocol job tracker protected map task attempt id simulator task in progress tasks used map slots used reduce slots boolean first heartbeat true last heartbeat response received short heartbeat response id set task attempt id orphan task completions the log object send messages used debugging log log log factory get log simulator task tracker heartbeat interval fuzz used randomly perturbing heartbeat timings random random simulator task tracker inter tracker protocol job tracker list simulator event accept simulator event event list simulator event init finish running task task status status list simulator event process task attempt completion event task attempt completion event create task attempt completion event list simulator event handle simulator launch task action list simulator event handle kill task action kill task action action list simulator event handle all maps completed task action progress task status simulator task in progress tip progress task statuses garbage collect completed tasks list task status collect and clone task statuses list simulator event handle heartbeat response heartbeat response response list simulator event process heartbeat event heartbeat event event simulator task in progress
1020	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorTaskTrackerStatus.java	heartbeat	package org apache hadoop mapred this exists pass current simulation time job tracker heartbeat call simulator task tracker status extends task tracker status current simulation time simulator task tracker status string tracker name string host get current simulation time
1021	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorThreadWakeUpEvent.java	unrelated	package org apache hadoop mapred simulator thread wake up event extends simulator event simulator thread wake up event simulator event listener listener
1022	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\TaskAttemptCompletionEvent.java	unrelated	package org apache hadoop mapred this used simulator task trackers signaling task attempt finishes the rationale redundant event sent way possible monitor task completion events centrally engine t ts used call heartbeat job tracker right task completed called crazy heartbeats waiting heartbeat interval if wanted simulate need decouple task completion monitoring periodic heartbeats task attempt completion event extends simulator event the status completed task task status status task attempt completion event simulator event listener listener returns status task task status get status protected string real to string
1023	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\DistributedRaidFileSystem.java	unrelated	package org apache hadoop hdfs this implementation hadoop raid filesystem this file system wraps instance distributed file system if file corrupted file system uses parity blocks regenerate bad block distributed raid file system extends filter file system alternate locations used read access decode info alternates configuration conf stripe length distributed raid file system throws io exception distributed raid file system file system fs throws io exception super fs alternates null stripe length information required decoding source file decode info path dest path erasure code type type configuration conf stripe length decode info configuration conf erasure code type type path dest path decoder create decoder initialize raid file system initialize uri name configuration conf throws io exception conf conf class clazz conf get class fs raid underlyingfs impl clazz null fs file system reflection utils new instance clazz null super initialize name conf find stripe length configured stripe length raid node get stripe length conf stripe length put xor rs alternates alternates new decode info path xor path raid node xor destination path conf fs alternates new decode info conf erasure code type xor xor path path rs path raid node rs destination path conf fs alternates new decode info conf erasure code type rs rs path returns underlying filesystem file system get file system throws io exception return fs fs data input stream open path f buffer size throws io exception ext fs data input stream fd new ext fs data input stream conf alternates f return fd close throws io exception fs null super close layered filesystem input stream this input stream tries reading alternate locations encoumters read errors primary location ext fs data input stream extends fs data input stream underlying block create input stream wraps reads positions seeking ext fs input stream extends fs input stream constructor ext input stream ext fs data input stream configuration conf distributed raid file system lfs
1024	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\RaidDFSUtil.java	unrelated	package org apache hadoop hdfs raid dfs util returns corrupt blocks file list located block corrupt blocks in file throws io exception located blocks get block locations make successive calls list corrupt files obtain corrupt files string get corrupt files distributed file system dfs
1025	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicyRaid.java	unrelated	package org apache hadoop hdfs server blockmanagement this block placement policy spreads group blocks used raid recovering this important availability blocks this used multiple threads it thread safe block placement policy raid extends block placement policy log log configuration conf stripe length xor parity length rs parity length string xor prefix null string rs prefix null string raid temp prefix null string raidrs temp prefix null string raid har temp prefix null string raidrs har temp prefix null fs namesystem namesystem null block placement policy default default policy cached located blocks cached located blocks cached full path names cached full path names inherit doc initialize configuration conf fs cluster stats stats datanode descriptor choose target string src path num of replicas datanode descriptor choose target string src path num of replicas verify block placement string src path located block blk inherit doc datanode descriptor choose replica to delete fs inode info inode add excluded nodes string file file type type hash map node node excluded datanode descriptor choose replica to delete map string integer count companion blocks node comparator implements comparator datanode descriptor list located block get companion blocks string path file type type list located block get companion blocks for har parity block list located block get companion blocks for parity block list located block get companion blocks for source block get block index string file block block throws io exception cached full path names cached located blocks extends cache string list located block cache k v string get source file string parity string prefix throws io exception string get parity file string src throws io exception string get parity file string parity prefix string src boolean har file string path enum file type file type get file type string path throws io exception
1026	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\datanode\RaidBlockSender.java	unrelated	package org apache hadoop hdfs server datanode reads block disk sends recipient raid block sender implements java io closeable fs constants log log data node log log client trace log data node client trace log extended block block block read the visible length replica replica visible length input stream block in data stream block in position updated using transfer to data input stream checksum in checksum datastream data checksum checksum checksum stream offset starting position read end offset ending position bytes per checksum chunk size checksum size checksum size boolean corrupt checksum ok need verify checksum boolean chunk offset ok need send chunk offset seqno sequence number packet boolean transfer to allowed true boolean block read fully set whole block read boolean verify checksum true check verified reading string client trace fmt format client trace log message minimum buffer used sending data clients used transfer to enabled kb large it could larger sure much improvement min buffer with transferto volatile chunk checksum last chunk checksum null raid block sender extended block block block length start offset length block block length start offset length raid block sender extended block block block length start offset length try catch io exception ioe close opened files close throws io exception io exception ioe null close checksum file checksum in null close data file block in null throw io exception ioe null converts io excpetion subclasses socket exception this typically done indicate upper layers error socket error rather often serious exceptions like disk errors io exception ioe to socket exception io exception ioe ioe get class equals io exception otherwise return exception return ioe sends upto max chunks chunks data when block in position assumes link socket output stream tries link socket output stream transfer to fully file channel send data updates block in position send chunks byte buffer pkt max chunks output stream sends multiple chunks one packet single write len math min end offset offset num chunks len bytes per checksum bytes per checksum packet len len num chunks checksum size boolean last data packet offset len end offset len pkt clear packet header header new packet header header put in buffer pkt checksum off pkt position checksum len num chunks checksum size byte buf pkt array checksum size checksum in null data off checksum off checksum len block in position try catch io exception e return len send block used read block metadata stream data either client another datanode wrapper stream this enables optimizations sending data e g link socket output stream transfer to fully file channel send block data output stream output stream base stream null initial offset offset total read output stream stream for send chunks start time client trace log info enabled system nano time try finally block read fully initial offset offset replica visible length return total read boolean block read fully return block read fully input stream factory input stream create stream offset throws io exception block input stream factory implements input stream factory extended block block fs dataset interface data block input stream factory extended block block fs
1027	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeRaidUtil.java	unrelated	package org apache hadoop hdfs server namenode utilities used raid accessing name node name node raid util accessing fs directory get file info hdfs file status get file info fs directory dir accessing fs namesystem get file info hdfs file status get file info fs namesystem namesystem accessing fs namesystem get block locations located blocks get block locations fs namesystem namesystem
1028	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\BlockFixer.java	unrelated	package org apache hadoop raid contains core functionality block fixer configuration options raid blockfix classname name block fixer implementation use raid blockfix interval interval checks corrupt files raid blockfix history interval interval fixing file raid blockfix read timeout read time raid blockfix write timeout write time block fixer extends configured implements runnable string blockfix classname raid blockfix classname string blockfix interval raid blockfix interval string blockfix history interval string blockfix read timeout string blockfix write timeout default blockfix interval min default blockfix history interval block fixer create block fixer configuration conf num files fixed volatile boolean running true interval checks corrupt files protected block fix interval interval fixing file protected history interval block fixer configuration conf run synchronized files fixed protected synchronized incr files fixed protected synchronized incr files fixed incr boolean source file path p string dest prefixes filter unfixable source files iterator path throws io exception block fixer helper extends configured
1029	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ConfigManager.java	unrelated	package org apache hadoop raid maintains configuration xml file read memory config manager log log log factory get log org apache hadoop raid config manager time wait checks config file reload interval time wait successive runs policies rescan interval har partfile size distraid max jobs distraid max files time wait config file modified reloading done prevent loading file fully written reload wait configuration conf hadoop configuration string config file name path config xml file last reload attempt last time tried reload config file last successful reload last time successfully reloaded config boolean last reload attempt failed false reload interval reload interval periodicity time runs policies har partfile size max jobs per policy max jobs running simultaneously max files per job max files raided job reload configuration boolean reload thread reload thread volatile boolean running false collection configured policies collection policy list policies new array list policy list config manager configuration conf throws io exception sax exception conf conf config file name conf get raid config file reload conf get boolean raid config reload true reload interval conf get long raid config reload interval reload interval periodicity conf get long raid policy rescan interval rescan interval har partfile size conf get long raid har partfile size har partfile size max jobs per policy conf get int raid distraid max jobs max files per job conf get int raid distraid max files config file name null reload configs last successful reload raid node last reload attempt raid node running true reload config file loaded returns true file reloaded synchronized boolean reload configs if necessary time raid node time last reload attempt reload interval return false updates memory data structures config file this file expected following whitespace separated format configuration src path prefix hdfs hadoop myhost com user warehouse u full src path configuration blank lines lines starting ignored reload configs throws io exception parser configuration exception config file name null file file new file config file name file exists create temporary hashmaps hold new allocs save fields parsed entire allocs file successfully list policy list new array list policy list periodicity value periodicity read parse configuration file allow files configuration file document builder factory doc builder factory document builder factory new instance doc builder factory set ignoring comments true doc builder factory set namespace aware true try catch unsupported operation exception e log error reloading config file file document builder builder doc builder factory new document builder document doc builder parse file element root doc get document element configuration equals ignore case root get tag name node list elements root get child nodes map string policy info existing policies loop configured source paths elements get length done src paths set all policies periodicity periodicity value return synchronized get periodicity return periodicity synchronized get har partfile size return har partfile size synchronized get max jobs per policy return max jobs per policy synchronized get max files per job return max files per job get collection policies synchronized collection policy list get all policies return new array list policies set collection policies protected synchronized set all
1030	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\Decoder.java	unrelated	package org apache hadoop raid represents generic decoder used read file corrupt blocks using parity file this concrete subclasses need implement fix erased block decoder log log log factory get log protected configuration conf protected stripe size protected parity size protected random rand protected buf size protected byte read bufs protected byte write bufs decoder configuration conf stripe size parity size allocate buffers configure buffers block size the generate decoded file using good portion source file parity file different fs case parity file part har archive additional errors source file discovered decode process source file success decode file recovers corrupt block local file different fs case parity file part har archive additional errors source file discovered decode process this prevent writing beyond end file recover block to file implementation specific mechanism writing fixed block different fs case parity file part har archive additional errors source file discovered decode process skipped writing output this needed output may portion block written source file new corruption discovered block bytes to skip this prevent writing beyond end file protected fix erased block
1031	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DirectoryTraversal.java	pooling	package org apache hadoop raid implements depth first traversal using stack object the traversal stopped time state traversal saved directory traversal log log log factory get log org apache hadoop raid directory traversal file system fs list file status paths path idx next path process stack node stack new stack node executor service executor num threads a file filter object used choose files directory traversal file filter boolean check file status f throws io exception represents directory node directory traversal node file status path path node represents file status elements elements node idx node file status path file status elements boolean next file status next file status path constructor directory traversal file system fs list file status start paths fs start paths directory traversal file system fs list file status start paths num threads fs fs paths start paths path idx num threads num threads executor executors new fixed thread pool num threads list file status get filtered files file filter filter limit list file status filtered new array list file status we need semaphore block number running workitems equal number threads fixed thread pool limits number threads queue size this way limit memory usage semaphore slots new semaphore num threads true try catch interrupted exception ie return filtered filter file work item implements runnable file filter filter node dir list file status filtered semaphore slots filter file work item file filter filter node dir list file status filtered run return next file file status get next file throws io exception check traversal done done traversal return null gets next directory tree the algorithm returns deeper directories first file status get next directory throws io exception node dir node get next directory node dir node null return null node get next directory node throws io exception check traversal done done traversal return null push new node file status stat throws io exception stat dir path p stat get path file status elements fs list status p node new node new node stat elements null new file status elements stack push new node boolean done traversal return paths empty stack empty
1032	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistBlockFixer.java	scheduler	package org apache hadoop raid distributed block fixer uses map reduce jobs fix corrupt files configuration options raid blockfix filespertask number corrupt files fix single map reduce task e one mapper node raid blockfix fairscheduler pool pool use block fixer jobs raid blockfix maxpendingfiles maximum number files fix simultaneously dist block fixer extends block fixer volatile sufficient since block fixer thread updates num jobs running threads may read volatile num jobs running string work dir prefix blockfixer string in file suffix string part prefix part string blockfix files per task raid blockfix filespertask string blockfix max pending files raid blockfix maxpendingfiles string blockfix pool raid blockfix fairscheduler pool mapred fairscheduler pool used local configuration passed block fixing job string mapred pool mapred fairscheduler pool default number files fix task default blockfix files per task l default number files fix simultaneously default blockfix max pending files l protected log log log factory get log dist block fixer number files fix task files per task number files fix simultaneously max pending files number files fixed right pending files pool name use may null case special pool used string pool name last check time simple date format date format new simple date format yyyy mm dd hh mm ss map string corrupt file info file index new hash map string corrupt file info map job list corrupt file info job index new hash map job list corrupt file info enum counter files succeeded files failed files noaction dist block fixer configuration conf super conf files per task dist block fixer files per task get conf max pending files dist block fixer max pending files get conf pending files l pool name conf get blockfix pool start due first iteration last check time system current time millis block fix interval determines many files fix single task protected files per task configuration conf return conf get long blockfix files per task determines many files fix simultaneously protected max pending files configuration conf return conf get long blockfix max pending files runs block fixer periodically run running checks corrupt blocks fixes check and fix blocks start time throws io exception interrupted exception class not found exception check jobs pending files max pending files list path corrupt files get corrupt files filter unfixable source files corrupt files iterator string start time str date format format new date start time log info found corrupt files size corrupt files corrupt files size handle failed job fail job job job throws io exception assume files fixed log error dist block fixer job job get job id job get job name corrupt file info file info job index get job num jobs running handle successful job succeed job job job files succeeded files failed throws io exception log info dist block fixer job job get job id job get job name files failed else report succeeded files metrics incr files fixed files succeeded num jobs running checks jobs completed updates job file index returns list failed files restarting check jobs throws io exception iterator job job iter job index key set iterator job
1033	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistRaid.java	scheduler	package org apache hadoop raid dist raid extends configured protected log log log factory get log dist raid string name dist raid string job dir label name job dir op list block size block size control file short op list replication replication factor control file string ops per task raid distraid opspertask default ops per task sync file max simple date format date form new simple date format yyyy mm dd hh mm enum counter files succeeded files failed processed blocks processed size meta blocks meta size dist raid configuration conf super conf random random new random protected string get random id return integer string random next int integer max value helper holds policy paths raid policy path pair policy info policy list file status src paths raid policy path pair policy info policy list file status src paths list raid policy path pair raid policy path pair list new array list raid policy path pair job running job string last report null responsible generating splits src file list dist raid input format extends list input split get splits job context job throws io exception the mapper raiding files dist raid mapper extends mapper text policy info text text boolean ignore failures false failcount succeedcount statistics st new statistics string get count string run file operation map text key policy info policy context context inherit doc close throws io exception set options specified raid scheduleroption the formatted key value key value set scheduler option configuration conf string scheduler option conf get raid scheduleroption scheduler option null creates new job object job create job configuration job conf throws io exception string job name name date form format new date raid node set scheduler option job conf job job job get instance job conf job name job set speculative execution false job set jar by class dist raid job set input format class dist raid input format job set output key class text job set output value class text job set mapper class dist raid mapper job set num reduce tasks return job add paths raided add raid paths policy info info list file status paths raid policy path pair list add new raid policy path pair info paths invokes map reduce job parallel raiding boolean start dist raid throws io exception assert raid policy path pair list size job job create job get conf create input file job try catch class not found exception e catch interrupted exception e checks map reduce job completed boolean check complete throws io exception catch interrupted exception e boolean successful throws io exception catch interrupted exception e set input file list input files create input file job job throws io exception configuration job conf job get configuration path job dir new path job dir label get random id path dir new path job dir path dir new path job dir file input format set input paths job dir file output format set output path job dir path op list new path dir name configuration tmp new configuration job conf the control file small size blocks this helps
1034	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistRaidNode.java	unrelated	package org apache hadoop raid implementation link raid node uses map reduce jobs raid files dist raid node extends raid node log log log factory get log dist raid node daemon thread monitor raid job progress job monitor job monitor null daemon job monitor thread null dist raid node configuration conf throws io exception inherit docs join inherit docs stop inherit docs raid files policy info info list file status paths throws io exception inherit docs get running jobs for policy string policy name
1035	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\Encoder.java	unrelated	package org apache hadoop raid represents generic encoder generate parity file source file this concrete subclasses need implement encode file impl encoder log log log factory get log protected configuration conf protected stripe size protected parity size protected random rand protected buf size protected byte read bufs protected byte write bufs a acts sink data similar dev null null output stream extends output stream write byte b throws io exception write b throws io exception write byte b len throws io exception encoder configuration conf stripe size parity size conf conf stripe size stripe size parity size parity size rand new random buf size conf get int raid encoder bufsize read bufs new byte stripe size write bufs new byte parity size allocate buffers allocate buffers stripe size parity size configure buffers block size buf size block size else block size buf size the use generate parity file this method called multiple times encoder object thus allowing reuse buffers allocated encoder object encode file file system fs path src file file system parity fs path parity file short parity repl progressable reporter throws io exception file status src stat fs get file status src file src size src stat get len block size src stat get block size configure buffers block size create tmp file write first path tmp dir get parity temp path parity fs mkdirs tmp dir path parity tmp new path tmp dir fs data output stream parity fs create try finally recovers corrupt block parity file local file the encoder generates parity size parity blocks source file stripe since want one parity blocks function creates null outputs blocks discarded recover parity block to file file system fs path src file src size block size path parity file corrupt offset file local block file throws io exception output stream new file output stream local block file try finally recovers corrupt block parity file local file the encoder generates parity size parity blocks source file stripe since want one parity blocks function creates null outputs blocks discarded recover parity block to stream file system fs path src file src size block size path parity file corrupt offset output stream throws io exception log info recovering parity block parity file corrupt offset get start offset corrupt block corrupt offset corrupt offset block size block size output streams block parity file stripe output stream outs new output stream parity size index of corrupt block in parity stripe log info index corrupt block parity stripe create real output stream block want recover create null streams rest parity size get stripe index start offset stripe stripe idx corrupt offset parity size block size stripe start stripe idx block size stripe size get input streams block source file stripe input stream blocks stripe inputs fs src file stripe start log info starting recovery using source stripe read data blocks write parity file encode stripe blocks stripe start block size outs recovers corrupt block parity file output stream the encoder generates parity size parity blocks source file stripe since one output provided blocks written files written
1036	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ErasureCode.java	unrelated	package org apache hadoop raid erasure code encodes given message significant bits the number data bits symbol size the number elements message stripe size significant bits the number parity bits symbol size the number elements code parity size encode message parity generates missing portions data first part array in integer relevant portion present least significant bits the number elements data stripe size parity size decode data erased locations erased values the number elements message stripe size the number elements code parity size number bits symbol symbol size
1037	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\GaloisField.java	unrelated	package org apache hadoop raid implementation galois field arithmetics p elements the input must unsigned integers galois field log table pow table mul table div table field size primitive period primitive polynomial field size good byte based system default field size primitive polynomial x x x x default primitive polynomial map integer galois field instances get object performs galois field arithmetics galois field get instance field size get object performs galois field arithmetics default setting galois field get instance galois field field size primitive polynomial return number elements field get field size return primitive polynomial gf get primitive polynomial compute sum two fields add x compute multiplication two fields multiply x compute division two fields divide x compute power n field power x n given vandermonde matrix v j x j vector solve z vz the output z placed replaced output vector solve vandermonde system x given vandermonde matrix v j x j vector solve z vz the output z placed replaced output vector solve vandermonde system x len compute multiplication two polynomials the index array corresponds power entry for example p constant term polynomial p multiply p q compute remainder dividend divisor pair the index array corresponds power entry for example p constant term polynomial p remainder dividend divisor compute sum two polynomials the index array corresponds power entry for example p constant term polynomial p add p q substitute x polynomial p x substitute p x
1038	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\HarIndex.java	unrelated	package org apache hadoop raid represents contents har index file the har assumed comprising raid parity files directories har index string index file name index list index entry entries new linked list index entry represents information single line har index file index entry string file name name file part file start offset start offset within part file length length file within part file mtime modification time file string part file name name part file index entry string file name start offset length string string constructor reads contents index file har index input stream max throws io exception line reader line reader new line reader text text new text nread nread max parses line extracts relevant information parse line string line throws unsupported encoding exception string splits line split boolean dir dir equals splits true false dir splits length finds index entry corresponding har part file offset index entry find entry string part name part file offset index entry e entries return null finds index entry corresponding file archive index entry find entry by file name string file name index entry e entries return null
1039	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\JobMonitor.java	unrelated	package org apache hadoop raid periodically monitors status jobs registered jobs submitted policy name kept list list kept map policy name key list value job monitor implements runnable log log log factory get log volatile boolean running true map string list dist raid jobs job monitor interval volatile jobs monitored volatile jobs succeeded job monitor configuration conf run periodically checks status running map reduce jobs monitor running jobs count string key monitor job string key dist raid job jobs monitored jobs succeeded add job map string list dist raid jobs map remove job map string list dist raid jobs map
1040	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\LocalBlockFixer.java	unrelated	package org apache hadoop raid this fixes source file blocks using parity file parity file blocks using source file it periodically fetches list corrupt files namenode figures location bad block reading corrupt file local block fixer extends block fixer log log log factory get log local block fixer java util hash map string java util date history block fixer helper helper local block fixer configuration conf throws io exception run fix throws interrupted exception io exception purge history list path get corrupt files throws io exception
1041	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\LocalRaidNode.java	unrelated	package org apache hadoop raid implementation link raid node performs raiding locally local raid node extends raid node log log log factory get log local raid node local raid node configuration conf throws io exception inherit docs raid files policy info info list file status paths throws io exception inherit docs get running jobs for policy string policy name
1042	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ParityInputStream.java	unrelated	package org apache hadoop raid wraps multiple input streams provides input stream xor streams parity input stream extends input stream default bufsize input stream streams byte xor byte buf buf size remaining available read pos parity input stream read throws io exception read byte b len throws io exception close throws io exception send contents stream sink drain output stream sink progressable reporter make bytes available reading internal buffer make available throws io exception read exact input stream byte bufs read throws io exception
1043	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidConfigurationException.java	unrelated	package org apache hadoop raid thrown config file link cron node malformed raid configuration exception extends exception serial version uid l raid configuration exception string message
1044	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidFilter.java	unrelated	package org apache hadoop raid raid filter statistics num raided num too new size too new num too small size too small aggregate statistics string string time based filter extends configured implements directory traversal file filter target repl path raid dest prefix mod time period start time statistics stats new statistics string current src path null mod time periods new string src paths new string time based filter configuration conf path dest prefix target repl time based filter configuration conf initialize other paths list policy info policies boolean check file status f throws io exception checks file chosen current policy boolean choose for current policy file status stat matching prefix length string string preference filter extends configured implements directory traversal file filter path first choice prefix directory traversal file filter second choice filter preference filter configuration conf preference filter configuration conf boolean check file status f throws io exception
1045	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidNode.java	unrelated	package org apache hadoop raid a base implements link raid protocol use raid classname specify implementation use raid node implements raid protocol log log log factory get log org apache hadoop raid raid node sleep time l seconds default port default stripe length parity length rs code default stripe length rs parity length default string rs parity length key hdfs raidrs paritylength string stripe length key hdfs raid stripe length string default raid location raid string raid location key hdfs raid locations string default raid tmp location tmp raid string raid tmp location key fs raid tmpdir string default raid har tmp location tmp raid har string raid har tmp location key fs raid hartmpdir string default raidrs location raidrs string raidrs location key hdfs raidrs locations string default raidrs tmp location tmp raidrs string raidrs tmp location key fs raidrs tmpdir string default raidrs har tmp location tmp raidrs har string raidrs har tmp location key fs raidrs hartmpdir string har suffix raid har pattern parity har partfile pattern string raidnode classname key raid classname rpc server server server rpc server address inet socket address server address null used testing purposes protected boolean stop requested false configuration manager config manager config mgr hadoop configuration protected configuration conf protected boolean initialized are initialized protected volatile boolean running are running deamon thread trigger policies daemon trigger thread null deamon thread delete obsolete parity files purge monitor purge monitor null daemon purge thread null deamon thread har raid directories daemon har thread null daemon thread fix corrupt files block fixer block fixer null daemon block fixer thread null statistics raw hdfs blocks this counts replicas block statistics startup options enum startup option raid node configuration conf throws io exception get protocol version string protocol protocol signature get protocol signature string protocol join stop inet socket address get address string address inet socket address get address configuration conf inet socket address get listener address initialize configuration conf inherit doc policy list get all policies throws io exception inherit doc string recover file string str corrupt offset throws io exception get running jobs for policy string policy name trigger monitor implements runnable raid files policy info info list file status paths path get original parity file path dest path prefix path src path parity file pair parity file pair get parity file path dest path prefix path src path configuration conf throws io exception parity file pair xor parity for source path src path configuration conf parity file pair rs parity for source path src path configuration conf parity file pair get parity file path dest path prefix path src path raid configuration conf policy info info list file status paths raid configuration conf policy info info raid configuration conf file status stat path dest path generate parity file configuration conf file status stat path un raid configuration conf path src path path un raid corrupt block configuration conf path src path purge monitor implements runnable returns number date files har percentage total number files har protected useful har har throws io exception interrupted exception recurse
1046	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidShell.java	unrelated	package org apache hadoop raid a link raid shell allows browsing configured raid policies raid shell extends configured implements tool configuration add default resource hdfs default xml configuration add default resource hdfs site xml log log log factory get log org apache hadoop raid shell raid protocol raidnode raid protocol rpc raidnode user group information ugi volatile boolean client running true configuration conf start raid shell p the raid shell connects specified raid node performs basic configuration options raid shell configuration conf throws io exception super conf conf conf initialize rpc configuration conf inet socket address address throws io exception ugi user group information get current user rpc raidnode create rpc raidnode address conf ugi raidnode create raidnode rpc raidnode initialize local configuration conf throws io exception ugi user group information get current user raid protocol create raidnode configuration conf throws io exception return create raidnode raid node get address conf conf raid protocol create raidnode inet socket address raid node addr return create raidnode create rpc raidnode raid node addr conf raid protocol create rpc raidnode inet socket address raid node addr throws io exception log debug raid shell connecting raid node addr return raid protocol rpc get proxy raid protocol raid protocol create raidnode raid protocol rpc raidnode throws io exception retry policy create policy retry policies retry up to maximum count with fixed sleep map class extends exception retry policy remote exception to policy map map class extends exception retry policy exception to policy map exception to policy map put remote exception retry policy method policy retry policies retry by exception map string retry policy method name to policy map new hash map string retry policy method name to policy map put create method policy return raid protocol retry proxy create raid protocol check open throws io exception client running close connection raid node synchronized close throws io exception client running displays format commands print usage string cmd string prefix usage java raid shell get simple name show config equals cmd else recover equals cmd else recover blocks equals cmd else run run string argv throws exception argv length exit code string cmd argv verify enough command line parameters show config equals cmd else recover equals cmd else fsck equals cmd try catch illegal argument exception arge catch remote exception e catch io exception e catch exception finally return exit code apply operation specified cmd parameters starting argv startindex show config string cmd string argv startindex throws io exception exit code startindex policy list raidnode get all policies policy list list return exit code recovers specified path parity file path recover string cmd string argv startindex throws io exception path paths new path argv length startindex j startindex argv length return paths recover and print string cmd string argv startindex throws io exception exit code path p recover cmd argv startindex return exit code recover blocks string args start index throws io exception log debug recovering blocks args length start index files block fixer block fixer helper fixer new block fixer block fixer helper conf start index args length
1047	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidUtils.java	unrelated	package org apache hadoop raid raid utils a link progressable nothing we could used reporter null would introduce dependency mapreduce dummy progressable implements progressable do nothing progress removes files matching trash file pattern filter trash configuration conf list path files remove files trash string trash pattern conf get raid blockfixer trash pattern iterator path files iterator next read till end input stream byte buf boolean eof ok throws io exception read buf length num read num read read copy bytes input stream output stream byte buf count throws io exception bytes read bytes read count zero input stream extends input stream end offset pos zero input stream end offset read throws io exception available throws io exception get pos throws io exception seek seek offset throws io exception boolean seek to new source target pos throws io exception read position byte buffer offset length read fully position byte buffer offset length read fully position byte buffer throws io exception
1048	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonCode.java	unrelated	package org apache hadoop raid reed solomon code implements erasure code stripe size parity size generating polynomial primitive root primitive power galois field gf galois field get instance err signature parity symbol locations data buff reed solomon code stripe size parity size encode message parity decode data erased location erased value stripe size parity size symbol size
1049	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonDecoder.java	unrelated	package org apache hadoop raid reed solomon decoder extends decoder log log log factory get log erasure code reed solomon code reed solomon decoder protected fix erased block protected build inputs file system fs path src file decode inputs provided write output write fixed block read from inputs perform decode byte read bufs byte write bufs
1050	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonEncoder.java	unrelated	package org apache hadoop raid reed solomon encoder extends encoder log log log factory get log erasure code reed solomon code reed solomon encoder protected encode stripe perform encode byte read bufs byte write bufs idx path get parity temp path
1051	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\XORDecoder.java	unrelated	package org apache hadoop raid xor decoder extends decoder log log log factory get log xor decoder protected fix erased block protected stripe offsets error offset block size protected parity offset error offset block size
1052	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\XOREncoder.java	unrelated	package org apache hadoop raid xor encoder extends encoder log log log factory get log xor encoder protected encode stripe path get parity temp path
1053	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\PolicyInfo.java	unrelated	package org apache hadoop raid protocol maintains information one policy policy info implements writable log log log factory get log protected simple date format date format path src path specified src path string policy name name policy erasure code type code type erasure code used string description a verbose description policy configuration conf hadoop configuration properties properties policy dependent properties reentrant read write lock plock protects policy operations enum erasure code type create empty object policy info create metadata describes policy policy info string policy name configuration conf copy fields another policy info copy from policy info sets input path policy applied set src path string throws io exception set erasure code type used policy set erasure code string code set description policy set description string des sets internal property set property string name string value returns value internal property string get property string name get name policy string get name get destination path policy erasure code type get erasure code get src path path get src path get expanded unglobbed forms src paths path get src path expanded throws io exception convert policy printable form string string writable register ctor write data output throws io exception read fields data input throws io exception
1054	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\PolicyList.java	unrelated	package org apache hadoop raid protocol maintains informatiom policies belong category these policies applied one time cannot run simultaneously policy list implements writable log log log factory get log list policy info category list policies path src path create new category policies policy list add new policy category add policy info info set src path configuration conf string src throws io exception path get src path returns policies category collection policy info get all writable register ctor write data output throws io exception read fields data input throws io exception
1055	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\RaidProtocol.java	unrelated	package org apache hadoop raid protocol raid protocol used user code link org apache hadoop raid raid shell communicate raid node user code manipulate configured policies raid protocol extends versioned protocol version id l policy list get all policies throws io exception string recover file string input path corrupt offset throws io exception
1056	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\AutoInputFormat.java	unrelated	package org apache hadoop streaming an link input format tries deduce types input files automatically it currently handle text sequence files auto input format extends file input format text input format text input format new text input format sequence file input format seq file input format configure job conf job record reader get record reader input split split job conf job
1057	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\DumpTypedBytes.java	unrelated	package org apache hadoop streaming utility program fetches files match given pattern dumps content stdout typed bytes this works files handled link org apache hadoop streaming auto input format dump typed bytes implements tool configuration conf dump typed bytes configuration conf dump typed bytes configuration get conf set conf configuration conf the main driver code dump typed bytes code run string args throws exception print usage dump given list files standard output typed bytes dump typed bytes list file status files throws io exception main string args throws exception
1058	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\Environment.java	unrelated	package org apache hadoop streaming this used get current environment host machines running map reduce this assumes setting environment streaming allowed windows ix linuz freebsd sunos solaris hp ux environment extends properties serial version uid l environment throws io exception used runtime exec string cmdarray string envp string array map string string map string get host
1059	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\HadoopStreaming.java	unrelated	package org apache hadoop streaming the main entry point usually invoked script bin hadoop jar hadoop streaming jar args hadoop streaming main string args throws exception print usage
1060	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\JarBuilder.java	unrelated	package org apache hadoop streaming this main generating job jar hadoop streaming jobs it includes files specified file option includes jar also hadoop streaming user level appplication hadoop streaming needed job also included job jar jar builder jar builder set verbose boolean v merge list src names list src unjar string dst jar throws io exception protected string file extension string file protected string get base path in jar out string source file add jar entries jar output stream dst jar file src throws io exception add named stream jar output stream dst string name input stream throws io exception add file stream jar output stream dst string jar base name file file throws io exception add directory jar output stream dst string jar base name file dir depth throws io exception test program main string args buff size byte buffer new byte buff size protected boolean verbose false
1061	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\LoadTypedBytes.java	unrelated	package org apache hadoop streaming utility program reads typed bytes standard input stores sequence file path given argument load typed bytes implements tool configuration conf load typed bytes configuration conf load typed bytes configuration get conf set conf configuration conf the main driver code load typed bytes code run string args throws exception print usage main string args throws exception
1062	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PathFinder.java	unrelated	package org apache hadoop streaming maps relative pathname absolute pathname using path environment path finder string pathenv pathnames string path sep path separator string file sep file separator directory construct path finder object using path java path path finder construct path finder object using path specified system environment variable path finder string envpath appends specified component path list prepend path component string str returns full path name file listed path file get absolute path string filename main string args throws io exception
1063	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeCombiner.java	unrelated	package org apache hadoop streaming pipe combiner extends pipe reducer string get pipe command job conf job boolean get do pipe
1064	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapper.java	unrelated	package org apache hadoop streaming a generic mapper bridge it delegates operations external program via stdin stdout pipe mapper extends pipe map red implements mapper boolean ignore key false boolean skipping false byte map output field separator byte map input field separator num of map output key fields string get pipe command job conf job boolean get do pipe configure job conf job do not declare default constructor map red creates reflectively map object key object value output collector output reporter reporter throws io exception close byte get input separator byte get field separator get num of key fields input writer create input writer throws io exception output reader create output reader throws io exception
1065	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapRed.java	unrelated	package org apache hadoop streaming shared functionality pipe mapper pipe reducer pipe map red protected log log log factory get log pipe map red get name returns configuration configuration get configuration return job returns data output client input written data output get client output return client out returns data input client output read data input get client input return client in returns input separator used byte get input separator returns field separator used byte get field separator returns number key fields get num of key fields returns command spawned subprocess mapper reducer operations delegate string get pipe command job conf job boolean get do pipe outside singleq doubleq buffer size string split args string args array list arg list new array list char ch args char array clen ch length state outside argstart c c clen c return string arg list array new string configure job conf job try catch io exception e catch interrupted exception e set stream job details job conf job string job get stream min rec written to enable skip null add job conf to environment job conf conf properties env iterator conf iterator next string safe env var name string var string buffer safe new string buffer len var length len return safe string add environment properties env string name vals encoding b c stream job name vals null return string nv name vals split nv length env put properties env string name string value log debug enabled env put name value start output threads output collector output reporter reporter throws io exception writer create input writer reader create output reader thread new mr output thread reader output reporter thread start err thread new mr error thread err thread set reporter reporter err thread start wait output threads throws io exception try catch interrupted exception e input writer create input writer throws io exception input writer create input writer class extends input writer input writer class throws io exception input writer input writer input writer initialize return input writer output reader create output reader throws io exception output reader create output reader class extends output reader output reader class throws io exception output reader output reader output reader initialize return output reader mr output thread extends thread mr output thread output reader reader output collector collector run output reader reader null output collector collector null reporter reporter null last stdout report mr error thread extends thread mr error thread set reporter reporter reporter run boolean matches reporter string line boolean matches counter string line boolean matches status string line incr counter string line set status string line last stderr report volatile reporter reporter string reporter prefix string counter prefix string status prefix map red finished try catch runtime exception e maybe log record num rec read next rec read log next rec read log next rec read log string get context string num rec info n min rec written to enable skip min rec written to enable skip envline host envline user envline hadoop user thread null return string envline string var return var stream util env
1066	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapRunner.java	unrelated	package org apache hadoop streaming pipe map runner k v k v extends map runner k v k v run record reader k v input output collector k v output
1067	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeReducer.java	unrelated	package org apache hadoop streaming a generic reducer bridge it delegates operations external program via stdin stdout pipe reducer extends pipe map red implements reducer byte reduce out field separator byte reduce input field separator num of reduce output key fields boolean skipping false string get pipe command job conf job boolean get do pipe configure job conf job reduce object key iterator values output collector output exit val else extra info subprocess still running n close byte get input separator byte get field separator get num of key fields input writer create input writer throws io exception output reader create output reader throws io exception
1068	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamBaseRecordReader.java	unrelated	package org apache hadoop streaming shared functionality hadoop streaming formats a custom reader defined record reader constructor selected option bin hadoop streaming inputreader stream base record reader implements record reader text text protected log log log factory get log stream base record reader get name custom job conf properties prefixed namespace string conf ns stream recordreader stream base record reader fs data input stream file split split reporter reporter split split start split get start length split get length end start length split name split get path get name reporter reporter job job fs fs status max record chars job get int conf ns statuschars record reader api read record implementation call num rec stats end boolean next text key text value throws io exception returns current position input synchronized get pos throws io exception return get pos close future operations synchronized close throws io exception close get progress throws io exception end start else text create key return new text text create value return new text stream base record reader api implementation seek forward first byte next record the initial byte offset stream arbitrary seek next record boundary throws io exception num rec stats byte record start len throws io exception num rec num rec next status rec last mem string get status char sequence record pos try catch io exception io string rec str record length status max record chars else string unqual split split get path get name string status hstr stream util host num rec pos pos unqual split status split name return status fs data input stream file split split start end length string split name reporter reporter job conf job file system fs num rec next status rec status max record chars
1069	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamInputFormat.java	unrelated	package org apache hadoop streaming an input format selects record reader based job conf property this used non standard record reader stream xml record reader for standard record readers appropriate input format used stream input format extends key value text input format record reader text text get record reader input split generic split job conf job reporter reporter throws io exception string c job get stream recordreader c null c index of line record reader return super get record reader generic split job reporter handling non standard record reader likely stream xml record reader file split split file split generic split log info get record reader start split split reporter set status split string open file seek start split file system fs split get path get file system job fs data input stream fs open split get path factory dispatch based available params class reader class reader class stream util good class or null job c null reader class null throw new runtime exception class found c constructor ctor try ctor reader class get constructor new class fs data input stream file split reporter job conf file system catch no such method exception nsm throw new runtime exception nsm record reader text text reader try reader record reader text text ctor new instance new object split reporter job fs catch exception nsm throw new runtime exception nsm return reader
1070	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamJob.java	unrelated	package org apache hadoop streaming all client side work happens jar packaging map red job submission monitoring stream job implements tool protected log log log factory get log stream job get name string reduce none none streaming cli implementation command line parser parser new basic parser options options configuration using link set conf configuration run link run string stream job string argv boolean may exit argv argv config new configuration stream job setup options config new configuration configuration get conf return config set conf configuration conf config conf run string args throws exception try argv args init pre process args parse argv print usage post process args set job conf catch illegal argument exception ex ignore since log already printed print log debug mode log debug error streaming job ex return return submit and monitor job this method creates streaming job given argument list the created object used submitted jobtracker execution job agent job control job conf create job string argv throws io exception stream job job new stream job job argv argv job init job pre process args job parse argv job post process args job set job conf return job job conf this method actually intializes job conf submits job jobtracker go throws io exception try return run argv catch exception ex throw new io exception ex get message protected init try env new environment catch io exception io throw new runtime exception io pre process args verbose false add task environment post process args throws io exception input specs size fail required argument input name output null fail required argument output msg add task environment add task environment string package file package files file f new file package file f file msg shipped canon files shipped canon files careful names map cmd unqualify if local path map cmd com cmd unqualify if local path com cmd red cmd unqualify if local path red cmd string unqualify if local path string cmd throws io exception cmd null else string prog cmd string args cmd index of string prog canon try catch io exception io boolean shipped shipped canon files contains prog canon msg shipped shipped prog canon shipped msg cmd cmd return cmd parse argv command line cmd line null try cmd line parser parse options argv catch exception oe log error oe get message exit usage argv length info equals argv cmd line null detailed usage cmd line option info cmd line option help detailed usage verbose cmd line option verbose background cmd line option background debug cmd line option debug debug debug string values cmd line get option values input values null values length output cmd line get option value output map cmd cmd line get option value mapper com cmd cmd line get option value combiner red cmd cmd line get option value reducer lazy output cmd line option lazy output values cmd line get option values file values null values length string fs name cmd line get option value dfs null fs name additional conf spec cmd line get option value additionalconfspec input format spec cmd
1071	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamKeyValUtil.java	unrelated	package org apache hadoop streaming stream key val util find first occured tab utf encoded find tab byte utf start length find first occured tab utf encoded find tab byte utf split utf byte array key value assuming delimilator splitpos split key val byte utf start length split utf byte array key value assuming delimilator splitpos split key val byte utf start length split utf byte array key value assuming delimilator splitpos split key val byte utf text key text val split pos split utf byte array key value assuming delimilator splitpos split key val byte utf text key text val split pos read utf encoded line data input stream read line line reader line reader text throws io exception
1072	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamUtil.java	unrelated	package org apache hadoop streaming utilities used streaming stream util it may seem strange silently switch behaviour string classname reason simplified usage pre mapper classname program instead explicit usage mapper program javamapper classname mapper javamapper mutually exclusive repeat reducer combiner pre class good class or null configuration conf string name string default package class clazz null try clazz conf get class by name name catch class not found exception cnf clazz null name index of default package null name default package name try clazz conf get class by name name catch class not found exception cnf return clazz string find in classpath string name return find in classpath name stream util get class loader string find in classpath string name class loader loader string rel path name rel path rel path replace rel path java net url url loader get resource rel path string code path url null boolean jar url get protocol equals jar code path url string code path starts with jar code path code path substring jar length code path starts with file code path code path substring file length jar a jar spec remove suffix path jar package class bang code path last index of code path code path substring bang else a spec remove package class portion pos code path last index of rel path pos throw new illegal argument exception invalid code path name name code path code path substring pos else code path null return code path string qualify host string url try return qualify host new url url string catch io exception io return url url qualify host url url try inet address inet address get by name url get host string qual host get canonical host name url q new url url get protocol qual host url get port url get file return q catch io exception io return url string regexp specials string regexp escape string plain string buffer buf new string buffer char ch plain char array csup ch length c c csup c regexp specials index of ch c buf append buf append ch c return buf string string slurp file f throws io exception len f length byte buf new byte len file input stream new file input stream f string contents null try read buf len contents new string buf utf finally close return contents string slurp hadoop path p file system fs throws io exception len fs get file status p get len byte buf new byte len fs data input stream fs open p string contents null try read fully get pos buf contents new string buf utf finally close return contents environment env string host try env new environment host env get host catch io exception io io print stack trace environment env env null return env try env new environment catch io exception io io print stack trace return env boolean local job tracker job conf job return job get jt config jt ipc address local equals local
1073	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamXmlRecordReader.java	unrelated	package org apache hadoop streaming a way interpret xml fragments mapper input records values xml subtrees delimited configurable tags keys could value certain attribute xml subtree left stream processor application the name value properties stream xml record reader understands string begin chars marking beginning record string end chars marking end record maxrec maximum record size lookahead maximum lookahead sync cdata boolean slowmatch stream xml record reader extends stream base record reader stream xml record reader fs data input stream file split split reporter reporter init throws io exception num next synchronized boolean next text key text value throws io exception seek next record boundary throws io exception boolean read until match begin throws io exception boolean read until match end data output buffer buf throws io exception boolean slow read until match pattern mark pattern boolean pat states cdata in cdata out cdata unk record accept inputs cdata begin cdata end record maybe also updates first match start next state state input buf pos pattern make pattern c data or mark string escaped mark add group string buffer pat string escaped group boolean fast read until match string text pat boolean pat data output buffer buf or null throws io exception string check job get string prop throws io exception string begin mark string end mark pattern begin pat pattern end pat boolean slow match look ahead bytes read try synch cdata non cdata should max record size max rec size buffered input stream bin wrap fs data input stream efficient backward seeks pos keep track position respect encapsulated fs data input stream na first match start candidate record boundary might cdata first match end boolean synched
1074	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\IdentifierResolver.java	unrelated	package org apache hadoop streaming io this used resolve identifier required io by extending pointing property tt stream io identifier resolver tt extension additional io added external code identifier resolver note identifiers case insensitive string text id text string raw bytes id rawbytes string typed bytes id typedbytes class extends input writer input writer class null class extends output reader output reader class null class output key class null class output value class null resolve string identifier class extends input writer get input writer class class extends output reader get output reader class class get output key class class get output value class protected set input writer class class extends input writer protected set output reader class class extends output reader protected set output key class class output key class protected set output value class class output value class
1075	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\InputWriter.java	unrelated	package org apache hadoop streaming io abstract base write client input input writer k v initializes input writer this method called calling methods initialize pipe map red pipe map red throws io exception writes input key write key k key throws io exception writes input value write value v value throws io exception
1076	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\OutputReader.java	unrelated	package org apache hadoop streaming io abstract base read client output output reader k v initializes output reader this method called calling methods initialize pipe map red pipe map red throws io exception read next key value pair outputted client boolean read key value throws io exception returns current key k get current key throws io exception returns current value v get current value throws io exception returns last output client string string get last output
1077	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\RawBytesInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input raw bytes raw bytes input writer extends input writer writable writable data output client out byte array output stream buffer out data output stream buffer data out initialize pipe map red pipe map red throws io exception write key writable key throws io exception write value writable value throws io exception write raw bytes writable writable throws io exception
1078	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\RawBytesOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output raw bytes raw bytes output reader extends output reader bytes writable bytes writable data input client in byte bytes bytes writable key bytes writable value initialize pipe map red pipe map red throws io exception boolean read key value throws io exception bytes writable get current key throws io exception bytes writable get current value throws io exception string get last output read length throws io exception byte read bytes length throws io exception
1079	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TextInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input text text input writer extends input writer object object data output client out byte input separator initialize pipe map red pipe map red throws io exception write key object key throws io exception write value object value throws io exception write object output stream using utf encoding write utf object object throws io exception
1080	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TextOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output text text output reader extends output reader text text line reader line reader byte bytes data input client in configuration conf num key fields byte separator text key text value text line initialize pipe map red pipe map red throws io exception boolean read key value throws io exception text get current key throws io exception text get current value throws io exception string get last output split utf line key value split key val byte line length text key text val
1081	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TypedBytesInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input typed bytes typed bytes input writer extends input writer object object typed bytes output tb out typed bytes writable output tbw out initialize pipe map red pipe map red throws io exception write key object key throws io exception write value object value throws io exception write typed bytes object value throws io exception
1082	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TypedBytesOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output typed bytes typed bytes output reader extends output reader typed bytes writable typed bytes writable byte bytes data input client in typed bytes writable key typed bytes writable value typed bytes input initialize pipe map red pipe map red throws io exception boolean read key value throws io exception typed bytes writable get current key throws io exception typed bytes writable get current value throws io exception string get last output
1083	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\Type.java	unrelated	package org apache hadoop typedbytes the possible type codes enum type codes supported types bytes byte bool int long float double string vector list map application specific codes writable low level codes marker code type code
1084	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesInput.java	unrelated	package org apache hadoop typedbytes provides functionality reading typed bytes typed bytes input data input typed bytes input set data input data input thread local tb in new thread local get thread local typed bytes input supplied link data input typed bytes input get data input creates new instance typed bytes input typed bytes input data input reads typed bytes sequence converts java object the first byte interpreted type code right number subsequent bytes read depending obtained type object read throws io exception reads typed bytes sequence the first byte interpreted type code right number subsequent bytes read depending obtained type reached byte read raw throws io exception reads type byte returns corresponding link type type read type throws io exception skips type byte boolean skip type throws io exception reads bytes following code type bytes code code byte read bytes throws io exception reads raw bytes following custom code byte read raw bytes code throws io exception reads raw bytes following code type bytes code code byte read raw bytes throws io exception reads byte following code type byte code code byte read byte throws io exception reads raw byte following code type byte code code byte read raw byte throws io exception reads boolean following code type bool code code boolean read bool throws io exception reads raw bytes following code type bool code code byte read raw bool throws io exception reads integer following code type int code code read int throws io exception reads raw bytes following code type int code code byte read raw int throws io exception reads following code type long code code read long throws io exception reads raw bytes following code type long code code byte read raw long throws io exception reads following code type float code code read float throws io exception reads raw bytes following code type float code code byte read raw float throws io exception reads following code type double code code read double throws io exception reads raw bytes following code type double code code byte read raw double throws io exception reads following code type string code code string read string throws io exception reads raw bytes following code type string code code byte read raw string throws io exception reads vector following code type vector code code array list read vector throws io exception reads raw bytes following code type vector code code byte read raw vector throws io exception reads header following code type vector code code read vector header throws io exception reads list following code type list code code list read list throws io exception reads raw bytes following code type list code code byte read raw list throws io exception reads map following code type map code code tree map read map throws io exception reads raw bytes following code type map code code byte read raw map throws io exception reads header following code type map code code read map header throws io exception
1085	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesOutput.java	unrelated	package org apache hadoop typedbytes provides functionality writing typed bytes typed bytes output data output typed bytes output set data output data output thread local tb out new thread local get thread local typed bytes output supplied link data output link data output typed bytes output get data output creates new instance typed bytes output typed bytes output data output writes java object typed bytes sequence write object obj throws io exception writes raw sequence typed bytes write raw byte bytes throws io exception writes raw sequence typed bytes write raw byte bytes offset length writes bytes array typed bytes sequence using given typecode length write bytes byte bytes code length throws io exception writes bytes array typed bytes sequence using given typecode write bytes byte bytes code throws io exception writes bytes array typed bytes sequence write bytes byte bytes throws io exception writes bytes buffer typed bytes sequence write bytes buffer buffer throws io exception writes byte typed bytes sequence write byte byte b throws io exception writes boolean typed bytes sequence write bool boolean b throws io exception writes integer typed bytes sequence write int throws io exception writes typed bytes sequence write long throws io exception writes typed bytes sequence write float f throws io exception writes typed bytes sequence write double throws io exception writes typed bytes sequence write string string throws io exception writes vector typed bytes sequence write vector array list vector throws io exception writes vector header write vector header length throws io exception writes list typed bytes sequence write list list list throws io exception writes list header write list header throws io exception writes list footer write list footer throws io exception writes map typed bytes sequence write map map map throws io exception writes map header write map header length throws io exception
1086	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesRecordInput.java	unrelated	package org apache hadoop typedbytes serializer records writes typed bytes typed bytes record input implements record input typed bytes input typed bytes record input set typed bytes input typed bytes input thread local tb in new thread local protected synchronized object initial value get thread local typed bytes record input supplied link typed bytes input link typed bytes input typed bytes record input get typed bytes input typed bytes record input bin typed bytes record input tb in get bin set typed bytes input return bin get thread local typed bytes record input supplied link data input link data input typed bytes record input get data input return get typed bytes input get creates new instance typed bytes record input typed bytes record input typed bytes input creates new instance typed bytes record input typed bytes record input data input new typed bytes input boolean read bool string tag throws io exception skip type return read bool buffer read buffer string tag throws io exception skip type return new buffer read bytes byte read byte string tag throws io exception skip type return read byte read double string tag throws io exception skip type return read double read float string tag throws io exception skip type return read float read int string tag throws io exception skip type return read int read long string tag throws io exception skip type return read long string read string string tag throws io exception skip type return read string start record string tag throws io exception skip type index start vector string tag throws io exception skip type return new typed bytes index read vector header index start map string tag throws io exception skip type return new typed bytes index read map header end record string tag throws io exception end vector string tag throws io exception end map string tag throws io exception typed bytes index implements index nelems typed bytes index nelems boolean done incr
1087	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesRecordOutput.java	unrelated	package org apache hadoop typedbytes deserialized records reads typed bytes typed bytes record output implements record output typed bytes output typed bytes record output set typed bytes output typed bytes output thread local tb out new thread local get thread local typed bytes record input supplied link typed bytes output link typed bytes output typed bytes record output get typed bytes output get thread local typed bytes record output supplied link data output link data output typed bytes record output get data output creates new instance typed bytes record output typed bytes record output typed bytes output creates new instance typed bytes record output typed bytes record output data output write bool boolean b string tag throws io exception write buffer buffer buf string tag throws io exception write byte byte b string tag throws io exception write double string tag throws io exception write float f string tag throws io exception write int string tag throws io exception write long string tag throws io exception write string string string tag throws io exception start record record r string tag throws io exception start vector array list v string tag throws io exception start map tree map string tag throws io exception end record record r string tag throws io exception end vector array list v string tag throws io exception end map tree map string tag throws io exception
1088	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritable.java	unrelated	package org apache hadoop typedbytes writable typed bytes typed bytes writable extends bytes writable create typed bytes writable typed bytes writable create typed bytes writable given byte array initial value typed bytes writable byte bytes set typed bytes given java object set value object obj get typed bytes java object object get value get type code embedded first byte type get type generate suitable representation string string
1089	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritableInput.java	unrelated	package org apache hadoop typedbytes provides functionality reading typed bytes writable objects typed bytes writable input implements configurable typed bytes input configuration conf typed bytes writable input conf new configuration set typed bytes input typed bytes input thread local tb in new thread local protected synchronized object initial value return new typed bytes writable input get thread local typed bytes writable input supplied link typed bytes input link typed bytes input typed bytes writable input get typed bytes input typed bytes writable input bin typed bytes writable input tb in get bin set typed bytes input return bin get thread local typed bytes writable input supplied link data input link data input typed bytes writable input get data input return get typed bytes input get creates new instance typed bytes writable input typed bytes writable input typed bytes input creates new instance typed bytes writable input typed bytes writable input data input din new typed bytes input din writable read throws io exception type type read type type null return null switch type case bytes return read bytes case byte return read byte case bool return read boolean case int return read v int case long return read v long case float return read float case double return read double case string return read text case vector return read array case map return read map case writable return read writable default throw new runtime exception unknown type class extends writable read type throws io exception type type read type type null return null switch type case bytes return bytes writable case byte return byte writable case bool return boolean writable case int return v int writable case long return v long writable case float return float writable case double return double writable case string return text case vector return array writable case map return map writable case writable return writable default throw new runtime exception unknown type bytes writable read bytes bytes writable bw throws io exception byte bytes read bytes bw null bw new bytes writable bytes else bw set bytes bytes length return bw bytes writable read bytes throws io exception return read bytes null byte writable read byte byte writable bw throws io exception bw null bw new byte writable bw set read byte return bw byte writable read byte throws io exception return read byte null boolean writable read boolean boolean writable bw throws io exception bw null bw new boolean writable bw set read bool return bw boolean writable read boolean throws io exception return read boolean null int writable read int int writable iw throws io exception iw null iw new int writable iw set read int return iw int writable read int throws io exception return read int null v int writable read v int v int writable iw throws io exception iw null iw new v int writable iw set read int return iw v int writable read v int throws io exception return read v int null long writable read long long writable lw throws io exception lw null lw new long writable
1090	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritableOutput.java	unrelated	package org apache hadoop typedbytes provides functionality writing writable objects typed bytes typed bytes writable output typed bytes output typed bytes writable output set typed bytes output typed bytes output thread local tb out new thread local get thread local typed bytes writable input supplied link typed bytes output link typed bytes output typed bytes writable output get typed bytes output get thread local typed bytes writable output supplied link data output link data output typed bytes writable output get data output creates new instance typed bytes writable output typed bytes writable output typed bytes output creates new instance typed bytes writable output typed bytes writable output data output dout write writable w throws io exception write typed bytes typed bytes writable tbw throws io exception write bytes bytes writable bw throws io exception write byte byte writable bw throws io exception write boolean boolean writable bw throws io exception write int int writable iw throws io exception write v int v int writable viw throws io exception write long long writable lw throws io exception write v long v long writable vlw throws io exception write float float writable fw throws io exception write double double writable dw throws io exception write text text throws io exception write array array writable aw throws io exception write map map writable mw throws io exception write sorted map sorted map writable smw throws io exception write record record r throws io exception write writable writable w throws io exception
1091	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\DiagnosticTest.java	unrelated	package org apache hadoop vaidya this base extended diagnostic test it implements runnable required multiple tests run parallel diagnostic test implements runnable highval mediumval lowval job statistics job execution stats element test config element impact level boolean evaluated boolean test passed boolean evaluated boolean istest passed init globals job statistics job execution stats element test config element string get prescription string get reference details evaluate job statistics job execution stats string get title throws exception string get description throws exception get importance throws exception get impact level throws exception get severity level throws exception get success threshold throws exception element get report element document doc node parent throws exception run protected get input element long value string element name default value protected get input element double value string element name default value protected string get input element string value string element name string default value truncate x
1092	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\JobDiagnoser.java	unrelated	package org apache hadoop vaidya this base driver job diagnostics various specialty drivers tests specific aspects job problems e g post ex performance diagnoser extends base job diagnoser document report document get report job diagnoser throws exception print report save report string filename
1093	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\postexdiagnosis\PostExPerformanceDiagnoser.java	unrelated	package org apache hadoop vaidya postexdiagnosis this acts driver rule engine executing post execution performance diagnostics tests map reduce job it prints saves diagnostic report xml document post ex performance diagnoser extends job diagnoser string job history file null input stream tests conf file is null string report file null string job conf file null job statistics job execution statistics string get report file string get job history file input stream get tests conf file is set tests conf file is input stream tests conf file is job statistics get job execution statistics post ex performance diagnoser string job conf file string job history file input stream tests conf file is job info read job information job conf job conf throws exception print help main string args
1094	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\JobStatistics.java	unrelated	package org apache hadoop vaidya statistics job job statistics implements job statistics interface pattern parsing counters pattern pattern pattern compile job configuration job conf job conf set job conf job conf job conf job conf job conf todo add job conf job array aggregated job level counters job history parser job info job info job stats java util hashtable enum string job job conf get job conf return job conf get job counters type get long value enum key job get key null else get job counters type double get double value enum key job get key null else get job counters type string string get string value enum key job get key null return else set key value type set value enum key value job put key long string value set key value type set value enum key value job put key double string value set key value type string set value enum key string value job put key value map task list sorted task id array list map task statistics map task list new array list map task statistics reduce task list sorted task id array list reduce task statistics reduce task list new array list reduce task statistics ctor job statistics job conf job conf job info job info throws parse exception job conf job conf job info job info job new hashtable enum string populate job job job info populate map reduce task lists map task list reduce task list add job type map reduce map only get long value job keys total reduces else populate map reduce task lists array list map task statistics map task list throws parse exception num tasks task map entry set size do need lists list task attempt info successful map attempt list new array list task attempt info list task attempt info successful reduce attempt list new array list task attempt info job history parser task info task info task map values get last successful task attempt added stats task attempt info get last successful task attempt task info task task attempt info ai task get all task attempts values return null popuate job stats populate job hashtable enum string job job info job info throws parse exception job put job keys finish time string value of job info get finish time job put job keys jobid job info get job id string job put job keys jobname job info get jobname job put job keys user job info get username job put job keys jobconf job info get job conf path job put job keys submit time string value of job info get submit time job put job keys launch time string value of job info get launch time job put job keys total maps string value of job info get total maps job put job keys total reduces string value of job info get total reduces job put job keys failed maps string value of job info get failed maps job put job keys failed reduces string value of job info get failed reduces job put job keys finished
1095	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\JobStatisticsInterface.java	unrelated	package org apache hadoop vaidya statistics job job statistics interface get job configuration job xml values job conf get job conf get job counters type get long value enum key get job counters type double get double value enum key get job counters type string string get string value enum key set key value type set value enum key value set key value type set value enum key valye set key value type string set value enum key string value if sort key null default map tasks sorted using map task ids array list map task statistics get map task list enum map task sort key key data type data type if sort key null default reduce tasks sorted using task ids array list reduce task statistics get reduce task list enum reduce task sort key key data type data type print job execution statistics print job execution statistics job task statistics key data types enum key data type job keys enum job keys map task keys enum map task keys reduce task keys enum reduce task keys
1096	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\MapTaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job map task statistics extends task statistics map task statistics extends task statistics
1097	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\ReduceTaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job reduce task statistics extends task statistics reduce task statistics extends task statistics
1098	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\TaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job task statistics stores task statistics enum string key value pairs hashtable enum string task new hashtable enum string get long key value get long value enum key get key type double get double value enum key get key type string string get string value enum key set key value set value enum key value set key value set value enum key value set string key value set value enum key string value print key values pairs task print keys
1099	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\util\XMLUtils.java	unrelated	package org apache hadoop vaidya util sample utility work dom document xml utils prints specified node prints children print dom node node string get element value string element name element element throws exception document parse input stream fs write xml to file string filename document document count by tag name string tag document document
1100	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaConfiguration.java	unrelated	package org apache hadoop vertica a container configuration property names jobs vertica input output the job configured using methods link vertica input format link vertica output format alternatively properties set configuration proper values string string string string string string string string vertica configuration vertica version constants integer version class name vertica jdbc driver string vertica driver class com vertica driver host names connect selected random string hostnames prop mapred vertica hostnames name database connect string database prop mapred vertica database user name vertica string username prop mapred vertica username password vertica string password prop mapred vertica password host names connect selected random string output hostnames prop mapred vertica hostnames output name database connect string output database prop mapred vertica database output user name vertica string output username prop mapred vertica username output password vertica string output password prop mapred vertica password output query run input string query prop mapred vertica input query query run retrieve parameters string query param prop mapred vertica input query paramquery static parameters query string query params prop mapred vertica input query params optional input delimiter streaming string input delimiter prop mapred vertica input delimiter optional input terminator streaming string input terminator prop mapred vertica input terminator whether marshal dates strings string date string mapred vertica date output table name string output table name prop mapred vertica output table name definition output table types string output table def prop mapred vertica output table def whether drop tables string output table drop mapred vertica output table drop optional output format delimiter string output delimiter prop mapred vertica output delimiter optional output format terminator string output terminator prop mapred vertica output terminator override sleep timer optimize poll new projetions refreshed string optimize poll timer prop mapred vertica optimize poll sets vertica database connection information link configuration configuration one hosts vertica cluster name vertica database vertica database username vertica database password configure vertica configuration conf string hostnames sets vertica database connection information link configuration configuration one hosts source cluster name source vertica database source vertica database source vertica database one hosts output cluster name output vertica database target vertica database target vertica database configure vertica configuration conf string hostnames configuration conf default record terminator writing output vertica string record terminater u default delimiter writing output vertica string delimiter u defulat optimize poll timeout optimize poll timer vertica configuration configuration conf configuration get configuration returns connection random host vertica cluster true connection writing connection get connection boolean output throws io exception string get input query get run query give results mappers set input query string input query return query used retrieve parameters input query set string get params query query used retrieve parameters input query the result set must match input query parameters preceisely set params query string segment params query return input parameters set collection list object get input parameters throws io exception sets collection lists each list passed input split used arguments input query set input params collection list object segment params for streaming return delimiter separate values mapper string get input delimiter for streaming set delimiter separate values mapper
1101	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaInputFormat.java	unrelated	package org apache hadoop vertica input formatter returns results query executed vertica the key record number within result set mapper the value vertica record uses similar jdbc result sets returning values vertica input format extends set input query job query run vertica set input job job string input query set parameterized input query job query returns parameters sql query parameters specified question marks sql query returns parameters input query set input job job string input query set input query number comma delimited literal list parameters sql query parameters specified question marks numer comma delimited strings literal parameters substitute input query set input job job string input query set input query collection parameter lists sql query parameters specified question marks collection ordered lists subtitute input query set input job job string inpu query inherit doc record reader long writable vertica record create record reader inherit doc list input split get splits job context context throws io exception
1102	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaInputSplit.java	unrelated	package org apache hadoop vertica input split reading data vertica vertica input split extends input split implements writable log log log factory get log vertica input split prepared statement stmt null connection connection null vertica configuration vtconfig null string input query null list object segment params null start end inherit doc vertica input split vertica input split string input query list object segment params inherit doc configure configuration conf throws exception list object get segment params result set execute query throws exception inherit doc close throws sql exception get start get end get length throws io exception inherit doc string get locations throws io exception inherit doc configuration get configuration inherit doc read fields data input throws io exception inherit doc write data output throws io exception
1103	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaOutputFormat.java	unrelated	package org apache hadoop vertica output formatter loading reducer output vertica vertica output format extends output format text vertica record string delimiter vertica configuration delimiter string terminator vertica configuration record terminater set output table set output job job string table name set output job table name false set output table whether drop loading set output job job string table name boolean drop table set output job table name drop table string null set output table whether drop loading create table specification exist list column definitions foo bar varchar set output job job string table name boolean drop table vertica configuration vtconfig new vertica configuration job vtconfig set output table name table name vtconfig set output table def table def vtconfig set drop table drop table todo handle collection output tables vertica table inherit doc check output specs job context context throws io exception vertica util check output specs context get configuration vertica configuration vtconfig new vertica configuration context delimiter vtconfig get output delimiter terminator vtconfig get output record terminator test check specs connect db true testing check output specs job context context boolean test vertica util check output specs context get configuration vertica configuration vtconfig new vertica configuration context delimiter vtconfig get output delimiter terminator vtconfig get output record terminator inherit doc record writer text vertica record get record writer vertica configuration config new vertica configuration context string name context get job name todo use explicit date formats string table config get output table name string copy stmt copy table from stdin delimiter try catch exception e vertica record get value configuration conf throws exception vertica configuration config new vertica configuration conf string table config get output table name connection conn config get connection true return new vertica record writer conn table config optionally called end job optimize newly created loaded tables useful new tables k records optimize configuration conf throws exception vertica configuration vtconfig new vertica configuration conf connection conn vtconfig get connection true todo consider tables skip tables non temp projections string table name vtconfig get output table name statement stmt conn create statement result set rs null string buffer design tables new string buffer table name hash set string tables with temp new hash set string fully qualify table name defaults table table name index of add single output table tables with temp add table name map table name set projection names hash map string collection string table proj new hash map string collection string rs stmt execute query select schemaname anchortablename projname vt projection rs next string table tables with temp string design name new integer conn hash code string stmt execute select create projection design design name vertica util vertica version conf true vertica configuration version else inherit doc output committer get output committer task attempt context context return new file output committer file output format get output path context
1104	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecord.java	unrelated	package org apache hadoop vertica serializable record records returned written vertica vertica record implements writable result set results null result set meta data meta null columns list integer types null list object values null list string names null boolean date string string delimiter vertica configuration delimiter string terminator vertica configuration record terminater date format datefmt new simple date format yyyy m mdd date format timefmt new simple date format h hmmss date format tmstmpfmt new simple date format yyyy m mdd h hmmss date format sqlfmt new simple date format yyyy mm dd hh mm ss list object get values return values list integer get types return types create new vertica record query result set result set returned running input split query true dates marshaled strings vertica record result set results boolean date string throws sql exception results results date string date string meta results get meta data columns meta get column count names new array list string columns types new array list integer columns values new array list object columns columns vertica record types new array list integer values new array list object vertica record list string names list integer types names names types types values new array list object suppress warnings unused integer type types columns values size vertica record list object values boolean parse types types new array list integer values values columns values size object types test junit tests require database vertica record list string names list integer types names names types types values values date string date string columns types size types size object get string name throws exception names null names size names index of name return get object get values size return values get set string name object value throws exception names null names size names index of name set value set value indexed set integer object value set value false set value indexed set integer object value boolean validate values size validate value null values set value boolean next throws sql exception results next return false object types object obj values string sql string return sql string delimiter terminator string sql string string delimiter arg string terminator arg string buffer sb new string buffer columns return sb string read fields data input throws io exception columns read int types size columns columns write data output throws io exception write int columns columns columns
1105	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecordReader.java	unrelated	package org apache hadoop vertica vertica record reader extends result set results null start pos length vertica input split split null long writable key null vertica record value null vertica record reader vertica input split split configuration job initialize input split split task attempt context context inherit doc close throws io exception get pos throws io exception get progress throws io exception boolean next long writable key vertica record value throws io exception long writable get current key throws io exception interrupted exception vertica record get current value throws io exception boolean next key value throws io exception interrupted exception
1106	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecordWriter.java	unrelated	package org apache hadoop vertica vertica record writer extends record writer text vertica record string writer table null connection connection null statement statement null com vertica pg statement string copy stmt null string delimiter vertica configuration delimiter string terminator vertica configuration record terminater methods com vertica pg statement method start copy in null method finish copy in null method add stream to copy in null vertica record writer connection connection string copy stmt vertica record get value throws sql exception close task attempt context context throws io exception write text table vertica record record throws io exception
1107	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingInput.java	unrelated	package org apache hadoop vertica vertica streaming input extends input format text text record reader text text create record reader input split split list input split get splits job context context throws io exception
1108	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingOutput.java	unrelated	package org apache hadoop vertica vertica streaming output extends output format text text log log log factory string delimiter vertica configuration delimiter string terminator vertica configuration record terminater check output specs job context context throws io exception record writer text text get record writer task attempt context context output committer get output committer task attempt context context
1109	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingRecordReader.java	unrelated	package org apache hadoop vertica vertica streaming record reader extends record reader text text result set results null vertica record internal record null start pos length vertica input split split null string delimiter vertica configuration delimiter string terminator vertica configuration record terminater text key new text text value new text vertica streaming record reader vertica input split split inherit doc initialize input split split task attempt context context inherit doc close throws io exception get pos throws io exception get progress throws io exception inherit doc text get current key throws io exception interrupted exception inherit doc text get current value throws io exception interrupted exception inherit doc boolean next key value throws io exception
1110	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingRecordWriter.java	unrelated	package org apache hadoop vertica vertica streaming record writer extends record writer text text log log log factory string writer table null connection connection null statement statement null com vertica pg statement string copy stmt null methods com vertica pg statement method start copy in null method finish copy in null method add stream to copy in null vertica streaming record writer connection connection string copy stmt close task attempt context context throws io exception write text table text record throws io exception
1111	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaUtil.java	unrelated	package org apache hadoop vertica vertica util log log log factory get log vertica util vertica version configuration conf boolean output throws io exception check output specs configuration conf throws io exception todo catch params required missing todo better error message count query bad list input split get splits job context context
1112	mapreduce\src\examples\org\apache\hadoop\examples\AggregateWordCount.java	unrelated	package org apache hadoop examples this example aggregated hadoop map reduce application it reads text input files breaks line words counts the output locally sorted list words count often occurred to run bin hadoop jar hadoop examples jar aggregatewordcount dir dir num of reducers textinputformat aggregate word count word count plug in class extends array list entry text text generate key val pairs object key the main driver word count map reduce program invoke method submit map reduce job when communication problems job tracker main string args throws io exception interrupted exception class not found exception job job value aggregator job create value aggregator job args job set jar by class aggregate word count ret job wait for completion true system exit ret
1113	mapreduce\src\examples\org\apache\hadoop\examples\AggregateWordHistogram.java	unrelated	package org apache hadoop examples this example aggregated hadoop map reduce application computes histogram words input texts to run bin hadoop jar hadoop examples jar aggregatewordhist dir dir num of reducers textinputformat aggregate word histogram aggregate word histogram plugin extends value aggregator base descriptor parse given value generate aggregation id value pair per word the id type value histogram word histogram real id the value word array list entry text text generate key val pairs object key the main driver word count map reduce program invoke method submit map reduce job when communication problems job tracker main string args throws io exception interrupted exception class not found exception job job value aggregator job create value aggregator job args job set jar by class aggregate word count ret job wait for completion true system exit ret
1114	mapreduce\src\examples\org\apache\hadoop\examples\BaileyBorweinPlouffe.java	unrelated	package org apache hadoop examples a map reduce program uses bailey borwein plouffe compute exact digits pi this program able calculate digit positions lower certain limit roughly if limit exceeded corresponding results may incorrect due overflow errors for computing higher bits pi consider using distbbp reference david h bailey peter b borwein simon plouffe on rapid computation various polylogarithmic constants math comp bailey borwein plouffe extends configured implements tool string description string name mapreduce bailey borwein plouffe get simple name custom job properties string working dir property name dir string hex file property name hex file string digit start property name digit start string digit size property name digit size string digit parts property name digit parts log log log factory get log bailey borwein plouffe mapper computing digits pi bbp mapper extends compute offset th offset length th digits protected map long writable offset int writable length reducer concatenating map outputs bbp reducer extends storing hex digits list byte hex new array list byte concatenate map outputs protected reduce long writable offset iterable bytes writable values write output files protected cleanup context context print elements nice format t print print stream iterator t iterator string builder sb new string builder n prefix length string spaces sb string print n prefix iterator next println input split link bbp input format bbp split extends input split implements writable string empty offset size public default constructor writable bbp split bbp split offset size get offset inherit doc get length no location needed string get locations inherit doc read fields data input throws io exception inherit doc write data output throws io exception input format link bbp mapper keys values represent offsets sizes respectively bbp input format inherit doc list input split get splits job context context inherit doc record reader long writable int writable create record reader create setup job job create job string name configuration conf job job new job conf name name configuration jobconf job get configuration job set jar by class bailey borwein plouffe setup mapper job set mapper class bbp mapper job set map output key class long writable job set map output value class bytes writable setup reducer job set reducer class bbp reducer job set output key class long writable job set output value class bytes writable job set num reduce tasks setup input job set input format class bbp input format disable task timeout jobconf set long mr job config task timeout use speculative execution jobconf set boolean mr job config map speculative false jobconf set boolean mr job config reduce speculative false return job run map reduce job compute pi compute start digit n digits n maps string name start digit n digits setup wroking directory println working directory working dir println file system fs file system get conf path dir fs make qualified new path working dir fs exists dir else fs mkdirs dir println start digit start digit println number digits n digits println number maps n maps setup job job job create job name conf path hexfile new path dir pi name hex
1115	mapreduce\src\examples\org\apache\hadoop\examples\DBCountPageView.java	unrelated	package org apache hadoop examples this demonstrative program uses db input format reading input data database db output format writing data database br the program first creates necessary tables populates input table runs mapred job br the input data mini access log code lt url referrer time gt code schema the output number pageviews url log schema code lt url pageview gt code when called arguments program starts local hsqldb server uses database storing retrieving data db count page view extends configured implements tool log log log factory get log db count page view connection connection boolean initialized false string access field names url referrer time string pageview field names url pageview string db url jdbc hsqldb hsql localhost url access string driver class org hsqldb jdbc driver server server start hsqldb server server new server server set database path server set database name url access server start create connection string driver class name class name driver class name connection driver manager get connection url connection set auto commit false shutdown try catch throwable ex finally initialize string driver class name string url throws exception initialized drop tables string drop access drop table access string drop pageview drop table pageview statement st null try catch sql exception ex create tables throws sql exception string create access string create pageview statement st connection create statement try finally populates access table generated records populate access throws sql exception prepared statement statement null try catch sql exception ex finally verifies results correct boolean verify throws sql exception check total num pageview string count access query select count from access string sum pageview query select sum pageview from pageview statement st null result set rs null try finally holds lt url referrer time gt tuple access record implements writable db writable string url string referrer time read fields data input throws io exception write data output throws io exception read fields result set result set throws sql exception write prepared statement statement throws sql exception holds lt url pageview gt tuple pageview record implements writable db writable string url pageview pageview record string url pageview read fields data input throws io exception write data output throws io exception read fields result set result set throws sql exception write prepared statement statement throws sql exception string string mapper extracts ur ls access record tuples db emits lt url gt pair access record pageview mapper extends long writable one new long writable l map long writable key access record value context context reducer sums pageviews emits pageview record correspond one tuple db pageview reducer extends null writable n null writable get reduce text key iterable long writable values usage db count page view driver class dburl run string args throws exception string driver class name driver class string url db url args length initialize driver class name url configuration conf get conf db configuration configure db conf driver class name url job job new job conf job set job name count pageviews ur ls job set jar by class db count page view job set mapper class pageview
1116	mapreduce\src\examples\org\apache\hadoop\examples\ExampleDriver.java	unrelated	package org apache hadoop examples a description example program based human readable description example driver main string argv
1117	mapreduce\src\examples\org\apache\hadoop\examples\Grep.java	unrelated	package org apache hadoop examples extracts matching regexs input files counts grep extends configured implements tool grep singleton run string args throws exception main string args throws exception
1118	mapreduce\src\examples\org\apache\hadoop\examples\Join.java	unrelated	package org apache hadoop examples given set sorted datasets keyed yielding equal partitions possible effect join datasets prior map the example facilitates to run bin hadoop jar build hadoop examples jar join r reduces format input format format output format key output key value output value join op lt inner outer gt dir dir dir join extends configured implements tool string reduces per host mapreduce join reduces per host print usage run string args throws exception main string args throws exception
1119	mapreduce\src\examples\org\apache\hadoop\examples\MultiFileWordCount.java	unrelated	package org apache hadoop examples multi file word count example demonstrate usage multi file input format this examples counts occurrences words text files given input directory multi file word count extends configured implements tool this record keeps lt filename offset gt pairs word offset implements writable comparable offset string file name read fields data input throws io exception write data output throws io exception compare to object boolean equals object obj hash code to use link combine file input format one extend return custom link record reader combine file input format uses link combine file split my input format extends combine file input format word offset text record reader word offset text create record reader input split split record reader responsible extracting records chunk combine file split combine file line record reader extends record reader word offset text start offset offset chunk end end chunk pos current pos file system fs path path word offset key text value fs data input stream file in line reader reader combine file line record reader combine file split split initialize input split split task attempt context context close throws io exception get progress throws io exception boolean next key value throws io exception word offset get current key text get current value throws io exception interrupted exception this mapper similar one link word count map class map class extends int writable one new int writable text word new text map word offset key text value context context print usage system println usage multifilewc input dir output run string args throws exception args length job job new job get conf job set job name multi file word count job set jar by class multi file word count set input format job input format job set input format class my input format keys words strings job set output key class text values counts ints job set output value class int writable use defined mapper job set mapper class map class use word count reducer job set combiner class int sum reducer job set reducer class int sum reducer file input format add input paths job args file output format set output path job new path args return job wait for completion true main string args throws exception ret tool runner run new multi file word count args system exit ret
1120	mapreduce\src\examples\org\apache\hadoop\examples\QuasiMonteCarlo.java	unrelated	package org apache hadoop examples a map reduce program estimates value pi using quasi monte carlo q mc method arbitrary integrals approximated numerically q mc methods in example use q mc method approximate integral i s f x dx s unit square x x x dimensional point f function describing inscribed circle square s f x x x f x otherwise it easy see pi equal i so approximation pi obtained i evaluated numerically there better methods computing pi we emphasize numerical approximation arbitrary integrals example for computing many digits pi consider using bbp the implementation discussed mapper generate points unit square count points inside outside inscribed circle square reducer accumulate points inside outside results mappers let num total num inside num outside the fraction num inside num total rational approximation value area circle area square i area inscribed circle pi area unit square finally estimated value pi num inside num total quasi monte carlo extends configured implements tool string description tmp directory input output path tmp dir new path dimensional halton sequence h h dimensional point index halton sequence used generate sample points pi estimation halton sequence bases p maximum number digits allowed k index x q initialize h startindex sequence begins h startindex halton sequence startindex compute next point assume current point h index compute h index next point mapper pi estimation generate points unit square count points inside outside inscribed circle square qmc mapper extends map method map long writable offset reducer pi estimation accumulate points inside outside results mappers qmc reducer extends num inside num outside accumulate number points inside outside results mappers reduce boolean writable inside reduce task done write output file cleanup context context throws io exception run map reduce job estimating pi big decimal estimate pi num maps num points job job new job conf setup job conf job set job name quasi monte carlo get simple name job set jar by class quasi monte carlo job set input format class sequence file input format job set output key class boolean writable job set output value class long writable job set output format class sequence file output format job set mapper class qmc mapper job set reducer class qmc reducer job set num reduce tasks turn speculative execution dfs handle multiple writers file job set speculative execution false setup input output directories path dir new path tmp dir path dir new path tmp dir file input format set input paths job dir file output format set output path job dir file system fs file system get conf fs exists tmp dir fs mkdirs dir try finally parse arguments runs map reduce job print output standard run string args throws exception args length n maps integer parse int args n samples long parse long args system println number maps n maps system println samples per map n samples system println estimated value pi return main method running stand alone command main string argv throws exception system exit tool runner run null new quasi monte carlo argv
1121	mapreduce\src\examples\org\apache\hadoop\examples\RandomTextWriter.java	unrelated	package org apache hadoop examples this program uses map reduce run distributed job interaction tasks task writes large unsorted random sequence words in order program generate data terasort words per key words per value following config xmp xml version xml stylesheet type text xsl href configuration xsl configuration property name mapreduce randomtextwriter minwordskey name value value property property name mapreduce randomtextwriter maxwordskey name value value property property name mapreduce randomtextwriter minwordsvalue name value value property property name mapreduce randomtextwriter maxwordsvalue name value value property property name mapreduce randomtextwriter totalbytes name value value property configuration xmp equivalently link random text writer also supports options ones supported link tool via command line to run bin hadoop jar hadoop version examples jar randomtextwriter format output format output random text writer extends configured implements tool string total bytes mapreduce randomtextwriter totalbytes string bytes per map mapreduce randomtextwriter bytespermap string maps per host mapreduce randomtextwriter mapsperhost string max value mapreduce randomtextwriter maxwordsvalue string min value mapreduce randomtextwriter minwordsvalue string min key mapreduce randomtextwriter minwordskey string max key mapreduce randomtextwriter maxwordskey print usage system println randomtextwriter tool runner print generic command usage system return user counters enum counters records written bytes written random text mapper extends mapper text text text text num bytes to write min words in key words in key range min words in value words in value range random random new random save configuration value need write data setup context context given output filename write bunch random records map text key text value text generate sentence words this main routine launching distributed random write job it runs maps node node writes gig data dfs file the reduce anything run string args throws exception args length configuration conf get conf job client client new job client conf cluster status cluster client get cluster status num maps per host conf get int maps per host num bytes to write per map conf get long bytes per map num bytes to write per map total bytes to write conf get long total bytes num maps total bytes to write num bytes to write per map num maps total bytes to write conf set int mr job config num maps num maps job job new job conf job set jar by class random text writer job set job name random text writer job set output key class text job set output value class text job set input format class random writer random input format job set mapper class random text mapper class extends output format output format class list string args new array list string args length job set output format class output format class file output format set output path job new path args get system println running num maps maps reducer none job set num reduce tasks date start time new date system println job started start time ret job wait for completion true date end time new date system println job ended end time system println the job took return ret main string args throws exception res tool runner run new configuration new random text writer
1122	mapreduce\src\examples\org\apache\hadoop\examples\RandomWriter.java	unrelated	package org apache hadoop examples this program uses map reduce run distributed job interaction tasks task write large unsorted random binary sequence file bytes writable in order program generate data terasort byte keys byte values following config xmp xml version xml stylesheet type text xsl href configuration xsl configuration property name mapreduce randomwriter minkey name value value property property name mapreduce randomwriter maxkey name value value property property name mapreduce randomwriter minvalue name value value property property name mapreduce randomwriter maxvalue name value value property property name mapreduce randomwriter totalbytes name value value property configuration xmp equivalently link random writer also supports options ones supported link generic options parser via command line random writer extends configured implements tool string total bytes mapreduce randomwriter totalbytes string bytes per map mapreduce randomwriter bytespermap string maps per host mapreduce randomwriter mapsperhost string max value mapreduce randomwriter maxvalue string min value mapreduce randomwriter minvalue string min key mapreduce randomwriter minkey string max key mapreduce randomwriter maxkey user counters enum counters records written bytes written a custom input format creates virtual inputs single map random input format extends input format text text generate requested number file splits filename set filename output file list input split get splits job context job throws io exception return single record filename filename taken file split random record reader extends record reader text text record reader text text create record reader input split split random mapper extends mapper writable comparable writable num bytes to write min key size key size range min value size value size range random random new random bytes writable random key new bytes writable bytes writable random value new bytes writable randomize bytes byte data offset length given output filename write bunch random records map writable comparable key save values configuaration need write data setup context context this main routine launching distributed random write job it runs maps node node writes gig data dfs file the reduce anything run string args throws exception args length path dir new path args configuration conf get conf job client client new job client conf cluster status cluster client get cluster status num maps per host conf get int maps per host num bytes to write per map conf get long bytes per map num bytes to write per map total bytes to write conf get long total bytes num maps total bytes to write num bytes to write per map num maps total bytes to write conf set int mr job config num maps num maps job job new job conf job set jar by class random writer job set job name random writer file output format set output path job dir job set output key class bytes writable job set output value class bytes writable job set input format class random input format job set mapper class random mapper job set reducer class reducer job set output format class sequence file output format system println running num maps maps reducer none job set num reduce tasks date start time new date system println job started start time ret job wait
1123	mapreduce\src\examples\org\apache\hadoop\examples\SecondarySort.java	unrelated	package org apache hadoop examples this example hadoop map reduce application it reads text input files must contain two integers per line the output sorted first second number grouped first number to run bin hadoop jar build hadoop examples jar secondarysort dir dir secondary sort define pair integers writable they serialized byte comparable format int pair first second set left right values set left right get first get second read two integers encoded min value min value max value read fields data input throws io exception write data output throws io exception hash code boolean equals object right a comparator compares serialized int pair comparator extends writable comparator register comparator compare to int pair partition based first part pair first partitioner extends partitioner int pair int writable get partition int pair key int writable value compare first part pair reduce called value first part first grouping comparator compare byte b byte b compare int pair int pair read two integers line generate key value pair left right right map class int pair key new int pair int writable value new int writable map long writable key text value a reducer emits sum input values reduce text separator text first new text reduce int pair key iterable int writable values main string args throws exception configuration conf new configuration string args new generic options parser conf args get remaining args args length job job new job conf secondary sort job set jar by class secondary sort job set mapper class map class job set reducer class reduce group partition first pair job set partitioner class first partitioner job set grouping comparator class first grouping comparator map output int pair int writable job set map output key class int pair job set map output value class int writable reduce output text int writable job set output key class text job set output value class int writable file input format add input path job new path args file output format set output path job new path args system exit job wait for completion true
1124	mapreduce\src\examples\org\apache\hadoop\examples\Sort.java	unrelated	package org apache hadoop examples this trivial map reduce program absolutely nothing use framework fragment sort input values to run bin hadoop jar build hadoop examples jar sort r reduces format input format format output format key output key value output value total order pcnt num samples max splits dir dir sort k v extends configured implements tool string reduces per host job job null print usage the main driver sort program invoke method submit map reduce job job tracker run string args throws exception main string args throws exception get last job run using instance job get result
1125	mapreduce\src\examples\org\apache\hadoop\examples\WordCount.java	unrelated	package org apache hadoop examples word count tokenizer mapper int writable one new int writable text word new text map object key text value context context int sum reducer int writable result new int writable reduce text key iterable int writable values main string args throws exception configuration conf new configuration string args new generic options parser conf args get remaining args args length job job new job conf word count job set jar by class word count job set mapper class tokenizer mapper job set combiner class int sum reducer job set reducer class int sum reducer job set output key class text job set output value class int writable file input format add input path job new path args file output format set output path job new path args system exit job wait for completion true
1126	mapreduce\src\examples\org\apache\hadoop\examples\dancing\DancingLinks.java	unrelated	package org apache hadoop examples dancing a generic solver tile laying problems using knuth dancing link algorithm it provides fast backtracking data structure problems expressed sparse boolean matrix goal select subset rows column exactly true the application gives column name row named set columns true solutions passed back giving selected rows names the type parameter column name application column names dancing links column name log log node column name column header column name extends node column name column header column name head list column header column name columns dancing links add column column name name boolean primary add column column name name get number columns string get column name index add row boolean values solution acceptor column name column header column name find best column cover column column header column name col uncover column column header column name col list column name get row name node column name row search list node column name partial solution acceptor column name output search prefixes depth choices list split depth node column name advance goal row rollback node column name row solve prefix solution acceptor column name output solve solution acceptor column name output
1127	mapreduce\src\examples\org\apache\hadoop\examples\dancing\DistributedPentomino.java	unrelated	package org apache hadoop examples dancing launch distributed pentomino solver it generates complete list prefixes length n unique prefix separate line a prefix sequence n integers denote index row choosen column order note next column heuristically choosen solver dependant previous choice that file given input map reduce the output key value move prefix solution text text distributed pentomino extends configured implements tool pent depth pent width pent height default maps each map takes line represents prefix move finds solutions start prefix the output prefix key solution value pent map extends width height depth pentomino pent text prefix string context context for solution generate prefix representation solution the solution starts newline output looks like prefix solution solution catcher implements dancing links solution acceptor pentomino column name break prefix moves sequence integer row ids selected column order find solutions prefix map writable comparable key text value context context setup context context create input file possible combinations given depth create input directory file system fs fs mkdirs dir list splits pent get splits depth path input new path dir part print stream file prefix splits file close return fs get file status input get len launch solver x board one sided pentominos this takes hours nodes cpus node splits job maps reduce main string args throws exception res tool runner run new configuration system exit res run string args throws exception args length configuration conf get conf width conf get int pentomino width pent width height conf get int pentomino height pent height depth conf get int pentomino depth pent depth class extends pentomino pent class conf get class pentomino class num maps conf get int mr job config num maps default maps path output new path args path input new path output input file system file sys file system get conf try
1128	mapreduce\src\examples\org\apache\hadoop\examples\dancing\OneSidedPentomino.java	unrelated	package org apache hadoop examples dancing of normal pentominos distinct shapes flipped this includes variants flippable shapes unflippable shapes total pieces clearly boards must boxes hold solutions one sided pentomino extends pentomino one sided pentomino one sided pentomino width height protected initialize pieces main string args
1129	mapreduce\src\examples\org\apache\hadoop\examples\dancing\Pentomino.java	unrelated	package org apache hadoop examples dancing pentomino string depth mapreduce pentomino depth string width mapreduce pentomino width string height mapreduce pentomino height string class mapreduce pentomino this marker types i expect get back column names protected column name nothing maintain information puzzle piece protected piece implements column name string name boolean shape rotations boolean flippable piece string name string shape string get name get rotations boolean get flippable flip boolean flip x max boolean get shape boolean flip rotate a point puzzle board this represents placement piece given point board point implements column name x point x convert solution puzzle returned model represents placement pieces onto board string stringify solution width height string picture new string height width string buffer result new string buffer piece placement list column name row solution put together picture length return result string enum solution category upper left mid x mid y center find whether solution x upper left quadrant x midline midline center solution category get category list list column name names piece x piece null find x piece piece p pieces find row containing x list column name row names return solution category upper left a solution printer writes solution stdout solution printer width height solution printer width height solution list list column name names protected width protected height protected list piece pieces new array list piece is piece fixed rotation protected one rotation new is piece identical rotated degrees protected two rotations new are rotations unique protected four rotations new fill pieces list protected initialize pieces pieces add new piece x x xxx x false one rotation pieces add new piece v x x xxx false four rotations pieces add new piece xxx x x false four rotations pieces add new piece w x xx xx false four rotations pieces add new piece u x x xxx false four rotations pieces add new piece xxxxx false two rotations pieces add new piece f xx xx x true four rotations pieces add new piece p xx xx x true four rotations pieces add new piece z xx x xx true two rotations pieces add new piece n xx xxx true four rotations pieces add new piece x xxxx true four rotations pieces add new piece x xxxx true four rotations is middle piece upper left side board given offset size piece this checks one dimension boolean side offset shape size board return offset shape size board for given piece generate potential placements add rows model used single piece eliminate trivial roations solution generate rows dancing links dancer rotation rotations piece get rotations rot index rot index rotations length rot index dancing links column name dancer new dancing links column name dancing links solution acceptor column name printer initialize pieces create model given pentomino set pieces board size pentomino width height initialize width height create object without initialization pentomino initialize width height width width height height height piece base dancer get number columns piece p pieces boolean row new boolean dancer get number columns idx idx pieces size idx printer new solution printer width height generate
1130	mapreduce\src\examples\org\apache\hadoop\examples\dancing\Sudoku.java	unrelated	package org apache hadoop examples dancing this uses dancing links algorithm knuth solve sudoku puzzles it solved x puzzles seconds sudoku board size square x size square y size protected column name string stringify solution size list list column name solution solution printer implements dancing links solution acceptor column name sudoku input stream stream throws io exception column constraint implements column name row constraint implements column name square constraint implements column name cell constraint implements column name boolean generate row boolean row values x num dancing links column name make model solve main string args throws io exception
1131	mapreduce\src\examples\org\apache\hadoop\examples\pi\Combinable.java	unrelated	package org apache hadoop examples pi a combinable object combined objects combinable t extends comparable t t combine t
1132	mapreduce\src\examples\org\apache\hadoop\examples\pi\Container.java	unrelated	package org apache hadoop examples pi a container contains element container t t get element
1133	mapreduce\src\examples\org\apache\hadoop\examples\pi\DistBbp.java	unrelated	package org apache hadoop examples pi a map reduce program uses bbp type method compute exact binary digits pi this program designed computing n th bit pi large n say n for computing lower bits pi consider using bbp the actually computation done dist sum jobs the steps launching jobs initialize parameters create list sums read computed values given local directory remove computed values sums partition remaining sums computation jobs submit computation jobs cluster wait results write job outputs given local directory combine job outputs print pi bits the command line format hadoop org apache hadoop examples pi dist bbp b n threads n jobs type n part remote dir local dir and parameters b the number bits skip e compute b th position n threads the number working threads n jobs the number jobs per sum type map side job r reduce side job x mix type n part the number parts per job remote dir remote directory submitting jobs local dir local directory storing output files note may take time finish jobs b large if program killed middle execution command different remote dir used resume execution for example suppose use following command compute th bit pi hadoop org apache hadoop examples pi dist bbp x remote local output it uses threads summit jobs concurrent jobs each sum totally sums partitioned jobs the jobs executed map side reduce side each job parts the remote directory jobs remote local directory storing output local output depends cluster configuration may take many days finish entire execution if execution killed may resume hadoop org apache hadoop examples pi dist bbp x remote b local output dist bbp extends configured implements tool string description util timer timer new util timer true inherit doc run string args throws exception execute dist sum computations execute dist sum distsum main main string args throws exception
1134	mapreduce\src\examples\org\apache\hadoop\examples\pi\DistSum.java	unrelated	package org apache hadoop examples pi the main computing sums using map reduce jobs a sum partitioned jobs a job may executed map side reduce side a map side job multiple maps zero reducer a reduce side job one map multiple reducers depending clusters status runtime mix type job may executed either side dist sum extends configured implements tool log log log factory get log dist sum string name dist sum get simple name string n parts mapreduce pi name n parts dist sum job parameters parameters abstract machine job execution machine map side extends machine reduce side extends machine mix machine extends machine util timer timer new util timer true parameters parameters get parameters parameters get parameters return parameters set parameters set parameters parameters p parameters p create job job create job string name summation sigma throws io exception start job compute sigma compute string name summation sigma throws io exception convert task result string string task result string name task result result convert string string task result pair map entry string task result task result string callable computation computation implements callable computation partition sigma execute computations summation execute string name summation sigma inherit doc run string args throws exception main main string args throws exception
1135	mapreduce\src\examples\org\apache\hadoop\examples\pi\Parser.java	unrelated	package org apache hadoop examples pi a parsing outputs parser string verbose property pi parser verbose boolean verbose parser boolean verbose println string parse line parse line string line map parameter list task result log info line line parse file directory tree parse file f map parameter list task result sums throws io exception parse path map parameter list task result parse string f throws io exception parse input write results map parameter list task result parse string inputpath string outputdir combine results t extends combinable t map parameter t combine map parameter list t main main string args throws io exception
1136	mapreduce\src\examples\org\apache\hadoop\examples\pi\SummationWritable.java	unrelated	package org apache hadoop examples pi a writable summation summation writable implements writable comparable summation writable container summation summation sigma summation writable summation writable summation sigma sigma sigma inherit doc string string return get class get simple name sigma inherit doc summation get element return sigma read sigma conf summation read class clazz configuration conf return summation value of conf get clazz get simple name sigma write sigma conf write summation sigma class clazz configuration conf conf set clazz get simple name sigma sigma string read summation data input summation read data input throws io exception summation writable new summation writable read fields return get element inherit doc read fields data input throws io exception arithmetic progression n arithmetic progression writable read arithmetic progression e arithmetic progression writable read sigma new summation n e read boolean write sigma data output write summation sigma data output throws io exception arithmetic progression writable write sigma n arithmetic progression writable write sigma e double v sigma get value v null else inherit doc write data output throws io exception write sigma inherit doc compare to summation writable return sigma compare to sigma inherit doc boolean equals object obj obj else obj null obj instanceof summation writable throw new illegal argument exception obj null obj null not supported hash code throw new unsupported operation exception a writable arithmetic progression arithmetic progression writable read arithmetic progression data input arithmetic progression read data input throws io exception write arithmetic progression data output write arithmetic progression ap data output
1137	mapreduce\src\examples\org\apache\hadoop\examples\pi\TaskResult.java	unrelated	package org apache hadoop examples pi a map task results reduce task results task result implements container summation combinable task result writable summation sigma duration task result task result summation sigma duration inherit doc summation get element return sigma get duration return duration inherit doc compare to task result inherit doc boolean equals object obj not supported hash code inherit doc task result combine task result inherit doc read fields data input throws io exception inherit doc write data output throws io exception inherit doc string string covert string task result task result value of string
1138	mapreduce\src\examples\org\apache\hadoop\examples\pi\Util.java	pooling	package org apache hadoop examples pi utility methods util output stream print stream system error stream print stream err system timer timer boolean accumulative start system current time millis previous start timer constructor timer boolean accumulative same tick null tick return tick null tick synchronized tick string covert milliseconds string string millis string n n else n string builder b new string builder millis n l millis n b insert string format n l n b insert string format n l n b insert n l days n l b insert days day days insert days n l return b string covert string this support comma separated number format string return long parse long trim replace covert string comma separated number format string n n string builder b new string builder n n n return n b string parse variable parse long variable string name string return parse string variable name parse variable string parse string variable string name string starts with name return substring name length execute callables number threads t e extends callable t execute n threads list e callables executor service executor executors new fixed thread pool n threads list future t futures executor invoke all callables future t f futures print usage messages print usage string args string usage err println args arrays list args err println err println usage java usage err println tool runner print generic command usage err return combine list items t extends combinable t list t combine collection t items list t sorted new array list t items sorted size collections sort sorted list t combined new array list t items size t prev sorted get sorted size combined add prev return combined check local directory check directory file dir dir exists dir directory simple date format date format new simple date format yyyy m mdd h hmmss sss create writer local file print writer create writer file dir string prefix throws io exception check directory dir print bits skipped message print bit skipped b println println b b convert pi value string string pi pi terms value pi l double precision acc bit accuracy terms false acc hex acc bit shift double precision acc bit return string format acc hex x acc hex x hex digits double precision mantissa size macheps exponent double precision estimate accuracy accuracy terms boolean print error terms math log terms math log bits macheps exponent math ceil error print return bits bits string job separation property pi job separation seconds semaphore job semaphore new semaphore run job run job string name job job machine machine string startmessage util timer timer job semaphore acquire uninterruptibly long starttime null try catch exception e finally read job outputs list task result read job outputs file system fs path outdir throws io exception list task result results new array list task result file status status fs list status outdir results empty return results write results write results string name list task result results file system fs string dir throws io exception path outfile new path dir name txt util println name
1139	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\ArithmeticProgression.java	unrelated	package org apache hadoop examples pi math an arithmetic progression arithmetic progression implements comparable arithmetic progression a symbol char symbol starting value value difference terms delta ending value limit constructor arithmetic progression char symbol value delta limit inherit doc boolean equals object obj not supported hash code inherit doc compare to arithmetic progression does contain boolean contains arithmetic progression skip steps skip steps get number steps get steps inherit doc string string convert string arithmetic progression arithmetic progression value of string
1140	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Bellard.java	unrelated	package org apache hadoop examples pi math bellard bbp type pi formula sum n infty n n n n n n n n n references david h bailey peter b borwein simon plouffe on rapid computation various polylogarithmic constants math comp fabrice bellard a new formula compute n th binary digit pi available http fabrice bellard free fr pi bellard parameters sums enum parameter sum k infty k k k k k p false p false p p p p sum k infty k k k k k k k k k k k p true p false p false p false p true p p p p p p p p p p boolean isplus j delta n delta e offset e parameter boolean isplus j delta n offset e parameter parameter p get parameter represented string parameter get string the sums bellard formula sum implements container summation iterable summation accuracy bit parameter parameter summation sigma summation parts tail tail constructor t extends container summation sum b parameter p n parts list t existing t extends container summation summation partition inherit doc string string set value sigma set value summation get value sigma get value inherit doc summation get element the sum tail tail inherit doc iterator summation iterator get sums bellard formula t extends container summation map parameter sum get sums map parameter sum sums new tree map parameter sum parameter p parameter values return sums compute bits pi results t extends container summation compute pi results size parameter values length pi parameter p parameter values return pi compute bits pi local machine compute pi b pi parameter p parameter values return pi estimate number terms bit terms b return b compute pi util timer b tick util pi compute pi b bit terms b main main string args throws io exception util timer new util timer false compute pi compute pi compute pi compute pi compute pi util print bit skipped compute pi compute pi b
1141	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\LongLong.java	unrelated	package org apache hadoop examples pi math support bit integer arithmetic long long bits per long mid bits per long size bits per long full mask l bits per long lower mask full mask mid upper mask lower mask mid set values long long set and operation mask shift right operation shift right n plus equal operation long long plus equal long long convert big integer big integer big integer inherit doc string string compute b store result r long long multiplication long long r b
1142	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Modular.java	unrelated	package org apache hadoop examples pi math modular arithmetics modular max sqrt long math sqrt long max value compute e mod n mod e n given x return x mod add mod x given x return x mod mod inverse x
1143	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Montgomery.java	unrelated	package org apache hadoop examples pi math montgomery method references richard crandall carl pomerance prime numbers a computational perspective springer verlag peter montgomery modular multiplication without trial division math comp montgomery protected product product new product protected n protected n i n protected r protected r r protected set modular initialize object montgomery set n n n n r long highest one bit n n i r modular mod inverse n r r r long number of trailing zeros r return compute mod n n odd mod p r n x p x n x n mask long highest one bit mask mask return product p product long long x new long long long long x n i new long long long long n new long long c
1144	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Summation.java	unrelated	package org apache hadoop examples pi math represent summation sum frac e mod n n summation implements container summation combinable summation variable n summation arithmetic progression n variable e summation arithmetic progression e double value null constructor summation arithmetic progression n arithmetic progression e constructor summation value n delta n constructor summation value n delta n limit n inherit doc summation get element return return number steps summation get steps return e get steps return value summation double get value return value set value summation set value v value v inherit doc string string inherit doc boolean equals object obj not supported hash code covert string summation summation value of string compute value summation compute max modular l compute value using link modular mod compute modular montgomery montgomery new montgomery compute value using link montgomery mod compute montgomery inherit doc compare to summation inherit doc summation combine summation find remaining terms t extends container summation list summation remaining terms list t sorted does contains boolean contains summation partition summation summation partition n parts
1145	mapreduce\src\examples\org\apache\hadoop\examples\terasort\GenSort.java	unrelated	package org apache hadoop examples terasort a single process data generator terasort data based gensort c version mar chris nyberg chris nyberg ordinal com gen sort generate binary record suitable sort benchmarks except penny sort generate record byte rec buf unsigned rand big integer make big integer x big integer ninety five new big integer generate ascii record suitable sort benchmarks including penny sort generate ascii record byte rec buf unsigned rand usage output records output stream main string args throws exception
1146	mapreduce\src\examples\org\apache\hadoop\examples\terasort\Random16.java	unrelated	package org apache hadoop examples terasort this implements bit linear congruential generator specifically x recently issued bit random number seed random number already generated next number generated x equal x x c mod x ed fc da df fccf c x the coefficient suggested pierre l ecuyer tables linear congruential generators different sizes good lattice structure mathematics computation pp http www ams org mcom s s pdf the constant c meets simple suggestion reference odd there also facility quickly advancing state generator fixed number steps facilitates parallel generation this based rand c chris nyberg chris nyberg ordinal com random random constant random constant gen array new random constant unsigned skip ahead unsigned advance next rand unsigned rand
1147	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraChecksum.java	unrelated	package org apache hadoop examples terasort tera checksum extends configured implements tool checksum mapper unsigned checksum new unsigned unsigned sum new unsigned checksum crc new pure java crc map text key text value cleanup context context checksum reducer reduce null writable key iterable unsigned values usage throws io exception system err println terasum dir report dir run string args throws exception job job job get instance new cluster get conf get conf args length tera input format set input paths job new path args file output format set output path job new path args job set job name tera sum job set jar by class tera checksum job set mapper class checksum mapper job set reducer class checksum reducer job set output key class null writable job set output value class unsigned force single reducer job set num reduce tasks job set input format class tera input format return job wait for completion true main string args throws exception res tool runner run new configuration new tera checksum args system exit res
1148	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraGen.java	unrelated	package org apache hadoop examples terasort generate official gray sort input data set the user specifies number rows output directory runs map reduce program generate data the format data ul li bytes key constant bytes bytes rowid constant bytes bytes filler constant bytes li the rowid right justified row id hex number ul p to run program b bin hadoop jar hadoop examples jar teragen dir b tera gen extends configured implements tool log log log factory get log tera sort enum counters checksum string num rows mapreduce terasort num rows range input format get number of rows job context job set number of rows job job num rows sort gen mapper usage throws io exception parse human long string str run string args main string args throws exception
1149	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraInputFormat.java	scheduler	package org apache hadoop examples terasort an input format reads first characters line key rest line value both key value represented text tera input format extends file input format text text string partition filename partition lst string num partitions mapreduce terasort num partitions string sample size mapreduce terasort partitions sample key length value length record length key length value length mr job config last context null list input split last result null tera file split extends file split string locations tera file split tera file split path file start length string hosts protected set locations string hosts string get locations string string text sampler implements indexed sortable array list text records new array list text compare j swap j add key text key find split points given sample the sample keys sorted sampled find even split points partitions the returned keys start respective partitions text create partitions num partitions use input splits take samples input generate sample keys by default reads keys locations input sorts picks n keys generate n equally sized partitions write partition file job context job system current time millis configuration conf job get configuration tera input format format new tera input format text sampler sampler new text sampler partitions job get num reduce tasks sample size conf get long sample size list input split splits format get splits job system current time millis system println computing input splits took ms samples math min conf get int num partitions splits size system println sampling samples splits splits size records per sample sample size samples sample step splits size samples thread sampler reader new thread samples take n samples different parts input samples file system fs part file get file system conf data output stream writer fs create part file true short samples text split sampler create partitions partitions writer close system current time millis system println computing parititions took ms tera record reader extends record reader text text fs data input stream offset length record length key length value length byte buffer new byte record length text key text value tera record reader throws io exception initialize input split split task attempt context context close throws io exception text get current key text get current value get progress throws io exception boolean next key value throws io exception record reader text text return new tera record reader protected file split make split path file start length return new tera file split file start length hosts list input split get splits job context job throws io exception job last context system current time millis last context job last result super get splits job system current time millis system println spent ms computing base splits job get configuration get boolean tera scheduler use true return last result
1150	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraOutputFormat.java	unrelated	package org apache hadoop examples terasort an output format writes key value appended together tera output format extends file output format text text string final sync attribute mapreduce terasort sync output committer committer null set requirement sync stream closed set final sync job context job boolean new value job get configuration set boolean final sync attribute new value does user want sync close boolean get final sync job context job return job get configuration get boolean final sync attribute false tera record writer extends record writer text text boolean sync false fs data output stream tera record writer fs data output stream synchronized write text key close task attempt context context throws io exception check output specs job context job ensure output directory set path dir get output path job dir null record writer text text get record writer task attempt context job path file get default work file job file system fs file get file system job get configuration fs data output stream file out fs create file return new tera record writer file out job output committer get output committer task attempt context context committer null return committer tera output committer extends file output committer tera output committer path output path task attempt context context commit job job context job context setup job job context job context setup task task attempt context task context
1151	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraScheduler.java	scheduler	package org apache hadoop examples terasort tera scheduler string use mapreduce terasort use terascheduler log log log factory get log tera scheduler split splits list host hosts new array list host slots per host remaining splits file split real splits null split string filename boolean assigned false list host locations new array list host split string filename string string host string hostname list split splits new array list split host string hostname string string list string read file string filename throws io exception list string result new array list string buffered reader new buffered reader new file reader filename string line read line line null close return result tera scheduler string split filename slots per host get hosts map string host host ids new hash map string host string host name read file node filename read blocks list string split lines read file split filename splits new split split lines size remaining splits string line split lines tera scheduler file split real splits real splits real splits slots per host conf get int tt config tt map slots map string host host table new hash map string host splits new split real splits length file split real split real splits host pick best host host result null splits integer max value host host hosts result null return result pick best splits host host tasks to pick math min slots per host split best new split tasks to pick split cur host splits chosen blocks remove locations tasks to pick non chosen blocks remove host split cur host splits solve throws io exception host host pick best host host null solve schedule modify file split array reflect new schedule it move placed splits front unplacable splits end best host host list input split get new file splits throws io exception solve file split result new file split real splits length left right real splits length splits length list input split ret new array list input split file split fs result return ret main string args throws io exception tera scheduler problem new tera scheduler block loc txt nodes host host problem hosts log info starting solve problem solve list split left overs new array list split problem splits length split cur left overs system println left left overs size log info done
1152	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraSort.java	unrelated	package org apache hadoop examples terasort generates sampled split points launches job waits finish p to run program b bin hadoop jar hadoop examples jar terasort dir dir b tera sort extends configured implements tool log log log factory get log tera sort string simple partitioner mapreduce terasort simplepartitioner string output replication mapreduce terasort output replication a partitioner splits text keys roughly equal partitions global sorted order total order partitioner extends partitioner text text trie node trie text split points configuration conf a generic trie node trie node an inner trie node contains children based next character inner trie node extends trie node a leaf trie node compares figure given key belongs lower upper leaf trie node extends trie node read cut points given sequence file text read partitions file system fs path p given sorted set cut points build trie find correct partition quickly trie node build trie text splits lower upper set conf configuration conf configuration get conf total order partitioner get partition text key text value num partitions a total order partitioner assigns keys based first prefix length bytes assuming flat distribution simple partitioner extends partitioner text text prefixes per reduce prefix length configuration conf null set conf configuration conf configuration get conf get partition text key text value num partitions boolean get use simple partitioner job context job return job get configuration get boolean simple partitioner false set use simple partitioner job job boolean value job get configuration set boolean simple partitioner value get output replication job context job return job get configuration get int output replication set output replication job job value job get configuration set int output replication value run string args throws exception log info starting job job job get instance new cluster get conf get conf path input dir new path args path output dir new path args boolean use simple partitioner get use simple partitioner job tera input format set input paths job input dir file output format set output path job output dir job set job name tera sort job set jar by class tera sort job set output key class text job set output value class text job set input format class tera input format job set output format class tera output format use simple partitioner else job get configuration set int dfs replication get output replication job tera output format set final sync job true ret job wait for completion true log info done return ret main string args throws exception res tool runner run new configuration new tera sort args system exit res
1153	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraValidate.java	unrelated	package org apache hadoop examples terasort generate mapper per file checks make sure keys sorted within file the mapper also generates file begin first key file end last key the reduce verifies start end items order any output reduce problem report p to run program b bin hadoop jar hadoop examples jar teravalidate dir report dir b p if output something wrong output reduce problem report tera validate extends configured implements tool text error new text error text checksum new text checksum string textify bytes text bytes writable b new bytes writable b set get bytes get length return b string validate mapper extends mapper text text text text text last key string filename unsigned checksum new unsigned unsigned tmp new unsigned checksum crc new pure java crc get part input name string get filename file split split map text key text value context context cleanup context context check boundaries output files making sure boundary keys always increasing also passes error reports along intact validate reducer extends reducer text text text text boolean first key true text last key new text text last value new text reduce text key iterable text values usage throws io exception system err println teravalidate dir report dir run string args throws exception job job job get instance new cluster get conf get conf args length tera input format set input paths job new path args file output format set output path job new path args job set job name tera validate job set jar by class tera validate job set mapper class validate mapper job set reducer class validate reducer job set output key class text job set output value class text force single reducer job set num reduce tasks force single split file input format set min input split size job long max value job set input format class tera input format return job wait for completion true main string args throws exception res tool runner run new configuration new tera validate args system exit res
1154	mapreduce\src\examples\org\apache\hadoop\examples\terasort\Unsigned16.java	unrelated	package org apache hadoop examples terasort an unsigned byte integer supports addition multiplication left shifts unsigned implements writable hi lo unsigned unsigned unsigned unsigned boolean equals object hash code unsigned string throws number format exception set string throws number format exception set get hex digit char ch throws number format exception unsigned ten new unsigned unsigned decimal string throws number format exception string string byte get byte b char get hex digit p get high get low multiply unsigned b add unsigned b shift left bits read fields data input throws io exception write data output throws io exception
1155	mapreduce\src\java\org\apache\hadoop\filecache\DistributedCache.java	unrelated	package org apache hadoop filecache distribute application specific large read files efficiently p code distributed cache code facility provided map reduce framework cache files text archives jars etc needed applications p p applications specify files via urls hdfs http cached via link org apache hadoop mapred job conf the code distributed cache code assumes files specified via urls already present link file system path specified url accessible every machine cluster p p the framework copy necessary files slave node tasks job executed node its efficiency stems fact files copied per job ability cache archives un archived slaves p p code distributed cache code used distribute simple read data text files complex types archives jars etc archives zip tar tgz tar gz files un archived slave nodes jars may optionally added classpath tasks rudimentary software distribution mechanism files execution permissions optionally users also direct symlink distributed cache file working directory task p p code distributed cache code tracks modification timestamps cache files clearly cache files modified application externally job executing p p here illustrative example use code distributed cache code p p blockquote pre setting cache application copy requisite files code file system code bin hadoop fs copy from local lookup dat myapp lookup dat bin hadoop fs copy from local map zip myapp map zip bin hadoop fs copy from local mylib jar myapp mylib jar bin hadoop fs copy from local mytar tar myapp mytar tar bin hadoop fs copy from local mytgz tgz myapp mytgz tgz bin hadoop fs copy from local mytargz tar gz myapp mytargz tar gz setup application code job conf code job conf job new job conf distributed cache add cache file new uri myapp lookup dat lookup dat job distributed cache add cache archive new uri myapp map zip job distributed cache add file to class path new path myapp mylib jar job distributed cache add cache archive new uri myapp mytar tar job distributed cache add cache archive new uri myapp mytgz tgz job distributed cache add cache archive new uri myapp mytargz tar gz job use cached files link org apache hadoop mapred mapper link org apache hadoop mapred reducer map class extends map reduce base implements mapper lt k v k v gt path local archives path local files configure job conf job get cached archives files local archives distributed cache get local cache archives job local files distributed cache get local cache files job map k key v value output collector lt k v gt output reporter reporter throws io exception use data cached archives files output collect k v pre blockquote p it also common use distributed cache using link org apache hadoop util generic options parser this includes methods used users specifically mentioned example well link distributed cache add archive to class path path configuration well methods intended use map reduce framework e g link org apache hadoop mapred job client distributed cache extends
1156	mapreduce\src\java\org\apache\hadoop\filecache\package-info.java	unrelated	package org apache hadoop filecache
1157	mapreduce\src\java\org\apache\hadoop\mapred\ACLsManager.java	audit	package org apache hadoop mapred manages map reduce cluster administrators access checks job level operations queue level operations uses job ac ls manager access checks job level operations queue manager queue operations ac ls manager log log log factory get log ac ls manager mr owner user started mapreduce cluster ugi user group information mr owner mapreduce cluster administrators access control list admin acl job ac ls manager job ac ls manager queue manager queue manager boolean acls enabled ac ls manager configuration conf job ac ls manager job ac ls manager user group information get mr owner access control list get admins acl job ac ls manager get job ac ls manager is calling user admin mapreduce cluster e either cluster owner cluster administrator boolean mr admin user group information caller ugi check ac ls user passed operation ul li if ac ls disabled allow users li li otherwise operation job operation eg submit job queue allow cluster owner started cluster b cluster administrators c members queue submit job acl queue li li if operation job operation allow job owner b cluster owner started cluster c cluster administrators members queue admins acl queue e members job acl job operation li ul check access job in progress job user group information caller ugi check ac ls user passed job operation ul li if ac ls disabled allow users li li otherwise allow job owner b cluster owner started cluster c cluster administrators members job acl job operation li ul check access job status job status user group information caller ugi check ac ls user passed operation ul li if ac ls disabled allow users li li otherwise operation job operation eg submit job queue allow cluster owner started cluster b cluster administrators c members queue submit job acl queue li li if operation job operation allow job owner b cluster owner started cluster c cluster administrators members queue admins acl queue e members job acl job operation li ul job operation check access string job id user group information caller ugi
1158	mapreduce\src\java\org\apache\hadoop\mapred\AdminOperationsProtocol.java	unrelated	package org apache hadoop mapred protocol admin operations this framework not to be used by users directly admin operations protocol extends versioned protocol version initial version added refresh queue acls version added node refresh facility version changed refresh queue acls refresh queues version id l refresh queues used jobtracker scheduler access control lists queue states refreshed refresh queues throws io exception refresh node list link job tracker refresh nodes throws io exception
1159	mapreduce\src\java\org\apache\hadoop\mapred\AuditLogger.java	audit	package org apache hadoop mapred manages map reduce audit logs audit logs provides information authorization authentication events success failure audit log format written key value pairs audit logger log log log factory get log audit logger enum keys user operation target result ip permissions constants string success success string failure failure string key val separator char pair separator some constants used others using audit logger some commonly used targets string jobtracker job tracker some commonly used operations string refresh queue refresh queue string refresh nodes refresh nodes some commonly used descriptions string unauthorized user unauthorized user a helper api creating audit log successful event this factored testing purpose string create success log string user string operation string target string builder b new string builder start keys user user b add remote ip b add keys operation operation b add keys target target b add keys result constants success b return b string create readable parseable audit log successful event commonly operated targets jobs job tracker queues etc br br note link audit logger uses tabs key val delimiter hence value fields contains tabs log success string user string operation string target log info enabled a helper api creating audit log failure event this factored testing purpose string create failure log string user string operation string perm string builder b new string builder start keys user user b add remote ip b add keys operation operation b add keys target target b add keys result constants failure b add keys description description b add keys permissions perm b return b string create readable parseable audit log failed event commonly operated targets jobs job tracker queues etc failed br br note link audit logger uses tabs key val delimiter hence value fields contains tabs log failure string user string operation string perm log warn enabled a helper api add remote ip address add remote ip string builder b inet address ip server get remote ip ip address null testcases ip null adds first key val pair passed builder following format key value start keys key string value string builder b b append key name append constants key val separator append value appends key val pair passed builder following format pair delim key value add keys key string value string builder b b append constants pair separator append key name append constants key val separator append value
1160	mapreduce\src\java\org\apache\hadoop\mapred\BackupStore.java	unrelated	package org apache hadoop mapred code backup store code utility used support mark reset functionality values iterator p it two caches memory cache file cache values stored iterated mark on reset values retrieved caches framework moves memory cache file cache memory cache becomes full backup store k v log log log factory get log backup store get name max vint size eof marker size max vint size task attempt id tid memory cache mem cache file cache file cache list segment k v segment list new linked list segment k v read segment index first segment offset current kv offset next kv offset data input buffer current key null data input buffer current value new data input buffer data input buffer current disk value new data input buffer boolean more false boolean reset false boolean clear mark flag false boolean last segment eof false backup store configuration conf task attempt id taskid throws io exception write data input buffer key data input buffer value throws io exception mark throws io exception reset throws io exception boolean next throws io exception next throws io exception data input buffer next value data input buffer next key reinitialize throws io exception exit reset mode throws io exception for writing first key value bytes directly data output stream get output stream length throws io exception this method called value iterators writing first update counters length clear mark throws io exception clear segment list throws io exception memory cache file cache backup ram manager implements ram manager
1161	mapreduce\src\java\org\apache\hadoop\mapred\BasicTypeSorterBase.java	unrelated	package org apache hadoop mapred this implements sort using primitive arrays data structures called basic type sorter base basic type sorter base implements buffer sorter protected output buffer key val buffer buffer used storing protected start offsets array used store start offsets protected key lengths array used store lengths protected value lengths array used store value lengths protected pointers array start offsets indices this protected raw comparator comparator comparator map output protected count number key values overhead arrays memory keyoffsets keylengths value lengths indices start offsets array pointers array ignored partpointers list buffered key val overhead initial array size maintain max lengths key val encounter during iteration sorted results create data output buffer return keys the max size data output buffer max keylength encounter expose value model memory accurately max key length max val length reference progressable object sending keep alive protected progressable reporter implementation methods sorter base configure job conf conf set progressable progressable reporter add key value record offset key length val length set input buffer output buffer buffer get memory utilized raw key value iterator sort close grow grow old new length basic type sorter base implementation methods raw key value iterator these methods must invoked iterate key vals sort done mr sort result iterator implements raw key value iterator count pointers start offsets key lengths val lengths curr start offset index curr index in pointers output buffer key val buffer data output buffer key new data output buffer in mem uncompressed bytes value new in mem uncompressed bytes mr sort result iterator output buffer key val buffer progress get progress data output buffer get key throws io exception value bytes get value throws io exception boolean next throws io exception close an implementation value bytes memory value buffers in mem uncompressed bytes implements value bytes in mem uncompressed bytes mr sort result iterator
1162	mapreduce\src\java\org\apache\hadoop\mapred\BufferSorter.java	unrelated	package org apache hadoop mapred this provides generic sort implemented specific sort algorithms the use case following a user writes key value records buffer finally wants sort buffer this defines methods user update implementation offsets records lengths keys values the user gives reference buffer latter wishes sort records written buffer far typically user decides point sort happen based memory consumed far buffer data structures maintained implementation that method provided get memory consumed far datastructures implementation buffer sorter extends job configurable pass progressable object sort call progress sorting set progressable progressable reporter when key value added particular offset key value buffer add key value recordoffset key length val length the user invokes method set buffer specific set input buffer output buffer buffer the framework invokes method get memory consumed far get memory utilized framework decides actually sort raw key value iterator sort framework invokes signal sorter cleanup close
1163	mapreduce\src\java\org\apache\hadoop\mapred\Child.java	unrelated	package org apache hadoop mapred the main child processes child log log volatile task attempt id taskid null volatile boolean cleanup main string args throws throwable
1164	mapreduce\src\java\org\apache\hadoop\mapred\CleanupQueue.java	unrelated	package org apache hadoop mapred cleanup queue log log log factory get log cleanup queue path cleanup thread cleanup thread create singleton path clean queue it used delete paths directories files separate thread this constructor creates clean thread also starts daemon callers instantiate one cleanup queue per jvm use deleting paths use link cleanup queue add to queue path deletion context add paths deletion cleanup queue synchronized path cleanup thread contains info related path file dir deleted path deletion context string full path full path file dir file system fs path deletion context file system fs string full path protected string get path for cleanup makes path subdirectories recursively fully deletable protected enable path for cleanup throws io exception adds paths queue paths deleted cleanup thread add to queue path deletion context contexts cleanup thread add to queue contexts protected boolean delete path path deletion context context context enable path for cleanup log debug enabled context fs exists new path context full path return true currently used tests protected boolean queue empty return cleanup thread queue size path cleanup thread extends thread cleanup queue deletes files directories paths queued linked blocking queue path deletion context queue path cleanup thread add to queue path deletion context contexts run
1165	mapreduce\src\java\org\apache\hadoop\mapred\Clock.java	unrelated	package org apache hadoop mapred a clock mocked testing clock get time
1166	mapreduce\src\java\org\apache\hadoop\mapred\ClusterStatus.java	unrelated	package org apache hadoop mapred status information current state map reduce cluster p code cluster status code provides clients information ol li size cluster li li name trackers li li task capacity cluster li li the number currently running map reduce tasks li li state code job tracker code li li details regarding black listed trackers li ol p p clients query latest code cluster status code via link job client get cluster status p cluster status implements writable class encapsulates information blacklisted tasktracker the information includes tasktracker name reasons getting blacklisted the string method print information whitespace separated fashion enable parsing black list info implements writable string tracker name string reason for black listing string black list report black list info gets blacklisted tasktracker name string get tracker name gets reason tasktracker blacklisted string get reason for black listing sets blacklisted tasktracker name set tracker name string tracker name sets reason tasktracker blacklisted set reason for black listing string reason for black listing gets descriptive report tasktracker blacklisted string get black list report sets descriptive report tasktracker blacklisted blacklisted set black list report string black list report read fields data input throws io exception write data output throws io exception print information related blacklisted tasktracker whitespace separated fashion the method changes newlines report describing tasktracker blacklisted enabling better parsing string string num active trackers collection string active trackers new array list string num blacklisted trackers num excluded nodes tt expiry interval map tasks reduce tasks max map tasks max reduce tasks job tracker status status collection black list info blacklisted trackers info new array list black list info cluster status construct new cluster status cluster status trackers blacklists tt expiry interval trackers blacklists tt expiry interval maps reduces max maps construct new cluster status cluster status trackers blacklists tt expiry interval num active trackers trackers num blacklisted trackers blacklists num excluded nodes num decommissioned nodes tt expiry interval tt expiry interval map tasks maps reduce tasks reduces max map tasks max maps max reduce tasks max reduces status status construct new cluster status cluster status collection string active trackers active trackers blacklisted trackers tt expiry interval maps reduces construct new cluster status cluster cluster status collection string active trackers active trackers size black listed tracker info size active trackers active trackers blacklisted trackers info black listed tracker info get number task trackers cluster get task trackers return num active trackers get names task trackers cluster collection string get active tracker names return active trackers get names task trackers cluster collection string get blacklisted tracker names array list string blacklisted trackers new array list string black list info bi blacklisted trackers info return blacklisted trackers get number blacklisted task trackers cluster get blacklisted trackers return num blacklisted trackers get number excluded hosts cluster get num excluded nodes return num excluded nodes get tasktracker expiry interval cluster get tt expiry interval return tt expiry interval get number currently running map tasks cluster get map tasks return map tasks get number currently running reduce tasks cluster get reduce tasks return reduce tasks get maximum
1167	mapreduce\src\java\org\apache\hadoop\mapred\CommitTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker commit output task commit task action extends task tracker action task attempt id task id commit task action commit task action task attempt id task id task attempt id get task id write data output throws io exception read fields data input throws io exception
1168	mapreduce\src\java\org\apache\hadoop\mapred\CompletedJobStatusStore.java	unrelated	package org apache hadoop mapred persists retrieves job info job dfs p if retain time zero jobs persisted p a daemon thread cleans job info files older retain time p the retain time set persist jobstatus hours configuration variable hours completed job status store implements runnable boolean active string job info dir retain time file system fs string job info store dir jobtracker jobs info ac ls manager acls manager log log hour sleep time hour fs permission job status store dir permission fs permission completed job status store configuration conf ac ls manager acls manager indicates job status persistency active boolean active run delete job status dirs path get info file path job id job id persists job dfs store job in progress job fs data input stream get job info file job id job id throws io exception job status read job status fs data input stream data in throws io exception job profile read job profile fs data input stream data in counters read counters fs data input stream data in throws io exception task completion event read events fs data input stream data in this method retrieves job status information dfs stored using store method job status read job status job id job id this method retrieves job profile information dfs stored using store method job profile read job profile job id job id this method retrieves counters information file stored using store method counters read counters job id job id throws access control exception this method retrieves task completion events information dfs stored using store method task completion event read job task completion events job id job id
1169	mapreduce\src\java\org\apache\hadoop\mapred\CompositeTaskTrackerInstrumentation.java	unrelated	package org apache hadoop mapred this task tracker instrumentation subclass forwards events receives list instrumentation objects thus used attack multiple instrumentation objects task tracker composite task tracker instrumentation extends task tracker instrumentation list task tracker instrumentation instrumentations composite task tracker instrumentation task tracker tt package getter methods tests list task tracker instrumentation get instrumentations complete task task attempt id timedout task task attempt id task failed ping task attempt id report task launch task attempt id file stdout file stderr report task end task attempt id status update task task task status task status
1170	mapreduce\src\java\org\apache\hadoop\mapred\Counters.java	unrelated	package org apache hadoop mapred a set named counters p code counters code represent global counters defined either map reduce framework applications each code counter code link enum type p p code counters code bunched link group comprising counters particular code enum code counters implements writable iterable counters group log log log factory get log counters char group open char group close char counter open char counter close char unit open char unit close char chars to escape group open group close log log log factory get log counters downgrade new link org apache hadoop mapreduce counters old counters counters downgrade org apache hadoop mapreduce counters new counters counters old counters new counters org apache hadoop mapreduce counter group new group new counters return old counters a counter record comprising name value counter extends org apache hadoop mapreduce counter counter counter string name string display name value set display name string new name returns compact stringified version counter format actual name display name value synchronized string make escaped compact string checks content equality two basic counters synchronized boolean content equals counter c what current value counter synchronized get counter code group code counters comprising counters particular counter link enum p code group code handles localization name counter names p group implements writable iterable counter string group name string display name map string counter subcounters new hash map string counter optional resource bundle localization group counter names resource bundle bundle null group string group name returns specified resource bundle throws exception resource bundle get resource bundle string enum class name returns raw name group this name enum group counters string get name returns localized name group this get name default different appropriate resource bundle found string get display name set display name set display name string display name returns compact stringified version group format actual name display name value compact strings counters within string make escaped compact string hash code checks content equality groups boolean equals object obj returns value specified counter counter exist synchronized get counter string counter name get counter given id create exist synchronized counter get counter id string name get counter given name create exist synchronized counter get counter for name string name returns number counters group synchronized size looks key resource bundle returns corresponding value if bundle key exist returns default value string localize string key string default value synchronized write data output throws io exception synchronized read fields data input throws io exception synchronized iterator counter iterator map group name enum name map enum ordinal counter record name value pair map string group counters new hash map string group a cache enum values associated counter dramatically speeds typical usage map enum counter cache new identity hash map enum counter returns names counter synchronized collection string get group names return counters key set synchronized iterator group iterator return counters values iterator returns named counter group empty group none specified name synchronized group get group string group name to provide support deprecated group names group name equals org apache hadoop mapred task counter else group name equals group result
1171	mapreduce\src\java\org\apache\hadoop\mapred\DefaultTaskController.java	unrelated	package org apache hadoop mapred the default implementation controlling tasks this provides implementation launching killing tasks need run tasktracker hence many initializing cleanup methods required br default task controller extends task controller log log launch task jvm task controller task controller context context initialize task task controller task controller context context initialize job job initialization context context terminate task task controller context context kill task task controller context context dump task stack task controller context context initialize distributed cache file distributed cache file context context initialize user initialization context context run debug script debug script context context throws io exception enable task for cleanup path deletion context context enable job for cleanup path deletion context context enable path for cleanup path deletion context context
1172	mapreduce\src\java\org\apache\hadoop\mapred\DeprecatedQueueConfigurationParser.java	unrelated	package org apache hadoop mapred class build queue hierarchy using deprecated conf mapred site xml generates single level queue hierarchy deprecated queue configuration parser extends queue configuration parser log log string mapred queue names key mapred queue names deprecated queue configuration parser configuration conf list queue create queues configuration conf only applicable leaf level queues parse ac ls queue configuration queue state get queue state string name configuration conf check queue properties configured passed configuration if yes print deprecation warning messages boolean deprecated conf configuration conf string get queue names configuration conf parse ac ls queue configuration map string access control list get queue acls
1173	mapreduce\src\java\org\apache\hadoop\mapred\DisallowedTaskTrackerException.java	unrelated	package org apache hadoop mapred this exception thrown tasktracker tries register communicate jobtracker appear list included nodes specifically excluded disallowed task tracker exception extends io exception serial version uid l disallowed task tracker exception task tracker status tracker
1174	mapreduce\src\java\org\apache\hadoop\mapred\EagerTaskInitializationListener.java	pooling	package org apache hadoop mapred a link job in progress listener initializes tasks job soon job added using link job added job in progress method eager task initialization listener extends job in progress listener default num threads log log log factory get log used init new jobs created job init manager implements runnable run init job implements runnable job in progress job init job job in progress job run job init manager job init manager new job init manager thread job init manager thread list job in progress job init queue new array list job in progress executor service thread pool num threads task tracker manager ttm eager task initialization listener configuration conf num threads thread pool executors new fixed thread pool num threads set task tracker manager task tracker manager ttm ttm ttm start throws io exception job init manager thread new thread job init manager job init manager job init manager thread set daemon true job init manager thread start terminate throws io exception job init manager thread null job init manager thread alive we add jip job init queue processed asynchronously handle split computation build right task tracker block mapping job added job in progress job synchronized job init queue sort jobs priority start time synchronized resort init queue comparator job in progress comp new comparator job in progress synchronized job init queue job removed job in progress job synchronized job init queue job updated job change event event event instanceof job status change event called job status changed job state changed job status change event event resort job queue job start time job priority changes event get event type event type start time changed
1175	mapreduce\src\java\org\apache\hadoop\mapred\FileAlreadyExistsException.java	unrelated	package org apache hadoop mapred used target file already exists operation configured overwritten file already exists exception serial version uid l file already exists exception file already exists exception string msg
1176	mapreduce\src\java\org\apache\hadoop\mapred\FileInputFormat.java	unrelated	package org apache hadoop mapred a base file based link input format p code file input format code base file based code input format code this provides generic implementation link get splits job conf subclasses code file input format code also link splitable file system path method ensure input files split processed whole link mapper instead file input format k v implements input format k v log log string num input files split slop slop min split size path filter hidden file filter new path filter protected set min split size min split size multi path filter implements path filter protected boolean splitable file system fs path filename record reader k v get record reader input split split set input path filter job conf conf path filter get input path filter job conf conf protected add input path recursively list file status result list input directories protected file status list status job conf job throws io exception protected file split make split path file start length splits files returned link list status job conf input split get splits job conf job num splits protected compute split size goal size min size protected get block index block location blk locations set input paths job conf conf string comma separated paths add input paths job conf conf string comma separated paths set input paths job conf conf path input paths add input path job conf conf path path this method escapes commas glob pattern given paths string get path strings string comma separated paths path get input paths job conf conf sort in descending order list node info mylist protected string get split hosts block location blk locations throws io exception string identify hosts replication factor string fake racks block location blk locations index throws io exception node info
1177	mapreduce\src\java\org\apache\hadoop\mapred\FileOutputCommitter.java	unrelated	package org apache hadoop mapred an link output committer commits files specified job output directory e mapreduce output fileoutputformat outputdir file output committer extends output committer log log log factory get log temporary directory name string temp dir name temporary string succeeded file name success string successful job output dir marker setup job job context context throws io exception true job requires output dir marked successful job note default set true boolean mark output dir job conf conf commit job job context context throws io exception create success file job output folder mark output dir successful job context context throws io exception cleanup job job context context throws io exception abort job job context context run state throws io exception setup task task attempt context context throws io exception commit task task attempt context context throws io exception move task outputs task attempt context context throws io exception abort task task attempt context context throws io exception path get final path path job output dir path task output boolean needs task commit task attempt context context throws io exception path get temp task output path task attempt context task context throws io exception path get work path task attempt context task context path base path throws io exception
1178	mapreduce\src\java\org\apache\hadoop\mapred\FileOutputFormat.java	unrelated	package org apache hadoop mapred a base link output format file output format k v implements output format k v set whether output job compressed set compress output job conf conf boolean compress conf set boolean org apache hadoop mapreduce lib output is job output compressed code false code otherwise boolean get compress output job conf conf return conf get boolean org apache hadoop mapreduce lib output set link compression codec used compress job outputs compress job outputs set output compressor class job conf conf set compress output conf true conf set class org apache hadoop mapreduce lib output get link compression codec compressing job outputs job outputs class extends compression codec get output compressor class job conf conf class extends compression codec codec class default value string name conf get org apache hadoop mapreduce lib output name null return codec class record writer k v get record writer file system ignored throws io exception check output specs file system ignored job conf job throws file already exists exception ensure output directory set already path dir get output path job dir null job get num reduce tasks dir null set link path output directory map reduce job map reduce job set output path job conf conf path output dir output dir new path conf get working directory output dir conf set org apache hadoop mapreduce lib output set link path task temporary output directory map reduce job p note task output path set framework p map reduce job set work output path job conf conf path output dir output dir new path conf get working directory output dir conf set job context task output dir output dir string get link path output directory map reduce job path get output path job conf conf string name conf get org apache hadoop mapreduce lib output return name null null new path name get link path task temporary output directory map reduce job id side effect files tasks side effect files p note the following valid link output committer link file output committer if code output committer code code file output committer code task temporary output directory link get output path job conf e tt mapreduce output fileoutputformat outputdir tt p p some applications need create write side files differ actual job outputs p in cases could issues instances tip running simultaneously e g speculative tasks trying open write file path hdfs hence application writer pick unique names per task attempt e g using attemptid say tt attempt tt per tip p p to get around map reduce framework helps application writer maintaining special tt mapreduce output fileoutputformat outputdir temporary taskid tt sub directory task attempt hdfs output task attempt goes on successful completion task attempt files tt mapreduce output fileoutputformat outputdir temporary taskid tt promoted tt mapreduce output fileoutputformat outputdir tt of course framework discards sub directory unsuccessful task attempts this completely transparent application p p the application writer take advantage creating side files required tt mapreduce task output dir tt execution reduce task e via link get work output path job conf framework
1179	mapreduce\src\java\org\apache\hadoop\mapred\FileSplit.java	unrelated	package org apache hadoop mapred a section input file returned link input format get splits job conf passed link input format get record reader input split job conf reporter instead file split extends org apache hadoop mapreduce input split org apache hadoop mapreduce lib input file split fs protected file split constructs split file split path file start length job conf conf constructs split host information file split path file start length string hosts file split org apache hadoop mapreduce lib input file split fs the file containing split data path get path return fs get path the position first byte file process get start return fs get start the number bytes file process get length return fs get length string string return fs string writable methods write data output throws io exception read fields data input throws io exception string get locations throws io exception
1180	mapreduce\src\java\org\apache\hadoop\mapred\HeartbeatResponse.java	heartbeat	package org apache hadoop mapred the response sent link job tracker hearbeat sent periodically link task tracker heartbeat response implements writable configurable configuration conf null short response id heartbeat interval task tracker action actions heartbeat response heartbeat response short response id task tracker action actions set response id short response id short get response id set actions task tracker action actions task tracker action get actions set conf configuration conf configuration get conf set heartbeat interval interval get heartbeat interval write data output throws io exception read fields data input throws io exception
1181	mapreduce\src\java\org\apache\hadoop\mapred\ID.java	unrelated	package org apache hadoop mapred a general identifier internally stores id integer this super link job id link task id link task attempt id id extends org apache hadoop mapreduce id constructs id object given id id protected id
1182	mapreduce\src\java\org\apache\hadoop\mapred\IFile.java	pooling	package org apache hadoop mapred code i file code simple key len value len key value format intermediate map outputs map reduce there code writer code write map outputs format code reader code read files format i file log log log factory get log i file eof marker end file marker code i file writer code write intermediate map outputs writer k extends object v extends object fs data output stream boolean output stream false start fs data output stream raw out compression output stream compressed out compressor compressor boolean compress output false decompressed bytes written compressed bytes written count records written disk num records written counters counter written records counter i file output stream checksum out class k key class class v value class serializer k key serializer serializer v value serializer data output buffer buffer new data output buffer writer configuration conf file system fs path file protected writer counters counter writes counter writer configuration conf fs data output stream writer configuration conf file system fs path file throws io exception close throws io exception append k key v value throws io exception append data input buffer key data input buffer value throws io exception required mark reset data output stream get output stream required mark reset update counters for external append length get raw length get compressed length code i file reader code read intermediate map outputs reader k extends object v extends object default buffer size max vint size count records read disk num records read counters counter read records counter input stream possibly decompressed stream read decompressor decompressor bytes read protected file length protected boolean eof false i file input stream checksum in protected byte buffer null protected buffer size default buffer size protected data input stream data in protected rec no protected current key length protected current value length byte key bytes new byte construct i file reader checksum bytes data end file reader configuration conf file system fs path file construct i file reader bytes reader configuration conf fs data input stream length get length get position throws io exception read upto len bytes buf starting offset read data byte buf len throws io exception protected boolean position to next record data input in throws io exception boolean next raw key data input buffer key throws io exception next raw value data input buffer value throws io exception close throws io exception reset offset disable checksum validation
1183	mapreduce\src\java\org\apache\hadoop\mapred\IFileInputStream.java	unrelated	package org apache hadoop mapred a checksum input stream used i files used validate checksum files created link i file output stream i file input stream extends input stream input stream the input stream verified checksum length the total length input file data length data checksum sum current offset byte b new byte byte csum null checksum size boolean disable checksum validation false create checksum input stream reads i file input stream input stream len close input stream note need read end stream validate checksum close throws io exception skip n throws io exception throw new io exception skip supported i file input stream get position get size read bytes stream at eof checksum validated checksum bytes passed back buffer read byte b len throws io exception read bytes stream at eof checksum validated sent back last four bytes buffer the caller handle bytes appropriately read with checksum byte b len throws io exception read byte b len throws io exception read throws io exception byte get checksum disable checksum validation
1184	mapreduce\src\java\org\apache\hadoop\mapred\IFileOutputStream.java	unrelated	package org apache hadoop mapred a checksum output stream checksum contents file calculated appended end file close stream used i files i file output stream extends filter output stream the output stream checksummed data checksum sum byte barray boolean closed false boolean finished false create checksum output stream writes bytes given stream i file output stream output stream close throws io exception finishes writing data output stream writing checksum bytes end the underlying stream closed finish throws io exception write bytes stream write byte b len throws io exception write b throws io exception
1185	mapreduce\src\java\org\apache\hadoop\mapred\IndexCache.java	unrelated	package org apache hadoop mapred index cache job conf conf total memory allowed atomic integer total memory used new atomic integer log log log factory get log index cache concurrent hash map string index information cache new concurrent hash map string index information linked blocking queue string queue new linked blocking queue string index cache job conf conf conf conf total memory allowed log info index cache created max memory total memory allowed this method gets index information given map id reduce it reads index file cache already present already present cache index record get index information string map id reduce throws io exception index information info cache get map id info null else info map spill record size return info map spill record get index reduce index information read index file to cache path index file name throws io exception index information info index information new ind new index information info cache put if absent map id new ind null log debug index cache miss map id map id found spill record tmp null try catch throwable e finally queue add map id total memory used add and get new ind get size total memory allowed return new ind this method removes map cache it called map output tracker discarded remove map string map id index information info cache remove map id info null else bring memory usage total memory allowed synchronized free index information total memory used get total memory allowed index information spill record map spill record get size
1186	mapreduce\src\java\org\apache\hadoop\mapred\InputFormat.java	unrelated	package org apache hadoop mapred code input format code describes input specification map reduce job p the map reduce framework relies code input format code job p ol li validate input specification job li split input file logical link input split assigned individual link mapper li li provide link record reader implementation used glean input records logical code input split code processing link mapper li ol p the default behavior file based link input format typically sub link file input format split input logical link input split based total size bytes input files however link file system blocksize input files treated upper bound input splits a lower bound split size set via href doc root mapred default html mapreduce input fileinputformat split minsize mapreduce input fileinputformat split minsize p p clearly logical splits based input size insufficient many applications since record boundaries respected in cases application also implement link record reader lies responsibilty respect record boundaries present record oriented view logical code input split code individual task input format k v logically split set input files job p each link input split assigned individual link mapper processing p p note the split logical split inputs input files physically split chunks for e g split could lt input file path start offset gt tuple input split get splits job conf job num splits throws io exception get link record reader given link input split p it responsibility code record reader code respect record boundaries processing logical split present record oriented view individual task p record reader k v get record reader input split split
1187	mapreduce\src\java\org\apache\hadoop\mapred\InputSplit.java	unrelated	package org apache hadoop mapred code input split code represents data processed individual link mapper p typically presents byte oriented view input responsibility link record reader job process present record oriented view input split extends writable get total number bytes data code input split code get length throws io exception get list hostnames input split located located array code string code string get locations throws io exception
1188	mapreduce\src\java\org\apache\hadoop\mapred\InterTrackerProtocol.java	heartbeat	package org apache hadoop mapred protocol task tracker central job tracker use communicate the job tracker server implements protocol server principal jt config jt user name client principal tt config tt user name inter tracker protocol extends versioned protocol version introduced replace emit hearbeat poll for new task poll for task with closed job link heartbeat task tracker status boolean boolean boolean short version changed task report hadoop version introduced removes locate map outputs instead uses get task completion events figure finished maps fetch outputs version adds max tasks task tracker status hadoop version replaces max tasks max map tasks max reduce tasks task tracker status hadoop version heartbeat response added next heartbeat interval version changes counter representation hadoop version changes task status representation hadoop version changes job id get task completion events version changes counters representation hadoop version added call get build version hadoop version replaced get filesystem name get system dir hadoop version changed format task task status hadoop version adds resource status task tracker status hadoop version changed format task task status hadoop version changed status message due changes task status version changed heartbeat piggyback job tracker restart information version changed status message due changes task status hadoop version changed information reported task tracker status resource status corresponding accessor methods hadoop version replaced parameter initial contact restarted heartbeat method hadoop version added parameter initial contact heartbeat method hadoop version changed format task task status hadoop version job i ds passed response job tracker restart version modified task id aware new task types version added num required slots task status mapreduce version adding node health status task status mapreduce version adding user name serialized task use tt version adding available memory cpu usage information tt task tracker status mapreduce version id l trackers ok unknown tasktracker called regularly link task tracker update status tasks within job tracker link job tracker responds link heartbeat response directs link task tracker undertake series actions see link org apache hadoop mapred task tracker action action type link task tracker must also indicate whether first interaction since state refresh acknowledge last response received link job tracker restarted code false code otherwise refresh code false code otherwise ready accept new tasks run link task tracker fresh instructions heartbeat response heartbeat task tracker status status throws io exception the task tracker calls discern find files referred job tracker string get filesystem name throws io exception report problem job tracker remote side report task tracker error string task tracker get task completion events jobid starting event id returns empty aray events available task completion event get task completion events job id jobid event id grab jobtracker system directory path job specific files placed string get system dir returns build version job tracker string get build version throws io exception
1189	mapreduce\src\java\org\apache\hadoop\mapred\InvalidFileTypeException.java	unrelated	package org apache hadoop mapred used file type differs desired file type like getting file directory expected or wrong file type invalid file type exception serial version uid l invalid file type exception invalid file type exception string msg
1190	mapreduce\src\java\org\apache\hadoop\mapred\InvalidInputException.java	unrelated	package org apache hadoop mapred this wraps list problems input user get list problems together instead finding fixing one one invalid input exception extends io exception serial version uid l list io exception problems invalid input exception list io exception probs list io exception get problems string get message
1191	mapreduce\src\java\org\apache\hadoop\mapred\InvalidJobConfException.java	unrelated	package org apache hadoop mapred this exception thrown jobconf misses mendatory attributes value attributes invalid invalid job conf exception serial version uid l invalid job conf exception invalid job conf exception string msg
1192	mapreduce\src\java\org\apache\hadoop\mapred\JobACLsManager.java	unrelated	package org apache hadoop mapred job ac ls manager configuration conf job ac ls manager configuration conf boolean ac ls enabled construct job ac ls configuration kept memory if authorization disabled jt nothing constructed empty map returned map job acl access control list construct job ac ls configuration conf if authorization enabled checks whether user caller ugi authorized perform operation specified job operation job checking user job owner part job acl specific job operation ul li the owner job operation job li li for users groups job acls checked li ul boolean check access user group information caller ugi
1193	mapreduce\src\java\org\apache\hadoop\mapred\JobChangeEvent.java	unrelated	package org apache hadoop mapred link job change event used capture state changes job a job change state w r priority progress run state etc job change event job in progress jip job change event job in progress jip get job object change reported job in progress get job in progress
1194	mapreduce\src\java\org\apache\hadoop\mapred\JobClient.java	scheduler	package org apache hadoop mapred code job client code primary user job interact link job tracker code job client code provides facilities submit jobs track progress access component tasks reports logs get map reduce cluster status information etc p the job submission process involves ol li checking input output specifications job li li computing link input split job li li setup requisite accounting information link distributed cache job necessary li li copying job jar configuration map reduce system directory distributed file system li li submitting job code job tracker code optionally monitoring status li ol p normally user creates application describes various facets job via link job conf uses code job client code submit job monitor progress p here example use code job client code p p blockquote pre create new job conf job conf job new job conf new configuration my job specify various job specific parameters job set job name myjob job set input path new path job set output path new path job set mapper class my job my mapper job set reducer class my job my reducer submit job poll progress job complete job client run job job pre blockquote p id job control job control p at times clients would chain map reduce jobs accomplish complex tasks cannot done via single map reduce job this fairly easy since output job typically goes distributed file system used input next job p p however also means onus ensuring jobs complete success failure lies squarely clients in situations various job control options ol li link run job job conf submits job returns job completed li li link submit job job conf submits job poll returned handle link running job query status make scheduling decisions li li link job conf set job end notification uri string setup notification job completion thus avoiding polling li ol p job client extends cli enum task status filter none killed failed succeeded all task status filter task output filter task status filter failed config util load resources a networked job implementation running job it holds job profile object provide info interacts remote service provide certain functionality networked job implements running job job job we store job profile timestamp last acquired job profile if job null cannot perform tasks the job might null job tracker completely forgotten job eg hours job completes networked job job status status cluster cluster throws io exception networked job job job throws io exception configuration get configuration an identifier job job id get id rather use link get id string get job id the user specified job name string get job name the name job file string get job file a url job status seen string get tracking url a indicating map work completed map progress throws io exception a indicating reduce work completed reduce progress throws io exception a indicating cleanup work completed cleanup progress throws io exception a indicating setup work completed setup progress throws io exception returns immediately whether whole job done yet synchronized boolean complete throws io exception true iff job completed successfully synchronized boolean successful throws
1195	mapreduce\src\java\org\apache\hadoop\mapred\JobConf.java	scheduler	package org apache hadoop mapred a map reduce job configuration p code job conf code primary user describe map reduce job hadoop framework execution the framework tries faithfully execute job described code job conf code however ol li some configuration parameters might marked href doc root org apache hadoop conf configuration html final params administrators hence cannot altered li li while job parameters straight forward set e g link set num reduce tasks parameters interact subtly rest framework job configuration relatively complex user control finely e g link set num map tasks li ol p p code job conf code typically specifies link mapper combiner link partitioner link reducer link input format link output format implementations used etc p optionally code job conf code used specify advanced facets job code comparator code used files put link distributed cache whether intermediate job outputs compressed debugability via user provided scripts link set map debug script string link set reduce debug script string post processing task logs task stdout stderr syslog etc p p here example configure job via code job conf code p p blockquote pre create new job conf job conf job new job conf new configuration my job specify various job specific parameters job set job name myjob file input format set input paths job new path file output format set output path job new path job set mapper class my job my mapper job set combiner class my job my reducer job set reducer class my job my reducer job set input format sequence file input format job set output format sequence file output format pre blockquote p job conf extends configuration log log log factory get log job conf config util load resources link mapred job reduce memory mb property string mapred task maxvmem property mapred task maxvmem string upper limit on task vmem property mapred task limit maxvmem string mapred task default maxvmem property mapred task default maxvmem string mapred task maxpmem property mapred task maxpmem a value set memory related configuration options indicates options turned disabled memory limit l property name configuration property mapreduce cluster local dir string mapred local dir property mr config local dir name queue jobs submitted queue name mentioned string default queue name default string mapred job map memory mb property string mapred job reduce memory mb property job context reduce memory mb pattern default unpacking behavior job jars pattern unpack jar pattern default pattern compile lib configuration key set java command line options child map reduce tasks java opts task tracker child processes the following symbol present interpolated taskid it replaced current task id any occurrences go unchanged for example enable verbose gc logging file named taskid tmp set heap maximum gigabyte pass value xmx verbose gc xloggc tmp taskid gc the configuration variable link mapred task ulimit used control maximum virtual memory child processes the configuration variable link mapred task env used pass environment variables child processes link mapred reduce task java opts string mapred task java opts mapred child java opts configuration key set java command line options map tasks java
1196	mapreduce\src\java\org\apache\hadoop\mapred\JobConfigurable.java	unrelated	package org apache hadoop mapred that may configured job configurable initializes new instance link job conf configure job conf job
1197	mapreduce\src\java\org\apache\hadoop\mapred\JobContext.java	unrelated	package org apache hadoop mapred job context extends org apache hadoop mapreduce job context get job configuration job conf get job conf get progress mechanism reporting progress progressable get progressible
1198	mapreduce\src\java\org\apache\hadoop\mapred\JobContextImpl.java	unrelated	package org apache hadoop mapred job context impl job conf job progressable progress job context impl job conf conf org apache hadoop mapreduce job id job id job context impl job conf conf org apache hadoop mapreduce job id job id get job configuration job conf get job conf get progress mechanism reporting progress progressable get progressible
1199	mapreduce\src\java\org\apache\hadoop\mapred\JobEndNotifier.java	unrelated	package org apache hadoop mapred job end notifier log log log factory get log job end notifier get name thread thread volatile boolean running blocking queue job end status info queue new delay queue job end status info start notifier running true thread new thread thread start stop notifier running false thread interrupt job end status info create notification job conf conf job end status info notification null string uri conf get job end notification uri uri null return notification register notification job conf job conf job status status job end status info notification create notification job conf status notification null http notification string uri throws io exception uri url new uri uri false http client client new http client http method method new get method url get escaped uri method set request header accept return client execute method method use local job runner without using thread queue simple synchronous way local runner notification job conf conf job status status job end status info notification create notification conf status notification null job end status info implements delayed string uri retry attempts retry interval delay time job end status info string uri retry attempts retry interval string get uri get retry attempts get retry interval get delay time boolean configure for retry get delay time unit unit compare to delayed boolean equals object hash code string string
1200	mapreduce\src\java\org\apache\hadoop\mapred\JobID.java	unrelated	package org apache hadoop mapred job id represents immutable unique identifier job job id consists two parts first part represents jobtracker identifier job id jobtracker map defined for cluster setup jobtracker start time local setting local second part job id job number br an example job id code job code represents third job running jobtracker started code code p applications never construct parse job id strings rather use appropriate constructors link name string method job id extends org apache hadoop mapreduce job id constructs job id object job id string jt identifier id job id downgrade new job id old one job id downgrade org apache hadoop mapreduce job id old job id read data input throws io exception construct job id object given job id name string str throws illegal argument exception returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching job run jobtracker started would use pre job id get task i ds pattern null pre return pre job pre string get job i ds pattern string jt identifier integer job id string builder get job i ds pattern wo prefix string jt identifier
1201	mapreduce\src\java\org\apache\hadoop\mapred\JobInfo.java	unrelated	package org apache hadoop mapred represents basic information saved per job job tracker receives submit job request the information saved job tracker recover incomplete jobs upon restart job info implements writable org apache hadoop mapreduce job id id text user path job submit dir job info job info org apache hadoop mapreduce job id id get job id org apache hadoop mapreduce job id get job id get configured job user name text get user get job submission directory path get job submit dir read fields data input throws io exception write data output throws io exception
1202	mapreduce\src\java\org\apache\hadoop\mapred\JobInProgress.java	audit	package org apache hadoop mapred job in progress maintains info keeping job straight narrow it keeps job profile latest job status plus set tables bookkeeping tasks job in progress used kill issued job initializing kill interrupted exception extends interrupted exception serial version uid l kill interrupted exception string msg log log log factory get log job in progress job profile profile job status status path job file null path local job file null task in progress maps new task in progress task in progress reduces new task in progress task in progress cleanup new task in progress task in progress setup new task in progress num map tasks num reduce tasks memory per map memory per reduce volatile num slots per map volatile num slots per reduce max task failures per tracker counters track currently running finished failed map reduce task attempts running map tasks running reduce tasks finished map tasks finished reduce tasks failed map tasks failed reduce tasks default completed maps percent for reduce slowstart f completed maps for reduce slowstart running map tasks speculative tasks need capture speculative tasks separately speculative map tasks speculative reduce tasks map failures percent reduce failures percent failed map ti ps failed reduce ti ps volatile boolean launched cleanup false volatile boolean launched setup false volatile boolean job killed false volatile boolean job failed false boolean job setup cleanup needed boolean task cleanup needed job priority priority job priority normal protected job tracker jobtracker protected credentials token storage job history job history network topology node set ti ps map node list task in progress non running map cache map network topology node set running ti ps map node set task in progress running map cache a list non local non running maps list task in progress non local maps a set non local running maps set task in progress non local running maps a list non running reduce ti ps list task in progress non running reduces a set running reduce ti ps set task in progress running reduces a list cleanup tasks map task attempts launched list task attempt id map cleanup tasks new linked list task attempt id a list cleanup tasks reduce task attempts launched list task attempt id reduce cleanup tasks new linked list task attempt id max level a special value indicating link find new map task task tracker status schedule available map tasks job including speculative tasks cache level a special value indicating link find new map task task tracker status schedule switch speculative map tasks job non local cache level task completion event tracker list task completion event task completion events the maximum percentage trackers cluster added blacklist cluster blacklist percent the maximum percentage fetch failures allowed map max allowed fetch failures percent no tasktrackers cluster volatile cluster size the tasktrackers conf get max task failures per tracker tasks failed volatile flaky task trackers map tracker host name task failures map string integer tracker to failures map new tree map string integer confine estimation algorithms oracle jip queries resource estimator resource estimator start time launch time finish
1203	mapreduce\src\java\org\apache\hadoop\mapred\JobInProgressListener.java	unrelated	package org apache hadoop mapred a listener changes link job in progress job lifecycle link job tracker job in progress listener invoked new job added link job tracker job added job in progress job throws io exception invoked job removed link job tracker job removed job in progress job invoked job updated link job tracker this change job tracker using link job change event job updated job change event event
1204	mapreduce\src\java\org\apache\hadoop\mapred\JobPriority.java	unrelated	package org apache hadoop mapred used describe priority running job enum job priority very high high normal low very low
1205	mapreduce\src\java\org\apache\hadoop\mapred\JobProfile.java	unrelated	package org apache hadoop mapred a job profile map reduce primitive tracks job whether living dead job profile implements writable register ctor string user job id jobid string job file string url string name string queue name construct empty link job profile job profile construct link job profile userid jobid job config file job details url job name job profile string user org apache hadoop mapreduce job id jobid construct link job profile userid jobid job config file job details url job name job profile string user org apache hadoop mapreduce job id jobid job profile string user string jobid string job file string url get user id string get user get job id job id get job id string get job id get configuration file job string get job file get link web ui details job url get url get user specified job name string get job name get name queue job submitted string get queue name writable write data output throws io exception read fields data input throws io exception
1206	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueClient.java	unrelated	package org apache hadoop mapred code job queue client code provided user order get job queue related information link job tracker it provides facility list job queues present ability view list jobs within specific job queue job queue client extends configured implements tool job client jc job queue client job queue client job conf conf throws io exception init job conf conf throws io exception run string argv throws exception format print information passed job queue print job queue info job queue info job queue info writer writer display queue list throws io exception expands hierarchy queues gives list queues depth first order list job queue info expand queue list job queue info root queues method used display information pertaining single job queue registered link queue manager display jobs determine boolean display queue info string queue boolean show jobs display queue acls info for current user throws io exception display usage string cmd main string argv throws exception
1207	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueInfo.java	unrelated	package org apache hadoop mapred class contains information regarding job queues maintained hadoop map reduce framework job queue info extends queue info default constructor job queue info job queue info construct new job queue info object using queue name scheduling information passed queue job queue info string queue name string scheduling info job queue info queue info queue set queue name job queue info protected set queue name string queue name set scheduling information associated particular job queue protected set scheduling info string scheduling info set state queue protected set queue state string state string get queue state protected set children list job queue info children list job queue info get children protected set properties properties props add child link job queue info link job queue info modify fully qualified name child link job queue info reflect hierarchy only testing add child job queue info child remove child link job queue info this also resets queue name child fully qualified name simple queue name only testing remove child job queue info child protected set job statuses org apache hadoop mapreduce job status stats
1208	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueJobInProgressListener.java	unrelated	package org apache hadoop mapred a link job in progress listener maintains jobs managed queue by default queue fifo possible use custom queue ordering using link job queue job in progress listener map constructor job queue job in progress listener extends job in progress listener a groups information link job in progress necessary scheduling job job scheduling info job priority priority start time job id id job scheduling info job in progress jip job scheduling info job status status job priority get priority return priority get start time return start time job id get job id return id comparator job scheduling info fifo job queue comparator new comparator job scheduling info compare job scheduling info job scheduling info map job scheduling info job in progress job queue job queue job in progress listener new tree map job scheduling info for clients want provide job priorities protected job queue job in progress listener map job scheduling info job queue collections synchronized map job queue returns synchronized view job queue collection job in progress get job queue return job queue values job added job in progress job job queue put new job scheduling info job get status job job removed job completes job removed job in progress job job completed job scheduling info old info job queue remove old info synchronized job updated job change event event job in progress job event get job in progress event instanceof job status change event reorder jobs job in progress job job scheduling info old info synchronized job queue
1209	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler keeps jobs queue priority order fifo default job queue task scheduler extends task scheduler min cluster size for padding log log log factory get log job queue task scheduler protected job queue job in progress listener job queue job in progress listener protected eager task initialization listener eager task initialization listener pad fraction job queue task scheduler synchronized start throws io exception synchronized terminate throws io exception synchronized set conf configuration conf synchronized list task assign tasks task tracker task tracker boolean exceeded padding boolean map task synchronized collection job in progress get jobs string queue name
1210	mapreduce\src\java\org\apache\hadoop\mapred\JobStatus.java	unrelated	package org apache hadoop mapred describes current status job this intended comprehensive piece data for look job profile job status extends org apache hadoop mapreduce job status running succeeded failed prep killed string unknown unknown string run states helper method get human readable state job string get job run state state org apache hadoop mapreduce job status state get enum state job status create job status object given jobid job status job id jobid map progress reduce progress create job status object given jobid job status job id jobid map progress reduce progress create job status object given jobid job status job id jobid map progress reduce progress create job status object given jobid job status job id jobid setup progress map progress job status job id jobid setup progress map progress job status downgrade org apache hadoop mapreduce job status stat string get job id return get job id string job id get job id return job id downgrade super get job id return priority job synchronized job priority get job priority protected synchronized set map progress p protected synchronized set cleanup progress p protected synchronized set setup progress p protected synchronized set reduce progress p protected synchronized set finish time finish time protected synchronized set history file string history file protected synchronized set tracking url string tracking url protected synchronized set retired protected synchronized set run state state synchronized get run state return super get state get value protected synchronized set start time start time protected synchronized set username string user name protected synchronized set scheduling info string scheduling info protected synchronized set job ac ls map job acl access control list acls set priority job defaulting normal synchronized set job priority job priority jp synchronized map progress return super get map progress synchronized cleanup progress synchronized setup progress synchronized reduce progress a utility convert new job runstates old ones get old new job run state
1211	mapreduce\src\java\org\apache\hadoop\mapred\JobStatusChangeEvent.java	unrelated	package org apache hadoop mapred link job status change event tracks change job status job status change w r run state e prep running failed killed succeeded start time priority note job times change job get restarted job status change event extends job change event events job status lead job status change enum event type run state changed start time changed priority changed job status old status job status new status event type event type job status change event job in progress jip event type event type create link job status change event indicating state changed note assume state change doesnt care old state job status change event job in progress jip event type event type job status status returns event type caused state change event type get event type get old job status job status get old status get new job status result events job status get new status
1212	mapreduce\src\java\org\apache\hadoop\mapred\JobTracker.java	audit	package org apache hadoop mapred job tracker central location submitting tracking mr jobs network environment job tracker implements mr constants inter tracker protocol client protocol task tracker manager refresh user mappings protocol refresh authorization policy protocol admin operations protocol get user mappings protocol jt config config util load resources tasktracker expiry interval delegation token gc interval hour delegation token secret manager secret manager the interval one fault tracker discarded faults update faulty tracker interval the maximum percentage trackers cluster added blacklist across jobs max blacklist percent a tracker blacklisted across jobs number blacklists x average number blacklists x blacklist threshold average blacklist threshold the maximum number blacklists tracker tracker could blacklisted across jobs max blacklists per tracker approximate number heartbeats could arrive job tracker second num heartbeats in second default num heartbeats in second min num heartbeats in second scaling factor heartbeats used testing heartbeats scaling factor min heartbeats scaling factor f default heartbeats scaling factor f minimum interval heartbeats regardless cluster size heartbeat interval min enum state initializing running state state state initializing fs access retry period string job info file job info dns to switch mapping dns to switch mapping network topology cluster map new network topology num task cache levels max level cache tasks link nodes at max level using key set link concurrent hash map safely written iterated via separate threads note it iterated single thread feasible since iteration done link job in progress link job tracker lock set node nodes at max level collections new set from map new concurrent hash map node boolean task scheduler task scheduler list job in progress listener job in progress listeners new copy on write array list job in progress listener list service plugin plugins system directory completely owned job tracker fs permission system dir permission fs permission create immutable short rwx system files permission fs permission system file permission fs permission create immutable short rwx clock clock null clock default clock new clock job history job history job token secret manager job token secret manager new job token secret manager job token secret manager get job token secret manager return job token secret manager mr async disk service async disk service returns delegation token secret manager instance job tracker delegation token secret manager get delegation token secret manager return secret manager a client tried submit job job tracker ready illegal state exception extends io exception serial version uid l illegal state exception string msg atomic integer next job id new atomic integer log log log factory get log job tracker returns job tracker clock note correct clock implementation obtained job tracker initialized if job tracker initialized default clock e link clock returned clock get clock return clock null default clock clock return jt job history handle job history get job history return job history start job tracker given configuration the conf modified reflect actual ports job tracker running user passes port code zero code job tracker start tracker job conf conf throws io exception interrupted exception return start tracker conf default clock job tracker start tracker job conf conf clock
1213	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerClientProtocolProvider.java	unrelated	package org apache hadoop mapred job tracker client protocol provider extends client protocol provider client protocol create configuration conf throws io exception client protocol create inet socket address addr configuration conf throws io exception client protocol create rpc proxy inet socket address addr close client protocol client protocol throws io exception
1214	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerInstrumentation.java	heartbeat	package org apache hadoop mapred job tracker instrumentation protected job tracker tracker job tracker instrumentation job tracker jt job conf conf launch map task attempt id task attempt id complete map task attempt id task attempt id failed map task attempt id task attempt id launch reduce task attempt id task attempt id complete reduce task attempt id task attempt id failed reduce task attempt id task attempt id submit job job conf conf job id id complete job job conf conf job id id terminate job job conf conf job id id finalize job job conf conf job id id add waiting maps job id id task dec waiting maps job id id task add waiting reduces job id id task dec waiting reduces job id id task set map slots slots set reduce slots slots add black listed map slots slots dec black listed map slots slots add black listed reduce slots slots dec black listed reduce slots slots add reserved map slots slots dec reserved map slots slots add reserved reduce slots slots dec reserved reduce slots slots add occupied map slots slots dec occupied map slots slots add occupied reduce slots slots dec occupied reduce slots slots failed job job conf conf job id id killed job job conf conf job id id add prep job job conf conf job id id dec prep job job conf conf job id id add running job job conf conf job id id dec running job job conf conf job id id add running maps tasks dec running maps tasks add running reduces tasks dec running reduces tasks killed map task attempt id task attempt id killed reduce task attempt id task attempt id add trackers trackers dec trackers trackers add black listed trackers trackers dec black listed trackers trackers set decommissioned trackers trackers heartbeat speculate map task attempt id task attempt id speculate reduce task attempt id task attempt id launch data local map task attempt id task attempt id launch rack local map task attempt id task attempt id
1215	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerMetricsInst.java	heartbeat	package org apache hadoop mapred job tracker metrics inst extends job tracker instrumentation implements updater metrics record metrics record num map tasks launched num map tasks completed num map tasks failed num reduce tasks launched num reduce tasks completed num reduce tasks failed num jobs submitted num jobs completed num waiting maps num waiting reduces num speculative maps num speculative reduces num data local maps num rack local maps cluster status fields volatile num map slots volatile num reduce slots num black listed map slots num black listed reduce slots num reserved map slots num reserved reduce slots num occupied map slots num occupied reduce slots num jobs failed num jobs killed num jobs preparing num jobs running num running maps num running reduces num map tasks killed num reduce tasks killed num trackers num trackers black listed num trackers decommissioned could well month worth heartbeats reasonable assumptions job tracker improvements num heartbeats l job tracker metrics inst job tracker tracker job conf conf since object registered updater method called periodically e g every seconds updates metrics context unused synchronized launch map task attempt id task attempt id synchronized launch data local map task attempt id task attempt id synchronized launch rack local map task attempt id task attempt id synchronized complete map task attempt id task attempt id synchronized failed map task attempt id task attempt id synchronized speculate map task attempt id task attempt id synchronized launch reduce task attempt id task attempt id synchronized complete reduce task attempt id task attempt id synchronized failed reduce task attempt id task attempt id synchronized speculate reduce task attempt id task attempt id synchronized submit job job conf conf job id id synchronized complete job job conf conf job id id synchronized add waiting maps job id id task synchronized dec waiting maps job id id task synchronized add waiting reduces job id id task synchronized dec waiting reduces job id id task synchronized set map slots slots synchronized set reduce slots slots synchronized add black listed map slots slots synchronized dec black listed map slots slots synchronized add black listed reduce slots slots synchronized dec black listed reduce slots slots synchronized add reserved map slots slots synchronized dec reserved map slots slots synchronized add reserved reduce slots slots synchronized dec reserved reduce slots slots synchronized add occupied map slots slots synchronized dec occupied map slots slots synchronized add occupied reduce slots slots synchronized dec occupied reduce slots slots synchronized failed job job conf conf job id id synchronized killed job job conf conf job id id synchronized add prep job job conf conf job id id synchronized dec prep job job conf conf job id id synchronized add running job job conf conf job id id synchronized dec running job job conf conf job id id synchronized add running maps task synchronized dec running maps task synchronized add running reduces task synchronized dec running reduces task synchronized killed map task attempt id task attempt id synchronized killed reduce task attempt id task attempt id synchronized add trackers trackers synchronized dec trackers
1216	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerStatistics.java	unrelated	package org apache hadoop mapred collects job tracker statistics job tracker statistics statistics collector collector map string task tracker stat tt stats new hash map string task tracker stat job tracker statistics collector new statistics collector collector start synchronized task tracker added string name task tracker stat stat tt stats get name stat null synchronized task tracker removed string name task tracker stat stat tt stats remove name stat null synchronized task tracker stat get task tracker stat string name return tt stats get name task tracker stat string total tasks key stat total tasks stat string succeeded tasks key stat succeeded tasks stat string health check failed key stat health check failed stat task tracker stat string tracker name synchronized incr total tasks synchronized incr succeeded tasks synchronized incr health check failed synchronized remove
1217	mapreduce\src\java\org\apache\hadoop\mapred\JSPUtil.java	unrelated	package org apache hadoop mapred jsp util lru based cache map string job info job history cache new linked hash map string job info log log log factory get log jsp util wraps link job in progress object contains boolean job view access allowed this usage js ps servlets job with view access check job in progress job null true user authorized view job boolean view allowed true job with view access check job in progress job job in progress get job boolean view job allowed set view access boolean view allowed validates current user view job if user authorized view job method modify response forwards error page returns job view job access flag set false view job access flag callers method check flag decide view allowed job null job given jobid doesnot exist job tracker job with view access check check access and get job job tracker jt job in progress job jt get job jobid job with view access check job new job with view access check job string user request get remote user user null job null jt ac ls enabled return job sets error code sc unauthorized response forwards error page contains error message back link set error and forward string err msg request set attribute error msg err msg request dispatcher dispatcher request get request dispatcher response set status http servlet response sc unauthorized dispatcher forward request response method used process request job page based request received for example like changing priority selected jobs boolean process buttons http servlet request request string user request get remote user actions allowed tracker conf actions allowed tracker conf return true method used generate job table job pages string generate job table string label collection job in progress jobs remove uninitialized jobs calling job in progress synchronized methods job initialization takes time iterator job in progress jobs iterator next boolean modifiable label equals running string builder sb new string builder sb append table border cellpadding cellspacing n jobs size else sb append table n return sb string string get abbreviated job name string name return name length name substring name string generate retired job table job tracker tracker row id throws io exception string builder sb new string builder sb append table border cellpadding cellspacing n iterator job status iterator iterator next else sb append table n return sb string generate retired job xml jsp writer job tracker tracker row id iterator job status iterator iterator next boolean actions allowed job conf conf return conf get boolean jt config private actions key false path get job conf file path path log file path log dir log file get parent org apache hadoop mapreduce job id job id return job history get conf file log dir job id read job history log file construct corresponding link job info also cache link job info quick serving requests job info get job info path log file file system fs string jobid job info job info null synchronized job history cache return job info check access users view job history pages return link job info
1218	mapreduce\src\java\org\apache\hadoop\mapred\JvmContext.java	unrelated	package org apache hadoop mapred jvm context implements writable log log jvm id jvm id string pid jvm context jvm context jvm id id string pid read fields data input throws io exception write data output throws io exception
1219	mapreduce\src\java\org\apache\hadoop\mapred\JVMId.java	unrelated	package org apache hadoop mapred jvm id extends id boolean map job id job id string jvm jvm number format id format number format get instance jvm id job id job id boolean map id jvm id string jt identifier job id boolean map id jvm id boolean map jvm job id get job id boolean equals object compare task in progress ids first job ids tip numbers reduces defined greater maps compare to org apache hadoop mapreduce id string string add unique id given string builder protected string builder append to string builder builder hash code read fields data input throws io exception write data output throws io exception construct jvm id object given jvm id name string str
1220	mapreduce\src\java\org\apache\hadoop\mapred\JvmManager.java	scheduler	package org apache hadoop mapred jvm manager log log log factory get log jvm manager jvm manager for type map jvm manager jvm manager for type reduce jvm manager jvm env construct jvm env list string setup vector string vargs return new jvm env setup vargs stdout stderr log size work dir env conf jvm manager task tracker tracker map jvm manager new jvm manager for type tracker get max current map tasks reduce jvm manager new jvm manager for type tracker get max current reduce tasks jvm manager for type get jvm manager for type task type type type equals task type map else type equals task type reduce return null stop map jvm manager stop reduce jvm manager stop boolean jvm known jvm id jvm id jvm id map jvm else saves pid given task jvm set pid to jvm jvm id jvm id string pid jvm id map jvm else returns pid task string get pid task runner null get task null return null launch jvm task runner jvm env env get task map task else task in progress get task for jvm jvm id jvm id jvm id map jvm else task finished task runner tr tr get task map task else task killed task runner tr tr get task map task else dump stack task runner tr tr get task map task else kill jvm jvm id jvm id jvm id map else adds task work dir cleanup queue task tracker asynchronous deletion work dir delete work dir task tracker tracker task task throws io exception tracker get cleanup thread add to queue jvm manager for type mapping jvm i ds running tasks map jvm id task runner jvm to running task mapping tasks jvm i ds map task runner jvm id running task to jvm mapping jvm i ds reduce jvm processes map jvm id jvm runner jvm id to runner max jvms boolean map task tracker tracker random rand new random system current time millis jvm manager for type max jvms boolean map synchronized set running task for jvm jvm id jvm id synchronized task in progress get task for jvm jvm id jvm id synchronized string get pid by running task task runner synchronized set pid for jvm jvm id jvm id string pid synchronized boolean jvmknown jvm id jvm id synchronized task finished task runner tr synchronized task killed task runner tr synchronized kill jvm jvm id jvm id synchronized kill jvm runner jvm runner jvm runner dump stack task runner tr synchronized stop synchronized remove jvm jvm id jvm id synchronized reap jvm synchronized string get details synchronized spawn new jvm job id job id jvm env env synchronized update on jvm exit jvm id jvm id jvm runner extends thread jvm env helper list string vargs list string setup file stdout file stderr file work dir log size job conf conf map string string env jvm env list string setup vector string vargs file stdout
1221	mapreduce\src\java\org\apache\hadoop\mapred\JvmTask.java	unrelated	package org apache hadoop mapred task abstraction serialized implements writable jvm task implements writable task boolean die jvm task task boolean die jvm task task get task boolean die write data output throws io exception read fields data input throws io exception
1222	mapreduce\src\java\org\apache\hadoop\mapred\KeyValueLineRecordReader.java	unrelated	package org apache hadoop mapred this treats line input key value pair separated separator character the separator specified config file attribute name mapreduce input keyvaluelinerecordreader key value separator the default separator tab character link org apache hadoop mapreduce lib input key value line record reader instead key value line record reader implements record reader text text line record reader line record reader byte separator byte long writable dummy key text inner value class get key class return text text create key text create value key value line record reader configuration job file split split find separator byte utf start length read key value pair line synchronized boolean next text key text value get progress throws io exception synchronized get pos throws io exception synchronized close throws io exception
1223	mapreduce\src\java\org\apache\hadoop\mapred\KeyValueTextInputFormat.java	unrelated	package org apache hadoop mapred an link input format plain text files files broken lines either linefeed carriage return used signal end line each line divided key value parts separator byte if byte exists key entire line value empty link org apache hadoop mapreduce lib input key value text input format instead key value text input format extends file input format text text implements job configurable compression codec factory compression codecs null configure job conf conf protected boolean splitable file system fs path file record reader text text get record reader input split generic split
1224	mapreduce\src\java\org\apache\hadoop\mapred\KillJobAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker kill task job cleanup resources kill job action extends task tracker action job id job id kill job action kill job action job id job id job id get job id write data output throws io exception read fields data input throws io exception
1225	mapreduce\src\java\org\apache\hadoop\mapred\KillTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker kill task kill task action extends task tracker action task attempt id task id kill task action kill task action task attempt id task id task attempt id get task id write data output throws io exception read fields data input throws io exception
1226	mapreduce\src\java\org\apache\hadoop\mapred\LaunchTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker launch new task launch task action extends task tracker action task task launch task action launch task action task task task get task write data output throws io exception read fields data input throws io exception
1227	mapreduce\src\java\org\apache\hadoop\mapred\LimitTasksPerJobTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler limits maximum number tasks running job the limit set means link jt config jt runningtasks per job property limit tasks per job task scheduler extends job queue task scheduler log log log factory get log max tasks per job limit tasks per job task scheduler synchronized start throws io exception synchronized set conf configuration conf synchronized list task assign tasks task tracker task tracker determine maximum number maps reduces willing run task tracker accept maximum local max map load maps local max reduce load reduces host protected synchronized get max map and reduce load local max map load
1228	mapreduce\src\java\org\apache\hadoop\mapred\LineRecordReader.java	pooling	package org apache hadoop mapred treats keys offset file value line link org apache hadoop mapreduce lib input line record reader instead line record reader implements record reader long writable text log log log factory get log line record reader get name compression codec factory compression codecs null start pos end line reader fs data input stream file in seekable file position max line length compression codec codec decompressor decompressor a provides line reader input stream line reader extends org apache hadoop util line reader line reader input stream line reader input stream buffer size line reader input stream configuration conf throws io exception line reader input stream byte record delimiter line reader input stream buffer size byte record delimiter line reader input stream configuration conf line record reader configuration job job split null line record reader configuration job file split split max line length job get int org apache hadoop mapreduce lib input start split get start end start split get length path file split get path compression codecs new compression codec factory job codec compression codecs get codec file open file seek start split file system fs file get file system job file in fs open file compressed input else if first split always throw away first record always except last split read one extra line next method start pos start line record reader input stream offset end offset offset end offset max line length null line record reader input stream offset end offset max line length max line length new line reader record delimiter start offset pos offset end end offset file position null line record reader input stream offset end offset throws io exception offset end offset job null line record reader input stream offset end offset throws io exception max line length job get int org apache hadoop mapreduce lib input new line reader job record delimiter start offset pos offset end end offset file position null long writable create key return new long writable text create value return new text boolean compressed input return codec null max bytes to consume pos return compressed input get file position throws io exception ret val compressed input null file position else return ret val read line synchronized boolean next long writable key text value throws io exception we always read one extra line lies outside upper split limit e end get file position end return false get progress within split synchronized get progress throws io exception start end else synchronized get pos throws io exception return pos synchronized close throws io exception try finally
1229	mapreduce\src\java\org\apache\hadoop\mapred\LinuxTaskController.java	unrelated	package org apache hadoop mapred a link task controller runs task jv ms user submits job this executes setuid executable implement methods link task controller including launching task jvm killing needed also initializing finalizing task environment p the setuid executable launched using command line p p task controller mapreduce job user name command command args p p mapreduce job user name name owner submits job p p command one cardinal value link linux task controller task controller commands enumeration p p command args depends command launched p in addition running killing tasks also sets appropriate access directories files used tasks linux task controller extends task controller log log name executable script contain child jvm command line see write command details string command file taskjvm sh path setuid executable string task controller exe linux task controller enum task controller commands setup throws io exception launch task jvm task controller task controller context context run debug script debug script context context throws io exception run command task controller commands task controller command list string build initialize task args task exec context context initialize task task controller context context list string build task cleanup args list string build job cleanup args enable task for cleanup path deletion context context enable job for cleanup path deletion context context enable path for cleanup task controller path deletion context c log output string output string get job id task exec context context list string build launch task args task exec context context get directory list directories configured configs local dir chosen storing data pertaining task string get directory chosen for task file directory shell command executor build task controller executor return task specific directory cache string get task cache directory task exec context context write jvm command line file specified directory note jvm launched using setuid executable could potentially contain strings defined user hence prevent special character attacks write command line file execute write command string cmd line list string build initialize job command args initialize job job initialization context context initialize distributed cache file distributed cache file context context initialize user initialization context context list string build kill task command args task controller context protected signal task task controller context context terminate task task controller context context kill task task controller context context dump task stack task controller context context protected string get task controller executable path string get run as user job conf conf
1230	mapreduce\src\java\org\apache\hadoop\mapred\LocalClientProtocolProvider.java	unrelated	package org apache hadoop mapred local client protocol provider extends client protocol provider client protocol create configuration conf throws io exception client protocol create inet socket address addr configuration conf close client protocol client protocol
1231	mapreduce\src\java\org\apache\hadoop\mapred\LocalJobRunner.java	pooling	package org apache hadoop mapred implements map reduce locally process debugging local job runner implements client protocol log log log factory get log local job runner the maximum number map tasks run parallel local job runner string local max maps mapreduce local map tasks maximum file system fs hash map job id job jobs new hash map job id job job conf conf atomic integer map tasks new atomic integer reduce tasks random rand new random job tracker instrumentation metrics null string job dir local runner counters empty counters new counters get protocol version string protocol client version return client protocol version id protocol signature get protocol signature string protocol return protocol signature get protocol signature job extends thread implements task umbilical protocol the job directory system job client places job configurations this analogous job tracker system directory path system job dir path system job file the job directory task analagous task job directory path local job dir path local job file job id id job conf job num map tasks partial map progress counters map counters counters reduce counters job status status list task attempt id map ids collections synchronized list job profile profile file system local fs boolean killed false tracker distributed cache manager tracker distributerd cache manager task distributed cache manager task distributed cache manager get protocol version string protocol client version protocol signature get protocol signature string protocol job job id jobid string job submit dir throws io exception job profile get profile a runnable instance handles map task run executor protected map task runnable implements runnable create runnables encapsulate map tasks use executor service protected list map task runnable get map task runnables initialize counters hold partial progress various task attempts synchronized init counters num maps creates executor service used run map tasks protected executor service create map executor num map tasks run task umbilical protocol methods jvm task get task jvm context context return null synchronized boolean status update task attempt id task id return current values counters job including tasks progress synchronized counters get current counters task reporting commit pending waiting commit response commit pending task attempt id taskid throws io exception interrupted exception report diagnostic info task attempt id taskid string trace report next record range task attempt id taskid boolean ping task attempt id taskid throws io exception boolean commit task attempt id taskid throws io exception done task attempt id task id throws io exception synchronized fs error task attempt id task id string message throws io exception shuffle error task attempt id task id string message throws io exception synchronized fatal error task attempt id task id string msg throws io exception map task completion events update get map completion events job id job id local job runner configuration conf throws io exception new job conf conf local job runner job conf conf throws io exception fs file system get local conf conf conf metrics new job tracker metrics inst null new job conf conf job submission protocol methods jobid synchronized org apache hadoop mapreduce job id get new job id
1232	mapreduce\src\java\org\apache\hadoop\mapred\MapFileOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes link map file link org apache hadoop mapreduce lib output map file output format instead map file output format extends file output format writable comparable writable record writer writable comparable writable get record writer file system ignored job conf job open output generated format map file reader get readers file system ignored path dir get entry output generated k extends writable comparable v extends writable writable get entry map file reader readers
1233	mapreduce\src\java\org\apache\hadoop\mapred\MapOutputFile.java	unrelated	package org apache hadoop mapred manipulate working area transient store maps reduces this used map reduce tasks identify directories need write read intermediate files the callers methods child space see mapreduce cluster local dir task tracker job cache job id attempt id this used task tracker space map output file job conf conf string reduce input file format string map map output file local dir allocator dir alloc path get output file path get output file for write size path get output index file path get output index file for write size path get spill file spill number path get spill file for write spill number size path get spill index file spill number path get spill index file for write spill number size path get input file map id path get input file for write org apache hadoop mapreduce task id map id removes files related task remove all set conf configuration conf
1234	mapreduce\src\java\org\apache\hadoop\mapred\Mapper.java	unrelated	package org apache hadoop mapred maps input key value pairs set intermediate key value pairs p maps individual tasks transform input records intermediate records the transformed intermediate records need type input records a given input pair may map zero many output pairs p p the hadoop map reduce framework spawns one map task link input split generated link input format job code mapper code implementations access link job conf job via link job configurable configure job conf initialize similarly use link closeable close method de initialization p p the framework calls link map object object output collector reporter key value pair code input split code task p p all intermediate values associated given output key subsequently grouped framework passed link reducer determine output users control grouping specifying code comparator code via link job conf set output key comparator class class p p the grouped code mapper code outputs partitioned per code reducer code users control keys hence records go code reducer code implementing custom link partitioner p users optionally specify code combiner code via link job conf set combiner class class perform local aggregation intermediate outputs helps cut amount data transferred code mapper code code reducer code p the intermediate grouped outputs always stored link sequence file applications specify intermediate outputs compressed link compression codec used via code job conf code p p if job href doc root org apache hadoop mapred job conf html reducer none zero reduces output code mapper code directly written link file system without grouping keys p p example p p blockquote pre my mapper lt k extends writable comparable v extends writable gt extends map reduce base implements mapper lt k v k v gt enum my counters num records string map task id string input file records configure job conf job map task id job get job context task attempt id input file job get job context map input file map k key v val output collector lt k v gt output reporter reporter throws io exception process lt key value gt pair assume takes let framework know alive kicking reporter progress process increment lt key value gt pairs processed records increment counters reporter incr counter num records every records update application level status records reporter set status map task id processed records input file input file output result output collect key val pre blockquote p p applications may write custom link map runnable exert greater control map processing e g multi threaded code mapper code etc p mapper k v k v extends job configurable closeable map k key v value output collector k v output reporter reporter throws io exception
1235	mapreduce\src\java\org\apache\hadoop\mapred\MapReduceBase.java	unrelated	package org apache hadoop mapred base link mapper link reducer implementations p provides default op implementations methods non trivial applications need p map reduce base implements closeable job configurable default implementation nothing close throws io exception default implementation nothing configure job conf job
1236	mapreduce\src\java\org\apache\hadoop\mapred\MapReducePolicyProvider.java	unrelated	package org apache hadoop mapred link policy provider map reduce protocols map reduce policy provider extends policy provider service map reduce services service get services
1237	mapreduce\src\java\org\apache\hadoop\mapred\MapRunnable.java	unrelated	package org apache hadoop mapred expert generic link mapper p custom implementations code map runnable code exert greater control map processing e g multi threaded asynchronous mappers etc p map runnable k v k v start mapping input tt lt key value gt tt pairs p mapping input records output records complete method returns p run record reader k v input output collector k v output
1238	mapreduce\src\java\org\apache\hadoop\mapred\MapRunner.java	unrelated	package org apache hadoop mapred default link map runnable implementation map runner k v k v mapper k v k v mapper boolean incr proc count configure job conf job run record reader k v input output collector k v output protected mapper k v k v get mapper
1239	mapreduce\src\java\org\apache\hadoop\mapred\MapTask.java	unrelated	package org apache hadoop mapred a map task map task extends task the size record index file map outputs map output index record length task split index split meta info new task split index approx header length log log log factory get log map task get name progress map phase progress sort phase set phase task set phase task status phase map get progress set status map map task super map task string job file task attempt id task id super job file task id partition num slots required split meta info split index boolean map task return true localize configuration job conf conf super localize configuration conf task runner create runner task tracker tracker return new map task runner tip tracker conf write data output throws io exception super write map or reduce read fields data input throws io exception super read fields map or reduce this wraps user record reader update counters progress records read tracked record reader k v record reader k v raw in counters counter file input byte counter counters counter input record counter task reporter reporter bytes in prev bytes in curr statistics fs stats tracked record reader task reporter reporter job conf job k create key v create value synchronized boolean next k key v value throws io exception protected incr counters protected synchronized boolean move to next k key v value get pos throws io exception return raw in get pos close throws io exception get progress throws io exception task reporter get task reporter get input bytes statistics stats this skips records based failed ranges previous attempts skipping record reader k v extends tracked record reader k v skip range iterator skip it sequence file writer skip writer boolean write skip recs task umbilical protocol umbilical counters counter skip rec counter rec index skipping record reader task umbilical protocol umbilical synchronized boolean next k key v value throws io exception protected synchronized boolean move to next k key v value throws io exception write skipped rec k key v value throws io exception run job conf job task umbilical protocol umbilical throws io exception class not found exception interrupted exception umbilical umbilical map task task reporter reporter start reporter umbilical boolean use new api job get use new mapper initialize job get job id reporter use new api check cleanup job task job cleanup job setup task cleanup use new api else done umbilical reporter t t get split details path file offset throws io exception file system fs file get file system conf fs data input stream file fs open file file seek offset string name text read string file class t cls try catch class not found exception ce serialization factory factory new serialization factory conf deserializer t deserializer deserializer open file t split deserializer deserialize null pos file get pos get counters find counter file close return split inkey invalue outkey outvalue run old mapper job conf job input split input split get split details new path split index get split location update job with split job input split reporter
1240	mapreduce\src\java\org\apache\hadoop\mapred\MapTaskRunner.java	unrelated	package org apache hadoop mapred runs map task map task runner extends task runner map task runner task in progress task task tracker tracker job conf conf string get child java opts job conf job conf string default value get child ulimit job conf job conf string get child env job conf job conf level get log level job conf job conf
1241	mapreduce\src\java\org\apache\hadoop\mapred\MapTaskStatus.java	unrelated	package org apache hadoop mapred map task status extends task status map finish time sort finish time map task status map task status task attempt id taskid progress num slots boolean get is map sets finish time set finish time finish time get shuffle finish time set shuffle finish time shuffle finish time get map finish time set map finish time map finish time get sort finish time set sort finish time sort finish time synchronized status update task status status read fields data input throws io exception write data output throws io exception add fetch failed map task attempt id map task id
1242	mapreduce\src\java\org\apache\hadoop\mapred\Merger.java	scheduler	package org apache hadoop mapred merger utility used map reduce tasks merging memory disk segments merger log log log factory get log merger local directories local dir allocator dir alloc k extends object v extends object raw key value iterator merge configuration conf file system fs throws io exception k extends object v extends object raw key value iterator merge configuration conf file system fs throws io exception k extends object v extends object raw key value iterator merge configuration conf file system fs k extends object v extends object raw key value iterator merge configuration conf file system fs k extends object v extends object raw key value iterator merge configuration conf file system fs k extends object v extends object k extends object v extends object raw key value iterator merge configuration conf file system fs return new merge queue k v conf fs segments comparator reporter k extends object v extends object write file raw key value iterator records writer k v writer throws io exception segment k extends object v extends object throws io exception boolean variable including considering merge part sort phase this true map task false reduce task it used calculating merge progress boolean final merge false consider final merge for progress merge queue k extends object v extends object extends priority queue segment k v implements raw key value iterator
1243	mapreduce\src\java\org\apache\hadoop\mapred\MergeSorter.java	unrelated	package org apache hadoop mapred this implements sort method basic type sorter base merge sort note really wrapper actual mergesort implementation util package the main intent providing setup input data util merge sort algo latter need bother various data structures created map output rather concentrate core algorithm thereby allowing easy integration mergesort implementation the bridge util merge sort comparator merge sorter extends basic type sorter base implements comparator int writable progress update frequency progress calls the sort method derived basic type sorter base overridden raw key value iterator sort the implementation compare method comparator note compare int writable int writable j add extra memory utilized sort method get memory utilized
1244	mapreduce\src\java\org\apache\hadoop\mapred\MRConstants.java	unrelated	package org apache hadoop mapred some handy constants mr constants timeouts constants counter update interval result codes success file not found the custom http header used map output length string map output length map output length the custom http header used raw map output length string raw map output length raw map output length the map task map output data transferred string from map task map task the reduce task number map output transferred string for reduce task reduce task string workdir work
1245	mapreduce\src\java\org\apache\hadoop\mapred\MultiFileInputFormat.java	unrelated	package org apache hadoop mapred an link input format returns link multi file split link get splits job conf method splits constructed files input paths each split returned contains nearly equal content length br subclasses implement link get record reader input split job conf reporter construct code record reader code code multi file split code multi file input format k v extends file input format k v input split get splits job conf job num splits find size split index avg length per split record reader k v get record reader input split split
1246	mapreduce\src\java\org\apache\hadoop\mapred\MultiFileSplit.java	unrelated	package org apache hadoop mapred a sub collection input files unlike link file split multi file split represent split file split input files smaller sets the atomic unit split file br multi file split used implement link record reader reading one record per file multi file split extends combine file split multi file split multi file split job conf job path files lengths string get locations throws io exception add to set set string set string array string string
1247	mapreduce\src\java\org\apache\hadoop\mapred\NodeHealthCheckerService.java	unrelated	package org apache hadoop mapred the provides functionality checking health node reporting back service health checker asked report node health checker service log log log factory get log node health checker service absolute path health script string node health script delay node health script executed interval time time script timedout script timeout timer used schedule node health monitoring script execution timer node health script scheduler shell command executor used execute monitoring script shell command executor shexec null configuration used checker configuration conf pattern used searching output node health script string error pattern error configuration keys string health check script property string health check interval property string health check failure interval property string health check script arguments property end configuration keys time error message string node health script timed out msg node health script timed default frequency running node health script default health check interval default script time period default health script failure interval default health check interval boolean healthy string health report last reported time timer task timer enum health checker exit status node health monitor executor extends timer task node health checker service configuration conf initialize configuration conf start stop boolean healthy synchronized set healthy boolean healthy string get health report synchronized set health report string health report get last reported time synchronized set last reported time last reported time boolean run configuration conf synchronized set health status boolean healthy string output synchronized set health status boolean healthy string output synchronized set health status task tracker health status health status xxx not used directly timer task get timer
1248	mapreduce\src\java\org\apache\hadoop\mapred\Operation.java	scheduler	package org apache hadoop mapred generic operation maps dependent set ac ls drive authorization operation enum operation view job counters queue acl administer jobs job acl view job view job details queue acl administer jobs job acl view job view task logs queue acl administer jobs job acl view job kill job queue acl administer jobs job acl modify job fail task queue acl administer jobs job acl modify job kill task queue acl administer jobs job acl modify job set job priority queue acl administer jobs job acl modify job submit job queue acl submit job null queue acl q acl needed job acl job acl needed operation queue acl q acl job acl job acl
1249	mapreduce\src\java\org\apache\hadoop\mapred\OutputCollector.java	unrelated	package org apache hadoop mapred collects code lt key value gt code pairs output link mapper link reducer p code output collector code generalization facility provided map reduce framework collect data output either code mapper code code reducer code e intermediate outputs output job p output collector k v adds key value pair output collect k key v value throws io exception
1250	mapreduce\src\java\org\apache\hadoop\mapred\OutputCommitter.java	unrelated	package org apache hadoop mapred code output committer code describes commit task output map reduce job p the map reduce framework relies code output committer code job p ol li setup job initialization for example create temporary output directory job initialization job li li cleanup job job completion for example remove temporary output directory job completion li li setup task temporary output li li check whether task needs commit this avoid commit procedure task need commit li li commit task output li li discard task commit li ol output committer for framework setup job output initialization setup job job context job context throws io exception for cleaning job output job completion link abort job job context instead cleanup job job context job context throws io exception for committing job output successful job completion note invoked jobs runstate successful commit job job context job context throws io exception for aborting unsuccessful job output note invoked jobs runstate link job status failed link job status killed abort job job context job context status throws io exception sets output task setup task task attempt context task context throws io exception check whether task needs commit boolean needs task commit task attempt context task context throws io exception to promote task temporary output output location the task output moved job output directory commit task task attempt context task context throws io exception discard task output abort task task attempt context task context throws io exception this method implements new calling old method note input types different new old apis bridge two setup job org apache hadoop mapreduce job context job context this method implements new calling old method note input types different new old apis bridge two link abort job org apache hadoop mapreduce job context org apache hadoop mapreduce job status state instead cleanup job org apache hadoop mapreduce job context context this method implements new calling old method note input types different new old apis bridge two commit job org apache hadoop mapreduce job context context this method implements new calling old method note input types different new old apis bridge two abort job org apache hadoop mapreduce job context context throws io exception this method implements new calling old method note input types different new old apis bridge two setup task org apache hadoop mapreduce task attempt context task context this method implements new calling old method note input types different new old apis bridge two boolean this method implements new calling old method note input types different new old apis bridge two commit task org apache hadoop mapreduce task attempt context task context this method implements new calling old method note input types different new old apis bridge two abort task org apache hadoop mapreduce task attempt context task context
1251	mapreduce\src\java\org\apache\hadoop\mapred\OutputFormat.java	unrelated	package org apache hadoop mapred code output format code describes output specification map reduce job p the map reduce framework relies code output format code job p ol li validate output specification job for e g check output directory already exist li provide link record writer implementation used write output files job output files stored link file system li ol output format k v get link record writer given job record writer k v get record writer file system ignored job conf job throws io exception check validity output specification job p this validate output specification job job submitted typically checks already exist throwing exception already exists output overwritten p check output specs file system ignored job conf job throws io exception
1252	mapreduce\src\java\org\apache\hadoop\mapred\OutputLogFilter.java	unrelated	package org apache hadoop mapred this filters log files directory given it doesnt accept paths logs this used list paths output directory follows path file list file util stat paths fs list status dir new output log filter link org apache hadoop mapred utils output file utils output log filter instead output log filter implements path filter path filter log filter boolean accept path path
1253	mapreduce\src\java\org\apache\hadoop\mapred\Partitioner.java	unrelated	package org apache hadoop mapred partitions key space p code partitioner code controls partitioning keys intermediate map outputs the key subset key used derive partition typically hash function the total number partitions number reduce tasks job hence controls code code reduce tasks intermediate key hence record sent reduction p partitioner k v extends job configurable get paritition number given key hence record given total number partitions e number reduce tasks job p typically hash function subset key p get partition k key v value num partitions
1254	mapreduce\src\java\org\apache\hadoop\mapred\Queue.java	unrelated	package org apache hadoop mapred a storing properties job queue queue implements comparable queue log log log factory get log queue queue name string name null acls list map string access control list acls queue state queue state state queue state running an object used schedulers fill arbitrary scheduling information the string method objects called framework get string displayed ui object scheduling info set queue children properties props queue queue string name map string access control list acls queue state state string get name set name string name map string access control list get acls set acls map string access control list acls queue state get state set state queue state state object get scheduling info set scheduling info object scheduling info copy scheduling info queue source queue add child queue child set queue get children set properties properties props properties get properties map string queue get inner queues map string queue get leaf queues compare to queue queue boolean equals object string string hash code job queue info get job queue info boolean hierarchy same as queue new state
1255	mapreduce\src\java\org\apache\hadoop\mapred\QueueACL.java	scheduler	package org apache hadoop mapred enum representing access control list drives set operations performed queue enum queue acl submit job acl submit job administer jobs acl administer jobs currently acl acl administer jobs checked operations fail task kill task kill job set job priority view job todo add acl list jobs ability authenticate users ui todo add acl change acl admin tool configuring queues string acl name queue acl string acl name string get acl name
1256	mapreduce\src\java\org\apache\hadoop\mapred\QueueAclsInfo.java	unrelated	package org apache hadoop mapred class encapsulate queue ac ls particular user queue acls info extends org apache hadoop mapreduce queue acls info default constructor queue acls info queue acls info construct new queue acls info object using queue name queue operations array queue acls info string queue name string operations queue acls info downgrade
1257	mapreduce\src\java\org\apache\hadoop\mapred\QueueConfigurationParser.java	unrelated	package org apache hadoop mapred class parsing mapred queues xml the format consists nesting queues within queues feature called hierarchical queues the parser expects queues defined within queues tag top level element xml document creates complete queue hieararchy queue configuration parser log log boolean acls enabled false default root protected queue root null xml tags mapred queues xml string name separator string queue tag queue string acl submit job tag acl submit job string acl administer job tag acl administer jobs the value read queues config file tag used to enable queue acls job acls mapreduce cluster acls enabled set mapred site xml string acls enabled tag acls enabled string properties tag properties string state tag state string queue name tag name string queues tag queues string property tag property string key tag key string value tag value default constructor deperacated queue configuration parser queue configuration parser queue configuration parser string conf file boolean acls enabled queue configuration parser input stream xml input boolean acls enabled load from input stream xml input set acls enabled boolean acls enabled boolean acls enabled queue get root set root queue root method load resource file generates root protected queue load resource input stream resource input queue parse resource element queues node queue create hierarchy string parent element queue node populate properties queue properties populate properties element field checks name tag queues checks queue children shouldnot acls state else throws exception validate node node string get simple queue name string full q name construct link element single queue constructing inner queue lt name gt lt properties gt lt state gt inner lt queue gt elements recursively element get queue element document document job queue info jqi
1258	mapreduce\src\java\org\apache\hadoop\mapred\QueueManager.java	scheduler	package org apache hadoop mapred class exposes information queues maintained hadoop map reduce framework p the map reduce framework configured one queues depending scheduler configured while schedulers work one queue schedulers support multiple queues some schedulers also support notion queues within queues feature called hierarchical queues p queue names unique used key lookup queues hierarchical queues named fully qualified name q q q q child queue q q child queue q p leaf level queues queues contain queues within jobs submitted leaf level queues p queues configured various properties some properties common schedulers handled schedulers might also associate several custom properties queues these properties parsed maintained per queue framework if schedulers need complicated structure maintain configuration per queue free use facilities provided framework define mechanisms in cases likely name queue used relate common properties queue scheduler specific properties p information related queue name properties scheduling information children exposed via serializable called link job queue info p queues configured configuration file mapred queues xml to support backwards compatibility queues also configured mapred site xml however configured latter support hierarchical queues queue manager log log log factory get log queue manager map queue name queue object map string queue leaf queues new hash map string queue map string queue queues new hash map string queue string queue conf file name mapred queues xml string queue conf default file name mapred queues default xml prefix configuration queue related keys string queue conf property name prefix mapred queue resource queue acls configured queue root null represents job queue acls enabled mapreduce cluster boolean acls enabled false queue configuration parser get queue configuration parser queue manager acls disabled queue manager boolean acls enabled queue manager configuration cluster conf queue manager string conf file boolean acls enabled initialize queue configuration parser cp synchronized set string get leaf queue names synchronized boolean access synchronized boolean running string queue name synchronized set scheduler info synchronized object get scheduler info string queue name string msg refresh failure with change of hierarchy string msg refresh failure with scheduler failure synchronized refresh queues configuration conf method internal use string full property name synchronized job queue info get job queue infos synchronized job queue info get job queue info string queue synchronized map string job queue info get job queue info mapping synchronized queue acls info get queue acls user group information ugi synchronized set queues queue queues job queue info get root queues job queue info get child queues string queue name queue get queue string queue name boolean acls enabled queue get root synchronized access control list get queue acl string queue name dump configuration writer configuration conf throws io exception dump configuration writer string config file dump configuration json generator dump generator
1259	mapreduce\src\java\org\apache\hadoop\mapred\RamManager.java	pooling	package org apache hadoop mapred code ram manager code manages memory pool configured limit ram manager reserve memory data coming given input stream else code false code boolean reserve requested size input stream throws interrupted exception return memory pool unreserve requested size
1260	mapreduce\src\java\org\apache\hadoop\mapred\RawKeyValueIterator.java	unrelated	package org apache hadoop mapred code raw key value iterator code iterator used iterate raw keys values sort merge intermediate data raw key value iterator gets current raw key data input buffer get key throws io exception gets current raw value data input buffer get value throws io exception sets current key value get key get value code false code otherwise boolean next throws io exception closes iterator underlying streams closed close throws io exception gets progress object indicating bytes processed iterator far progress get progress
1261	mapreduce\src\java\org\apache\hadoop\mapred\RecordReader.java	unrelated	package org apache hadoop mapred code record reader code reads lt key value gt pairs link input split p code record reader code typically converts byte oriented view input provided code input split code presents record oriented view link mapper link reducer tasks processing it thus assumes responsibility processing record boundaries presenting tasks keys values p record reader k v reads next key value pair input processing boolean next k key v value throws io exception create object appropriate type used key k create key create object appropriate type used value v create value returns current position input get pos throws io exception close link input split future operations close throws io exception how much input link record reader consumed e processed get progress throws io exception
1262	mapreduce\src\java\org\apache\hadoop\mapred\RecordWriter.java	unrelated	package org apache hadoop mapred code record writer code writes output lt key value gt pairs output file p code record writer code implementations write job outputs link file system record writer k v writes key value pair write k key v value throws io exception close code record writer code future operations close reporter reporter throws io exception
1263	mapreduce\src\java\org\apache\hadoop\mapred\Reducer.java	unrelated	package org apache hadoop mapred reduces set intermediate values share key smaller set values p the number code reducer code job set user via link job conf set num reduce tasks code reducer code implementations access link job conf job via link job configurable configure job conf method initialize similarly use link closeable close method de initialization p p code reducer code primary phases p ol li id shuffle shuffle p code reducer code input grouped output link mapper in phase framework code reducer code fetches relevant partition output code mapper code via http p li li id sort sort p the framework groups code reducer code inputs code key code since different code mapper code may output key stage p p the shuffle sort phases occur simultaneously e outputs fetched merged p id secondary sort secondary sort p if equivalence rules keys grouping intermediates different grouping keys reduction one may specify code comparator code via link job conf set output value grouping comparator class since link job conf set output key comparator class class used control intermediate keys grouped used conjunction simulate secondary sort values p for example say want find duplicate web pages tag url best known example you would set job like ul li map input key url li li map input value document li li map output key document checksum url pagerank li li map output value url li li partitioner checksum li li output key comparator checksum decreasing pagerank li li output value grouping comparator checksum li ul li li id reduce reduce p in phase link reduce object iterator output collector reporter method called code lt key list values code pair grouped inputs p p the output reduce task typically written link file system via link output collector collect object object p li ol p the output code reducer code b sorted b p p example p p blockquote pre my reducer lt k extends writable comparable v extends writable gt extends map reduce base implements reducer lt k v k v gt enum my counters num records string reduce task id keys configure job conf job reduce task id job get job context task attempt id reduce k key iterator lt v gt values output collector lt k v gt output reporter reporter throws io exception process values values next v value values next increment values key values process lt key value gt pair assume takes let framework know alive kicking values reporter progress process output lt key value gt output collect key value increment lt key list values gt pairs processed keys increment counters reporter incr counter num records every keys update application level status keys reporter set status reduce task id processed keys pre blockquote p reducer k v k v extends job configurable closeable reduce k key iterator v values
1264	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTask.java	unrelated	package org apache hadoop mapred a reduce task reduce task extends task register ctor writable factories set factory log log log factory get log reduce task get name num maps compression codec codec get progress set status reduce set phase task status phase shuffle phase start progress copy phase progress sort phase progress reduce phase counters counter shuffled maps counter get counters find counter task counter shuffled maps counters counter reduce shuffle bytes get counters find counter task counter reduce shuffle bytes counters counter reduce input key counter get counters find counter task counter reduce input groups counters counter reduce input value counter get counters find counter task counter reduce input records counters counter reduce output counter get counters find counter task counter reduce output records counters counter reduce combine input counter get counters find counter task counter combine input records counters counter reduce combine output counter get counters find counter task counter combine output records counters counter file output byte counter get counters find counter file output format counter bytes written a custom comparator map output files here ordering determined file size path in case files size different file paths first parameter considered smaller second one in case files size path considered equal comparator file status map output file comparator new comparator file status a sorted set keeping set map output files disk sorted set file status map output files on disk new tree set file status map output file comparator reduce task super reduce task string job file task attempt id task id super job file task id partition num slots required num maps num maps compression codec init codec check map outputs compressed conf get compress map output return null task runner create runner task tracker tracker task in progress tip throws io exception return new reduce task runner tip tracker conf boolean map task return false get num maps return num maps localize given job conf specific task localize configuration job conf conf throws io exception super localize configuration conf conf set num map tasks num maps write data output throws io exception super write write int num maps write number maps read fields data input throws io exception super read fields num maps read int get input files reducer path get map files file system fs boolean local throws io exception list path file list new array list path local else return file list array new path reduce values iterator key value reduce values iterator raw key value iterator value next protected value move to next inform reduce progress skipping reduce values iterator key value extends reduce values iterator key value skip range iterator skip it task umbilical protocol umbilical counters counter skip group counter counters counter skip rec counter grp index class key key class class value val class sequence file writer skip writer boolean write skip recs boolean next task reporter reporter skipping reduce values iterator raw key value iterator next key throws io exception boolean may be skip throws io exception write skipped rec key key value value throws io exception run job conf
1265	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTaskRunner.java	unrelated	package org apache hadoop mapred runs reduce task reduce task runner extends task runner reduce task runner task in progress task task tracker tracker close throws io exception string get child java opts job conf job conf string default value get child ulimit job conf job conf string get child env job conf job conf level get log level job conf job conf
1266	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTaskStatus.java	unrelated	package org apache hadoop mapred reduce task status extends task status shuffle finish time sort finish time list task attempt id failed fetch tasks new array list task attempt id reduce task status reduce task status task attempt id taskid progress num slots object clone boolean get is map set finish time finish time get shuffle finish time set shuffle finish time shuffle finish time get sort finish time set sort finish time sort finish time get map finish time set map finish time shuffle finish time list task attempt id get fetch failed maps add fetch failed map task attempt id map task id synchronized status update task status status synchronized clear status read fields data input throws io exception write data output throws io exception
1267	mapreduce\src\java\org\apache\hadoop\mapred\ReinitTrackerAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker reinitialize reinit tracker action extends task tracker action reinit tracker action write data output throws io exception read fields data input throws io exception
1268	mapreduce\src\java\org\apache\hadoop\mapred\Reporter.java	unrelated	package org apache hadoop mapred a facility map reduce applications report progress update counters status information etc p link mapper link reducer use code reporter code provided report progress indicate alive in scenarios application takes insignificant amount time process individual key value pairs crucial since framework might assume task timed kill task p applications also update link counters via provided code reporter code p reporter extends progressable a constant reporter type nothing reporter null new reporter set status description task set status string status get link counter given group given name counter get counter enum name get link counter given group given name counter get counter string group string name increments counter identified key link enum type specified amount code enum code incremented incr counter enum key amount increments counter identified group counter name specified amount incremented incr counter string group string counter amount get link input split object map input split get input split get progress task progress represented number inclusive get progress
1269	mapreduce\src\java\org\apache\hadoop\mapred\ResourceEstimator.java	unrelated	package org apache hadoop mapred class responsible modeling resource consumption running tasks for temp space maps there one resource estimator per job in progress resource estimator log job in progress log log log factory get log completed maps input size completed maps output size completed maps updates job in progress job threshhold to use resource estimator job in progress job protected synchronized update with completed task task status ts protected synchronized get estimated total map output size get estimated map output size get estimated reduce input size
1270	mapreduce\src\java\org\apache\hadoop\mapred\RunningJob.java	scheduler	package org apache hadoop mapred code running job code user query details running map reduce job p clients get hold code running job code via link job client query running job details name configuration progress etc p running job get underlying job configuration configuration get configuration get job identifier job id get id rather use link get id string get job id get name job string get job name get path submitted job configuration string get job file get url job progress information displayed string get tracking url get progress job map tasks when map tasks completed function returns map progress throws io exception get progress job reduce tasks when reduce tasks completed function returns reduce progress throws io exception get progress job cleanup tasks when cleanup tasks completed function returns cleanup progress throws io exception get progress job setup tasks when setup tasks completed function returns setup progress throws io exception check job finished this non blocking call boolean complete throws io exception check job completed successfully boolean successful throws io exception blocks job complete wait for completion throws io exception returns current state job link job status get job state throws io exception kill running job blocks job tasks killed well if job longer running simply returns kill job throws io exception set priority running job set job priority string priority throws io exception get events indicating completion success failure component tasks task completion event get task completion events start from throws io exception kill indicated task attempt list otherwise killed w affecting job failure status kill task task attempt id task id boolean fail throws io exception kill task string task id boolean fail throws io exception gets counters job counters get counters throws io exception gets diagnostic messages given task attempt string get task diagnostics task attempt id taskid throws io exception get url history file archived returns empty history file available yet string get history url throws io exception check whether job removed job tracker memory retired on retire job history file copied location known link get history url boolean retired throws io exception
1271	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsBinaryInputFormat.java	unrelated	package org apache hadoop mapred input format reading keys values sequence files binary raw format link org apache hadoop mapreduce lib input sequence file as binary input format instead sequence file as binary input format extends sequence file input format bytes writable bytes writable sequence file as binary input format super record reader bytes writable bytes writable get record reader return new sequence file as binary record reader job file split split read records sequence file binary raw bytes sequence file as binary record reader sequence file reader start end boolean done false data output buffer buffer new data output buffer sequence file value bytes vbytes sequence file as binary record reader configuration conf file split split bytes writable create key bytes writable create value retrieve name key sequence file string get key class name retrieve name value sequence file string get value class name read raw bytes sequence file synchronized boolean next bytes writable key bytes writable val get pos throws io exception close throws io exception return progress within input split get progress throws io exception
1272	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsBinaryOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes keys values link sequence file binary raw format link org apache hadoop mapreduce lib output sequence file as binary output format instead sequence file as binary output format extends sequence file output format bytes writable bytes writable inner used append raw protected writable value bytes extends org apache hadoop mapreduce set key link sequence file p this allows user specify key different actual link bytes writable used writing p set sequence file output key class job conf conf conf set class org apache hadoop mapreduce lib output set value link sequence file p this allows user specify value different actual link bytes writable used writing p set sequence file output value class job conf conf conf set class org apache hadoop mapreduce lib output get key link sequence file class extends writable comparable get sequence file output key class job conf conf return conf get class org apache hadoop mapreduce lib output get value link sequence file class extends writable get sequence file output value class job conf conf return conf get class org apache hadoop mapreduce lib output record writer bytes writable bytes writable throws io exception get path temporary output file path file file output format get task output path job name file system fs file get file system job compression codec codec null compression type compression type compression type none get compress output job default codec sequence file writer return new record writer bytes writable bytes writable check output specs file system ignored job conf job super check output specs ignored job get compress output job
1273	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsTextInputFormat.java	unrelated	package org apache hadoop mapred this similar sequence file input format except generates sequence file as text record reader converts input keys values string forms calling string method link org apache hadoop mapreduce lib input sequence file as text input format instead sequence file as text input format extends sequence file input format text text sequence file as text input format record reader text text get record reader input split split
1274	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsTextRecordReader.java	unrelated	package org apache hadoop mapred this converts input keys values string forms calling string method this sequence file as text input format line record reader text input format link org apache hadoop mapreduce lib input sequence file as text record reader instead sequence file as text record reader implements record reader text text sequence file record reader writable comparable writable sequence file record reader writable comparable inner key writable inner value sequence file as text record reader configuration conf file split split text create key text create value read key value pair line synchronized boolean next text key text value throws io exception get progress throws io exception synchronized get pos throws io exception synchronized close throws io exception
1275	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileInputFilter.java	unrelated	package org apache hadoop mapred a allows map red job work sample sequence files the sample decided filter set job link org apache hadoop mapreduce lib input sequence file input filter instead sequence file input filter k v extends sequence file input format k v string filter class org apache hadoop mapreduce lib sequence file input filter create record reader given split record reader k v get record reader input split split set filter set filter class configuration conf class filter class filter extends filter base extends org apache hadoop mapreduce records filter matching key regex regex filter extends filter base this returns percentage records percent filter extends filter base this returns set records examing md digest md filter extends filter base filter record reader k v
1276	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileInputFormat.java	unrelated	package org apache hadoop mapred an link input format link sequence file link org apache hadoop mapreduce lib input sequence file input format instead sequence file input format k v extends file input format k v sequence file input format protected file status list status job conf job throws io exception record reader k v get record reader input split split
1277	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes link sequence file link org apache hadoop mapreduce lib output sequence file output format instead sequence file output format k v extends file output format k v record writer k v get record writer open output generated format sequence file reader get readers configuration conf path dir get link compression type output link sequence file defaulting link compression type record compression type get output compression type job conf conf set link compression type output link sequence file link sequence file set output compression type job conf conf
1278	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileRecordReader.java	unrelated	package org apache hadoop mapred an link record reader link sequence file sequence file record reader k v implements record reader k v sequence file reader start end boolean true protected configuration conf sequence file record reader configuration conf file split split throws io exception path path split get path file system fs path get file system conf new sequence file reader fs path conf end split get start split get length conf conf split get start get position start get position start end the key must passed link next object object class get key class return get key class the value must passed link next object object class get value class return get value class k create key return k reflection utils new instance get key class conf v create value return v reflection utils new instance get value class conf synchronized boolean next k key v value throws io exception return false pos get position boolean remaining next key null remaining pos end sync seen else return protected synchronized boolean next k key throws io exception return false pos get position boolean remaining next key null pos end sync seen else return protected synchronized get current value v value throws io exception get current value value return progress within input split get progress throws io exception end start else synchronized get pos throws io exception return get position protected synchronized seek pos throws io exception seek pos synchronized close throws io exception close
1279	mapreduce\src\java\org\apache\hadoop\mapred\SkipBadRecords.java	unrelated	package org apache hadoop mapred utility skip bad records functionality it contains various settings related skipping bad records p hadoop provides optional mode execution bad records detected skipped attempts p this feature used map reduce tasks crashes deterministically certain input this happens due bugs map reduce function the usual course would fix bugs but sometimes possible perhaps bug third party libraries source code available due task never reaches completion even multiple attempts complete data task lost p p with feature small portion data lost surrounding bad record may acceptable user applications see link skip bad records set mapper max skip records configuration p p the skipping mode gets kicked certain failures see link skip bad records set attempts to start skipping configuration p p in skipping mode map reduce task maintains record range getting processed times before giving input map reduce function sends record range task tracker if task crashes task tracker knows one last reported range on attempts range get skipped p skip bad records string counter group skipping task counters string counter map processed records string counter reduce processed groups string attempts to start skipping string auto incr map proc count string auto incr reduce proc count string out path job context skip outdir string mapper max skip records string reducer max skip groups get attempts to start skipping configuration conf set attempts to start skipping configuration conf boolean get auto incr mapper proc count configuration conf set auto incr mapper proc count configuration conf boolean get auto incr reducer proc count configuration conf set auto incr reducer proc count configuration conf path get skip output path configuration conf set skip output path job conf conf path path get mapper max skip records configuration conf set mapper max skip records configuration conf get reducer max skip groups configuration conf set reducer max skip groups configuration conf
1280	mapreduce\src\java\org\apache\hadoop\mapred\SortedRanges.java	unrelated	package org apache hadoop mapred keeps ranges sorted start index the added ranges always ensured non overlapping provides skip range iterator skips ranges stored object sorted ranges implements writable log log log factory get log sorted ranges tree set range ranges new tree set range indices count get iterator skips stored ranges the iterator next call return index starting synchronized skip range iterator skip range iterator return new skip range iterator ranges iterator get indices stored ranges synchronized get indices count return indices count get sorted set ranges synchronized sorted set range get ranges return ranges add range indices it ensured added range overlap existing ranges if overlaps existing overlapping ranges removed single range superset removed ranges range added if range length anything synchronized add range range range empty start index range get start index end index range get end index make sure overlapping ranges sorted set range head set ranges head set range head set size iterator range tail set it ranges tail set range iterator tail set it next add start index end index remove range indices if range found existing ranges existing ranges shrunk if range length anything synchronized remove range range range empty start index range get start index end index range get end index make sure overlapping ranges sorted set range head set ranges head set range head set size iterator range tail set it ranges tail set range iterator tail set it next add start end end start synchronized read fields data input throws io exception indices count read long ranges new tree set range size read int size synchronized write data output throws io exception write long indices count write int ranges size iterator range ranges iterator next string string string buffer sb new string buffer iterator range ranges iterator next return sb string index range comprises start index length a range length also the range stores indices type range implements comparable range writable start index length range start index length range get start index start index inclusive get start index get end index end index exclusive get end index get length get length range empty length zero code false code otherwise boolean empty boolean equals object hash code compare to range read fields data input throws io exception write data output throws io exception string string index iterator skips stored ranges skip range iterator implements iterator long iterator range range iterator range range new range next constructor skip range iterator iterator range range iterator returns true till index reaches long max value code false code otherwise synchronized boolean next get next available index the index starts synchronized long next next skip if in range get whether ranges skipped code false code otherwise synchronized boolean skipped all ranges remove supported doesn apply remove
1281	mapreduce\src\java\org\apache\hadoop\mapred\SpillRecord.java	unrelated	package org apache hadoop mapred spill record backing store byte buffer buf view backing storage longs long buffer entries spill record num partitions spill record path index file name job conf job throws io exception spill record path index file name job conf job string expected index owner spill record path index file name job conf job checksum crc return number index record entries spill size get spill offsets given partition index record get index partition set spill offsets given partition put index index record rec partition write spill record location provided write to file path loc job conf job write to file path loc job conf job checksum crc index record start offset raw length part length index record index record start offset raw length part length
1282	mapreduce\src\java\org\apache\hadoop\mapred\StatisticsCollector.java	unrelated	package org apache hadoop mapred collects statistics time windows statistics collector default period time window since start new time window since start time window last week new time window last week time window last day new time window last day time window last hour new time window last hour time window last minute new time window last minute time window default collect windows statistics collector since start statistics collector last day statistics collector last hour period boolean started map time window stat updater updaters new linked hash map time window stat updater map string stat statistics new hash map string stat statistics collector default period statistics collector period period period synchronized start started timer timer new timer timer thread monitoring true timer task task new timer task millis period timer schedule at fixed rate task millis millis started true protected synchronized update stat updater c updaters values map time window stat updater get updaters return collections unmodifiable map updaters map string stat get statistics return collections unmodifiable map statistics synchronized stat create stat string name return create stat name default collect windows synchronized stat create stat string name time window windows statistics get name null map time window time stat time stats time window window windows stat stat new stat name time stats statistics put name stat return stat synchronized stat remove stat string name stat stat statistics remove name stat null return stat time window string name window size update granularity time window string name window size update granularity hash code boolean equals object obj stat string name map time window time stat time stats stat string name map time window time stat time stats synchronized inc incr synchronized inc synchronized map time window time stat get values time stat stat updater protected map string time stat stat to collect synchronized add time stat string name time stat synchronized time stat remove time stat string name synchronized update updates time window statistics buckets time window stat updater extends stat updater collect buckets updates per bucket updates buckets time window stat updater time window w update period synchronized update
1283	mapreduce\src\java\org\apache\hadoop\mapred\Task.java	scheduler	package org apache hadoop mapred base tasks task implements writable configurable log log string merged output prefix merged protected string get file system counter names string uri scheme protected string filesystem counter group file system counters helper methods construct task output paths construct output file names output directory listing number format number format number format get instance synchronized string get output name partition fields string job file job configuration file string user user running job task attempt id task id unique includes job id partition id within job task status task status current status task protected job status state job run state for cleanup protected boolean job cleanup false protected boolean job setup false protected boolean task cleanup false an opaque data field used attach extra data task this used hadoop scheduler mesos associate mesos task id task recover i ds task tracker protected bytes writable extra data new bytes writable skip ranges based failed ranges previous attempts sorted ranges skip ranges new sorted ranges boolean skipping false boolean write skip recs true currently processing record start index volatile current rec start index iterator long current rec index iterator resource calculator plugin resource calculator null init cpu cumulative time protected job conf conf protected map output file map output file new map output file protected local dir allocator dir alloc max retries protected job context job context protected task attempt context task context protected org apache hadoop mapreduce output format output format protected org apache hadoop mapreduce output committer committer protected counters counter spilled records counter protected counters counter failed shuffle counter protected counters counter merged map outputs counter num slots required protected task umbilical protocol umbilical protected secret key token secret protected gc time updater gc updater constructors task task string job file task attempt id task id partition accessors set job file string job file job file job file string get job file return job file task attempt id get task id return task id get num slots required counters get counters return counters job id get job id set job token secret secret key token secret secret key get job token secret get partition synchronized task status phase get phase protected synchronized set phase task status phase phase protected boolean write skip recs protected set write skip recs boolean write skip recs protected report fatal error task attempt id id throwable throwable protected statistics get fs statistics path path configuration conf throws io exception sorted ranges get skip ranges set skip ranges sorted ranges skip ranges boolean skipping set skipping boolean skipping synchronized task status state get state synchronized set state task status state state set task cleanup task boolean task cleanup task boolean job cleanup task boolean job abort task boolean job setup task set job setup task set job cleanup task set job cleanup task state job status state status boolean map or reduce string get user set user string user writable methods write data output throws io exception read fields data input throws io exception string string return task id string localize configuration job conf conf
1284	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptContext.java	unrelated	package org apache hadoop mapred instead task attempt context task attempt id get task attempt id progressable get progressible job conf get job conf
1285	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptContextImpl.java	unrelated	package org apache hadoop mapred instead task attempt context impl reporter reporter task attempt context impl job conf conf task attempt id taskid task attempt context impl job conf conf task attempt id taskid get task attempt id task attempt id get task attempt id progressable get progressible job conf get job conf get progress counter get counter enum counter name counter get counter string group name string counter name report progress progress set current status task given set status string status
1286	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptID.java	unrelated	package org apache hadoop mapred task attempt id represents immutable unique identifier task attempt each task attempt one particular instance map reduce task identified task id task attempt id consists parts first part link task id task attempt id belongs second part task attempt number br an example task attempt id code attempt code represents zeroth task attempt fifth map task third job running jobtracker started code code p applications never construct parse task attempt id strings rather use appropriate constructors link name string method task attempt id extends org apache hadoop mapreduce task attempt id constructs task attempt id object given link task id task attempt id task id task id id constructs task id object given parts task attempt id string jt identifier job id boolean map id constructs task id object given parts task attempt id string jt identifier job id task type type task attempt id downgrade new task attempt id old one task attempt id downgrade org apache hadoop mapreduce task attempt id old task id get task id job id get job id task attempt id read data input throws io exception construct task attempt id object given task attempt id name string str returns regex pattern matches task attempt i ds arguments given null case part regex generic for example obtain regex matching task attempt i ds jobtracker job first map task would use pre task attempt id get task attempt i ds pattern null null true null pre return pre attempt pre string get task attempt i ds pattern string jt identifier map task type map task type reduce task id attempt id returns regex pattern matches task attempt i ds arguments given null case part regex generic for example obtain regex matching task attempt i ds jobtracker job first map task would use pre task attempt id get task attempt i ds pattern null null task type map null pre return pre attempt pre string get task attempt i ds pattern string jt identifier string builder get task attempt i ds pattern wo prefix string jt identifier
1287	mapreduce\src\java\org\apache\hadoop\mapred\TaskCompletionEvent.java	unrelated	package org apache hadoop mapred this used track task completion events job tracker link org apache hadoop mapreduce task completion event instead task completion event enum status failed killed succeeded obsolete tipfailed task completion event empty array default constructor writable task completion event constructor event id created externally incremented per event job incrementally starting task completion event event id task completion event downgrade returns task id string get task id returns task id task attempt id get task attempt id returns enum status sucess status failure status get task status sets task id set task id string task id sets task id protected set task attempt id task attempt id task id set task status protected set task status status status set task completion time protected set task run time task completion time set event id assigned incrementally starting protected set event id event id set task tracker http location protected set task tracker http string task tracker http
1288	mapreduce\src\java\org\apache\hadoop\mapred\TaskController.java	unrelated	package org apache hadoop mapred controls initialization finalization clean tasks also launching killing task jv ms this defines api initializing finalizing cleaning tasks also launching killing task jv ms subclasses implement logic required performing actual actions br task controller implements configurable configuration conf log log log factory get log task controller configuration get conf the list directory paths specified variable configs local dir this used determine among list directories picked storing data particular task protected string mapred local dirs set conf configuration conf setup throws io exception initialize job job initialization context context throws io exception initialize distributed cache file distributed cache file context context launch task jvm task controller context context destroy task jvm task controller context context perform initializing actions required task run initialize task task controller context context task exec context task controller context extends task exec context task controller path deletion context extends path deletion context contains info related path file dir deleted this info task controller task path deletion context extends task controller path deletion context contains info related path file dir deleted this info task controller job path deletion context extends task controller path deletion context initialization context distributed cache file context extends initialization context job initialization context extends initialization context debug script context extends task exec context terminate task task controller context context kill task task controller context context dump task stack task controller context context initialize user initialization context context run debug script debug script context context enable task for cleanup path deletion context context enable job for cleanup path deletion context context string get run as user job conf conf
1289	mapreduce\src\java\org\apache\hadoop\mapred\TaskGraphServlet.java	unrelated	package org apache hadoop mapred the servlet outputs svg graphics map reduce task statuses task graph servlet extends http servlet serial version uid l height graph w margins width height graph w margins height margin space axis ymargin margin space x axis xmargin one third f f get http servlet request request http servlet response response computes average progress per bar get map avarage progress tasks per bar index computes average progresses per bar get reduce avarage progresses tasks per bar index print rect print writer width height print line print writer x x print text print writer x string text
1290	mapreduce\src\java\org\apache\hadoop\mapred\TaskID.java	unrelated	package org apache hadoop mapred task id represents immutable unique identifier map reduce task each task id encompasses multiple attempts made execute map reduce task uniquely indentified task attempt id task id consists parts first part link job id task in progress belongs second part task id either r representing whether task map task reduce task and third part task number br an example task id code task code represents fifth map task third job running jobtracker started code code p applications never construct parse task id strings rather use appropriate constructors link name string method task id extends org apache hadoop mapreduce task id constructs task id object given link job id task id org apache hadoop mapreduce job id job id boolean map id constructs task in progress id object given parts task id string jt identifier job id boolean map id constructs task id object given link job id task id org apache hadoop mapreduce job id job id task type type id constructs task in progress id object given parts task id string jt identifier job id task type type id task id downgrade new task id old one task id downgrade org apache hadoop mapreduce task id old task id read data input throws io exception job id get job id returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching first map task jobtracker job would use pre task id get task i ds pattern null null true pre return pre task pre integer string get task i ds pattern string jt identifier integer job id map task type map task type reduce task id returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching first map task jobtracker job would use pre task id get task i ds pattern null null true pre return pre task pre string get task i ds pattern string jt identifier integer job id string builder get task i ds pattern wo prefix string jt identifier task id name string str
1291	mapreduce\src\java\org\apache\hadoop\mapred\TaskInProgress.java	scheduler	package org apache hadoop mapred task in progress maintains info needed task lifetime owning job a given task might speculatively executed reexecuted need level indirection running id br a given task in progress contains multiple taskids might executing one time that allows speculative execution a taskid never recycled a tip allocates enough taskids account speculation failures ever handle once tip dead task in progress max task execs max nonspec tasks run concurrently max task attempts speculative lag num attempts per restart log log log factory get log task in progress defines tip string job file null task split meta info split info num maps partition job tracker jobtracker job history job history task id id job in progress job num slots required status tip success event number num task failures num killed tasks progress old progress rate string state last dispatch time recent time task attempt given tt exec start time started first task attempt exec finish time completes boolean failed false boolean killed false max skip records failed ranges failed ranges new failed ranges volatile boolean skipping false boolean job cleanup false boolean job setup false the next usable taskid tip next task id the taskid took tip success task attempt id successful task id the first taskid tip task attempt id first task id map task id task tracker id contains tasks currently runnings tree map task attempt id string active tasks new tree map task attempt id string all attempt ids tip tree set task attempt id tasks new tree set task attempt id job conf conf map task attempt id list string task diagnostic data new tree map task attempt id list string map task id task status tree map task attempt id task status task statuses new tree map task attempt id task status map task id task tracker id contains cleanup attempts ran tree map task attempt id string cleanup tasks new tree map task attempt id string tree set string machines where failed new tree set string tree set task attempt id tasks reported closed new tree set task attempt id list tasks kill taskid fail tree map task attempt id boolean tasks to kill new tree map task attempt id boolean task commit taskattemptid task attempt id task to commit volatile counters counters new counters hash map task attempt id long dispatch time map new hash map task attempt id long string user constructor map task task in progress job id jobid string job file job file job file split info split jobtracker jobtracker job job conf conf partition partition max skip records skip bad records get mapper max skip records conf num slots required num slots required set max task attempts init jobid jobtracker null user job get user constructor reduce task task in progress job id jobid string job file job file job file num maps num maps partition partition jobtracker jobtracker job job conf conf max skip records skip bad records get reducer max skip groups conf num slots required num slots required set max task attempts init jobid jobtracker null
1292	mapreduce\src\java\org\apache\hadoop\mapred\TaskLog.java	unrelated	package org apache hadoop mapred a simple logger handle task specific user logs this uses system property code hadoop log dir code task log log log string userlogs dir name userlogs file log dir local fs set used write to index file local file system local fs null file get task log file task attempt id taskid boolean cleanup file get real task log file location task attempt id taskid log file detail log file detail get log file detail task attempt id taskid throws io exception file get tmp index file task attempt id taskid boolean cleanup file get index file task attempt id taskid boolean cleanup string obtain log dir owner task attempt id taskid throws io exception string get base log dir file get attempt dir task attempt id taskid boolean cleanup prev out length prev err length prev log length write to index file string log location throws io exception reset prev lengths string log location volatile task attempt id current taskid null synchronized sync logs string log location throws io exception enum log name reader extends input stream string bash command bash string tail command tail get task log length job conf conf list string capture out and error list string cmd list string capture out and error list string setup list string capture out and error list string setup list string capture out and error list string setup string build command line list string setup list string cmd string build debug script command line list string cmd string debugout throws io exception string add command list string cmd boolean executable throws io exception list string capture debug out list string cmd file get user log dir file get job dir job id jobid task log
1293	mapreduce\src\java\org\apache\hadoop\mapred\TaskLogAppender.java	unrelated	package org apache hadoop mapred a simple log j appender task child map reduce system logs task log appender extends file appender string task id task id managed string rather task id object log j configure configuration log j properties integer max events queue logging event tail null boolean cleanup system properties passed jvm runner string iscleanup property hadoop tasklog iscleanup string logsize property hadoop tasklog total log file size string taskid property hadoop tasklog taskid activate options the task runner passes options system properties set options setters already called synchronized set options from system properties append logging event event flush synchronized close getter setter methods log j synchronized string get task id synchronized set task id string task id event size synchronized get total log file size synchronized set total log file size log size set whether task cleanup attempt true task cleanup attempt false otherwise synchronized set is cleanup boolean cleanup get whether task cleanup attempt synchronized boolean get is cleanup
1294	mapreduce\src\java\org\apache\hadoop\mapred\TaskLogServlet.java	unrelated	package org apache hadoop mapred a servlet run task trackers provide task logs via http task log servlet extends http servlet serial version uid l log log boolean task log task attempt id task id boolean cleanup construct task log url string get task log url string task tracker host name print task log http servlet response response throws io exception validates given user job view permissions job conf contains job owner job view ac ls we allow job owner super user e mr owner cluster administrators users groups specified configuration using mapreduce job acl view job view job check access for task logs job conf conf string user string job id builds job conf object reading job acls xml file this load default resources returns null job acls xml userlogs jobid local file system this happen restart cluster job level authorization enabled disabled earlier cluster viewing task logs old jobs e jobs finished earlier unsecure cluster job conf get conf from job ac ls file job id job id get logs via http get http servlet request request
1295	mapreduce\src\java\org\apache\hadoop\mapred\TaskMemoryManagerThread.java	unrelated	package org apache hadoop mapred manages memory usage tasks running tt kills task trees overflow step memory limits task memory manager thread extends thread log log log factory get log task memory manager thread task tracker task tracker monitoring interval max memory allowed for all tasks max rss memory allowed for all tasks map task attempt id process tree info process tree info map map task attempt id process tree info tasks to be added list task attempt id tasks to be removed string memory usage string memory usage process tree task id virutal bytes task memory manager thread task tracker task tracker task tracker get total memory allotted for tasks on tt l task tracker task tracker reserved rss memory task tracker get reserved physical memory on tt total physical memory on tt task tracker get total physical memory on tt reserved rss memory job conf disabled memory limit else mainly test purposes note tasktracker variable set task memory manager thread max memory allowed for all tasks set name get class get name process tree info map new hash map task attempt id process tree info tasks to be added new hash map task attempt id process tree info tasks to be removed new array list task attempt id max memory allowed for all tasks max memory allowed for all tasks monitoring interval monitoring interval add task task attempt id tid mem limit mem limit physical synchronized tasks to be added remove task task attempt id tid synchronized tasks to be removed process tree info task attempt id tid string pid procfs based process tree p tree mem limit mem limit physical process tree info task attempt id tid string pid task attempt id get tid string get pid set pid string pid procfs based process tree get process tree set process tree procfs based process tree p tree get mem limit get mem limit physical run log info starting thread get class true is total physical memory check enabled boolean check physical memory return max rss memory allowed for all tasks job conf disabled memory limit is total virtual memory check enabled boolean check virtual memory return max memory allowed for all tasks job conf disabled memory limit check whether task process tree current memory usage limit when java process exec program could momentarily account size memory jvm fork exec fork time creates copy parent memory if monitoring thread detects memory used task tree instance could assume limit kill tree fault process we counter problem employing heuristic check process tree exceeds memory limit twice killed immediately process tree processes older monitoring interval exceeding memory limit even time killed else given benefit doubt lie around one iteration iteration task tree processes tree older thread monitoring interval exceed memory limit false otherwise boolean process tree over limit string id boolean over limit false current mem usage limit else cur mem usage of aged processes limit return over limit method provided easy testing purposes boolean process tree over limit procfs based process tree p tree current mem usage p tree get cumulative vmem
1296	mapreduce\src\java\org\apache\hadoop\mapred\TaskReport.java	unrelated	package org apache hadoop mapred a report state task task report extends org apache hadoop mapreduce task report task report creates new task report object task report task id taskid progress string state creates new task report object task report task id taskid progress string state task report downgrade task report downgrade array org apache hadoop the id task task id get task id return task id downgrade super get task id counters get counters set successful attempt id task set successful attempt task attempt id get attempt id took task completion task attempt id get successful task attempt set running attempt task set running task attempts get running task attempt i ds task collection task attempt id get running task attempts set finish time task protected set finish time finish time set start time task protected set start time start time
1297	mapreduce\src\java\org\apache\hadoop\mapred\TaskRunner.java	unrelated	package org apache hadoop mapred base runs task separate process tasks run separate process order isolate map reduce system code bugs user supplied map reduce functions task runner extends thread log log log factory get log task runner volatile boolean killed false task tracker task in progress tip task object lock new object volatile boolean done false exit code boolean exit code set false string system path separator system get property path separator task tracker tracker task distributed cache manager task distributed cache manager protected job conf conf jvm manager jvm manager task runner task tracker task in progress tip task tracker tracker tip tip tip get task tracker tracker conf conf jvm manager tracker get jvm manager instance task get task return task tracker task in progress get task in progress return tip task tracker get tracker return tracker jvm manager get jvm manager return jvm manager called task output longer needed this method run parent process child exits it execute user code system code close throws io exception get java command line options child map reduce tasks via link job conf mapred map task java opts link job conf mapred reduce task java opts string get child java opts job conf job conf string default value return job conf get job conf mapred task java opts default value get maximum virtual memory child map reduce tasks none specified link job conf mapred map task ulimit link job conf mapred reduce task ulimit get child ulimit job conf job conf return job conf get int job conf mapred task ulimit get environment variables child map reduce tasks code null code unspecified set via link job conf mapred map task env link job conf mapred reduce task env string get child env job conf job conf return job conf get job conf mapred task env get log link level child map reduce tasks level get log level job conf job conf run string error info child error try catch fs error e catch throwable throwable finally launch jvm and wait list string setup vector string vargs file stdout jvm manager launch jvm jvm manager construct jvm env setup vargs stdout synchronized lock prepare log files task file prepare log files task attempt id taskid boolean cleanup file log files new file log files task log get task log file taskid cleanup log files task log get task log file taskid cleanup file log dir log files get parent file boolean b log dir mkdirs b else return log files write child configuration disk set configuration child pick setup child task configuration local dir allocator dir alloc path local task file write child task configuration file local disk write local task file local task file string conf set job file task the child needs know correct path job xml so set path accordingly set job file local task file string list string get vm setup cmd ulimit get child ulimit conf ulimit list string setup null string ulimit cmd shell get ulimit memory command ulimit ulimit cmd null return setup parse given return
1298	mapreduce\src\java\org\apache\hadoop\mapred\TaskScheduler.java	scheduler	package org apache hadoop mapred used link job tracker schedule link task link task tracker p link task scheduler typically use one link job in progress listener receive notifications jobs p it responsibility link task scheduler initialize tasks job calling link job in progress init tasks job added link job in progress listener job added job in progress called tasks job assigned link assign tasks task tracker task scheduler implements configurable protected configuration conf protected task tracker manager task tracker manager configuration get conf return conf set conf configuration conf conf conf synchronized set task tracker manager task tracker manager task tracker manager lifecycle method allow scheduler start work separate threads start throws io exception nothing lifecycle method allow scheduler stop work terminate throws io exception nothing returns tasks like task tracker execute right list task assign tasks task tracker task tracker throws io exception returns collection jobs order specific particular scheduler collection job in progress get jobs string queue name abstract queue refresher scheduler extend return instance link get queue refresher method the link refresh queues list method instance invoked link queue manager whenever gets request administrator refresh queue configuration this method documented contract link queue manager link task scheduler before calling queue refresher caller must hold lock corresponding link task scheduler generally link job tracker queue refresher refresh queue configuration scheduler this method following contract ol li before method link queue manager validation new queue configuration for e g currently addition new queues removal queues level hierarchy supported link queue manager supported schedulers li li schedulers passed list link job queue info root queues e queues top level all descendants properly linked top level queues li li schedulers use scheduler specific queue properties new root queues validate properties apply internally li li once method returns successfully schedulers assumed refresh queue properties successful throughout committed internally link queue manager it guaranteed point successful return scheduler queue refresh queue manager failed if ever abnormalities happen queue framework inconsistent need jt restart li li if scheduler throws exception link refresh queues link queue manager throws away newly read configuration retains old consistent configuration informs request issuer error appropriately li ol refresh queues list job queue info new root queues get link queue refresher scheduler by default link queue refresher exists scheduler set null schedulers need return instance link queue refresher wish refresh queue configuration link queue manager refreshes queue configuration via administrator request queue refresher get queue refresher return null
1299	mapreduce\src\java\org\apache\hadoop\mapred\TaskStatus.java	heartbeat	package org apache hadoop mapred describes current status task this intended comprehensive piece data task status implements writable cloneable log log enumeration reporting current phase task enum phase starting map shuffle sort reduce cleanup state task enum state running succeeded failed unassigned killed task attempt id taskid progress volatile state run state string diagnostic info string state string string task tracker num slots start time ms finish time output size l volatile phase phase phase starting counters counters boolean counters sorted ranges range next record range new sorted ranges range max task status size max string size testcases link get max string size control max size strings link task status note link task status never exposed clients users e map reduce hence users cannot api pass large strings link task status protected get max string size task status task status task attempt id taskid progress num slots task attempt id get task id return taskid boolean get is map get num slots get progress return progress set progress progress state get run state return run state string get task tracker return task tracker set task tracker string tracker task tracker tracker set run state state run state run state run state string get diagnostic info return diagnostic info set diagnostic info string info string get state string return state string set state link task status set state string string state string get next record range going processed task sorted ranges range get next record range set next record range going processed task set next record range sorted ranges range next record range get task finish time shuffle finish time sort finish time set set finish time it takes care case shuffle sort finish completed heartbeat interval reported separately task state task status failed finish time represents task failed get finish time sets finish time task status start time set passed finish time greater zero set finish time finish time get shuffle finish time task if shuffle finish time set due shuffle sort finish phases ending within heartbeat interval set finish time next phase e sort task finish set returns approximate shuffle finish time get shuffle finish time set shuffle finish time set shuffle finish time shuffle finish time get map phase finish time task if map finsh time set due sort phase ending within heartbeat interval set finish time next phase e sort phase set returns approximate map finish time get map finish time set map phase finish time set map finish time map finish time get sort finish time task if sort finish time set due sort reduce phase finishing heartebat interval set finish time finish time set finish time set else returns finish time get sort finish time sets sort finish time shuffle finish time set set sort finish time set sort finish time sort finish time get start time task get start time set start time task start time greater zero set start time start time get current phase task phase map case map tasks reduce one phase shuffle phase sort phase reduce phase get phase set current phase
1300	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerAction.java	unrelated	package org apache hadoop mapred a generic directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker take action task tracker action implements writable ennumeration various actions link job tracker directs link task tracker perform periodically enum action type a factory method create objects given link action type task tracker action create action action type action type action type action type protected task tracker action action type action type return link action type action type get action id write data output throws io exception read fields data input throws io exception
1301	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerInstrumentation.java	unrelated	package org apache hadoop mapred task tracker instrumentation defines number instrumentation points associated task trackers by default instrumentation points nothing subclasses arbitrary instrumentation monitoring points task tracker instrumentation interfaces associated uniquely task tracker we want inner subclasses direct access associated task tracker task tracker instrumentation protected task tracker tt task tracker instrumentation task tracker complete task task attempt id timedout task task attempt id task failed ping task attempt id report task launch task attempt id file stdout file stderr report task end task attempt id status update task task task status task status
1302	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerManager.java	heartbeat	package org apache hadoop mapred manages information link task tracker running cluster this exits primarily test link job tracker intended implemented users task tracker manager managed collection task tracker status task trackers get number of unique hosts cluster status get cluster status registers link job in progress listener updates link task tracker manager add job in progress listener job in progress listener listener unregisters link job in progress listener link task tracker manager remove job in progress listener job in progress listener listener return link queue manager manages queues link task tracker manager queue manager get queue manager return current heartbeat interval used link task tracker get next heartbeat interval kill job identified jobid kill job job id jobid obtain job object identified jobid job in progress get job job id jobid mark task attempt identified taskid killed boolean kill task task attempt id taskid boolean fail initialize job init job job in progress job fail job fail job job in progress job
1303	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerMetricsInst.java	unrelated	package org apache hadoop mapred task tracker metrics inst extends task tracker instrumentation metrics record metrics record num completed tasks timedout tasks tasks failed ping task tracker metrics inst task tracker synchronized complete task task attempt id synchronized timedout task task attempt id synchronized task failed ping task attempt id since object registered updater method called periodically e g every seconds updates metrics context unused
1304	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerStatus.java	unrelated	package org apache hadoop mapred a task tracker status map reduce primitive keeps info task tracker the job tracker maintains set recent task tracker status objects unique task tracker knows task tracker status implements writable log log log factory get log task tracker status register ctor writable factories set factory string tracker name string host http port failures list task status task reports volatile last seen max map tasks max reduce tasks task tracker health status health status unavailable class representing collection resources tasktracker resource status implements writable total virtual memory total physical memory map slot memory size on tt reduce slot memory size on tt available space available virtual memory unavailable byte available physical memory unavailable byte num processors unavailable cumulative cpu time unavailable millisecond cpu frequency unavailable k hz cpu usage unavailable resource status set maximum amount virtual memory tasktracker set total virtual memory total mem get maximum amount virtual memory tasktracker if link job conf disabled memory limit ignored used computation get total virtual memory set maximum amount physical memory tasktracker bytes set total physical memory total ram get maximum amount physical memory tasktracker if link job conf disabled memory limit ignored used computation get total physical memory set memory size map slot tt this used jt accounting slots jobs use memory set map slot memory size on tt mem get memory size map slot tt see link set map slot memory size on tt get map slot memory size on tt set memory size reduce slot tt this used jt accounting slots jobs use memory set reduce slot memory size on tt mem get memory size reduce slot tt see link set reduce slot memory size on tt get reduce slot memory size on tt set available disk space tt set available space avail space will return long max space measured yet get available space set amount available virtual memory tasktracker if input valid number set unavailable bytes set available virtual memory available mem get amount available virtual memory tasktracker will return unavailable cannot obtained bytes get availabel virtual memory set amount available physical memory tasktracker if input valid number set unavailable tasktracker bytes set available physical memory available ram get amount available physical memory tasktracker will return unavailable cannot obtained get available physical memory set cpu frequency task tracker if input valid number set unavailable set cpu frequency cpu frequency get cpu frequency task tracker will return unavailable cannot obtained get cpu frequency set number processors task tracker if input valid number set unavailable set num processors num processors get number processors task tracker will return unavailable cannot obtained get num processors set cumulative cpu time task tracker since it set unavailable currently unavailable set cumulative cpu time cumulative cpu time get cumulative cpu time task tracker since will return unavailable cannot obtained get cumulative cpu time set cpu usage task tracker set cpu usage cpu usage get cpu usage task tracker will return unavailable cannot obtained get cpu usage write data output throws io exception read fields data input throws io exception resource status res status
1305	mapreduce\src\java\org\apache\hadoop\mapred\TaskUmbilicalProtocol.java	unrelated	package org apache hadoop mapred protocol task child process uses contact parent process the parent daemon polls central master new map reduce task runs child process all communication child parent via protocol task umbilical protocol extends versioned protocol changed version since new method get map outputs changed version progress return boolean changed version since replaced task umbilical protocol progress string string org apache hadoop mapred task status phase counters status update string task status version changed counters representation hadoop version changes task status representation hadoop version changes done api via hadoop it expects whether task output needs promoted version changes job tip task id use corresponding objects rather strings version changes counter representation hadoop version changed task status format added report next record range hadoop version adds rp cs task commit part hadoop version get map completion events also indicates events stale hence return type encapsulates events whether reset events index version changed get task method signature hadoop version changed get task method signature hadoop version adds failed unclean killed unclean states hadoop version change signature get task hadoop version modified task id aware new task types version added num required slots task status mapreduce version added fatal error child communicate fatal errors tt version id l called child task process starts get task launched jvm task get task jvm context context throws io exception report child progress parent boolean status update task attempt id task id task status task status throws io exception interrupted exception report error messages back parent calls sparing since messages held job tracker report diagnostic info task attempt id taskid string trace throws io exception report record range going process next task report next record range task attempt id taskid sorted ranges range range throws io exception periodically called child check parent still alive boolean ping task attempt id taskid throws io exception report task successfully completed failure assumed task process exits without calling done task attempt id taskid throws io exception report task complete commit pending commit pending task attempt id task id task status task status throws io exception interrupted exception polling know whether task go ahead commit boolean commit task attempt id taskid throws io exception report reduce task shuffle map outputs shuffle error task attempt id task id string message throws io exception report task encounted local filesystem error fs error task attempt id task id string message throws io exception report task encounted fatal error fatal error task attempt id task id string message throws io exception called reduce task get map output locations finished maps returns update centered around map task completion events the update also piggybacks information whether events copy task tracker changed this trigger action child process fetched map task completion events update get map completion events job id job id throws io exception
1306	mapreduce\src\java\org\apache\hadoop\mapred\TextInputFormat.java	unrelated	package org apache hadoop mapred an link input format plain text files files broken lines either linefeed carriage return used signal end line keys position file values line text instead text input format extends file input format long writable text implements job configurable compression codec factory compression codecs null configure job conf conf protected boolean splitable file system fs path file record reader long writable text get record reader
1307	mapreduce\src\java\org\apache\hadoop\mapred\TextOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes plain text files link org apache hadoop mapreduce lib output text output format instead text output format k v extends file output format k v protected line record writer k v implements record writer k v string utf utf byte newline protected data output stream byte key value separator line record writer data output stream string key value separator line record writer data output stream write object byte stream handling text special case write object object throws io exception synchronized write k key v value synchronized close reporter reporter throws io exception record writer k v get record writer file system ignored throws io exception boolean compressed get compress output job string key value separator job get mapreduce output textoutputformat separator compressed else
1308	mapreduce\src\java\org\apache\hadoop\mapred\TIPStatus.java	unrelated	package org apache hadoop mapred the states link task in progress seen job tracker enum tip status pending running complete killed failed
1309	mapreduce\src\java\org\apache\hadoop\mapred\UserLogCleaner.java	unrelated	package org apache hadoop mapred this thread task tracker cleanup user logs responsibilities thread ol li removing old user logs li ol user log cleaner extends thread log log log factory get log user log cleaner default user log retain hours day default thread sleep time hour map job id long completed jobs collections thread sleep time mr async disk service log async disk clock clock user log cleaner configuration conf throws io exception set clock clock clock clock get clock run process completed jobs throws io exception clears logs userlog directory adds job directories deletion default retain hours deletes directories this usually called reinit restart task tracker clear old user logs configuration conf throws io exception get userlog retain millis configuration conf adds job user log directory cleanup thread delete logs user log retain hours if configuration null user log retain hours configured deleted value user log cleaner default user log retain hours job completion time millis the configuration user log retain hours read job id user logs deleted mark job logs for deletion job completion time configuration conf remove job user log deletion unmark job from log deletion job id jobid deletes log path this path removed immediately link mr async disk service delete log path string log path throws io exception
1310	mapreduce\src\java\org\apache\hadoop\mapred\Utils.java	unrelated	package org apache hadoop mapred a utility it provides a path filter utility filter output part files output dir utils output file utils this filters output part files given directory it accept files filenames logs success this used list paths output directory follows path file list file util stat paths fs list status dir new output files filter output files filter extends output log filter this filters log files directory given it doesnt accept paths logs this used list paths output directory follows path file list file util stat paths fs list status dir new output log filter output log filter implements path filter
1311	mapreduce\src\java\org\apache\hadoop\mapred\jobcontrol\Job.java	unrelated	package org apache hadoop mapred jobcontrol job extends controlled job log log log factory get log job success waiting running ready failed dependent failed construct job job job conf job conf array list depending jobs throws io exception job job conf conf throws io exception mapred framework job id get assigned job id jobid set framework set assigned job id job id mapred job id synchronized job conf get job conf set mapred job conf job synchronized set job conf job conf job conf synchronized get state job client get job client array list job get depending jobs
1312	mapreduce\src\java\org\apache\hadoop\mapred\jobcontrol\JobControl.java	unrelated	package org apache hadoop mapred jobcontrol link org apache hadoop mapreduce lib jobcontrol job control instead job control extends construct job control group jobs job control string group name array list job cast to job list list controlled job cjobs array list job get waiting jobs array list job get running jobs array list job get ready jobs array list job get successful jobs array list job get failed jobs add collection jobs add jobs collection job jobs get state
1313	mapreduce\src\java\org\apache\hadoop\mapred\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop mapred join this provides implementation resetable iterator the implementation uses link java util array list store elements added replaying requested prefer link stream backed iterator link org apache hadoop mapreduce lib join array list backed iterator instead array list backed iterator x extends writable extends array list backed iterator array list backed iterator array list x data
1314	mapreduce\src\java\org\apache\hadoop\mapred\join\ComposableInputFormat.java	unrelated	package org apache hadoop mapred join refinement input format requiring implementors provide composable record reader instead record reader link org apache hadoop mapreduce lib join composable input format instead composable input format k extends writable comparable composable record reader k v get record reader input split split
1315	mapreduce\src\java\org\apache\hadoop\mapred\join\ComposableRecordReader.java	unrelated	package org apache hadoop mapred join additional operations required record reader participate join link org apache hadoop mapreduce lib join composable record reader instead composable record reader k extends writable comparable extends record reader k v comparable composable record reader k return position collector occupies id return key record reader would supply call next k v k key clone key head record reader object provided key k key throws io exception returns true stream empty provides guarantee call next k v succeed boolean next skip key value pairs keys less equal key provided skip k key throws io exception while key value pairs record reader match given key register join collector provided accept composite record reader join collector jc k key throws io exception
1316	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeInputFormat.java	unrelated	package org apache hadoop mapred join an input format capable performing joins set data sources sorted partitioned way a user may define new join types setting property tt mapred join define lt ident gt tt classname in expression tt mapred join expr tt identifier assumed composable record reader tt mapred join keycomparator tt classname used compare keys join link org apache hadoop mapreduce lib join composite input format instead composite input format k extends writable comparable expression parse tree if requests proxied parser node root composite input format interpret given composite expression code func ident func func func tbl path see java lang class name java lang string path see org apache hadoop fs path path java lang string reads expression tt mapred join expr tt property user supplied join types tt mapred join define lt ident gt tt types paths supplied tt tbl tt given input paths input format listed set format job conf job throws io exception add defaults add user identifiers job root parser parse job get mapred join expr null job adds default set identifiers parser protected add defaults try catch no such method exception e inform parser user defined types add user identifiers job conf job throws io exception pattern x pattern compile mapred join define w map entry string string kv job build composite input split child input formats assigning ith split child ith composite split input split get splits job conf job num splits throws io exception set format job job set long mapred min split size long max value return root get splits job num splits construct composite record reader children input format defined init expression the outermost join need composable necessarily composite mandating tuple writable strictly correct composable record reader k tuple writable get record reader set format job return root get record reader split job reporter convenience method constructing composite formats given input format inf path p return code tbl inf p string compose class extends input format inf string path return compose inf get name intern path new string buffer string convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op class extends input format inf string infname inf get name string buffer ret new string buffer op string p path ret set char at ret length return ret string convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op class extends input format inf array list string tmp new array list string path length path p path return compose op inf tmp array new string string buffer compose string inf string path sb append tbl inf sb append path sb append return sb
1317	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeInputSplit.java	unrelated	package org apache hadoop mapred join this input split contains set child input splits any input split inserted collection must default constructor link org apache hadoop mapreduce lib join composite input split instead composite input split implements input split fill totsize l input split splits composite input split composite input split capacity add input split collection capacity reached add input split throws io exception get ith child input split input split get return aggregate length child input splits currently added get length throws io exception get length ith child input split get length throws io exception collect set hosts child input splits string get locations throws io exception get locations ith input split string get location throws io exception write splits following format code count classn split split splitn write data output throws io exception inherit doc faliing access checks read fields data input throws io exception
1318	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeRecordReader.java	unrelated	package org apache hadoop mapred join a record reader effect joins record readers sharing common key type partitioning link org apache hadoop mapreduce lib join composite record reader instead composite record reader k extends writable comparable key type v extends writable accepts record reader k v children x extends writable emits writables type implements configurable id configuration conf resetable iterator x empty new resetable iterator empty x writable comparator cmp class extends writable comparable keyclass priority queue composable record reader k q protected join collector jc protected composable record reader k extends v kids protected boolean combine object srcs tuple writable value create record reader tt capacity tt children position tt id tt parent reader the id root composite record reader convention relying recommended composite record reader id capacity assert capacity invalid capacity id id null cmpcl jc new join collector capacity kids new composable record reader capacity return position collector occupies id return id inherit doc set conf configuration conf conf conf inherit doc configuration get conf return conf return sorted list record readers composite protected priority queue composable record reader k get record reader queue return q return comparator defining ordering record readers composite protected writable comparator get comparator return cmp add record reader collection the id record reader determines tuple entry appear adding record readers id undefined behavior add composable record reader k extends v rr throws io exception kids rr id rr null q rr next collector join values this accumulates values given key child record readers if one child rr contain duplicate keys emit cross product associated values exhausted join collector k key resetable iterator x iters pos boolean first true construct collector capable handling specified number children join collector card register given iterator position id add id resetable iterator x return key associated collection k key codify contents collector iterated when called record readers registered key added resetable iterators reset k key clear state information clear returns false exhausted reset k called protected boolean next populate tuple iterators it case given iterators n values sources n sharing key k repeated calls next yield i x i protected boolean next tuple writable val throws io exception replay last tuple emitted boolean replay tuple writable val throws io exception close child iterators close throws io exception write next value key value accepted operation associated set record readers boolean flush tuple writable value throws io exception return key current join value top record reader heap k key jc next q empty return null clone key top rr given object key k key throws io exception writable utils clone into key key return true possible could emit values boolean next return jc next q empty pass skip key child r rs skip k key throws io exception array list composable record reader k tmp q empty cmp compare q peek key key composable record reader k rr tmp obtain iterator child r rs apropos value type ultimately emitted join protected resetable iterator x get delegate if key provided matches composite give join collector iterator values may emit accept composite record
1319	mapreduce\src\java\org\apache\hadoop\mapred\join\InnerJoinRecordReader.java	unrelated	package org apache hadoop mapred join full inner join link org apache hadoop mapreduce lib join inner join record reader instead inner join record reader k extends writable comparable inner join record reader id job conf conf capacity return true iff tuple full data sources contain key protected boolean combine object srcs tuple writable dst
1320	mapreduce\src\java\org\apache\hadoop\mapred\join\JoinRecordReader.java	unrelated	package org apache hadoop mapred join base composite joins returning tuples arbitrary writables link org apache hadoop mapreduce lib join join record reader instead join record reader k extends writable comparable join record reader id job conf conf capacity boolean next k key tuple writable value throws io exception inherit doc tuple writable create value protected resetable iterator tuple writable get delegate protected join delegation iterator
1321	mapreduce\src\java\org\apache\hadoop\mapred\join\MultiFilterRecordReader.java	unrelated	package org apache hadoop mapred join base composite join returning values derived multiple sources generally tuples link org apache hadoop mapreduce lib join multi filter record reader instead multi filter record reader k extends writable comparable class extends writable valueclass tuple writable ivalue multi filter record reader id job conf conf capacity protected v emit tuple writable dst throws io exception protected boolean combine object srcs tuple writable dst inherit doc boolean next k key v value throws io exception inherit doc v create value protected resetable iterator v get delegate protected multi filter delegation iterator
1322	mapreduce\src\java\org\apache\hadoop\mapred\join\OuterJoinRecordReader.java	unrelated	package org apache hadoop mapred join full outer join link org apache hadoop mapreduce lib join outer join record reader instead outer join record reader k extends writable comparable outer join record reader id job conf conf capacity emit everything collector protected boolean combine object srcs tuple writable dst
1323	mapreduce\src\java\org\apache\hadoop\mapred\join\OverrideRecordReader.java	unrelated	package org apache hadoop mapred join prefer quot rightmost quot data source key for example tt s s s tt prefer values s s values s s keys emitted sources link org apache hadoop mapreduce lib join override record reader instead override record reader k extends writable comparable override record reader id job conf conf capacity emit value highest position tuple protected v emit tuple writable dst instead filling join collector iterators data sources fill rightmost key this saves space discarding sources also emits number key value pairs preferred record reader instead repeating stream n times n cardinality cross product discarded streams given key protected fill join collector k iterkey throws io exception
1324	mapreduce\src\java\org\apache\hadoop\mapred\join\Parser.java	unrelated	package org apache hadoop mapred join very simple shift reduce parser join expressions this sufficient user extension permitted ought replaced parser generator complex grammars supported in particular quot shift reduce quot parser states each set formals requires different internal node type responsible interpreting list tokens receives this sufficient current grammar several annoying properties might inhibit extension in particular parenthesis always function calls algebraic filter grammar would require node type must also work around internals parser for cases adding hierarchy particularly extending join record reader multi filter record reader fairly straightforward one need relevant method usually link composite record reader combine property map value identifier parser parser enum t type cif ident comma lparen rparen quot num tagged union type tokens join expression token t type type token t type type t type get type return type node get node throws io exception get num throws io exception string get str throws io exception num token extends token num num token num get num return num node token extends token node node node token node node node get node str token extends token string str str token t type type string str string get str simple lexer wrapping stream tokenizer this encapsulates creation tagged union tokens initializes steam tokenizer lexer stream tokenizer tok lexer string token next throws io exception node implements composable input format return node type registered particular identifier by default c node composite node w node quot wrapped quot nodes user nodes likely composite nodes node ident string ident throws io exception class ncstr sig string protected for given identifier add mapping nodetype parse tree composable record reader created including formals required invoke constructor the nodetype constructor signature filled child node protected add identifier string ident class mcstr sig inst protected id protected string ident protected class extends writable comparator cmpcl protected node string ident protected set id id protected set key comparator class extends writable comparator cmpcl parse list token args job conf job throws io exception nodetype parse tree quot wrapped quot input formats w node extends node class cstr sig add identifier string ident string indir input format inf w node string ident let first actual define input format second define tt mapred input dir tt property parse list token job conf job throws io exception job conf get conf job conf job input split get splits job conf job num splits composable record reader get record reader string string internal nodetype quot composite quot input formats c node extends node class cstr sig add identifier string ident inst array list node kids new array list node c node string ident set key comparator class extends writable comparator cmpcl combine input splits child input formats link composite input split input split get splits job conf job num splits composable record reader get record reader parse list comma separated nodes parse list token args job conf job throws io exception string string token reduce stack token st job conf job throws io exception linked list token args new linked list token st empty t type lparen equals st
1325	mapreduce\src\java\org\apache\hadoop\mapred\join\ResetableIterator.java	unrelated	package org apache hadoop mapred join this defines stateful iterator replay elements added directly note extend link java util iterator link org apache hadoop mapreduce lib join resetable iterator instead resetable iterator t extends writable extends org apache hadoop mapreduce lib join resetable iterator t empty u extends writable
1326	mapreduce\src\java\org\apache\hadoop\mapred\join\StreamBackedIterator.java	unrelated	package org apache hadoop mapred join this provides implementation resetable iterator this implementation uses byte array store elements added link org apache hadoop mapreduce lib join stream backed iterator instead stream backed iterator x extends writable
1327	mapreduce\src\java\org\apache\hadoop\mapred\join\TupleWritable.java	unrelated	package org apache hadoop mapred join writable type storing multiple link org apache hadoop io writable this general purpose tuple type in almost cases users encouraged implement serializable types perform better validation provide efficient encodings capable tuple writable relies join framework type safety assumes instances rarely persisted assumptions incompatible contrary general case link org apache hadoop mapreduce lib join tuple writable instead tuple writable tuple writable tuple writable writable vals set written clear written clear written
1328	mapreduce\src\java\org\apache\hadoop\mapred\join\WrappedRecordReader.java	unrelated	package org apache hadoop mapred join proxy record reader participating join framework this keeps track quot head quot key value pair provided record reader keeps store values matching key source participating join link org apache hadoop mapreduce lib join wrapped record reader instead wrapped record reader k extends writable comparable boolean empty false record reader k u rr id index values inserted collector k khead key top rr u vhead value assoc khead writable comparator cmp resetable iterator u vjoin wrapped record reader id record reader k u rr inherit doc id k key key k qkey throws io exception boolean next skip k key throws io exception protected boolean next throws io exception accept composite record reader join collector k key boolean next k key u value throws io exception k create key u create value get progress throws io exception get pos throws io exception close throws io exception compare to composable record reader k boolean equals object hash code
1329	mapreduce\src\java\org\apache\hadoop\mapred\lib\BinaryPartitioner.java	unrelated	package org apache hadoop mapred lib partition link binary comparable keys using configurable part bytes array returned link binary comparable get bytes link org apache hadoop mapreduce lib partition binary partitioner instead binary partitioner v extends org apache hadoop mapreduce lib partition binary partitioner v implements partitioner binary comparable v configure job conf job
1330	mapreduce\src\java\org\apache\hadoop\mapred\lib\Chain.java	unrelated	package org apache hadoop mapred lib the chain provides common functionality link chain mapper link chain reducer chain extends org apache hadoop mapreduce lib chain chain string mapper by value chain mapper value string reducer by value chain reducer value job conf chain job conf list mapper mappers new array list mapper reducer reducer cache key value output serializations chain element avoid everytime lookup list serialization mappers key serialization list serialization mappers value serialization serialization reducer key serialization serialization reducer value serialization chain boolean map k v k v add mapper boolean map job conf job conf k v k v set reducer job conf job conf configure job conf job conf protected job conf get chain job conf mapper get first map reducer get reducer output collector get mapper collector mapper index output collector get reducer collector output collector output close throws io exception using thread local reuse byte array output stream used ser deser thread local would break used multi threaded map runner thread local data output buffer thread local data output buffer chain output collector k v implements output collector k v
1331	mapreduce\src\java\org\apache\hadoop\mapred\lib\ChainMapper.java	unrelated	package org apache hadoop mapred lib the chain mapper allows use multiple mapper within single map task p the mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p the key functionality feature mappers chain need aware executed chain this enables reusable specialized mappers combined perform composite operations within single task p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use maching output input key value conversion done chaining code p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p important there need specify output key value chain mapper done add mapper last mapper chain p chain mapper usage pattern p pre conf set job name chain conf set input format text input format conf set output format text output format p job conf map a conf new job conf false chain mapper add mapper conf a map long writable text text text true map a conf p job conf map b conf new job conf false chain mapper add mapper conf b map text text long writable text false map b conf p job conf reduce conf new job conf false chain reducer set reducer conf x reduce long writable text text text true reduce conf p chain reducer add mapper conf c map text text long writable text false null p chain reducer add mapper conf d map long writable text long writable long writable true null p file input format set input paths conf dir file output format set output path conf dir p job client jc new job client conf running job job jc submit job conf pre use link org apache hadoop mapreduce lib chain chain mapper instead chain mapper implements mapper k v k v add mapper job conf job chain chain chain mapper configure job conf job map object key object value output collector output close throws io exception
1332	mapreduce\src\java\org\apache\hadoop\mapred\lib\ChainReducer.java	unrelated	package org apache hadoop mapred lib the chain reducer allows chain multiple mapper reducer within reducer task p for record output reducer mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p the key functionality feature mappers chain need aware executed reducer chain this enables reusable specialized mappers combined perform composite operations within single task p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use maching output input key value conversion done chaining code p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p important there need specify output key value chain reducer done set reducer add mapper last element chain p chain reducer usage pattern p pre conf set job name chain conf set input format text input format conf set output format text output format p job conf map a conf new job conf false chain mapper add mapper conf a map long writable text text text true map a conf p job conf map b conf new job conf false chain mapper add mapper conf b map text text long writable text false map b conf p job conf reduce conf new job conf false chain reducer set reducer conf x reduce long writable text text text true reduce conf p chain reducer add mapper conf c map text text long writable text false null p chain reducer add mapper conf d map long writable text long writable long writable true null p file input format set input paths conf dir file output format set output path conf dir p job client jc new job client conf running job job jc submit job conf pre use link org apache hadoop mapreduce lib chain chain reducer instead chain reducer implements reducer k v k v set reducer job conf job k v k v add mapper job conf job chain chain chain reducer configure job conf job reduce object key iterator values output collector output close throws io exception
1333	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileInputFormat.java	pooling	package org apache hadoop mapred lib an link org apache hadoop mapred input format returns link combine file split link org apache hadoop mapred input format get splits job conf method splits constructed files input paths a split cannot files different pools each split returned may contain blocks different files if max split size specified blocks node combined form single split blocks left combined blocks rack if max split size specified blocks rack combined single split attempt made create node local splits if max split size equal block size similar default spliting behaviour hadoop block locally processed split subclasses implement link org apache hadoop mapred input format get record reader input split job conf reporter construct code record reader code code combine file split code link org apache hadoop mapreduce lib input combine file input format combine file input format k v extends org apache hadoop mapreduce lib input combine file input format k v implements input format k v combine file input format input split get splits job conf job num splits protected create pool job conf conf list path filter filters protected create pool job conf conf path filter filters record reader k v get record reader input split split method super implemented return null org apache hadoop mapreduce record reader k v create record reader
1334	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileRecordReader.java	unrelated	package org apache hadoop mapred lib a generic record reader hand different record readers chunk link combine file split a combine file split combine data chunks multiple files this allows using different record readers processing data chunks different files link org apache hadoop mapreduce lib input combine file record reader combine file record reader k v implements record reader k v class constructor signature new class protected combine file split split protected job conf jc protected reporter reporter protected class record reader k v rr class protected constructor record reader k v rr constructor protected file system fs protected idx protected progress protected record reader k v cur reader boolean next k key v value throws io exception k create key v create value get pos throws io exception close throws io exception get progress throws io exception combine file record reader job conf job combine file split split protected boolean init next record reader throws io exception
1335	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileSplit.java	unrelated	package org apache hadoop mapred lib link org apache hadoop mapreduce lib input combine file split combine file split extends job conf job combine file split combine file split job conf job path files start combine file split job conf job path files lengths copy constructor combine file split combine file split old throws io exception job conf get job
1336	mapreduce\src\java\org\apache\hadoop\mapred\lib\DelegatingInputFormat.java	unrelated	package org apache hadoop mapred lib an link input format delegates behaviour paths multiple input formats link org apache hadoop mapreduce lib input delegating input format instead delegating input format k v implements input format k v input split get splits job conf conf num splits throws io exception record reader k v get record reader input split split job conf conf
1337	mapreduce\src\java\org\apache\hadoop\mapred\lib\DelegatingMapper.java	unrelated	package org apache hadoop mapred lib an link mapper delegates behaviour paths multiple mappers link org apache hadoop mapreduce lib input delegating mapper instead delegating mapper k v k v implements mapper k v k v job conf conf mapper k v k v mapper map k key v value output collector k v output collector configure job conf conf close throws io exception
1338	mapreduce\src\java\org\apache\hadoop\mapred\lib\FieldSelectionMapReduce.java	unrelated	package org apache hadoop mapred lib this implements mapper reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the reducer extracts output key value pairs similar manner except key never ignored link field selection reducer instead field selection map reduce k v string map output key value spec boolean ignore input key string field separator list integer map output key field list new array list integer list integer map output value field list new array list integer map value fields from string reduce output key value spec list integer reduce output key field list new array list integer list integer reduce output value field list new array list integer reduce value fields from log log log factory get log field selection map reduce string spec to string map k key v val parse output key value spec configure job conf job close throws io exception reduce text key iterator text values
1339	mapreduce\src\java\org\apache\hadoop\mapred\lib\FilterOutputFormat.java	unrelated	package org apache hadoop mapred lib filter output format convenience wraps output format link org apache hadoop mapreduce lib output filter output format instead filter output format k v implements output format k v protected output format k v base out filter output format filter output format output format k v record writer k v get record writer file system ignored job conf job check output specs file system ignored job conf job throws io exception output format k v get base out throws io exception filter record writer k v implements record writer k v
1340	mapreduce\src\java\org\apache\hadoop\mapred\lib\HashPartitioner.java	unrelated	package org apache hadoop mapred lib partition keys link object hash code link org apache hadoop mapreduce lib partition hash partitioner instead hash partitioner k v implements partitioner k v configure job conf job use link object hash code partition get partition k key v value
1341	mapreduce\src\java\org\apache\hadoop\mapred\lib\IdentityMapper.java	unrelated	package org apache hadoop mapred lib implements identity function mapping inputs directly outputs identity mapper k v the identify function input key value pair written directly output map k key v val
1342	mapreduce\src\java\org\apache\hadoop\mapred\lib\IdentityReducer.java	unrelated	package org apache hadoop mapred lib performs reduction writing input values directly output identity reducer k v writes keys values directly output reduce k key iterator v values
1343	mapreduce\src\java\org\apache\hadoop\mapred\lib\InputSampler.java	unrelated	package org apache hadoop mapred lib link org apache hadoop mapreduce lib partition input sampler input sampler k v extends org apache hadoop mapreduce lib partition input sampler k v input sampler job conf conf k v write partition file job conf job sampler k v sampler
1344	mapreduce\src\java\org\apache\hadoop\mapred\lib\InverseMapper.java	unrelated	package org apache hadoop mapred lib a link mapper swaps keys values instead inverse mapper k v the inverse function input keys values swapped map k key v value
1345	mapreduce\src\java\org\apache\hadoop\mapred\lib\KeyFieldBasedComparator.java	unrelated	package org apache hadoop mapred lib this comparator implementation provides subset features provided unix gnu sort in particular supported features n sort numerically r reverse result comparison k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options nr described we assume fields key separated link job context map output key field seperator link org apache hadoop mapreduce lib partition key field based comparator instead key field based comparator k v extends configure job conf job
1346	mapreduce\src\java\org\apache\hadoop\mapred\lib\KeyFieldBasedPartitioner.java	unrelated	package org apache hadoop mapred lib defines way partition keys based certain key fields also see link key field based comparator the key specification supported form k pos pos pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field link org apache hadoop mapreduce lib partition key field based partitioner instead key field based partitioner k v extends org apache hadoop mapreduce lib partition key field based partitioner k v implements partitioner k v configure job conf job
1347	mapreduce\src\java\org\apache\hadoop\mapred\lib\LazyOutputFormat.java	unrelated	package org apache hadoop mapred lib a convenience creates output lazily link org apache hadoop mapreduce lib output lazy output format instead lazy output format k v extends filter output format k v set output format class job conf job record writer k v get record writer file system ignored job conf job check output specs file system ignored job conf job throws io exception get base output format job conf job throws io exception lazy record writer k v extends filter record writer k v
1348	mapreduce\src\java\org\apache\hadoop\mapred\lib\LongSumReducer.java	unrelated	package org apache hadoop mapred lib a link reducer sums values instead long sum reducer k extends map reduce base reduce k key iterator long writable values
1349	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleInputs.java	unrelated	package org apache hadoop mapred lib this supports map reduce jobs multiple input paths different link input format link mapper path link org apache hadoop mapreduce lib input multiple inputs instead multiple inputs add input path job conf conf path path add input path job conf conf path path map path input format get input format map job conf conf map path class extends mapper get mapper type map job conf conf
1350	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends file output format allowing write output data different output files there three basic use cases case one this used map reduce job least one reducer the reducer wants write data different files depending actual keys it assumed key value encodes actual key value desired location actual key value case two this used map job the job wants use output file name either part input file name input data derivation case three this used map job the job wants use output file name depends keys input file name link org apache hadoop mapreduce lib output multiple outputs instead multiple output format k v extends file output format k v record writer k v get record writer file system fs job conf job protected string generate leaf file name string name protected string generate file name for key value k key v value string name protected k generate actual key k key v value protected v generate actual value k key v value protected string get input file based output file name job conf job string name protected record writer k v get base record writer file system fs
1351	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleOutputs.java	unrelated	package org apache hadoop mapred lib the multiple outputs simplifies writting additional outputs job default output via code output collector code passed code map code code reduce code methods code mapper code code reducer code implementations p each additional output named output may configured code output format code key value p a named output single file multi file the later refered multi named output p a multi named output unbound set files sharing code output format code key value configuration p when named outputs used within code mapper code implementation key values written name output part reduce phase key values written job code output collector code part reduce phase p multiple outputs supports counters default disabled the counters group link multiple outputs name p the names counters named outputs for multi named outputs name counter concatenation named output underscore multiname p job configuration usage pattern pre job conf conf new job conf conf set input path dir file output format set output path conf dir conf set mapper class mo map conf set reducer class mo reduce defines additional single text based output text job multiple outputs add named output conf text text output format long writable text defines additional multi sequencefile based output sequence job multiple outputs add multi named output conf seq sequence file output format long writable text job client jc new job client running job job jc submit job conf pre p job configuration usage pattern pre mo reduce implements reducer lt writable comparable writable gt multiple outputs mos configure job conf conf mos new multiple outputs conf reduce writable comparable key iterator lt writable gt values output collector output reporter reporter throws io exception mos get collector text reporter collect key new text hello mos get collector seq a reporter collect key new text bye mos get collector seq b reporter collect key new text chau close throws io exception mos close pre link org apache hadoop mapreduce lib output multiple outputs instead multiple outputs string named outputs mo named outputs string mo prefix mo named output string format format string key key string value value string multi multi string counters enabled mo counters string counters group multiple outputs get name check named output job conf conf string named output check token name string named output check named output name string named output list string get named outputs list job conf conf boolean multi named output job conf conf string named output class extends output format get named output format class class get named output key class job conf conf object class get named output value class job conf conf add named output job conf conf string named output add multi named output job conf conf string named output add named output job conf conf string named output set counters enabled job conf conf boolean enabled boolean get counters enabled job conf conf instance code used mapper reducer code job conf conf output format output format set string named outputs map string record writer record writers boolean counters enabled multiple outputs job conf job iterator string get
1352	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleSequenceFileOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends multiple output format allowing write output data different output files sequence file output format link org apache hadoop mapreduce lib output multiple outputs instead multiple sequence file output format k v extends multiple output format k v protected record writer k v get base record writer file system fs throws io exception
1353	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleTextOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends multiple output format allowing write output data different output files text output format link org apache hadoop mapreduce lib output multiple outputs instead multiple text output format k v text output format k v text output format null protected record writer k v get base record writer file system fs job conf job
1354	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultithreadedMapRunner.java	pooling	package org apache hadoop mapred lib multithreaded implementation link org apache hadoop mapred map runnable p it used instead default implementation bound order improve throughput p map implementations using map runnable must thread safe p the map reduce job configured use map runnable using job conf set map runner class method number thread thread pool use code mapred map multithreadedrunner threads code property default value threads p multithreaded map runner k v k v log log job conf job mapper k v k v mapper executor service executor service volatile io exception io exception volatile runtime exception runtime exception boolean incr proc count configure job conf job conf blocking array queue extends array blocking queue runnable check for exceptions from processing threads run record reader k v input output collector k v output mapper invoke runable implements runnable
1355	mapreduce\src\java\org\apache\hadoop\mapred\lib\NLineInputFormat.java	unrelated	package org apache hadoop mapred lib n line input format splits n lines input one split in many pleasantly parallel applications process mapper processes input file computations controlled different parameters referred parameter sweeps one way achieve specify set parameters one set per line input control file input path map reduce application input dataset specified via config variable job conf the n line input format used applications splits input file default one line fed value one map task key offset e k v long writable text the location hints span whole mapred cluster link org apache hadoop mapreduce lib input n line input format instead n line input format extends file input format long writable text n record reader long writable text get record reader throws io exception logically splits set input files job splits n lines input one split input split get splits job conf job num splits throws io exception configure job conf conf
1356	mapreduce\src\java\org\apache\hadoop\mapred\lib\NullOutputFormat.java	unrelated	package org apache hadoop mapred lib consume outputs put dev null link org apache hadoop mapreduce lib output null output format instead null output format k v implements output format k v record writer k v get record writer file system ignored job conf job check output specs file system ignored job conf job
1357	mapreduce\src\java\org\apache\hadoop\mapred\lib\RegexMapper.java	unrelated	package org apache hadoop mapred lib a link mapper extracts text matching regular expression regex mapper k extends map reduce base pattern pattern group configure job conf job map k key text value
1358	mapreduce\src\java\org\apache\hadoop\mapred\lib\TaggedInputSplit.java	unrelated	package org apache hadoop mapred lib an link input split tags another input split extra data use link delegating input format link delegating mapper link org apache hadoop mapreduce lib input tagged input split instead tagged input split implements configurable input split class extends input split input split class input split input split class extends input format input format class class extends mapper mapper class configuration conf tagged input split default constructor creates new tagged input split tagged input split input split input split configuration conf input split class input split get class input split input split conf conf input format class input format class mapper class mapper class retrieves original input split input split get input split return input split retrieves input format use split class extends input format get input format class return input format class retrieves mapper use split class extends mapper get mapper class return mapper class get length throws io exception return input split get length string get locations throws io exception return input split get locations read fields data input throws io exception input split class class extends input split read class input split input split reflection utils input split read fields input format class class extends input format read class mapper class class extends mapper read class class read class data input throws io exception string name text read string try catch class not found exception e write data output throws io exception text write string input split class get name input split write text write string input format class get name text write string mapper class get name configuration get conf return conf set conf configuration conf conf conf
1359	mapreduce\src\java\org\apache\hadoop\mapred\lib\TokenCountMapper.java	unrelated	package org apache hadoop mapred lib a link mapper maps text values token freq pairs uses link string tokenizer break text tokens link org apache hadoop mapreduce lib map token counter mapper instead token count mapper k extends map reduce base map k key text value
1360	mapreduce\src\java\org\apache\hadoop\mapred\lib\TotalOrderPartitioner.java	unrelated	package org apache hadoop mapred lib partitioner effecting total order reading split points externally generated source link org apache hadoop mapreduce lib partition total order partitioner total order partitioner k extends writable comparable v total order partitioner configure job conf job
1361	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\DoubleValueSum.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator sums sequence values link org apache hadoop mapreduce lib aggregate double value sum instead double value sum
1362	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueMax.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain maximum sequence values link org apache hadoop mapreduce lib aggregate long value max instead long value max
1363	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueMin.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain minimum sequence values link org apache hadoop mapreduce lib aggregate long value min instead long value min
1364	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueSum.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator sums sequence values link org apache hadoop mapreduce lib aggregate long value sum instead long value sum
1365	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\StringValueMax.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain biggest sequence strings link org apache hadoop mapreduce lib aggregate string value max instead string value max
1366	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\StringValueMin.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain smallest sequence strings link org apache hadoop mapreduce lib aggregate string value min instead string value min
1367	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\UniqValueCount.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator dedupes sequence objects link org apache hadoop mapreduce lib aggregate uniq value count instead uniq value count uniq value count uniq value count max num
1368	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\UserDefinedValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this implements wrapper user defined value aggregator descriptor it servs two functions one create object value aggregator descriptor name user defined may dynamically loaded the deligate inviokations generate key val pairs function created object link org apache hadoop mapreduce lib aggregate user defined value aggregator descriptor instead user defined value aggregator descriptor extends org apache hadoop object create instance string name user defined value aggregator descriptor string name job conf job configure job conf job
1369	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregator.java	unrelated	package org apache hadoop mapred lib aggregate this defines minimal protocol value aggregators link org apache hadoop mapreduce lib aggregate value aggregator instead value aggregator e extends org apache hadoop mapreduce lib aggregate value aggregator e
1370	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorBaseDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this implements common functionalities subclasses value aggregator descriptor link org apache hadoop mapreduce lib aggregate value aggregator base descriptor instead value aggregator base descriptor extends org apache hadoop mapreduce string uniq value count org apache hadoop mapreduce string long value sum org apache hadoop mapreduce string double value sum org apache hadoop mapreduce string value histogram org apache hadoop mapreduce string long value max org apache hadoop mapreduce string long value min org apache hadoop mapreduce string string value max org apache hadoop mapreduce string string value min org apache hadoop mapreduce max num items long max value entry text text generate entry string type string id text val value aggregator generate value aggregator string type configure job conf job
1371	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorCombiner.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic combiner aggregate link org apache hadoop mapreduce lib aggregate value aggregator combiner instead value aggregator combiner k extends writable comparable extends value aggregator job base k v configure job conf job combines values given key reduce text key iterator text values close throws io exception map k arg v arg output collector text text arg
1372	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this defines contract value aggregator descriptor must support such descriptor configured job conf object its main function generate list aggregation id value pairs an aggregation id encodes aggregation type used guide way aggregate value reduce combiner phrase aggregate based job the mapper aggregate based map reduce job may create one value aggregator descriptor objects configuration time for input key value pair mapper use objects create aggregation id value pairs link org apache hadoop mapreduce lib aggregate value aggregator descriptor instead value aggregator descriptor extends string type separator org apache hadoop mapreduce text one org apache hadoop mapreduce configure object job conf object may contain information used configure object configure job conf job
1373	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorJob.java	unrelated	package org apache hadoop mapred lib aggregate this main creating map reduce job using aggregate framework the aggregate specialization map reduce framework specilizing performing various simple aggregations generally speaking order implement application using map reduce model developer implement map reduce functions possibly combine function however lot applications related counting statistics computing similar characteristics aggregate abstracts general patterns functions implementing patterns in particular package provides generic mapper redducer combiner set built value aggregators generic utility helps user create map reduce jobs using generic the built aggregators sum numeric values count number distinct values compute histogram values compute minimum maximum media average standard deviation numeric values the developer using aggregate need provide plugin conforming following value aggregator descriptor array list entry generate key val pairs object key object value configure job confjob the package also provides base value aggregator base descriptor implementing the user extend base implement generate key val pairs accordingly the primary work generate key val pairs emit one key value pairs based input key value pair the key output key value pair encode two pieces information aggregation type aggregation id the value aggregated onto aggregation id according aggregation type this offers function generate map reduce job using aggregate framework the function takes following parameters input directory spec input format text sequence file output directory file specifying user plugin link org apache hadoop mapreduce lib aggregate value aggregator job instead value aggregator job job control create value aggregator jobs string args job control create value aggregator jobs string args throws io exception job conf create value aggregator job string args job conf create value aggregator job string args throws io exception set aggregator descriptors job conf job main string args throws io exception
1374	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorJobBase.java	unrelated	package org apache hadoop mapred lib aggregate this implements common functionalities generic mapper reducer combiner aggregate link org apache hadoop mapreduce lib aggregate value aggregator job base instead value aggregator job base k extends writable comparable implements mapper k v text text reducer text text text text protected array list value aggregator descriptor aggregator descriptor list null configure job conf job value aggregator descriptor get value aggregator descriptor array list value aggregator descriptor get aggregator descriptors job conf job initialize my spec job conf job protected log spec close throws io exception
1375	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorMapper.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic mapper aggregate link org apache hadoop mapreduce lib aggregate value aggregator mapper instead value aggregator mapper k extends writable comparable extends value aggregator job base k v map k key v value reduce text arg iterator text arg
1376	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorReducer.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic reducer aggregate link org apache hadoop mapreduce lib aggregate value aggregator reducer instead value aggregator reducer k extends writable comparable extends value aggregator job base k v reduce text key iterator text values map k arg v arg output collector text text arg
1377	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueHistogram.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator computes histogram sequence strings link org apache hadoop mapreduce lib aggregate value histogram instead value histogram
1378	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBConfiguration.java	unrelated	package org apache hadoop mapred lib db link org apache hadoop mapreduce lib db db configuration instead db configuration extends org apache hadoop mapreduce lib db db configuration the jdbc driver name string driver class property org apache hadoop mapreduce lib db db configuration driver class property jdbc database access url string url property org apache hadoop mapreduce lib db db configuration url property user name access database string username property org apache hadoop mapreduce lib db db configuration username property password access database string password property org apache hadoop mapreduce lib db db configuration password property input table name string input table name property org apache hadoop mapreduce lib db db configuration input table name property field names input table string input field names property org apache hadoop mapreduce lib db db configuration input field names property where clause input select statement string input conditions property org apache hadoop mapreduce lib db db configuration input conditions property order by clause input select statement string input order by property org apache hadoop mapreduce lib db db configuration input order by property whole input query exluding limit offset string input query org apache hadoop mapreduce lib db db configuration input query input query get count records string input count query org apache hadoop mapreduce lib db db configuration input count query class name implementing db writable hold input tuples string input class property org apache hadoop mapreduce lib db db configuration input class property output table name string output table name property org apache hadoop mapreduce lib db db configuration output table name property field names output table string output field names property org apache hadoop mapreduce lib db db configuration output field names property number fields output table string output field count property org apache hadoop mapreduce lib db db configuration output field count property sets db access related fields job conf configure db job conf job string driver class string db url job set driver class property driver class job set url property db url user name null passwd null sets db access related fields job conf configure db job conf job string driver class string db url configure db job driver class db url null null db configuration job conf job super job
1379	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBInputFormat.java	unrelated	package org apache hadoop mapred lib db use link org apache hadoop mapreduce lib db db input format instead db input format t extends db writable extends org apache hadoop mapreduce lib db db input format t implements input format long writable t job configurable a record reader reads records sql table emits long writables containing record number key db writables value protected db record reader extends protected db record reader db input split split class t input class inherit doc long writable create key inherit doc t create value get pos throws io exception inherit doc boolean next long writable key t value throws io exception a record reader implementation passes wrapped record reader built new api db record reader wrapper t extends db writable org apache hadoop mapreduce lib db db record reader t rr db record reader wrapper close throws io exception long writable create key t create value get progress throws io exception get pos throws io exception boolean next long writable key t value throws io exception a class nothing implementing db writable null db writable extends a input split spans set rows protected db input split extends default constructor db input split convenience constructor db input split start end inherit doc configure job conf job super set conf job inherit doc record reader long writable t get record reader input split split wrap dbrr shim deal api differences return new db record reader wrapper t inherit doc input split get splits job conf job chunks throws io exception list org apache hadoop mapreduce input split new splits input split ret new input split new splits size org apache hadoop mapreduce input split new splits return ret initializes map part job appropriate input settings java object holding tuple fields and length set input job conf job class extends db writable input class job set input format db input format db configuration db conf new db configuration job db conf set input class input class db conf set input table name table name db conf set input field names field names db conf set input conditions conditions db conf set input order by order by initializes map part job appropriate input settings java object holding tuple fields select f f f from mytable order by f table example select count f from mytable set input job conf job class extends db writable input class job set input format db input format db configuration db conf new db configuration job db conf set input class input class db conf set input query input query db conf set input count query input count query
1380	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBOutputFormat.java	unrelated	package org apache hadoop mapred lib db db output format k extends db writable v extends org apache hadoop mapreduce lib db db output format k v implements output format k v a record writer writes reduce output sql table protected db record writer extends protected db record writer connection connection inherit doc close reporter reporter throws io exception inherit doc check output specs file system filesystem job conf job throws io exception inherit doc record writer k v get record writer file system filesystem org apache hadoop mapreduce record writer k v w super get record writer org apache hadoop mapreduce lib db db output format db record writer writer org apache hadoop mapreduce lib db db output format db record writer w try catch sql exception se initializes reduce part job appropriate output settings set output job conf job string table name string field names field names length field names null else initializes reduce part job appropriate output settings set output job conf job string table name field count db configuration db conf set output job table name db conf set output field count field count db configuration set output job conf job string table name job set output format db output format job set reduce speculative execution false db configuration db conf new db configuration job db conf set output table name table name return db conf
1381	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBWritable.java	unrelated	package org apache hadoop mapred lib db use link org apache hadoop mapreduce lib db db writable instead db writable extends org apache hadoop mapreduce lib db db writable
1382	mapreduce\src\java\org\apache\hadoop\mapred\pipes\Application.java	authenticate	package org apache hadoop mapred pipes this responsible launching communicating child process application k extends writable comparable v extends writable log log log factory get log application get name server socket server socket process process socket client socket output handler k v handler downward protocol k v downlink boolean windows system get property os name starts with windows application job conf conf string get security challenge write password to local file string local password file downward protocol k v get downlink wait for authentication throws io exception boolean wait for finish throws throwable abort throwable throws io exception cleanup throws io exception process run client list string command string create digest byte password string data
1383	mapreduce\src\java\org\apache\hadoop\mapred\pipes\BinaryProtocol.java	unrelated	package org apache hadoop mapred pipes this protocol binary implementation pipes protocol binary protocol k extends writable comparable v extends writable implements downward protocol k v current protocol version the buffer size command socket buffer size data output stream stream data output buffer buffer new data output buffer log log log factory get log binary protocol get name uplink reader thread uplink the integer codes represent different messages these must match c codes massive confusion result enum message type start code message type code uplink reader thread k extends writable comparable extends thread data input stream stream upward protocol k v handler k key v value boolean auth pending true uplink reader thread input stream stream close connection throws io exception run read object writable obj throws io exception an output stream save copy data file tee output stream extends filter output stream output stream file tee output stream string filename output stream base throws io exception write byte b len throws io exception write b throws io exception flush throws io exception close throws io exception create proxy object speak binary protocol socket upward messages passed specified handler downward downward messages methods object binary protocol socket sock output stream raw sock get output stream if debugging save copy downlink commands file submitter get keep command file config stream new data output stream new buffered output stream raw uplink new uplink reader thread k v sock get input stream uplink set name pipe uplink handler uplink start close connection shutdown handler thread close throws io exception interrupted exception log debug closing connection stream close uplink close connection uplink interrupt uplink join authenticate string digest string challenge log debug sending authentication req digest digest challenge writable utils write v int stream message type authentication req code text write string stream digest text write string stream challenge start throws io exception log debug starting downlink writable utils write v int stream message type start code writable utils write v int stream current protocol version set job conf job conf job throws io exception writable utils write v int stream message type set job conf code list string list new array list string map entry string string itm job writable utils write v int stream list size string entry list set input types string key type writable utils write v int stream message type set input types code text write string stream key type text write string stream value type run map input split split num reduces writable utils write v int stream message type run map code write object split writable utils write v int stream num reduces writable utils write v int stream piped input map item writable comparable key writable utils write v int stream message type map item code write object key write object value run reduce reduce boolean piped output throws io exception writable utils write v int stream message type run reduce code writable utils write v int stream reduce writable utils write v int stream piped output reduce key writable comparable key throws io exception writable utils write
1384	mapreduce\src\java\org\apache\hadoop\mapred\pipes\DownwardProtocol.java	unrelated	package org apache hadoop mapred pipes the description downward java c pipes protocol all calls asynchronous return message processed downward protocol k extends writable comparable v extends writable request authentication authenticate string digest string challenge throws io exception start communication start throws io exception set job conf task set job conf job conf conf throws io exception set input types maps set input types string key type string value type throws io exception run map task child run map input split split num reduces for maps piped input key value pairs sent via messaage map item k key v value throws io exception run reduce task child run reduce reduce boolean piped output throws io exception the reduce given new key reduce key k key throws io exception the reduce given new value reduce value v value throws io exception the task input coming finish processing input end of input throws io exception the task stop soon possible something gone wrong abort throws io exception flush data buffers flush throws io exception close connection close throws io exception interrupted exception
1385	mapreduce\src\java\org\apache\hadoop\mapred\pipes\OutputHandler.java	authenticate	package org apache hadoop mapred pipes handles upward c java messages application output handler k extends writable comparable implements upward protocol k v reporter reporter output collector k v collector progress value f boolean done false throwable exception null record reader float writable null writable record reader null map integer counters counter registered counters string expected digest null boolean digest received false create handler handle records output application output handler output collector k v collector reporter reporter the task output normal record output k key v value throws io exception the task output record partition number attached partitioned output reduce k key update status message task status string msg float writable progress key new float writable f null writable null value null writable get update amount done call progress reporter progress progress throws io exception the task finished successfully done throws io exception get current amount done get progress the task failed exception failed throwable e wait task finish abort synchronized boolean wait for finish throws throwable register counter id string group string name throws io exception increment counter id amount throws io exception synchronized boolean authenticate string digest throws io exception this called application blocks thread authentication response received synchronized wait for authentication
1386	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesMapRunner.java	unrelated	package org apache hadoop mapred pipes an adaptor run c mapper pipes map runner k extends writable comparable v extends writable job conf job get new configuration configure job conf job run map task run record reader k v input output collector k v output
1387	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesNonJavaInputFormat.java	unrelated	package org apache hadoop mapred pipes dummy input format used non java link record reader used pipes application the useful thing set map reduce job get link pipes dummy record reader everything else left actual input format specified user given mapreduce pipes inputformat pipes non java input format implements input format float writable null writable record reader float writable null writable get record reader return new pipes dummy record reader job generic split input split get splits job conf job num splits throws io exception delegate generation input splits original input format return reflection utils new instance a dummy link org apache hadoop mapred record reader help track progress hadoop pipes applications using non java code record reader code the code pipes dummy record reader code informed progress task link output handler progress calls link next float writable null writable progress code key code pipes dummy record reader implements record reader float writable null writable progress f pipes dummy record reader configuration job input split split throws io exception float writable create key null writable create value synchronized close throws io exception synchronized get pos throws io exception get progress synchronized boolean next float writable key null writable value
1388	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesPartitioner.java	unrelated	package org apache hadoop mapred pipes this partitioner one either set manually per record fall back onto java partitioner set user pipes partitioner k extends writable comparable implements partitioner k v thread local integer cache new thread local integer partitioner k v part null configure job conf conf set next key given partition set next partition new value if partition result set manually return otherwise call java partitioner get partition k key v value
1389	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesReducer.java	unrelated	package org apache hadoop mapred pipes this used talk c reduce task pipes reducer k extends writable comparable v extends writable log log log factory get log pipes reducer get name job conf job application k v k v application null downward protocol k v downlink null boolean ok true boolean skipping false configure job conf job reduce k key iterator v values start application output collector k v output reporter reporter throws io exception close throws io exception
1390	mapreduce\src\java\org\apache\hadoop\mapred\pipes\Submitter.java	unrelated	package org apache hadoop mapred pipes the main entry point job submitter it may either used command line based api based method launch pipes jobs submitter extends configured implements tool protected log log log factory get log submitter string preserve commandfile mapreduce pipes commandfile preserve string executable mapreduce pipes executable string interpretor mapreduce pipes executable interpretor string is java map mapreduce pipes isjavamapper string is java rr mapreduce pipes isjavarecordreader string is java rw mapreduce pipes isjavarecordwriter string is java reduce mapreduce pipes isjavareducer string partitioner mapreduce pipes partitioner string input format mapreduce pipes inputformat string port mapreduce pipes command port submitter new configuration submitter configuration conf set conf conf get uri application executable string get executable job conf conf return conf get submitter executable set uri application executable normally hdfs location set executable job conf conf string executable conf set submitter executable executable set whether job using java record reader set is java record reader job conf conf boolean value conf set boolean submitter is java rr value check whether job using java record reader boolean get is java record reader job conf conf return conf get boolean submitter is java rr false set whether mapper written java set is java mapper job conf conf boolean value conf set boolean submitter is java map value check whether job using java mapper boolean get is java mapper job conf conf return conf get boolean submitter is java map false set whether reducer written java set is java reducer job conf conf boolean value conf set boolean submitter is java reduce value check whether job using java reducer boolean get is java reducer job conf conf return conf get boolean submitter is java reduce false set whether job use java record writer set is java record writer job conf conf boolean value conf set boolean submitter is java rw value will reduce use java record writer boolean get is java record writer job conf conf return conf get boolean submitter is java rw false set configuration already value given key set if unset job conf conf string key string value conf get key null save away user original partitioner set java partitioner job conf conf class cls conf set submitter partitioner cls get name get user original partitioner class extends partitioner get java partitioner job conf conf return conf get class submitter partitioner does user want keep command file debugging if true pipes write copy command data file task directory named downlink data may used run c program debugger you probably also want set job conf set keep failed task files true keep entire directory deleted to run using data file set environment variable mapreduce pipes commandfile point file boolean get keep command file job conf conf return conf get boolean submitter preserve commandfile false set whether keep command file debugging set keep command file job conf conf boolean keep conf set boolean submitter preserve commandfile keep submit job map reduce cluster all necessary modifications job run pipes made configuration running job submit job job conf conf throws io exception return run job
1391	mapreduce\src\java\org\apache\hadoop\mapred\pipes\UpwardProtocol.java	authenticate	package org apache hadoop mapred pipes the messages come child all calls asynchronous return message processed upward protocol k extends writable comparable v extends writable output record child output k key v value throws io exception map functions application defined partition function output records along partition partitioned output reduce k key update task status message status string msg throws io exception report making progress current progress progress progress throws io exception report application finished processing inputs successfully done throws io exception report application likely communication failed failed throwable e register counter given id group name register counter id string group string name throws io exception increment value registered counter increment counter id amount throws io exception handles authentication response client it must notify threads waiting authentication response boolean authenticate string digest throws io exception
1392	mapreduce\src\java\org\apache\hadoop\mapred\tools\GetGroups.java	unrelated	package org apache hadoop mapred tools mr implementation tool getting groups given user belongs get groups extends get groups base get groups configuration conf get groups configuration conf print stream protected inet socket address get protocol address configuration conf main string argv throws exception
1393	mapreduce\src\java\org\apache\hadoop\mapred\tools\MRAdmin.java	unrelated	package org apache hadoop mapred tools administrative access hadoop map reduce currently provides ability connect link job tracker refresh service level authorization policy refresh queue acl properties mr admin extends configured implements tool mr admin mr admin configuration conf print help string cmd string refresh service acl refresh service acl reload service level authorization policy file n string refresh queues string refresh user to groups mappings string refresh super user groups configuration string refresh nodes string help help cmd displays help given command commands none n refresh service acl equals cmd else refresh queues equals cmd else refresh user to groups mappings equals cmd else refresh nodes equals cmd else refresh super user groups configuration equals cmd else help equals cmd else displays format commands print usage string cmd user group information get ugi configuration conf refresh authorization policy throws io exception refresh super user groups configuration link job tracker refresh super user groups configuration throws io exception refresh user groups mappings link job tracker refresh user to groups mappings throws io exception refresh queues throws io exception command ask jobtracker reread hosts excluded hosts file usage java mr admin refresh nodes refresh nodes throws io exception run string args throws exception main string args throws exception
1394	mapreduce\src\java\org\apache\hadoop\mapred\tools\package-info.java	unrelated	package org apache hadoop mapred tools
1395	mapreduce\src\java\org\apache\hadoop\mapreduce\Cluster.java	unrelated	package org apache hadoop mapreduce provides way access information map reduce cluster cluster enum job tracker status initializing running client protocol provider client protocol provider client protocol client user group information ugi configuration conf file system fs null path sys dir null path staging area dir null path job history dir null cluster configuration conf throws io exception cluster inet socket address job track addr configuration conf client protocol get client configuration get conf close code cluster code synchronized close throws io exception job get jobs job status stats throws io exception get file system job specific files stored synchronized file system get file system get job corresponding jobid job get job job id job id throws io exception interrupted exception get queues cluster queue info get queues throws io exception interrupted exception get queue information specified name queue info get queue string name get current cluster status cluster metrics get cluster status throws io exception interrupted exception get active trackers cluster task tracker info get active task trackers get blacklisted trackers task tracker info get black listed task trackers get jobs cluster job get all jobs throws io exception interrupted exception get job status jobs cluster job status get all job statuses throws io exception interrupted exception grab jobtracker system directory path job specific files placed path get system dir throws io exception interrupted exception grab jobtracker view staging directory path job specific files placed path get staging area dir throws io exception interrupted exception get job history file path given job id the job history file path may may existing depending job completion state the file present completed jobs string get job history url job id job id throws io exception gets queue ac ls current user queue acls info get queue acls for current user gets root level queues queue info get root queues throws io exception interrupted exception returns immediate children queue name queue info get child queues string queue name get job tracker state state get job tracker state throws io exception interrupted exception get job tracker status job tracker status get job tracker status throws io exception get tasktracker expiry interval cluster get task tracker expiry interval throws io exception get delegation token user job tracker token delegation token identifier renew delegation token renew delegation token token delegation token identifier token cancel delegation token job tracker cancel delegation token token delegation token identifier token
1396	mapreduce\src\java\org\apache\hadoop\mapreduce\ClusterMetrics.java	unrelated	package org apache hadoop mapreduce status information current state map reduce cluster p code cluster metrics code provides clients information ol li size cluster li li number blacklisted decommissioned trackers li li slot capacity cluster li li the number currently occupied reserved map reduce slots li li the number currently running map reduce tasks li li the number job submissions li ol p p clients query latest code cluster metrics code via link cluster get cluster status p cluster metrics implements writable running maps running reduces occupied map slots occupied reduce slots reserved map slots reserved reduce slots total map slots total reduce slots total job submissions num trackers num blacklisted trackers num decommissioned trackers cluster metrics cluster metrics running maps running reduces get number running map tasks cluster get running maps get number running reduce tasks cluster get running reduces get number occupied map slots cluster get occupied map slots get number occupied reduce slots cluster get occupied reduce slots get number reserved map slots cluster get reserved map slots get number reserved reduce slots cluster get reserved reduce slots get total number map slots cluster get map slot capacity get total number reduce slots cluster get reduce slot capacity get total number job submissions cluster get total job submissions get number active trackers cluster get task tracker count get number blacklisted trackers cluster get black listed task tracker count get number decommissioned trackers cluster get decommissioned task tracker count read fields data input throws io exception write data output throws io exception
1397	mapreduce\src\java\org\apache\hadoop\mapreduce\Counter.java	unrelated	package org apache hadoop mapreduce a named counter tracks progress map reduce job p code counters code represent global counters defined either map reduce framework applications each code counter code named link enum value p p code counters code bunched groups comprising counters particular code enum code counter implements writable string name string display name value protected counter protected counter string name string display name create counter counter string name string display name value protected synchronized set display name string display name read binary representation counter synchronized read fields data input throws io exception write binary representation counter synchronized write data output throws io exception synchronized string get name get name counter synchronized string get display name what current value counter synchronized get value set counter given value synchronized set value value increment counter given value synchronized increment incr synchronized boolean equals object generic right synchronized hash code
1398	mapreduce\src\java\org\apache\hadoop\mapreduce\CounterGroup.java	unrelated	package org apache hadoop mapreduce a group link counter logically belong together typically link enum subclass counters values counter group implements writable iterable counter string name string display name tree map string counter counters new tree map string counter optional resource bundle localization group counter names resource bundle bundle null resource bundle get resource bundle string enum class name protected counter group string name create counter group counter group string name string display name synchronized string get name synchronized string get display name add counter group synchronized add counter counter counter counter find counter string counter name string display name synchronized counter find counter string counter name synchronized iterator counter iterator synchronized write data output throws io exception synchronized read fields data input throws io exception string localize string key string default value synchronized size synchronized boolean equals object generic right synchronized hash code synchronized incr all counters counter group right group
1399	mapreduce\src\java\org\apache\hadoop\mapreduce\Counters.java	unrelated	package org apache hadoop mapreduce counters implements writable iterable counter group a cache enum values associated counter dramatically speeds typical usage map enum counter cache new identity hash map enum counter tree map string counter group groups counters utility method create counters object org apache hadoop mapred counters counters org apache hadoop mapred counters counters add group add group counter group group counter find counter string group name string counter name find counter given enum the enum always return counter synchronized counter find counter enum key returns names counter synchronized collection string get group names iterator counter group iterator returns named counter group empty group none specified name synchronized counter group get group string group name returns total number counters summing number counters group synchronized count counters write set groups the external format groups group name group e number groups followed groups group form group display name counters false true counter counter form name false true display name value synchronized write data output throws io exception read set groups synchronized read fields data input throws io exception return textual representation counter values synchronized string string increments multiple counters amounts another counters instance synchronized incr all counters counters boolean equals object generic right hash code
1400	mapreduce\src\java\org\apache\hadoop\mapreduce\ID.java	unrelated	package org apache hadoop mapreduce a general identifier internally stores id integer this super link job id link task id link task attempt id id implements writable comparable id protected char separator protected id constructs id object given id id protected id returns represents identifier get id string string hash code boolean equals object compare i ds associated numbers compare to id read fields data input throws io exception write data output throws io exception
1401	mapreduce\src\java\org\apache\hadoop\mapreduce\InputFormat.java	unrelated	package org apache hadoop mapreduce code input format code describes input specification map reduce job p the map reduce framework relies code input format code job p ol li validate input specification job li split input file logical link input split assigned individual link mapper li li provide link record reader implementation used glean input records logical code input split code processing link mapper li ol p the default behavior file based link input format typically sub link file input format split input logical link input split based total size bytes input files however link file system blocksize input files treated upper bound input splits a lower bound split size set via href doc root mapred default html mapreduce input fileinputformat split minsize mapreduce input fileinputformat split minsize p p clearly logical splits based input size insufficient many applications since record boundaries respected in cases application also implement link record reader lies responsibility respect record boundaries present record oriented view logical code input split code individual task input format k v logically split set input files job p each link input split assigned individual link mapper processing p p note the split logical split inputs input files physically split chunks for e g split could lt input file path start offset gt tuple the input format also creates link record reader read link input split create record reader given split the framework call link record reader initialize input split task attempt context split used
1402	mapreduce\src\java\org\apache\hadoop\mapreduce\InputSplit.java	unrelated	package org apache hadoop mapreduce code input split code represents data processed individual link mapper p typically presents byte oriented view input responsibility link record reader job process present record oriented view input split get size split input splits sorted size get length throws io exception interrupted exception get list nodes name data split would local the locations need serialized
1403	mapreduce\src\java\org\apache\hadoop\mapreduce\Job.java	scheduler	package org apache hadoop mapreduce the job submitter view job p it allows user configure job submit control execution query state the set methods work job submitted afterwards throw illegal state exception p p normally user creates application describes various facets job via link job submits job monitor progress p p here example submit job p p blockquote pre create new job job job new job new configuration job set jar by class my job specify various job specific parameters job set job name myjob job set input path new path job set output path new path job set mapper class my job my mapper job set reducer class my job my reducer submit job poll progress job complete job wait for completion true pre blockquote p job extends job context impl implements job context log log log factory get log job enum job state define running max jobstatus age string output filter mapreduce client output filter key mapred xml sets completion poll inverval millis string completion poll interval key mapreduce client completion pollinterval default completion poll interval millis ms default completion poll interval key mapred xml sets prog monitor poll interval millis string progress monitor poll interval key mapreduce client progressmonitor pollinterval default prog monitor poll interval millis ms default monitor poll interval string used generic parser mapreduce client genericoptionsparser used string submit replication mapreduce client submit file replication string tasklog pull timeout key default tasklog timeout enum task status filter none killed failed succeeded all config util load resources job state state job state define job status status statustime cluster cluster job throws io exception new configuration job configuration conf throws io exception new cluster conf conf job configuration conf string job name throws io exception conf set job name job name job cluster cluster throws io exception cluster new configuration job cluster cluster configuration conf throws io exception super conf null cluster cluster job cluster cluster job status status cluster conf set job id status get job id status status state job state running creates new link job particular link cluster a cluster created generic link configuration job get instance throws io exception create null cluster return get instance new configuration creates new link job particular link cluster a cluster created conf parameter needed job get instance configuration conf throws io exception create null cluster return new job null conf creates new link job particular link cluster given job name a cluster created conf parameter needed job get instance configuration conf string job name create null cluster job result new job null conf result set job name job name return result job get instance cluster cluster throws io exception return new job cluster job get instance cluster cluster configuration conf return new job cluster conf job get instance cluster cluster job status status return new job cluster status conf ensure state job state state throws illegal state exception state state state job state running cluster null some methods rely recent job status object refresh necessary synchronized ensure fresh status system current time millis statustime max jobstatus age some methods
1404	mapreduce\src\java\org\apache\hadoop\mapreduce\JobACL.java	scheduler	package org apache hadoop mapreduce job related ac ls enum job acl acl viewing job dictates view job related details view job mr job config job acl view job acl modifying job dictates modify job e g killing job killing failing task job setting priority job modify job mr job config job acl modify job string acl name job acl string name get name acl here name configuration property specifying acl job string get acl name
1405	mapreduce\src\java\org\apache\hadoop\mapreduce\JobContext.java	unrelated	package org apache hadoop mapreduce a read view job provided tasks running job context extends mr job config return configuration job configuration get configuration get credentials job credentials get credentials get unique id job job id get job id get configured number reduce tasks job defaults code code get num reduce tasks get current working directory default file system path get working directory throws io exception get key job output data class get output key class get value job outputs class get output value class get key map output data if set use output key this allows map output key different output key class get map output key class get value map output data if set use output value this allows map output value different output value class get map output value class get user specified job name this used identify job user string get job name get link input format job class extends input format get input format class get link mapper job class extends mapper get mapper class get combiner job class extends reducer get combiner class get link reducer job class extends reducer get reducer class get link output format job class extends output format get output format class get link partitioner job class extends partitioner get partitioner class get link raw comparator comparator used compare keys raw comparator get sort comparator get pathname job jar string get jar get user defined link raw comparator comparator grouping keys inputs reduce raw comparator get grouping comparator get whether job setup job cleanup needed job boolean get job setup cleanup needed get whether task cleanup needed job boolean get task cleanup needed get whether task profiling enabled boolean get profile enabled get profiler configuration arguments the default value property agentlib hprof cpu samples heap sites force n thread verbose n file string get profile params get range maps reduces profile integer ranges get profile task range boolean map get reported username job string get user this method checks see symlinks create localized cache files current working directory boolean get symlink get archive entries classpath array path path get archive class paths get cache archives set configuration uri get cache archives throws io exception get cache files set configuration uri get cache files throws io exception return path array localized caches path get local cache archives throws io exception return path array localized files path get local cache files throws io exception get file entries classpath array path path get file class paths get timestamps archives used internal distributed cache map reduce code string get archive timestamps get timestamps files used internal distributed cache map reduce code string get file timestamps get configured number maximum attempts made run map task specified code mapred map max attempts code property if property already set default attempts get max map attempts get configured number maximum attempts made run reduce task specified code mapred reduce max attempts code property if property already set default attempts get max reduce attempts
1406	mapreduce\src\java\org\apache\hadoop\mapreduce\JobCounter.java	unrelated	package org apache hadoop mapreduce per job counters enum job counter num failed maps num failed reduces total launched maps total launched reduces other local maps data local maps rack local maps slots millis maps slots millis reduces fallow slots millis maps fallow slots millis reduces
1407	mapreduce\src\java\org\apache\hadoop\mapreduce\JobID.java	unrelated	package org apache hadoop mapreduce job id represents immutable unique identifier job job id consists two parts first part represents jobtracker identifier job id jobtracker map defined for cluster setup jobtracker start time local setting local second part job id job number br an example job id code job code represents third job running jobtracker started code code p applications never construct parse job id strings rather use appropriate constructors link name string method job id extends org apache hadoop mapred id protected string job job jobid regex various tools framework components string jobid regex text jt identifier protected number format id format number format get instance constructs job id object job id string jt identifier id job id string get jt identifier boolean equals object compare job ids first jt identifiers job numbers compare to id add stuff job prefix given builder this useful sub ids use substring start string builder append to string builder builder hash code string string read fields data input throws io exception write data output throws io exception construct job id object given job id name string str throws illegal argument exception
1408	mapreduce\src\java\org\apache\hadoop\mapreduce\JobPriority.java	unrelated	package org apache hadoop mapreduce used describe priority running job enum job priority very high high normal low very low
1409	mapreduce\src\java\org\apache\hadoop\mapreduce\JobStatus.java	unrelated	package org apache hadoop mapreduce describes current status job job status implements writable cloneable register ctor current state job enum state job id jobid map progress reduce progress cleanup progress setup progress state run state start time string user string queue job priority priority string scheduling info na map job acl access control list job ac ls string job name string job file finish time boolean retired string history file string tracking url job status create job status object given jobid job status job id jobid setup progress map progress sets map progress job protected synchronized set map progress p sets cleanup progress job protected synchronized set cleanup progress p sets setup progress job protected synchronized set setup progress p sets reduce progress job protected synchronized set reduce progress p set priority job defaulting normal protected synchronized set priority job priority jp set finish time job protected synchronized set finish time finish time set job history file url completed job protected synchronized set history file string history file set link web ui details job protected synchronized set tracking url string tracking url set job retire flag true protected synchronized set retired change current run state job protected synchronized set state state state set start time job protected synchronized set start time start time protected synchronized set username string user name used set scheduling information associated particular job protected synchronized set scheduling info string scheduling info set job acls protected synchronized set job ac ls map job acl access control list acls set queue name protected synchronized set queue string queue get queue name synchronized string get queue synchronized get map progress return map progress synchronized get cleanup progress return cleanup progress synchronized get setup progress return setup progress synchronized get reduce progress return reduce progress synchronized state get state return run state synchronized get start time return start time object clone job id get job id return jobid synchronized string get username return user gets scheduling information associated particular job synchronized string get scheduling info return scheduling info get job acls synchronized map job acl access control list get job ac ls return priority job synchronized job priority get priority return priority returns true status completed job synchronized boolean job complete writable synchronized write data output throws io exception synchronized read fields data input throws io exception get user specified job name string get job name get configuration file job string get job file get link web ui details job synchronized string get tracking url get finish time job synchronized get finish time check whether job retired synchronized boolean retired completed history file available return null synchronized string get history file string string
1410	mapreduce\src\java\org\apache\hadoop\mapreduce\JobSubmissionFiles.java	unrelated	package org apache hadoop mapreduce a utility manage job submission files job submission files job submission directory fs permission job dir permission job files world wide readable owner writable fs permission job file permission path get job split file path job submission dir path get job split meta file path job submission dir get job conf path path get job conf path path job submit dir get job jar path path get job jar path job submit dir get job distributed cache files path path get job dist cache files path job submit dir get job distributed cache archives path path get job dist cache archives path job submit dir get job distributed cache libjars path path get job dist cache libjars path job submit dir initializes staging directory returns path it also keeps track necessary ownership permissions path get staging dir cluster cluster configuration conf throws io exception interrupted exception
1411	mapreduce\src\java\org\apache\hadoop\mapreduce\JobSubmitter.java	unrelated	package org apache hadoop mapreduce job submitter protected log log log factory get log job submitter file system jt fs client protocol submit client string submit host name string submit host address job submitter file system submit fs client protocol submit client throws io exception submit client submit client jt fs submit fs see two file systems boolean compare fs file system src fs file system dest fs uri src uri src fs get uri uri dst uri dest fs get uri src uri get scheme null src uri get scheme equals dst uri get scheme string src host src uri get host string dst host dst uri get host src host null dst host null else src host null dst host null else src host null dst host null check ports src uri get port dst uri get port return true copies file jobtracker filesystem returns path copied path copy remote files path parent dir check need copy files jt using file system checking uri strings dns lookups see filesystems this optimal avoids name resolution file system remote fs null remote fs original path get file system conf compare fs remote fs jt fs might name collisions copy throw exception parse original path create new path path new path new path parent dir original path get name file util copy remote fs original path jt fs new path false conf jt fs set replication new path replication return new path configures files libjars archives copy and configure files job job path submit job dir configuration conf job get configuration conf get boolean job used generic parser false get command line arguments passed user conf string files conf get tmpfiles string libjars conf get tmpjars string archives conf get tmparchives string job jar job get jar figure fs job tracker using copy job temporary name this allows dfs work local fs also provides unix like object loading semantics job file deleted right submission still run submission completion create number filenames job tracker fs namespace log debug default file system jt fs get uri jt fs exists submit job dir submit job dir jt fs make qualified submit job dir submit job dir new path submit job dir uri get path fs permission mapred sys perms new fs permission job submission files job dir permission file system mkdirs jt fs submit job dir mapred sys perms path files dir job submission files get job dist cache files submit job dir path archives dir job submission files get job dist cache archives submit job dir path libjars dir job submission files get job dist cache libjars submit job dir add command line files jars archive first copy jobtrackers filesystem files null libjars null archives null job jar null copy jar job tracker fs else set timestamps archives files client distributed cache manager determine timestamps conf set visibility archives files client distributed cache manager determine cache visibilities conf get delegation token cached file client distributed cache manager get delegation tokens conf job uri get path uri path dest path string fragment uri path uri dest
1412	mapreduce\src\java\org\apache\hadoop\mapreduce\MapContext.java	unrelated	package org apache hadoop mapreduce the context given link mapper map context keyin valuein keyout valueout extends task input output context keyin valuein keyout valueout get input split map input split get input split
1413	mapreduce\src\java\org\apache\hadoop\mapreduce\Mapper.java	unrelated	package org apache hadoop mapreduce maps input key value pairs set intermediate key value pairs p maps individual tasks transform input records intermediate records the transformed intermediate records need type input records a given input pair may map zero many output pairs p p the hadoop map reduce framework spawns one map task link input split generated link input format job code mapper code implementations access link configuration job via link job context get configuration p the framework first calls link setup org apache hadoop mapreduce mapper context followed link map object object context key value pair code input split code finally link cleanup context called p p all intermediate values associated given output key subsequently grouped framework passed link reducer determine output users control sorting grouping specifying two key link raw comparator p p the code mapper code outputs partitioned per code reducer code users control keys hence records go code reducer code implementing custom link partitioner p users optionally specify code combiner code via link job set combiner class class perform local aggregation intermediate outputs helps cut amount data transferred code mapper code code reducer code p applications specify intermediate outputs compressed link compression codec used via code configuration code p p if job zero reduces output code mapper code directly written link output format without sorting keys p p example p p blockquote pre token counter mapper extends mapper lt object text text int writable gt int writable one new int writable text word new text map object key text value context context throws io exception interrupted exception string tokenizer itr new string tokenizer value string itr more tokens word set itr next token context write word one pre blockquote p p applications may link run context method exert greater control map processing e g multi threaded code mapper code etc p mapper keyin valuein keyout valueout context protected setup context context protected map keyin key valuein value protected cleanup context context run context context throws io exception interrupted exception
1414	mapreduce\src\java\org\apache\hadoop\mapreduce\MarkableIterator.java	unrelated	package org apache hadoop mapreduce code markable iterator code wrapper iterator implements link markable iterator interface markable iterator value implements markable iterator interface value markable iterator interface value base iterator markable iterator iterator value itr mark throws io exception reset throws io exception clear mark throws io exception boolean next value next remove
1415	mapreduce\src\java\org\apache\hadoop\mapreduce\MarkableIteratorInterface.java	unrelated	package org apache hadoop mapreduce code markable iterator interface code iterator supports mark reset functionality p mark called point iteration process reset go back last record call previous mark markable iterator interface value extends iterator value mark current record a subsequent call reset rewind iterator record mark throws io exception reset iterator last record call previous mark reset throws io exception clear previously set mark clear mark throws io exception
1416	mapreduce\src\java\org\apache\hadoop\mapreduce\MRConfig.java	unrelated	package org apache hadoop mapreduce place holder cluster level configuration keys these keys used link job tracker link task tracker the keys mapreduce cluster prefix mr config cluster level configuration parameters string temp dir mapreduce cluster temp dir string local dir mapreduce cluster local dir string mapmemory mb mapreduce cluster mapmemory mb string reducememory mb string mr acls enabled mapreduce cluster acls enabled string mr admins string mr supergroup delegation token related keys string delegation key update interval key delegation key update interval default string delegation token renew interval key delegation token renew interval default string delegation token max lifetime key delegation token max lifetime default string framework name mapreduce framework name
1417	mapreduce\src\java\org\apache\hadoop\mapreduce\MRJobConfig.java	scheduler	package org apache hadoop mapreduce mr job config put attribute names job job context consistent string input format class attr mapreduce job inputformat string map class attr mapreduce job map string combine class attr mapreduce job combine string reduce class attr mapreduce job reduce string output format class attr mapreduce job outputformat string partitioner class attr mapreduce job partitioner string setup cleanup needed mapreduce job committer setup cleanup needed string task cleanup needed mapreduce job committer task cleanup needed string jar mapreduce job jar string id mapreduce job id string job name mapreduce job name string jar unpack pattern mapreduce job jar unpack pattern string user name mapreduce job user name string priority mapreduce job priority string queue name mapreduce job queuename string jvm numtasks torun mapreduce job jvm numtasks string split file mapreduce job splitfile string num maps mapreduce job maps string max task failures per tracker mapreduce job maxtaskfailures per tracker string completed maps for reduce slowstart mapreduce job reduce slowstart completedmaps string num reduces mapreduce job reduces string skip records mapreduce job skiprecords string skip outdir mapreduce job skip outdir string speculative slownode threshold mapreduce job speculative slownodethreshold string speculative slowtask threshold mapreduce job speculative slowtaskthreshold string speculativecap mapreduce job speculative speculativecap string job local dir mapreduce job local dir string output key class mapreduce job output key string output value class mapreduce job output value string key comparator mapreduce job output key comparator string group comparator class mapreduce job output group comparator string working dir mapreduce job working dir string end notification url mapreduce job end notification url string end notification retries mapreduce job end notification retry attempts string end notification retrie interval mapreduce job end notification retry interval string classpath archives mapreduce job classpath archives string classpath files mapreduce job classpath files string cache files mapreduce job cache files string cache archives mapreduce job cache archives string cache files sizes mapreduce job cache files filesizes internal use string cache archives sizes mapreduce job cache archives filesizes ditto string cache localfiles mapreduce job cache local files string cache localarchives mapreduce job cache local archives string cache file timestamps mapreduce job cache files timestamps string cache archives timestamps mapreduce job cache archives timestamps string cache file visibilities mapreduce job cache files visibilities string cache archives visibilities mapreduce job cache archives visibilities string cache symlink mapreduce job cache symlink create string user log retain hours mapreduce job userlog retain hours string io sort factor mapreduce task io sort factor string io sort mb mapreduce task io sort mb string index cache memory limit mapreduce task index cache limit bytes string preserve failed task files mapreduce task files preserve failedtasks string preserve files pattern mapreduce task files preserve filepattern string task temp dir mapreduce task tmp dir string task debugout lines mapreduce task debugout lines string records before progress mapreduce task merge progress records string skip start attempts mapreduce task skip start attempts string task attempt id mapreduce task attempt id string task ismap mapreduce task ismap string task partition mapreduce task partition string task profile mapreduce task profile string task
1418	mapreduce\src\java\org\apache\hadoop\mapreduce\OutputCommitter.java	unrelated	package org apache hadoop mapreduce code output committer code describes commit task output map reduce job p the map reduce framework relies code output committer code job p ol li setup job initialization for example create temporary output directory job initialization job li li cleanup job job completion for example remove temporary output directory job completion li li setup task temporary output li li check whether task needs commit this avoid commit procedure task need commit li li commit task output li li discard task commit li ol output committer for framework setup job output initialization setup job job context job context throws io exception for cleaning job output job completion link abort job job context job status state instead cleanup job job context job context throws io exception for committing job output successful job completion note invoked jobs runstate successful commit job job context job context throws io exception for aborting unsuccessful job output note invoked jobs runstate link job status state failed link job status state killed abort job job context job context job status state state throws io exception sets output task setup task task attempt context task context throws io exception check whether task needs commit boolean needs task commit task attempt context task context throws io exception to promote task temporary output output location the task output moved job output directory commit task task attempt context task context throws io exception discard task output abort task task attempt context task context throws io exception
1419	mapreduce\src\java\org\apache\hadoop\mapreduce\OutputFormat.java	unrelated	package org apache hadoop mapreduce code output format code describes output specification map reduce job p the map reduce framework relies code output format code job p ol li validate output specification job for e g check output directory already exist li provide link record writer implementation used write output files job output files stored link file system li ol output format k v get link record writer given task record writer k v check validity output specification job p this validate output specification job job submitted typically checks already exist throwing exception already exists output overwritten p check output specs job context context get output committer output format this responsible ensuring output committed correctly output committer get output committer task attempt context context
1420	mapreduce\src\java\org\apache\hadoop\mapreduce\Partitioner.java	unrelated	package org apache hadoop mapreduce partitions key space p code partitioner code controls partitioning keys intermediate map outputs the key subset key used derive partition typically hash function the total number partitions number reduce tasks job hence controls code code reduce tasks intermediate key hence record sent reduction p note if require partitioner obtain job configuration object implement link configurable partitioner key value get partition key key value value num partitions
1421	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueAclsInfo.java	unrelated	package org apache hadoop mapreduce class encapsulate queue ac ls particular user queue acls info implements writable string queue name string operations default constructor queue acls info queue acls info construct new queue acls info object using queue name queue operations array queue acls info string queue name string operations get queue name string get queue name protected set queue name string queue name get opearations allowed queue string get operations read fields data input throws io exception write data output throws io exception
1422	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueInfo.java	unrelated	package org apache hadoop mapreduce class contains information regarding job queues maintained hadoop map reduce framework queue info implements writable string queue name the scheduling information object read back string once scheduling information set way recover string scheduling info queue state queue state jobs submitted queue job status stats list queue info children properties props default constructor queue info queue info construct new queue info object using queue name scheduling information passed queue queue info string queue name string scheduling info queue info string queue name string scheduling info queue state state set queue name job queue info protected set queue name string queue name get queue name job queue info string get queue name set scheduling information associated particular job queue protected set scheduling info string scheduling info gets scheduling information associated particular job queue if nothing set would return b n a b string get scheduling info set state queue protected set state queue state state return queue state queue state get state protected set job statuses job status stats get immediate children list queue info get queue children protected set queue children list queue info children get properties properties get properties protected set properties properties props get jobs submitted queue job status get job statuses read fields data input throws io exception write data output throws io exception
1423	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueState.java	unrelated	package org apache hadoop mapreduce enum representing queue state enum queue state stopped stopped running running undefined undefined string state name map string queue state enum map queue state string state name string get state name queue state get state string state string string
1424	mapreduce\src\java\org\apache\hadoop\mapreduce\RecordReader.java	unrelated	package org apache hadoop mapreduce the record reader breaks data key value pairs input link mapper record reader keyin valuein implements closeable called initialization initialize input split split read next key value pair boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception interrupted exception the current progress record reader data get progress throws io exception interrupted exception close record reader close throws io exception
1425	mapreduce\src\java\org\apache\hadoop\mapreduce\RecordWriter.java	unrelated	package org apache hadoop mapreduce code record writer code writes output lt key value gt pairs output file p code record writer code implementations write job outputs link file system record writer k v writes key value pair write k key v value close code record writer code future operations close task attempt context context
1426	mapreduce\src\java\org\apache\hadoop\mapreduce\ReduceContext.java	unrelated	package org apache hadoop mapreduce the context passed link reducer reduce context keyin valuein keyout valueout start processing next unique key boolean next key throws io exception interrupted exception iterable valuein get values throws io exception interrupted exception value iterator valuein extends markable iterator interface valuein
1427	mapreduce\src\java\org\apache\hadoop\mapreduce\Reducer.java	unrelated	package org apache hadoop mapreduce reduces set intermediate values share key smaller set values p code reducer code implementations access link configuration job via link job context get configuration method p p code reducer code primary phases p ol li id shuffle shuffle p the code reducer code copies sorted output link mapper using http across network p li li id sort sort p the framework merge sorts code reducer code inputs code key code since different code mapper code may output key p p the shuffle sort phases occur simultaneously e outputs fetched merged p id secondary sort secondary sort p to achieve secondary sort values returned value iterator application extend key secondary key define grouping comparator the keys sorted using entire key grouped using grouping comparator decide keys values sent call reduce the grouping comparator specified via link job set grouping comparator class class the sort order controlled link job set sort comparator class class p for example say want find duplicate web pages tag url best known example you would set job like ul li map input key url li li map input value document li li map output key document checksum url pagerank li li map output value url li li partitioner checksum li li output key comparator checksum decreasing pagerank li li output value grouping comparator checksum li ul li li id reduce reduce p in phase link reduce object iterable context method called code lt key collection values gt code sorted inputs p p the output reduce task typically written link record writer via link context write object object p li ol p the output code reducer code b sorted b p p example p p blockquote pre int sum reducer lt key gt extends reducer lt key int writable key int writable gt int writable result new int writable reduce key key iterable lt int writable gt values context context throws io exception interrupted exception sum int writable val values sum val get result set sum context write key result pre blockquote p reducer keyin valuein keyout valueout context protected setup context context protected reduce keyin key iterable valuein values context context protected cleanup context context run context context throws io exception interrupted exception
1428	mapreduce\src\java\org\apache\hadoop\mapreduce\StatusReporter.java	unrelated	package org apache hadoop mapreduce status reporter counter get counter enum name counter get counter string group string name progress get current progress progress get progress set status string status
1429	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskAttemptContext.java	unrelated	package org apache hadoop mapreduce the context task attempts task attempt context extends job context progressable get unique name task attempt task attempt id get task attempt id set current status task given set status string msg get last set status message string get status the current progress task attempt progress get progress get link counter given code counter name code counter get counter enum counter name get link counter given code group name code code counter name code code counter name code counter get counter string group name string counter name
1430	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskAttemptID.java	unrelated	package org apache hadoop mapreduce task attempt id represents immutable unique identifier task attempt each task attempt one particular instance map reduce task identified task id task attempt id consists parts first part link task id task attempt id belongs second part task attempt number br an example task attempt id code attempt code represents zeroth task attempt fifth map task third job running jobtracker started code code p applications never construct parse task attempt id strings rather use appropriate constructors link name string method task attempt id extends org apache hadoop mapred id protected string attempt attempt task id task id constructs task attempt id object given link task id task attempt id task id task id id constructs task id object given parts task attempt id string jt identifier job id task type type task attempt id returns link job id object task attempt belongs job id get job id returns link task id object task attempt belongs task id get task id returns task type task attempt id task type get task type boolean equals object add unique string builder protected string builder append to string builder builder read fields data input throws io exception write data output throws io exception hash code compare task ids first tip ids task numbers compare to id string string construct task attempt id object given task attempt id name string str
1431	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskCompletionEvent.java	unrelated	package org apache hadoop mapreduce this used track task completion events job tracker task completion event implements writable enum status failed killed succeeded obsolete tipfailed event id string task tracker http task run time using since runtime time difference task attempt id task id status status boolean map false id within job task completion event empty array default constructor writable task completion event constructor event id created externally incremented per event job incrementally starting task completion event event id returns event id get event id returns task id task attempt id get task attempt id returns enum status sucess status failure status get status http location tasktracker task ran string get task tracker http returns time millisec task took complete get task run time set task completion time protected set task run time task completion time set event id assigned incrementally starting protected set event id event id sets task id protected set task attempt id task attempt id task id set task status protected set task status status status set task tracker http location protected set task tracker http string task tracker http string string boolean equals object hash code boolean map task id within job writable write data output throws io exception read fields data input throws io exception
1432	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskCounter.java	unrelated	package org apache hadoop mapreduce counters used task enum task counter map input records map output records map skipped records map output bytes map output materialized bytes split raw bytes combine input records combine output records reduce input groups reduce shuffle bytes reduce input records reduce output records reduce skipped groups reduce skipped records spilled records shuffled maps failed shuffle merged map outputs gc time millis cpu milliseconds physical memory bytes virtual memory bytes committed heap bytes
1433	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskID.java	unrelated	package org apache hadoop mapreduce task id represents immutable unique identifier map reduce task each task id encompasses multiple attempts made execute map reduce task uniquely indentified task attempt id task id consists parts first part link job id task in progress belongs second part task id either r representing whether task map task reduce task and third part task number br an example task id code task code represents fifth map task third job running jobtracker started code code p applications never construct parse task id strings rather use appropriate constructors link name string method task id extends org apache hadoop mapred id protected string task task protected number format id format number format get instance id format set grouping used false id format set minimum integer digits job id job id task type type constructs task id object given link job id task id job id job id task type type id super id job id null job id job id type type constructs task in progress id object given parts task id string jt identifier job id task type type id new job id jt identifier job id type id task id job id new job id returns link job id object tip belongs job id get job id return job id get type task task type get task type return type boolean equals object super equals task id task id return type type job id equals job id compare task in progress ids first job ids tip numbers reduces defined greater maps compare to id task id task id job comp job id compare to job id job comp else return job comp string string return append to new string builder task string add unique given builder protected string builder append to string builder builder return job id append to builder hash code return job id hash code id read fields data input throws io exception super read fields job id read fields type writable utils read enum task type write data output throws io exception super write job id write writable utils write enum type construct task id object given task id name string str throws illegal argument exception str null try catch exception ex fall throw new illegal argument exception task id str gets character representing link task type char get representing character task type type return char task type maps get representing character type gets link task type corresponding character task type get task type char c return char task type maps get task type c string get all task types return char task type maps task types maintains mapping character representation task type enum task type constants char task type maps enum map task type character type to char map map character task type char to type map string task types r c setup task type to char mapping setup char to task type mapping char get representing character task type type task type get task type char c
1434	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskInputOutputContext.java	unrelated	package org apache hadoop mapreduce a context object allows input output task it supplied link mapper link reducer task input output context keyin valuein keyout valueout advance next key value pair returning null end boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception interrupted exception generate output key value pair write keyout key valueout value get link output committer task attempt output committer get output committer
1435	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskReport.java	unrelated	package org apache hadoop mapreduce a report state task task report implements writable task id taskid progress string state string diagnostics start time finish time counters counters tip status current status collection task attempt id running attempts task attempt id successful attempt new task attempt id task report creates new task report object task report task id taskid progress string state the id task task id get task id return taskid the amount completed zero one get progress return progress the recent state reported reporter string get state return state a list error messages string get diagnostics return diagnostics a table counters counters get task counters return counters the current status tip status get current status get finish time task get finish time set finish time task protected set finish time finish time get start time task get start time set start time task protected set start time start time set successful attempt id task protected set successful attempt id task attempt id get attempt id took task completion task attempt id get successful task attempt id set running attempt task protected set running task attempt ids get running task attempt i ds task collection task attempt id get running task attempt ids boolean equals object hash code writable write data output throws io exception read fields data input throws io exception
1436	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskTrackerInfo.java	unrelated	package org apache hadoop mapreduce information task tracker task tracker info implements writable string name boolean blacklisted false string reason for blacklist string blacklist report task tracker info construct active tracker task tracker info string name construct blacklisted tracker task tracker info string name string reason for blacklist gets tasktracker name string get task tracker name whether tracker blacklisted false otherwise boolean blacklisted gets reason tasktracker blacklisted string get reason for blacklist gets descriptive report tasktracker blacklisted string get blacklist report read fields data input throws io exception write data output throws io exception
1437	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskType.java	unrelated	package org apache hadoop mapreduce enum map reduce job setup job cleanup task cleanup task types enum task type map reduce job setup job cleanup task cleanup
1438	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\ClientDistributedCacheManager.java	unrelated	package org apache hadoop mapreduce filecache manages internal configuration cache client job submission client distributed cache manager determines timestamps files cached stores configuration this intended used internally job client cache files added this internal method determine timestamps configuration job throws io exception for archive cache file get corresponding delegation token get delegation tokens configuration job determines visibilities distributed cache files archives the visibility cache path leaf component read permissions others parent subdirs execute permissions others determine cache visibilities configuration job throws io exception this check visibility archives localized the order order archives added set archive visibilities configuration conf string booleans this check visibility files localized the order order files added set file visibilities configuration conf string booleans this check timestamp archives localized the order order archives added set archive timestamps configuration conf string timestamps this check timestamp files localized the order order files added set file timestamps configuration conf string timestamps returns link file status given cache file hdfs file status get file status configuration conf uri cache returns boolean denote whether cache file visible boolean public configuration conf uri uri throws io exception returns true ancestors specified path execute permission set users e users traverse directory heirarchy given path boolean ancestors have execute permissions file system fs path path checks given path whether other permissions imply permission passed fs action boolean check permission of other file system fs path path
1439	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\DistributedCache.java	unrelated	package org apache hadoop mapreduce filecache distribute application specific large read files efficiently p code distributed cache code facility provided map reduce framework cache files text archives jars etc needed applications p p applications specify files via urls hdfs http cached via link org apache hadoop mapred job conf the code distributed cache code assumes files specified via urls already present link file system path specified url accessible every machine cluster p p the framework copy necessary files slave node tasks job executed node its efficiency stems fact files copied per job ability cache archives un archived slaves p p code distributed cache code used distribute simple read data text files complex types archives jars etc archives zip tar tgz tar gz files un archived slave nodes jars may optionally added classpath tasks rudimentary software distribution mechanism files execution permissions optionally users also direct symlink distributed cache file working directory task p p code distributed cache code tracks modification timestamps cache files clearly cache files modified application externally job executing p p here illustrative example use code distributed cache code p p blockquote pre setting cache application copy requisite files code file system code bin hadoop fs copy from local lookup dat myapp lookup dat bin hadoop fs copy from local map zip myapp map zip bin hadoop fs copy from local mylib jar myapp mylib jar bin hadoop fs copy from local mytar tar myapp mytar tar bin hadoop fs copy from local mytgz tgz myapp mytgz tgz bin hadoop fs copy from local mytargz tar gz myapp mytargz tar gz setup application code job conf code job conf job new job conf distributed cache add cache file new uri myapp lookup dat lookup dat job distributed cache add cache archive new uri myapp map zip job distributed cache add file to class path new path myapp mylib jar job distributed cache add cache archive new uri myapp mytar tar job distributed cache add cache archive new uri myapp mytgz tgz job distributed cache add cache archive new uri myapp mytargz tar gz job use cached files link org apache hadoop mapred mapper link org apache hadoop mapred reducer map class extends map reduce base implements mapper lt k v k v gt path local archives path local files configure job conf job get cached archives files local archives distributed cache get local cache archives job local files distributed cache get local cache files job map k key v value output collector lt k v gt output reporter reporter throws io exception use data cached archives files output collect k v pre blockquote p it also common use distributed cache using link org apache hadoop util generic options parser this includes methods used users specifically mentioned example well link distributed cache add archive to class path path configuration well methods intended use map reduce framework e g link org apache hadoop mapred job client distributed cache set cache archives uri archives configuration conf set cache files uri files configuration conf uri get cache archives configuration conf throws io exception uri get
1440	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\package-info.java	unrelated	package org apache hadoop mapreduce filecache
1441	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\TaskDistributedCacheManager.java	unrelated	package org apache hadoop mapreduce filecache helper link tracker distributed cache manager represents cached files single task this used task runner local job runner parse job configuration setup local caches task distributed cache manager tracker distributed cache manager distributed cache manager configuration task conf list cache file cache files new array list cache file list string paths new array list string boolean setup called false cache file task distributed cache manager setup local dir allocator dir alloc file work dir list cache file get cache files string stringify path list list path p list string get class paths throws io exception release throws io exception class loader make class loader class loader parent
1442	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\TrackerDistributedCacheManager.java	pooling	package org apache hadoop mapreduce filecache manages single machine instance cross job cache this would typically instantiated task tracker something emulates like local job runner tracker distributed cache manager cache id cache status mapping linked hash map string cache status cached archives default total cache size gb default cache size l default cache subdir limit this default pulled air set based read world use cases default cache keep around pct f allowed cache size allowed cache subdirs allowed cache size cleanup goal allowed cache subdirs cleanup goal log log local file system local fs local dir allocator dir allocator task controller task controller configuration tracker conf random random new random mr async disk service async disk service protected base dir manager base dir manager new base dir manager protected cleanup thread cleanup thread tracker distributed cache manager configuration conf tracker distributed cache manager configuration conf path get local cache uri cache configuration conf release cache uri cache configuration conf time stamp get reference count uri cache configuration conf time stamp string get localized cache owner boolean public throws io exception delete local path mr async disk service async disk service string make relative uri cache configuration conf string get key uri cache configuration conf time stamp string user file status get file status configuration conf uri cache get timestamp configuration conf uri cache throws io exception check cache status validity configuration conf create symlink configuration conf uri cache method actually copies caches locally unjars unzips chmod files path localize cache configuration conf throws io exception set permissions configuration conf cache status cache status boolean tar file string filename ensure file hdfs modified since job started check stamp since job started configuration conf file system fs throws io exception checks cache already localized fresh boolean exists and fresh configuration conf file system fs throws io exception create all symlink configuration conf file job cache dir cache status purge cache task distributed cache manager new task distributed cache manager string get file visibilities configuration conf string get archive visibilities configuration conf set local archives configuration conf string str set local files configuration conf string str protected cleanup thread extends thread cache dir protected base dir manager start cleanup thread stop cleanup thread
1443	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\EventReader.java	scheduler	package org apache hadoop mapreduce jobhistory event reader implements closeable string version schema schema data input stream decoder decoder datum reader reader create new event reader event reader file system fs path name throws io exception create new event reader event reader data input stream throws io exception get next event stream history event get next event throws io exception close event reader close throws io exception counters avro jh counters counters
1444	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\EventWriter.java	unrelated	package org apache hadoop mapreduce jobhistory event writer utility used write events underlying stream typically one event writer translates one stream created per job event writer string version avro json fs data output stream datum writer event writer encoder encoder event writer fs data output stream throws io exception synchronized write history event event throws io exception flush throws io exception close throws io exception schema groups schema counters jh counters avro counters counters jh counters avro counters counters string name
1445	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\HistoryEvent.java	unrelated	package org apache hadoop mapreduce jobhistory interface event wrapper implementations wrap avro generated adding constructors accessor methods history event return event type event type get event type return avro datum wrapped object get datum set avro datum wrapped set datum object datum
1446	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\HistoryViewer.java	unrelated	package org apache hadoop mapreduce jobhistory history viewer used parse view job history files history viewer simple date format date format new simple date format mmm yyyy hh mm ss file system fs job info job string job id boolean print all constructs history viewer object history viewer string history file print all print all string error msg unable initialize history viewer try catch exception e print job task attempt summary information print throws io exception print job details print task summary print job analysis print tasks task type job setup task status state failed string print tasks task type job setup task status state killed string print tasks task type map task status state failed string print tasks task type map task status state killed string print tasks task type reduce task status state failed string print tasks task type reduce task status state killed string print tasks task type job cleanup task status state failed string print tasks task type job cleanup print all filtered job filter new filtered job job print failed attempts filter filter new filtered job job print failed attempts filter print job details string buffer job details new string buffer job details append n hadoop job append job get job id job details append n job details append n user append job get username job details append n job name append job get jobname job details append n job conf append job get job conf path job details append n submitted at append string utils job details append n launched at append string utils job details append n finished at append string utils job details append n status append job get job status null print counters job details job get total counters job get map counters job details append n job details append n system println job details string print counters string buffer buff counters total counters killed jobs might counters total counters null buff append n counters n n buff append string format buff append n string group name total counters get group names print all task attempts task type task type map task id task info tasks job get all tasks string buffer task list new string buffer task list append n append task type task list append task list append job get job id task list append n task id start time task type reduce equals task type task list append finish time host name error task logs task list append n system println task list string job history parser task info task tasks values print task summary summarized job ts new summarized job job string buffer task summary new string buffer task summary append n task summary task summary append n task summary append n kind total task summary append successful failed killed start time finish time task summary append n task summary append n setup append ts total setups task summary append append ts num finished setups task summary append append ts num failed setups task summary append append ts num killed setups task summary append append string utils get formatted
1447	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion job job finished event implements history event job finished datum new job finished create event record successful job completion job finished event job id id finish time job finished event object get datum return datum set datum object datum datum job finished datum event type get event type get job id job id get jobid return job id name datum jobid string get job finish time get finish time return datum finish time get number finished maps job get finished maps return datum finished maps get number finished reducers job get finished reduces return datum finished reduces get number failed maps job get failed maps return datum failed maps get number failed reducers job get failed reduces return datum failed reduces get counters job counters get total counters get map counters job counters get map counters get reduce counters job counters get reduce counters
1448	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobHistory.java	pooling	package org apache hadoop mapreduce jobhistory job history responsible creating maintaining job history information job history log log log factory get log job history job history block size map job id meta info file map thread pool executor executor null fs permission history dir permission fs permission history file permission job tracker job tracker default history max age l week file system log dir fs log dir fs file system done dir fs done dir fs path log dir null path done null folder completed jobs string old suffix old version prefix history files string history version history cleaner history cleaner thread null map job id moved file info job history file map job history filename regex pattern jobhistory filename regex job history conf filename regex pattern conf filename regex moved file info init job tracker jt job conf conf string hostname initialize done directory start history cleaner thread init done job conf conf file system fs throws io exception mark completed job id id throws io exception shut job history stopping history cleaner shut down path get job history location path get completed job history location path get job history file path dir job id job id job id get job id from history file path path job history file path string get user from history file path path job history file path string get history file path job id job id setup event writer job id job id job conf job conf throws io exception close event writer id close writer job id id log event history event event job id id move to done now path path path path throws io exception start file mover threads path get conf file path log dir job id job id string get old file suffix string identifier move old files throws io exception move to done job id id string get user name job conf job conf meta info history cleaner extends thread
1449	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobHistoryParser.java	unrelated	package org apache hadoop mapreduce jobhistory default parser job history files typical usage job history parser parser new job history parser fs history file job parser parse job history parser fs data input stream job info info null create job history parser given history file using given file system job history parser file system fs string file throws io exception fs new path file create job history parser given history file using given file system job history parser file system fs path history file throws io exception fs open history file create history parser based input stream job history parser fs data input stream parse entire history file populate job info object the first invocation populate object subsequent calls return already parsed object the input stream closed return synchronized job info parse throws io exception info null event reader reader new event reader history event event info new job info try finally return info handle event history event event throws io exception event type type event get event type switch type case job submitted case job status changed case job info changed case job inited case job priority changed case job failed case job killed case job finished case task started case task failed case task updated case task finished case map attempt started case cleanup attempt started case reduce attempt started case setup attempt started case map attempt failed case cleanup attempt failed case reduce attempt failed case setup attempt failed case map attempt killed case cleanup attempt killed case reduce attempt killed case setup attempt killed case map attempt finished case reduce attempt finished case setup attempt finished case cleanup attempt finished default handle task attempt finished event task attempt finished event event task info task info info tasks map get event get task id task attempt info attempt info attempt info finish time event get finish time attempt info status event get task status attempt info state event get state attempt info counters event get counters attempt info hostname event get hostname handle reduce attempt finished event reduce attempt finished event event task info task info info tasks map get event get task id task attempt info attempt info attempt info finish time event get finish time attempt info status event get task status attempt info state event get state attempt info shuffle finish time event get shuffle finish time attempt info sort finish time event get sort finish time attempt info counters event get counters attempt info hostname event get hostname handle map attempt finished event map attempt finished event event task info task info info tasks map get event get task id task attempt info attempt info attempt info finish time event get finish time attempt info status event get task status attempt info state event get state attempt info map finish time event get map finish time attempt info counters event get counters attempt info hostname event get hostname handle task attempt failed event task info task info info tasks map get event get task id task attempt info attempt info attempt info finish time event
1450	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobInfoChangeEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record changes submit launch time job job info change event implements history event job info change datum new job info change create event record submit launch time job job info change event job id id submit time launch time job info change event object get datum return datum set datum object datum get job id job id get job id return job id name datum jobid string get job submit time get submit time return datum submit time get job launch time get launch time return datum launch time event type get event type
1451	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobInitedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record initialization job job inited event implements history event job inited datum new job inited create event record job initialization job inited event job id id launch time total maps job inited event object get datum return datum set datum object datum datum job inited datum get job id job id get job id return job id name datum jobid string get launch time get launch time return datum launch time get total number maps get total maps return datum total maps get total number reduces get total reduces return datum total reduces get status string get status return datum job status string get event type event type get event type
1452	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobPriorityChangeEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record change priority job job priority change event implements history event job priority change datum new job priority change generate event record changes job priority job priority change event job id id job priority priority job priority change event object get datum return datum set datum object datum get job id job id get job id return job id name datum jobid string get job priority job priority get priority get event type event type get event type
1453	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobStatusChangedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record change status job job status changed event implements history event job status changed datum new job status changed create event record change job status job status changed event job id id string job status job status changed event object get datum return datum set datum object datum get job id job id get job id return job id name datum jobid string get event status string get status return datum job status string get event type event type get event type
1454	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobSubmittedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record submission job job submitted event implements history event job submitted datum new job submitted create event record job submission job submitted event job id id string job name string user name job submitted event object get datum return datum set datum object datum get job id job id get job id return job id name datum jobid string get job name string get job name return datum job name string get job queue name string get job queue name get user name string get user name return datum user name string get submit time get submit time return datum submit time get path job configuration file string get job conf path return datum job conf path string get acls configured job map job acl access control list get job acls get event type event type get event type return event type job submitted
1455	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobUnsuccessfulCompletionEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record failed killed completion jobs job unsuccessful completion event implements history event job unsuccessful completion datum create event record unsuccessful completion killed failed jobs job unsuccessful completion event job id id finish time job unsuccessful completion event object get datum return datum set datum object datum get job id job id get job id return job id name datum jobid string get job finish time get finish time return datum finish time get number finished maps get finished maps return datum finished maps get number finished reduces get finished reduces return datum finished reduces get status string get status return datum job status string get event type event type get event type
1456	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\MapAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion map attempt map attempt finished event implements history event map attempt finished datum new map attempt finished create event successful completion map attempts map attempt finished event task attempt id id map attempt finished event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get attempt id task attempt id get attempt id get task type task type get task type get task status string get task status return datum task status string get map phase finish time get map finish time return datum map finish time get attempt finish time get finish time return datum finish time get host name string get hostname return datum hostname string get state string get state return datum state string get counters counters get counters return event reader avro datum counters get event type event type get event type
1457	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\package-info.java	unrelated	package org apache hadoop mapreduce jobhistory
1458	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\ReduceAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion reduce attempt reduce attempt finished event implements history event reduce attempt finished datum create event record completion reduce attempt reduce attempt finished event task attempt id id reduce attempt finished event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get attempt id task attempt id get attempt id get task type task type get task type get task status string get task status return datum task status string get finish time sort phase get sort finish time return datum sort finish time get finish time shuffle phase get shuffle finish time return datum shuffle finish time get finish time attempt get finish time return datum finish time get name host attempt ran string get hostname return datum hostname string get state string get state return datum state string get counters attempt counters get counters return event reader avro datum counters get event type event type get event type
1459	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful task completion task attempt finished event implements history event task attempt finished datum new task attempt finished create event record successful finishes setup cleanup attempts task attempt finished event task attempt id id task attempt finished event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get task attempt id task attempt id get attempt id get task type task type get task type get task status string get task status return datum task status string get attempt finish time get finish time return datum finish time get host attempt executed string get hostname return datum hostname string get state string get state return datum state string get counters attempt counters get counters return event reader avro datum counters get event type event type get event type
1460	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptStartedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record start task attempt task attempt started event implements history event task attempt started datum new task attempt started create event record start attempt task attempt started event task attempt id attempt id task attempt started event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get tracker name string get tracker name return datum tracker name string get start time get start time return datum start time get task type task type get task type get http port get http port return datum http port get attempt id task attempt id get task attempt id get event type event type get event type return get task id get task type task type map
1461	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptUnsuccessfulCompletionEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record unsuccessful killed failed completion task attempts task attempt unsuccessful completion event implements history event task attempt unsuccessful completion datum create event record unsuccessful completion attempts task attempt unsuccessful completion event task attempt id id task attempt unsuccessful completion event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get task type task type get task type get attempt id task attempt id get task attempt id get finish time get finish time return datum finish time get name host attempt executed string get hostname return datum hostname string get error string get error return datum error string get task status string get task status return datum status string get event type event type get event type
1462	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskFailedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record failure task task failed event implements history event task failed datum new task failed create event record task failure task failed event task id id finish time task failed event object get datum return datum set datum object datum datum task failed datum get task id task id get task id return task id name datum taskid string get error string get error return datum error string get finish time attempt get finish time return datum finish time get task type task type get task type get attempt id due task failed task attempt id get failed attempt id get task status string get task status return datum status string get event type event type get event type return event type task failed
1463	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion task task finished event implements history event task finished datum new task finished create event record successful completion task task finished event task id id finish time task finished event object get datum return datum set datum object datum get task id task id get task id return task id name datum taskid string get task finish time get finish time return datum finish time get task counters counters get counters return event reader avro datum counters get task type task type get task type get task status string get task status return datum status string get event type event type get event type
1464	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskStartedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record start task task started event implements history event task started datum new task started create event record start task task started event task id id start time task started event object get datum return datum set datum object datum datum task started datum get task id task id get task id return task id name datum taskid string get split locations applicable map tasks string get split locations return datum split locations string get start time task get start time return datum start time get task type task type get task type get event type event type get event type
1465	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskUpdatedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record updates task task updated event implements history event task updated datum new task updated create event record task updates task updated event task id id finish time task updated event object get datum return datum set datum object datum datum task updated datum get task id task id get task id return task id name datum taskid string get task finish time get finish time return datum finish time get event type event type get event type
1466	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\DoubleValueSum.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator sums sequence values double value sum implements value aggregator string sum double value sum add next value object val add next value val string get report get sum reset array list string get combiner output
1467	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueMax.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain maximum sequence values long value max implements value aggregator string max val long min value long value max add next value object val add next value new val get val string get report reset array list string get combiner output
1468	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueMin.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain minimum sequence values long value min implements value aggregator string min val long max value long value min add next value object val add next value new val get val string get report reset array list string get combiner output
1469	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueSum.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator sums sequence values long value sum implements value aggregator string sum long value sum add next value object val add next value val get sum string get report reset array list string get combiner output
1470	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\StringValueMax.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain biggest sequence strings string value max implements value aggregator string string max val null string value max add next value object val string get val string get report reset array list string get combiner output
1471	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\StringValueMin.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain smallest sequence strings string value min implements value aggregator string string min val null string value min add next value object val string get val string get report reset array list string get combiner output
1472	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\UniqValueCount.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator dedupes sequence objects uniq value count implements value aggregator object string max num unique values tree map object object uniq items null num items max num items long max value uniq value count uniq value count max num set max items n add next value object val string get report set object get unique items reset array list object get combiner output
1473	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\UserDefinedValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements wrapper user defined value aggregator descriptor it serves two functions one create object value aggregator descriptor name user defined may dynamically loaded the delegate invocations generate key val pairs function created object user defined value aggregator descriptor implements string name protected value aggregator descriptor aggregator descriptor null class arg array new class object create instance string name create aggregator configuration conf user defined value aggregator descriptor string name array list entry text text generate key val pairs object key string string configure configuration conf
1474	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregator.java	unrelated	package org apache hadoop mapreduce lib aggregate this defines minimal protocol value aggregators value aggregator e add value aggregator add next value object val reset aggregator reset string get report array list e get combiner output
1475	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorBaseDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements common functionalities subclasses value aggregator descriptor value aggregator base descriptor string uniq value count uniq value count string long value sum long value sum string double value sum double value sum string value histogram value histogram string long value max long value max string long value min long value min string string value max string value max string string value min string value min string input file null my entry implements entry text text entry text text generate entry string type value aggregator generate value aggregator string type uniq count array list entry text text generate key val pairs object key configure configuration conf
1476	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorCombiner.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic combiner aggregate value aggregator combiner k extends writable comparable extends reducer text text text text combines values given key reduce text key iterable text values context context
1477	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this defines contract value aggregator descriptor must support such descriptor configured link configuration object its main function generate list aggregation id value pairs an aggregation id encodes aggregation type used guide way aggregate value reduce combiner phrase aggregate based job the mapper aggregate based map reduce job may create one value aggregator descriptor objects configuration time for input key value pair mapper use objects create aggregation id value pairs value aggregator descriptor string type separator text one new text generate list aggregation id value pairs given key value pair this function usually called mapper aggregate based job input key input value aggregation type used guide way aggregate value reduce combiner phrase aggregate based job array list entry text text generate key val pairs object key configure object configuration object may contain information used configure object configure configuration conf
1478	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorJob.java	unrelated	package org apache hadoop mapreduce lib aggregate this main creating map reduce job using aggregate framework the aggregate specialization map reduce framework specializing performing various simple aggregations generally speaking order implement application using map reduce model developer implement map reduce functions possibly combine function however lot applications related counting statistics computing similar characteristics aggregate abstracts general patterns functions implementing patterns in particular package provides generic mapper redducer combiner set built value aggregators generic utility helps user create map reduce jobs using generic the built aggregators sum numeric values count number distinct values compute histogram values compute minimum maximum media average standard deviation numeric values the developer using aggregate need provide plugin conforming following value aggregator descriptor array list entry generate key val pairs object key object value configure configuration conf the package also provides base value aggregator base descriptor implementing the user extend base implement generate key val pairs accordingly the primary work generate key val pairs emit one key value pairs based input key value pair the key output key value pair encode two pieces information aggregation type aggregation id the value aggregated onto aggregation id according aggregation type this offers function generate map reduce job using aggregate framework the function takes following parameters input directory spec input format text sequence file output directory file specifying user plugin value aggregator job job control create value aggregator jobs string args throws io exception job control create value aggregator jobs string args job create value aggregator job configuration conf string args job create value aggregator job string args configuration set aggregator descriptors main string args
1479	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorJobBase.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements common functionalities generic mapper reducer combiner aggregate value aggregator job base k extends writable comparable string descriptor mapreduce aggregate descriptor string descriptor num string user jar mapreduce aggregate user jar file protected array list value aggregator descriptor aggregator descriptor list null setup configuration job protected value aggregator descriptor get value aggregator descriptor protected array list value aggregator descriptor get aggregator descriptors initialize my spec configuration conf protected log spec
1480	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorMapper.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic mapper aggregate value aggregator mapper k extends writable comparable extends mapper k v text text setup context context map k key v value
1481	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorReducer.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic reducer aggregate value aggregator reducer k extends writable comparable extends reducer text text text text setup context context reduce text key iterable text values
1482	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueHistogram.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator computes histogram sequence strings value histogram implements value aggregator string tree map object object items null value histogram add next value object val string get report string get report details array list string get combiner output tree map object object get report items reset
1483	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\Chain.java	unrelated	package org apache hadoop mapreduce lib chain the chain provides common functionality link chain mapper link chain reducer chain protected string chain mapper mapreduce chain mapper protected string chain reducer mapreduce chain reducer protected string chain mapper size size protected string chain mapper class mapper protected string chain mapper config mapper config protected string chain reducer class reducer protected string chain reducer config reducer config protected string mapper input key class protected string mapper input value class protected string mapper output key class protected string mapper output value class protected string reducer input key class protected string reducer input value class protected string reducer output key class protected string reducer output value class protected boolean map list mapper mappers new array list mapper reducer reducer list configuration conf list new array list configuration configuration r conf list thread threads new array list thread list chain blocking queue blocking queues throwable throwable null protected chain boolean map key value pair k v chain record reader either reads blocking queue task context chain record reader keyin valuein extends chain record writer either writes blocking queue task context chain record writer keyout valueout extends synchronized throwable get throwable synchronized boolean set if unset throwable throwable th map runner keyin valuein keyout valueout extends thread reduce runner keyin valuein keyout valueout extends thread configuration get conf index keyin valuein keyout valueout mapper keyin valuein keyout valueout context create map context run mapper task input output context context index throws io exception add mapper task input output context input context add mapper chain blocking queue key value pair input add mapper chain blocking queue key value pair input keyin valuein keyout valueout reducer keyin valuein keyout valueout context create reduce context run reducer directly keyin valuein keyout valueout run reducer add reducer task input output context input context start threads start all threads wait till threads finish join all threads throws io exception interrupted exception interrupt threads synchronized interrupt all threads protected string get prefix boolean map protected get index configuration conf string prefix protected configuration get chain element conf configuration job conf protected add mapper boolean map job job reducer chain check reducer already set protected check reducer already set boolean map protected validate key value types boolean map protected set mapper conf boolean map configuration job conf protected set reducer job job class extends reducer klass protected set reducer conf configuration job conf setup configuration job conf list mapper get all mappers reducer get reducer chain blocking queue key value pair create blocking queue chain blocking queue e
1484	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainMapContextImpl.java	unrelated	package org apache hadoop mapreduce lib chain a simple wrapper delegates functionality underlying context overrides methods record readers record writers configuration chain map context impl keyin valuein keyout valueout implements record reader keyin valuein reader record writer keyout valueout output task input output context keyin valuein keyout valueout base configuration conf chain map context impl keyin get current key throws io exception interrupted exception valuein get current value throws io exception interrupted exception boolean next key value throws io exception interrupted exception input split get input split counter get counter enum counter name counter get counter string group name string counter name output committer get output committer write keyout key valueout value throws io exception string get status task attempt id get task attempt id set status string msg path get archive class paths string get archive timestamps uri get cache archives throws io exception uri get cache files throws io exception class extends reducer get combiner class configuration get configuration path get file class paths string get file timestamps raw comparator get grouping comparator class extends input format get input format class string get jar job id get job id string get job name boolean get job setup cleanup needed boolean get task cleanup needed path get local cache archives throws io exception path get local cache files throws io exception class get map output key class class get map output value class class extends mapper get mapper class get max map attempts get max reduce attempts get num reduce tasks class extends output format get output format class class get output key class class get output value class class extends partitioner get partitioner class boolean get profile enabled string get profile params integer ranges get profile task range boolean map class extends reducer get reducer class raw comparator get sort comparator boolean get symlink string get user path get working directory throws io exception progress credentials get credentials get progress
1485	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainMapper.java	unrelated	package org apache hadoop mapreduce lib chain the chain mapper allows use multiple mapper within single map task p the mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p p the key functionality feature mappers chain need aware executed chain this enables reusable specialized mappers combined perform composite operations within single task p p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use matching output input key value conversion done chaining code p p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p p important there need specify output key value chain mapper done add mapper last mapper chain p chain mapper usage pattern p pre job new job conf p configuration map a conf new configuration false chain mapper add mapper job a map long writable text text text true map a conf p configuration map b conf new configuration false chain mapper add mapper job b map text text long writable text false map b conf p p job wait for complettion true pre chain mapper keyin valuein keyout valueout extends add mapper job job class extends mapper klass chain chain protected setup context context run context context throws io exception interrupted exception
1486	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainReduceContextImpl.java	unrelated	package org apache hadoop mapreduce lib chain a simple wrapper delegates functionality underlying context overrides methods record writer configuration chain reduce context impl keyin valuein keyout valueout implements reduce context keyin valuein keyout valueout base record writer keyout valueout rw configuration conf chain reduce context impl iterable valuein get values throws io exception interrupted exception boolean next key throws io exception interrupted exception counter get counter enum counter name counter get counter string group name string counter name keyin get current key throws io exception interrupted exception valuein get current value throws io exception interrupted exception output committer get output committer boolean next key value throws io exception interrupted exception write keyout key valueout value throws io exception string get status task attempt id get task attempt id set status string msg path get archive class paths string get archive timestamps uri get cache archives throws io exception uri get cache files throws io exception class extends reducer get combiner class configuration get configuration path get file class paths string get file timestamps raw comparator get grouping comparator class extends input format get input format class string get jar job id get job id string get job name boolean get job setup cleanup needed boolean get task cleanup needed path get local cache archives throws io exception path get local cache files throws io exception class get map output key class class get map output value class class extends mapper get mapper class get max map attempts get max reduce attempts get num reduce tasks class extends output format get output format class class get output key class class get output value class class extends partitioner get partitioner class boolean get profile enabled string get profile params integer ranges get profile task range boolean map class extends reducer get reducer class raw comparator get sort comparator boolean get symlink string get user path get working directory throws io exception progress credentials get credentials get progress
1487	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainReducer.java	unrelated	package org apache hadoop mapreduce lib chain the chain reducer allows chain multiple mapper reducer within reducer task p for record output reducer mapper invoked chained piped fashion the output reducer becomes input first mapper output first becomes input second last mapper output last mapper written task output p p the key functionality feature mappers chain need aware executed reducer chain this enables reusable specialized mappers combined perform composite operations within single task p p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use matching output input key value conversion done chaining code p p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p p important there need specify output key value chain reducer done set reducer add mapper last element chain p chain reducer usage pattern p pre job new job conf p configuration reduce conf new configuration false chain reducer set reducer job x reduce long writable text text text true reduce conf p chain reducer add mapper job c map text text long writable text false null p chain reducer add mapper job d map long writable text long writable long writable true null p p job wait for completion true pre chain reducer keyin valuein keyout valueout extends set reducer job job class extends reducer klass add mapper job job class extends mapper klass chain chain protected setup context context run context context throws io exception interrupted exception
1488	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\BigDecimalSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter big decimal values big decimal splitter implements db splitter log log log factory get log big decimal splitter list input split split configuration conf result set results string col name big decimal min increment new big decimal double min value divide numerator denominator if impossible exact mode use rounding protected big decimal try divide big decimal numerator big decimal denominator returns list big decimals one element longer list input splits this represents boundaries input splits all splits open top end except last one so list would represent splits capturing intervals note closed interval last split list big decimal split big decimal num splits big decimal min val big decimal max val
1489	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\BooleanSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter boolean values boolean splitter implements db splitter list input split split configuration conf result set results string col name
1490	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DataDrivenDBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table operates like db input format instead using limit offset demarcate splits tries generate where clauses separate data roughly equivalent shards data driven db input format t extends db writable extends db input format t implements configurable log log log factory get log data driven db input format if users providing query following expected string substitute token conditions a input split spans set rows data driven db input split extends db input format db input split string lower bound clause string upper bound clause default constructor data driven db input split convenience constructor data driven db input split string lower string upper get length throws io exception inherit doc read fields data input input throws io exception inherit doc write data output output throws io exception string get lower clause string get upper clause protected db splitter get splitter sql data type switch sql data type case types numeric case types decimal case types bit case types boolean case types integer case types tinyint case types smallint case types bigint case types real case types float case types double case types char case types varchar case types longvarchar case types date case types time case types timestamp default inherit doc list input split get splits job context job throws io exception target num tasks job get configuration get int mr job config num maps target num tasks result set results null statement statement null connection connection get connection try catch sql exception e finally order column the min value first column max value second column results protected string get bounding vals query if user provided query use instead string user query get db conf get input bounding query null user query auto generate one based table name provided string builder query new string builder string split col get db conf get input order by query append select min append split col append query append max append split col append from query append get db conf get input table name string conditions get db conf get input conditions null conditions return query string set user defined bounding query use user defined query set bounding query configuration conf string query null query conf set db configuration input bounding query query protected record reader long writable t create db record reader db input split split db configuration db conf get db conf class t input class class t db conf get input class string db product name get db product name log debug creating db record reader db product db product name try catch sql exception ex configuration methods superclass ensure proper data driven db input format gets used note order by column called split by version we reuse field strictly ordering partitioning results set input job job db input format set input job input class table name conditions split by field names job set input format class data driven db input format set input takes custom query separate bounding query use set input job job db input format set input
1491	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records sql table using data driven where clause splits emits long writables containing record number key db writables value data driven db record reader t extends db writable extends db record reader t log log log factory get log data driven db record reader string db product name database manufacturer data driven db record reader db input format db input split split returns query selecting records subclasses custom behaviour protected string get select query
1492	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DateSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter date time values make use logic integer splitter since date time longs java date splitter extends integer splitter log log log factory get log date splitter list input split split configuration conf result set results string col name retrieve value column type appropriate manner return result set col to long result set rs col num sql data type throws sql exception parse valued timestamp appropriate sql date type date to date val sql data type given date format use sql date comparison operation quotation characters etc protected string date to string date
1493	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBConfiguration.java	unrelated	package org apache hadoop mapreduce lib db a container configuration property names jobs db input output the job configured using methods link db input format link db output format alternatively properties set configuration proper values db configuration the jdbc driver name string driver class property mapreduce jdbc driver jdbc database access url string url property mapreduce jdbc url user name access database string username property mapreduce jdbc username password access database string password property mapreduce jdbc password input table name string input table name property mapreduce jdbc input table name field names input table string input field names property mapreduce jdbc input field names where clause input select statement string input conditions property mapreduce jdbc input conditions order by clause input select statement string input order by property mapreduce jdbc input orderby whole input query exluding limit offset string input query mapreduce jdbc input query input query get count records string input count query mapreduce jdbc input count query input query get max min values jdbc input query string input bounding query class name implementing db writable hold input tuples string input class property mapreduce jdbc input output table name string output table name property mapreduce jdbc output table name field names output table string output field names property mapreduce jdbc output field names number fields output table string output field count property mapreduce jdbc output field count sets db access related fields link configuration configure db configuration conf string driver class conf set driver class property driver class conf set url property db url user name null passwd null sets db access related fields job conf configure db configuration job string driver class configure db job driver class db url null null configuration conf db configuration configuration job conf job returns connection object db connection get connection class name conf get db configuration driver class property conf get db configuration username property null else configuration get conf return conf string get input table name return conf get db configuration input table name property set input table name string table name conf set db configuration input table name property table name string get input field names return conf get strings db configuration input field names property set input field names string field names conf set strings db configuration input field names property field names string get input conditions return conf get db configuration input conditions property set input conditions string conditions conditions null conditions length string get input order by return conf get db configuration input order by property set input order by string orderby orderby null orderby length string get input query return conf get db configuration input query set input query string query query null query length string get input count query return conf get db configuration input count query set input count query string query query null query length set input bounding query string query query null query length string get input bounding query return conf get db configuration input bounding query class get input class return conf get class db configuration input class property set input class class
1494	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table p db input format emits long writables containing record number key db writables value the sql query input using one two set input methods db input format t extends db writable string db product name default null db writable implements db writable writable db input split extends input split implements writable string conditions connection connection string table name string field names db configuration db conf inherit doc set conf configuration conf configuration get conf db configuration get db conf connection get connection string get db product name protected record reader long writable t create db record reader db input split split inherit doc record reader long writable t create record reader input split split inherit doc list input split get splits job context job throws io exception returns query getting total number rows protected string get count query set input job job set input job job protected close connection
1495	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBOutputFormat.java	unrelated	package org apache hadoop mapreduce lib db a output format sends reduce output sql table p link db output format accepts lt key value gt pairs key type extending db writable returned link record writer writes b key b database batch sql query db output format k extends db writable v extends output format k v log log log factory get log db output format check output specs job context context output committer get output committer task attempt context context return new file output committer file output format get output path context a record writer writes reduce output sql table db record writer connection connection prepared statement statement db record writer throws sql exception db record writer connection connection connection get connection prepared statement get statement inherit doc close task attempt context context throws io exception inherit doc write k key v value throws io exception constructs query used prepared statement insert data table insert fields insert if field names unknown supply array nulls string construct query string table string field names field names null string builder query new string builder query append insert into append table field names length field names null query append values field names length query append return query string inherit doc record writer k v get record writer task attempt context context db configuration db conf new db configuration context get configuration string table name db conf get output table name string field names db conf get output field names field names null try catch exception ex initializes reduce part job appropriate output settings set output job job string table name field names length field names null else initializes reduce part job appropriate output settings set output job job string table name db configuration db conf set output job table name db conf set output field count field count db configuration set output job job job set output format class db output format job set reduce speculative execution false db configuration db conf new db configuration job get configuration db conf set output table name table name return db conf
1496	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records sql table emits long writables containing record number key db writables value db record reader t extends db writable extends log log log factory get log db record reader result set results null class t input class configuration conf db input format db input split split pos long writable key null t value null connection connection protected prepared statement statement db configuration db conf string conditions string field names string table name db record reader db input format db input split split protected result set execute query string query throws sql exception returns query selecting records subclasses custom behaviour protected string get select query inherit doc close throws io exception initialize input split split task attempt context context inherit doc long writable get current key inherit doc t get current value t create value get pos throws io exception boolean next long writable key t value throws io exception inherit doc get progress throws io exception inherit doc boolean next key value throws io exception protected db input format db input split get split protected string get field names protected string get table name protected string get conditions protected db configuration get db conf protected connection get connection protected prepared statement get statement protected set statement prepared statement stmt
1497	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBSplitter.java	unrelated	package org apache hadoop mapreduce lib db db splitter generate db input splits use data driven db input format data driven db input format needs interpolate two values represent lowest highest valued records import depending data type column requires different behavior db splitter implementations perform data type family data types db splitter given result set containing one record already advanced record two columns low value high value type determine set splits span given values list input split split configuration conf result set results string col name throws sql exception
1498	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBWritable.java	unrelated	package org apache hadoop mapreduce lib db objects read written database implement code db writable code db writable similar link writable except link write prepared statement method takes link prepared statement link read fields result set takes link result set p implementations responsible writing fields object prepared statement reading fields object result set p example p if following table database pre create table my table counter integer not null timestamp bigint not null pre read write tuples table p pre my writable implements writable db writable some data counter timestamp writable write implementation write data output throws io exception write int counter write long timestamp writable read fields implementation read fields data input throws io exception counter read int timestamp read long write prepared statement statement throws sql exception statement set int counter statement set long timestamp read fields result set result set throws sql exception counter result set get int timestamp result set get long pre p db writable write prepared statement statement throws sql exception read fields result set result set throws sql exception
1499	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\FloatSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter floating point values float splitter implements db splitter log log log factory get log float splitter min increment double min value list input split split configuration conf result set results string col name
1500	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\IntegerSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter integer values integer splitter implements db splitter list input split split configuration conf result set results string col name returns list longs one element longer list input splits this represents boundaries input splits all splits open top end except last one so list would represent splits capturing intervals note closed interval last split list long split num splits min val max val
1501	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\MySQLDataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records my sql table via data driven db record reader my sql data driven db record reader t extends db writable my sql data driven db record reader db input format db input split split execute statements mysql unbuffered mode protected result set execute query string query throws sql exception
1502	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\MySQLDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records my sql table my sqldb record reader t extends db writable extends db record reader t my sqldb record reader db input format db input split split execute statements mysql unbuffered mode protected result set execute query string query throws sql exception
1503	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDataDrivenDBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table oracle db oracle data driven db input format t extends db writable protected db splitter get splitter sql data type protected record reader long writable t create db record reader db input split split
1504	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records oracle table via data driven db record reader oracle data driven db record reader t extends db writable oracle data driven db record reader db input format db input split split
1505	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDateSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter date time values returned oracle db make use logic date splitter since needs use oracle specific functions formatting end generating input splits oracle date splitter extends date splitter protected string date to string date
1506	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records oracle sql table oracle db record reader t extends db writable extends db record reader t configuration key set timezone string session timezone key oracle session time zone log log log factory get log oracle db record reader oracle db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception super split input class conf conn db config cond fields table set session time zone conf conn returns query selecting records oracle db protected string get select query string builder query new string builder db configuration db conf get db conf string conditions get conditions string table name get table name string field names get field names oracle specific codepath use rownum instead limit offset db conf get input query null query append select field names length query append from append table name conditions null conditions length string order by db conf get input order by order by null order by length else prebuilt query query append db conf get input query try db input format db input split split get split split get length split get start catch io exception ex ignore throw return query string set session time zone we read oracle session time zone property set session time zone configuration conf connection conn throws sql exception need use reflection call method set session time zone oracle connection oracle specific java libraries accessible context method method try method conn get class get method catch exception ex log error could find method set session time zone conn get class get name ex rethrow sql exception throw new sql exception ex need set time zone order java correctly access column timestamp with local time zone we easily get correct oracle specific timezone java let user set timezone property string client time zone conf get session timezone key gmt try method set accessible true method invoke conn client time zone log info time zone set client time zone catch exception ex log warn time zone client time zone log warn setting default time zone gmt try catch exception ex
1507	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\TextSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter text strings text splitter extends big decimal splitter log log log factory get log text splitter this method needs determine splits two user provided strings in case user strings a z hard could create two splits a m m z splits strings beginning letter etc if user provided us strings ham haze however need create splits differ third letter the algorithm used follows since unicode characters interpret characters digits base given containing characters n interpret number n base having mapped low high strings floating point values use big decimal splitter establish even split points map resulting floating point values back strings list input split split configuration conf result set results string col name list string split num splits string min string string max string string common prefix big decimal one place new big decimal maximum number characters convert this prevent rounding errors repeating fractions near bottom getting control note still gives us huge number possible splits max chars return big decimal representation str suitable use numerically sorting order big decimal to big decimal string str return encoded big decimal repeatedly multiply input value integer portion multiplication represents single character base convert back char create data left string big decimal to string big decimal bd
1508	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionHelper.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements mapper reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the reducer extracts output key value pairs similar manner except key never ignored field selection helper text empty text new text string data field seperator string map output key value spec string reduce output key value spec extract fields string field list spec string select fields string fields list integer field list parse output key value spec string key value spec string spec to string string field separator string key value spec text key null text value null field selection helper field selection helper text key text val text get key text get value extract output key value string key string val
1509	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionMapper.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements mapper used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values field selection mapper k v string map output key value spec boolean ignore input key string field separator list integer map output key field list new array list integer list integer map output value field list new array list integer map value fields from log log log factory get log field selection map reduce setup context context map k key v val context context
1510	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionReducer.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form reduce output keys list fields form reduce output values the fields union key value the field separator attribute mapreduce fieldsel data field separator the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values field selection reducer k v string field separator string reduce output key value spec list integer reduce output key field list new array list integer list integer reduce output value field list new array list integer reduce value fields from log log log factory get log field selection map reduce setup context context reduce text key iterable text values context context
1511	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileInputFormat.java	pooling	package org apache hadoop mapreduce lib input an link input format returns link combine file split link input format get splits job context method splits constructed files input paths a split cannot files different pools each split returned may contain blocks different files if max split size specified blocks node combined form single split blocks left combined blocks rack if max split size specified blocks rack combined single split attempt made create node local splits if max split size equal block size similar default splitting behavior hadoop block locally processed split subclasses implement link input format create record reader input split task attempt context construct code record reader code code combine file split code combine file input format k v extends file input format k v string split minsize pernode string split minsize perrack ability limit size single split max split size min split size node min split size rack a pool input paths filters a split cannot blocks files across multiple pools array list multi path filter pools new array list multi path filter mapping rack name set nodes rack hash map string set string rack to nodes protected set max split size max split size protected set min split size node min split size node protected set min split size rack min split size rack protected create pool list path filter filters protected create pool path filter filters protected boolean splitable job context context path file combine file input format list input split get splits job context job get more splits job context job path paths add created split list input split split list record reader k v create record reader input split split one file info one block info protected block location get file block locations add host to rack hash map string set string rack to nodes set string get hosts set string racks multi path filter implements path filter
1512	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileRecordReader.java	unrelated	package org apache hadoop mapreduce lib input a generic record reader hand different record readers chunk link combine file split a combine file split combine data chunks multiple files this allows using different record readers processing data chunks different files combine file record reader k v extends record reader k v class constructor signature new class protected combine file split split protected class extends record reader k v rr class protected constructor extends record reader k v rr constructor protected file system fs protected task attempt context context protected idx protected progress protected record reader k v cur reader initialize input split split boolean next key value throws io exception interrupted exception k get current key throws io exception interrupted exception v get current value throws io exception interrupted exception close throws io exception get progress throws io exception interrupted exception combine file record reader combine file split split protected boolean init next record reader throws io exception
1513	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileSplit.java	unrelated	package org apache hadoop mapreduce lib input a sub collection input files unlike link file split combine file split represent split file split input files smaller sets a split may contain blocks different file blocks split probably local rack br combine file split used implement link record reader reading one record per file combine file split extends input split implements writable path paths startoffset lengths string locations tot length default constructor combine file split combine file split path files start combine file split path files lengths init split path files start copy constructor combine file split combine file split old throws io exception get length returns array containing start offsets files split get start offsets returns array containing lengths files split get lengths returns start offset sup th sup path get offset returns length sup th sup path get length returns number paths split get num paths returns sup th sup path path get path returns paths split path get paths returns paths input split resides string get locations throws io exception read fields data input throws io exception write data output throws io exception string string
1514	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format delegates behavior paths multiple input formats delegating input format k v extends input format k v list input split get splits job context job throws io exception interrupted exception configuration conf job get configuration job job copy new job conf list input split splits new array list input split map path input format format map multiple inputs get input format map job map path class extends mapper mapper map multiple inputs get mapper type map job map class extends input format list path format paths new hash map class extends input format list path first build map input formats paths entry path input format entry format map entry set format paths contains key entry get value get class format paths put entry get value get class new linked list path format paths get entry get value get class add entry get key entry class extends input format list path format entry format paths entry set class extends input format format class format entry get key input format format input format reflection utils new instance format class conf list path paths format entry get value map class extends mapper list path mapper paths new hash map class extends mapper list path now set paths common input format build map mappers paths used path path paths class extends mapper mapper class mapper map get path mapper paths contains key mapper class mapper paths put mapper class new linked list path mapper paths get mapper class add path now set paths common input format mapper added job split together entry class extends mapper list path map entry mapper paths entry set paths map entry get value class extends mapper mapper class map entry get key mapper class null try mapper class job get mapper class catch class not found exception e throw new io exception mapper found e file input format set input paths job copy paths array new path paths size get splits input path tag input format mapper types wrapping tagged input split list input split path splits format get splits job copy input split path split path splits splits add new tagged input split path split conf format get class mapper class return splits record reader k v create record reader input split split task attempt context context throws io exception interrupted exception return new delegating record reader k v split context
1515	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingMapper.java	unrelated	package org apache hadoop mapreduce lib input an link mapper delegates behavior paths multiple mappers delegating mapper k v k v extends mapper k v k v mapper k v k v mapper protected setup context context run context context
1516	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this delegating record reader delegates functionality underlying record reader link tagged input split delegating record reader k v extends record reader k v record reader k v original rr constructs delegating record reader delegating record reader input split split task attempt context context close throws io exception k get current key throws io exception interrupted exception v get current value throws io exception interrupted exception get progress throws io exception interrupted exception initialize input split split task attempt context context boolean next key value throws io exception interrupted exception
1517	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileInputFormat.java	unrelated	package org apache hadoop mapreduce lib input a base file based link input format p code file input format code base file based code input format code this provides generic implementation link get splits job context subclasses code file input format code also link splitable job context path method ensure input files split processed whole link mapper file input format k v extends input format k v string input dir string split maxsize string split minsize string pathfilter class string num input files log log log factory get log file input format split slop slop path filter hidden file filter new path filter multi path filter implements path filter protected get format min split size protected boolean splitable job context context path filename set input path filter job job set min input split size job job get min split size job context job set max input split size job job get max split size job context context path filter get input path filter job context context list input directories protected list file status list status job context job protected file split make split path file start length list input split get splits job context job throws io exception protected compute split size block size min size protected get block index block location blk locations set input paths job job add input paths job job set input paths job job add input path job job this method escapes commas glob pattern given paths string get path strings string comma separated paths path get input paths job context context
1518	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileInputFormatCounter.java	unrelated	package org apache hadoop mapreduce lib input counters used task enum file input format counter bytes read
1519	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileSplit.java	unrelated	package org apache hadoop mapreduce lib input a section input file returned link input format get splits job context passed link input format create record reader input split task attempt context file split extends input split implements writable path file start length string hosts file split constructs split host information file split path file start length string hosts the file containing split data path get path return file the position first byte file process get start return start the number bytes file process get length return length string string return file start length writable methods write data output throws io exception read fields data input throws io exception string get locations throws io exception
1520	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\InvalidInputException.java	unrelated	package org apache hadoop mapreduce lib input this wraps list problems input user get list problems together instead finding fixing one one invalid input exception extends io exception serial version uid l list io exception problems invalid input exception list io exception probs list io exception get problems string get message
1521	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\KeyValueLineRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this treats line input key value pair separated separator character the separator specified config file attribute name mapreduce input keyvaluelinerecordreader key value separator the default separator tab character key value line record reader extends record reader text text string key value seperator line record reader line record reader byte separator byte text inner value text key text value key value line record reader configuration conf initialize input split generic split find separator byte utf start length set key value text key text value byte line read key value pair line synchronized boolean next key value text get current key text get current value get progress throws io exception synchronized close throws io exception
1522	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\KeyValueTextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format plain text files files broken lines either line feed carriage return used signal end line each line divided key value parts separator byte if byte exists key entire line value empty key value text input format extends file input format text text protected boolean splitable job context context path file record reader text text create record reader input split generic split
1523	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\LineRecordReader.java	pooling	package org apache hadoop mapreduce lib input treats keys offset file value line line record reader extends record reader long writable text log log log factory get log line record reader string max line length compression codec factory compression codecs null start pos end line reader fs data input stream file in seekable file position max line length long writable key null text value null compression codec codec decompressor decompressor byte record delimiter bytes line record reader line record reader byte record delimiter initialize input split generic split boolean compressed input max bytes to consume pos get file position throws io exception boolean next key value throws io exception long writable get current key text get current value get progress within split get progress throws io exception synchronized close throws io exception
1524	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\MultipleInputs.java	unrelated	package org apache hadoop mapreduce lib input this supports map reduce jobs multiple input paths different link input format link mapper path multiple inputs string dir formats string dir mappers add input path job job path path add input path job job path path map path input format get input format map job context job map path class extends mapper
1525	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\NLineInputFormat.java	unrelated	package org apache hadoop mapreduce lib input n line input format splits n lines input one split in many pleasantly parallel applications process mapper processes input file computations controlled different parameters referred parameter sweeps one way achieve specify set parameters one set per line input control file input path map reduce application input dataset specified via config variable job conf the n line input format used applications splits input file default one line fed value one map task key offset e k v long writable text the location hints span whole mapred cluster n line input format extends file input format long writable text string lines per map record reader long writable text create record reader logically splits set input files job splits n lines input one split list input split get splits job context job throws io exception list file split get splits for file file status status set number lines per split set num lines per split job job num lines get number lines per split get num lines per split job context job
1526	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsBinaryInputFormat.java	unrelated	package org apache hadoop mapreduce lib input input format reading keys values sequence files binary raw format sequence file as binary input format extends sequence file input format bytes writable bytes writable sequence file as binary input format super record reader bytes writable bytes writable create record reader return new sequence file as binary record reader read records sequence file binary raw bytes sequence file as binary record reader sequence file reader start end boolean done false data output buffer buffer new data output buffer sequence file value bytes vbytes bytes writable key null bytes writable value null initialize input split split task attempt context context bytes writable get current key bytes writable get current value retrieve name key sequence file string get key class name retrieve name value sequence file string get value class name read raw bytes sequence file synchronized boolean next key value close throws io exception return progress within input split get progress throws io exception interrupted exception
1527	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsTextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input this similar sequence file input format except generates sequence file as text record reader converts input keys values string forms calling string method sequence file as text input format extends sequence file input format text text sequence file as text input format record reader text text create record reader input split split
1528	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsTextRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this converts input keys values string forms calling string method this sequence file as text input format line record reader text input format sequence file as text record reader extends record reader text text sequence file record reader writable comparable writable text key text value sequence file as text record reader initialize input split split task attempt context context text get current key text get current value read key value pair line synchronized boolean next key value get progress throws io exception interrupted exception synchronized close throws io exception
1529	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileInputFilter.java	unrelated	package org apache hadoop mapreduce lib input a allows map red job work sample sequence files the sample decided filter set job sequence file input filter k v log log log factory get log file input format string filter class string filter frequency string filter regex sequence file input filter create record reader given split record reader k v create record reader input split split set filter set filter class job job class filter class filter extends configurable filter base implements filter records filter matching key regex regex filter extends filter base this returns percentage records percent filter extends filter base this returns set records examing md digest md filter extends filter base filter record reader k v
1530	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format link sequence file sequence file input format k v extends file input format k v record reader k v create record reader input split split protected get format min split size protected list file status list status job context job
1531	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileRecordReader.java	unrelated	package org apache hadoop mapreduce lib input an link record reader link sequence file sequence file record reader k v extends record reader k v sequence file reader start end boolean true k key null v value null protected configuration conf initialize input split split boolean next key value throws io exception interrupted exception k get current key v get current value return progress within input split get progress throws io exception synchronized close throws io exception close
1532	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\TaggedInputSplit.java	unrelated	package org apache hadoop mapreduce lib input an link input split tags another input split extra data use link delegating input format link delegating mapper tagged input split extends input split implements configurable writable class extends input split input split class input split input split class extends input format input format class class extends mapper mapper class configuration conf tagged input split default constructor creates new tagged input split tagged input split input split input split configuration conf input split class input split get class input split input split conf conf input format class input format class mapper class mapper class retrieves original input split input split get input split return input split retrieves input format use split class extends input format get input format class return input format class retrieves mapper use split class extends mapper get mapper class return mapper class get length throws io exception interrupted exception return input split get length string get locations throws io exception interrupted exception return input split get locations read fields data input throws io exception input split class class extends input split read class input format class class extends input format read class mapper class class extends mapper read class input split input split reflection utils serialization factory factory new serialization factory conf deserializer deserializer factory get deserializer input split class deserializer open data input stream input split input split deserializer deserialize input split class read class data input throws io exception string name text read string try catch class not found exception e write data output throws io exception text write string input split class get name text write string input format class get name text write string mapper class get name serialization factory factory new serialization factory conf serializer serializer serializer open data output stream serializer serialize input split configuration get conf return conf set conf configuration conf conf conf
1533	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\TextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format plain text files files broken lines either linefeed carriage return used signal end line keys position file values line text text input format extends file input format long writable text record reader long writable text protected boolean splitable job context context path file
1534	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\jobcontrol\ControlledJob.java	unrelated	package org apache hadoop mapreduce lib jobcontrol this encapsulates map reduce job dependency it monitors states depending jobs updates state job a job starts waiting state if depending jobs depending jobs success state job state become ready if depending jobs fail job fail when ready state job submitted hadoop execution state changing running state from running state job get success failed state depending status job execution controlled job a job one following states enum state success waiting running ready failed string create dir mapreduce jobcontrol createdir ifnotexist state state string control id assigned used job control job job mapreduce job executed info human consumption e g reason job failed string message jobs current job depends list controlled job depending jobs controlled job job job list controlled job depending jobs controlled job configuration conf throws io exception string string string get job name set job name string job name string get job id set job id string id job id get mapred job id synchronized job get job synchronized set job job job synchronized state get job state protected synchronized set job state state state synchronized string get message synchronized set message string message list controlled job get dependent jobs synchronized boolean add depending job controlled job depending job synchronized boolean completed synchronized boolean ready kill job throws io exception interrupted exception check running state throws io exception interrupted exception protected synchronized submit
1535	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\jobcontrol\JobControl.java	unrelated	package org apache hadoop mapreduce lib jobcontrol this encapsulates set map reduce jobs dependency it tracks states jobs placing different tables according states this provides ap is client app add job group get jobs group different states when job added id unique group assigned job this thread submits jobs become ready monitors states running jobs updates states jobs based state changes depending jobs states the provides ap is suspending resuming thread stopping thread job control implements runnable the thread one following state enum thread state running suspended stopped stopping ready map string controlled job waiting jobs map string controlled job ready jobs map string controlled job running jobs map string controlled job successful jobs map string controlled job failed jobs next job id string group name job control string group name list controlled job list list controlled job get waiting job list list controlled job get running job list list controlled job get ready jobs list list controlled job get successful job list list controlled job get failed job list string get next job id add to queue controlled job job add to queue controlled job job map string controlled job get queue state state synchronized string add job controlled job job add job collection collection controlled job jobs thread state get thread state stop suspend resume synchronized check running jobs synchronized check waiting jobs synchronized start ready jobs synchronized boolean finished run
1536	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop mapreduce lib join this provides implementation resetable iterator the implementation uses link java util array list store elements added replaying requested prefer link stream backed iterator array list backed iterator x extends writable iterator x iter array list x data x hold null configuration conf new configuration array list backed iterator array list backed iterator array list x data boolean next boolean next x val throws io exception boolean replay x val throws io exception reset add x item throws io exception close throws io exception clear
1537	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ComposableInputFormat.java	unrelated	package org apache hadoop mapreduce lib join refinement input format requiring implementors provide composable record reader instead record reader composable input format k extends writable comparable composable record reader k v create record reader
1538	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ComposableRecordReader.java	unrelated	package org apache hadoop mapreduce lib join additional operations required record reader participate join composable record reader k extends writable comparable extends record reader k v implements comparable composable record reader k return position collector occupies id return key record reader would supply call next k v k key clone key head record reader object provided key k key throws io exception create instance key k create key create instance value v create value returns true stream empty provides guarantee call next k v succeed boolean next skip key value pairs keys less equal key provided skip k key throws io exception interrupted exception while key value pairs record reader match given key register join collector provided accept composite record reader join collector jc k key
1539	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeInputFormat.java	unrelated	package org apache hadoop mapreduce lib join an input format capable performing joins set data sources sorted partitioned way a user may define new join types setting property tt mapreduce join define lt ident gt tt classname in expression tt mapreduce join expr tt identifier assumed composable record reader tt mapreduce join keycomparator tt classname used compare keys join composite input format k extends writable comparable extends input format k tuple writable string join expr mapreduce join expr string join comparator mapreduce join keycomparator expression parse tree if requests proxied parser node root composite input format interpret given composite expression code func ident func func func tbl path see java lang class name java lang string path see org apache hadoop fs path path java lang string reads expression tt mapreduce join expr tt property user supplied join types tt mapreduce join define lt ident gt tt types paths supplied tt tbl tt given input paths input format listed set format configuration conf throws io exception add defaults add user identifiers conf root parser parse conf get join expr null conf adds default set identifiers parser protected add defaults try catch no such method exception e inform parser user defined types add user identifiers configuration conf throws io exception pattern x pattern compile mapreduce join define w map entry string string kv conf build composite input split child input formats assigning ith split child ith composite split list input split get splits job context job set format job get configuration job get configuration set long mapreduce input fileinputformat split minsize long max value return root get splits job construct composite record reader children input format defined init expression the outermost join need composable necessarily composite mandating tuple writable strictly correct record reader k tuple writable create record reader input split split set format task context get configuration return root create record reader split task context convenience method constructing composite formats given input format inf path p return code tbl inf p string compose class extends input format inf return compose inf get name intern path convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op string infname inf get name string buffer ret new string buffer op string p path ret set char at ret length return ret string convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op array list string tmp new array list string path length path p path return compose op inf tmp array new string string buffer compose string inf string path sb append tbl inf sb append path sb append return sb
1540	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeInputSplit.java	unrelated	package org apache hadoop mapreduce lib join this input split contains set child input splits any input split inserted collection must default constructor composite input split extends input split implements writable fill totsize l input split splits configuration conf new configuration composite input split composite input split capacity add input split collection capacity reached add input split throws io exception interrupted exception get ith child input split input split get return aggregate length child input splits currently added get length throws io exception get length ith child input split get length throws io exception interrupted exception collect set hosts child input splits string get locations throws io exception interrupted exception get locations ith input split string get location throws io exception interrupted exception write splits following format code count classn split split splitn write data output throws io exception inherit doc failing access checks read fields data input throws io exception
1541	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeRecordReader.java	scheduler	package org apache hadoop mapreduce lib join a record reader effect joins record readers sharing common key type partitioning composite record reader k extends writable comparable key type v extends writable accepts record reader k v children x extends writable emits writables type extends composable record reader k x implements configurable id protected configuration conf resetable iterator x empty new resetable iterator empty x writable comparator cmp protected class extends writable comparable keyclass null priority queue composable record reader k q protected join collector jc protected composable record reader k extends v kids protected boolean combine object srcs tuple writable value protected k key protected x value create record reader tt capacity tt children position tt id tt parent reader the id root composite record reader convention relying recommended composite record reader id capacity assert capacity invalid capacity id id null cmpcl jc new join collector capacity kids new composable record reader capacity initialize input split split task attempt context context kids null return position collector occupies id return id inherit doc set conf configuration conf conf conf inherit doc configuration get conf return conf return sorted list record readers composite protected priority queue composable record reader k get record reader queue return q return comparator defining ordering record readers composite protected writable comparator get comparator return cmp add record reader collection the id record reader determines tuple entry appear adding record readers id undefined behavior add composable record reader k extends v rr kids rr id rr collector join values this accumulates values given key child record readers if one child rr contain duplicate keys emit cross product associated values exhausted join collector k key resetable iterator x iters pos boolean first true construct collector capable handling specified number children join collector card register given iterator position id add id resetable iterator x return key associated collection k key codify contents collector iterated when called record readers registered key added resetable iterators reset k key clear state information clear returns false exhausted reset k called boolean next populate tuple iterators it case given iterators n values sources n sharing key k repeated calls next yield i x i protected boolean next tuple writable val throws io exception replay last tuple emitted boolean replay tuple writable val throws io exception close child iterators close throws io exception write next value key value accepted operation associated set record readers boolean flush tuple writable value throws io exception return key current join value top record reader heap k key jc next q empty return null clone key top rr given object key k key throws io exception reflection utils copy conf key key k get current key return key return true possible could emit values boolean next return jc next q empty pass skip key child r rs skip k key throws io exception interrupted exception array list composable record reader k tmp q empty cmp compare q peek key key composable record reader k rr tmp obtain iterator child r rs apropos value type ultimately emitted join protected resetable iterator x get delegate
1542	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\InnerJoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join full inner join inner join record reader k extends writable comparable inner join record reader id configuration conf capacity return true iff tuple full data sources contain key protected boolean combine object srcs tuple writable dst
1543	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\JoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join base composite joins returning tuples arbitrary writables join record reader k extends writable comparable join record reader id configuration conf capacity boolean next key value tuple writable create value protected resetable iterator tuple writable get delegate protected join delegation iterator
1544	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\MultiFilterRecordReader.java	scheduler	package org apache hadoop mapreduce lib join base composite join returning values derived multiple sources generally tuples multi filter record reader k extends writable comparable tuple writable ivalue null multi filter record reader id configuration conf capacity protected v emit tuple writable dst throws io exception protected boolean combine object srcs tuple writable dst inherit doc boolean next key value throws io exception interrupted exception initialize input split split task attempt context context protected resetable iterator v get delegate protected multi filter delegation iterator
1545	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\OuterJoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join full outer join outer join record reader k extends writable comparable outer join record reader id configuration conf capacity emit everything collector protected boolean combine object srcs tuple writable dst
1546	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\OverrideRecordReader.java	unrelated	package org apache hadoop mapreduce lib join prefer quot rightmost quot data source key for example tt s s s tt prefer values s s values s s keys emitted sources override record reader k extends writable comparable extends multi filter record reader k v override record reader id configuration conf capacity super id conf capacity cmpcl class extends writable valueclass null emit value highest position tuple protected v emit tuple writable dst return v dst iterator next v create value null valueclass valueclass equals null writable return v reflection utils new instance valueclass null instead filling join collector iterators data sources fill rightmost key this saves space discarding sources also emits number key value pairs preferred record reader instead repeating stream n times n cardinality cross product discarded streams given key protected fill join collector k iterkey priority queue composable record reader k q q null q empty
1547	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\Parser.java	unrelated	package org apache hadoop mapreduce lib join very simple shift reduce parser join expressions this sufficient user extension permitted ought replaced parser generator complex grammars supported in particular quot shift reduce quot parser states each set formals requires different internal node type responsible interpreting list tokens receives this sufficient current grammar several annoying properties might inhibit extension in particular parenthesis always function calls algebraic filter grammar would require node type must also work around internals parser for cases adding hierarchy particularly extending join record reader multi filter record reader fairly straightforward one need relevant method usually link composite record reader combine property map value identifier parser parser enum t type cif ident comma lparen rparen quot num tagged union type tokens join expression token t type type token t type type t type get type return type node get node throws io exception get num throws io exception string get str throws io exception num token extends token num num token num get num return num node token extends token node node node token node node node get node str token extends token string str str token t type type string str string get str simple lexer wrapping stream tokenizer this encapsulates creation tagged union tokens initializes steam tokenizer lexer stream tokenizer tok lexer string token next throws io exception node extends composable input format return node type registered particular identifier by default c node composite node w node quot wrapped quot nodes user nodes likely composite nodes node ident string ident throws io exception class ncstr sig string protected map string constructor extends for given identifier add mapping nodetype parse tree composable record reader created including formals required invoke constructor the nodetype constructor signature filled child node protected add identifier string ident class mcstr sig inst protected id protected string ident protected class extends writable comparator cmpcl protected node string ident protected set id id protected set key comparator parse list token args configuration conf nodetype parse tree quot wrapped quot input formats w node extends node class cstr sig add identifier string ident string indir input format inf w node string ident let first actual define input format second define tt mapred input dir tt property parse list token configuration conf throws io exception configuration get conf configuration jconf throws io exception list input split get splits job context context composable record reader create record reader input split split string string wrapped status reporter extends status reporter task attempt context context wrapped status reporter task attempt context context counter get counter enum name counter get counter string group string name progress get progress set status string status internal nodetype quot composite quot input formats c node extends node class cstr sig add identifier string ident inst array list node kids new array list node c node string ident set key comparator class extends writable comparator cmpcl combine input splits child input formats link composite input split list input split get splits job context job composable record reader parse list comma separated nodes parse list token args configuration conf string string
1548	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ResetableIterator.java	unrelated	package org apache hadoop mapreduce lib join this defines stateful iterator replay elements added directly note extend link java util iterator resetable iterator t extends writable empty u extends writable implements resetable iterator u boolean next return false reset close throws io exception clear boolean next u val throws io exception boolean replay u val throws io exception add u item throws io exception true call next may return value this permitted false positives false negatives boolean next assign next value actual it required elements added resetable iterator returned order call link reset fifo note call may fail nested joins e elements available none satisfying constraints join boolean next t val throws io exception assign last value returned actual boolean replay t val throws io exception set iterator return start range must called calling link add avoid concurrent modification exception reset add element collection elements iterate add t item throws io exception close datasources release resources calling methods iterator calling close undefined behavior xxx necessary close throws io exception close datasources release internal resources calling method permit object reused different datasource clear
1549	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\StreamBackedIterator.java	unrelated	package org apache hadoop mapreduce lib join this provides implementation resetable iterator this implementation uses byte array store elements added stream backed iterator x extends writable replayable byte input stream extends byte array input stream byte array output stream outbuf new byte array output stream data output stream outfbuf new data output stream outbuf replayable byte input stream inbuf data input stream infbuf stream backed iterator boolean next boolean next x val throws io exception boolean replay x val throws io exception reset add x item throws io exception close throws io exception clear
1550	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\TupleWritable.java	unrelated	package org apache hadoop mapreduce lib join writable type storing multiple link org apache hadoop io writable this general purpose tuple type in almost cases users encouraged implement serializable types perform better validation provide efficient encodings capable tuple writable relies join framework type safety assumes instances rarely persisted assumptions incompatible contrary general case tuple writable implements writable iterable writable protected bit set written writable values tuple writable tuple writable writable vals boolean writable get size boolean equals object hash code iterator writable iterator string string writable writes writable code code write data output throws io exception read fields data input throws io exception set written clear written clear written write bit set data output stream nbits read bit set data input stream nbits
1551	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\WrappedRecordReader.java	unrelated	package org apache hadoop mapreduce lib join proxy record reader participating join framework this keeps track quot head quot key value pair provided record reader keeps store values matching key source participating join wrapped record reader k extends writable comparable protected boolean empty false record reader k u rr id index values inserted collector protected writable comparator cmp null k key key top rr u value value assoc key resetable iterator u vjoin configuration conf new configuration class extends writable comparable keyclass null class extends writable valueclass null protected wrapped record reader id wrapped record reader id record reader k u rr throws io exception interrupted exception initialize input split split throws io exception interrupted exception k create key u create value inherit doc id k key key k qkey throws io exception boolean next skip k key throws io exception interrupted exception accept composite record reader join collector k key boolean next key value throws io exception interrupted exception boolean next throws io exception interrupted exception k get current key throws io exception interrupted exception u get current value throws io exception interrupted exception get progress throws io exception interrupted exception close throws io exception compare to composable record reader k boolean equals object hash code
1552	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\InverseMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper swaps keys values inverse mapper k v extends mapper k v v k the inverse function input keys values swapped map k key v value context context
1553	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\MultithreadedMapper.java	pooling	package org apache hadoop mapreduce lib map multithreaded implementation link org apache hadoop mapreduce mapper p it used instead default implementation bound order improve throughput p mapper implementations using map runnable must thread safe p the map reduce job configured mapper use via link set mapper class configuration class number thread thread pool use link get number of threads configuration method the default value threads p multithreaded mapper k v k v extends mapper k v k v log log log factory get log multithreaded mapper string num threads mapreduce mapper multithreadedmapper threads string map class mapreduce mapper multithreadedmapper mapclass class extends mapper k v k v map class context outer list map runner runners the number threads thread pool run map function get number of threads job context job return job get configuration get int num threads set number threads pool running maps set number of threads job job threads job get configuration set int num threads threads get application mapper k v k v class mapper k v k v get mapper class job context job return class mapper k v k v set application mapper k v k v set mapper class job job multithreaded mapper assignable from cls job get configuration set class map class cls mapper run application maps using thread pool run context context throws io exception interrupted exception outer context number of threads get number of threads context map class get mapper class context log debug enabled runners new array list map runner number of threads number of threads number of threads sub map record reader extends record reader k v k key v value configuration conf close throws io exception get progress throws io exception interrupted exception initialize input split split boolean next key value throws io exception interrupted exception k get current key v get current value sub map record writer extends record writer k v close task attempt context context throws io exception write k key v value throws io exception sub map status reporter extends status reporter counter get counter enum name counter get counter string group string name progress set status string status get progress map runner extends thread mapper k v k v mapper context subcontext throwable throwable record reader k v reader new sub map record reader map runner context context throws io exception interrupted exception run
1554	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\RegexMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper extracts text matching regular expression regex mapper k extends mapper k text text long writable string pattern mapreduce mapper regex string group mapreduce mapper regexmapper group pattern pattern group setup context context map k key text value
1555	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\TokenCounterMapper.java	unrelated	package org apache hadoop mapreduce lib map tokenize input values emit word count token counter mapper extends mapper object text text int writable int writable one new int writable text word new text map object key text value context context
1556	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\WrappedMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper wraps given one allow custom link mapper context implementations wrapped mapper keyin valuein keyout valueout extends mapper keyin valuein keyout valueout get wrapped link mapper context custom implementations mapper keyin valuein keyout valueout context get map context map context keyin valuein keyout valueout map context return new context map context context protected map context keyin valuein keyout valueout map context context map context keyin valuein keyout valueout map context get input split map input split get input split keyin get current key throws io exception interrupted exception valuein get current value throws io exception interrupted exception boolean next key value throws io exception interrupted exception counter get counter enum counter name counter get counter string group name string counter name output committer get output committer write keyout key valueout value throws io exception string get status task attempt id get task attempt id set status string msg path get archive class paths string get archive timestamps uri get cache archives throws io exception uri get cache files throws io exception class extends reducer get combiner class configuration get configuration path get file class paths string get file timestamps raw comparator get grouping comparator class extends input format get input format class string get jar job id get job id string get job name boolean get job setup cleanup needed boolean get task cleanup needed path get local cache archives throws io exception path get local cache files throws io exception class get map output key class class get map output value class class extends mapper get mapper class get max map attempts get max reduce attempts get num reduce tasks class extends output format get output format class class get output key class class get output value class class extends partitioner get partitioner class class extends reducer get reducer class raw comparator get sort comparator boolean get symlink path get working directory throws io exception progress boolean get profile enabled string get profile params integer ranges get profile task range boolean map string get user credentials get credentials get progress
1557	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputCommitter.java	unrelated	package org apache hadoop mapreduce lib output an link output committer commits files specified job output directory e mapreduce output fileoutputformat outputdir file output committer extends output committer log log log factory get log file output committer temporary directory name protected string temp dir name temporary string succeeded file name success string successful job output dir marker file system output file system null path output path null path work path null create file output committer file output committer path output path create temporary directory root task work directories setup job job context context throws io exception true job requires output dir marked successful job note default set true boolean mark output dir configuration conf create success file job output dir mark output dir successful mr job config context throws io exception delete temporary directory including work directories create success file make successful commit job job context context throws io exception cleanup job job context context throws io exception delete temporary directory including work directories abort job job context context job status state state throws io exception no task setup required setup task task attempt context context throws io exception move files work directory job output directory commit task task attempt context context throws io exception move files work directory output move task outputs task attempt context context throws io exception delete work directory abort task task attempt context context throws io exception find name given output file given job output directory work directory path get final path path job output dir path task output did task write files work directory boolean needs task commit task attempt context context get directory task write results path get work path throws io exception
1558	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output a base link output format read link file system file output format k v extends output format k v construct output file names output directory listing sorted lexicographically positions correspond output partitions number format number format number format get instance protected string base output name mapreduce output basename protected string part part number format set minimum integer digits number format set grouping used false file output committer committer null string compress mapreduce output fileoutputformat compress string compress codec mapreduce output fileoutputformat compress codec string compress type mapreduce output fileoutputformat compress type string outdir mapreduce output fileoutputformat outputdir set whether output job compressed set compress output job job boolean compress job get configuration set boolean file output format compress compress is job output compressed code false code otherwise boolean get compress output job context job return job get configuration get boolean set link compression codec used compress job outputs compress job outputs set output compressor class job job set compress output job true job get configuration set class file output format compress codec get link compression codec compressing job outputs job outputs class extends compression codec get output compressor class job context job class extends compression codec codec class default value configuration conf job get configuration string name conf get file output format compress codec name null return codec class record writer k v check output specs job context job ensure output directory set already path dir get output path job dir null get delegation token dir file system token cache obtain tokens for namenodes job get credentials dir get file system job get configuration exists dir set link path output directory map reduce job map reduce job set output path job job path output dir throws io exception output dir output dir get file system job get configuration make qualified job get configuration set file output format outdir output dir string get link path output directory map reduce job path get output path job context job string name job get configuration get file output format outdir return name null null new path name get link path task temporary output directory map reduce job id side effect files tasks side effect files p some applications need create write side files differ actual job outputs p in cases could issues instances tip running simultaneously e g speculative tasks trying open write file path hdfs hence application writer pick unique names per task attempt e g using attemptid say tt attempt tt per tip p p to get around map reduce framework helps application writer maintaining special tt mapreduce output fileoutputformat outputdir temporary taskid tt sub directory task attempt hdfs output task attempt goes on successful completion task attempt files tt mapreduce output fileoutputformat outputdir temporary taskid tt promoted tt mapreduce output fileoutputformat outputdir tt of course framework discards sub directory unsuccessful task attempts this completely transparent application p p the application writer take advantage creating side files required work directory execution task e via link get work output path task input output context framework move similarly thus
1559	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputFormatCounter.java	unrelated	package org apache hadoop mapreduce lib output counters used task enum file output format counter bytes written
1560	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FilterOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output filter output format convenience wraps output format filter output format k v extends output format k v protected output format k v base out filter output format filter output format output format k v base out record writer k v get record writer task attempt context context throws io exception interrupted exception check output specs job context context throws io exception interrupted exception output committer get output committer task attempt context context throws io exception interrupted exception output format k v get base out throws io exception filter record writer k v extends record writer k v
1561	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\LazyOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output a convenience creates output lazily lazy output format k v extends filter output format k v string output format set output format class job job get base output format configuration conf throws io exception record writer k v get record writer task attempt context context throws io exception interrupted exception check output specs job context context throws io exception interrupted exception output committer get output committer task attempt context context throws io exception interrupted exception lazy record writer k v extends filter record writer k v
1562	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\MapFileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link org apache hadoop mapreduce output format writes link map file map file output format record writer writable comparable writable get record writer open output generated format map file reader get readers path dir get entry output generated k extends writable comparable v extends writable
1563	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\MultipleOutputs.java	unrelated	package org apache hadoop mapreduce lib output the multiple outputs simplifies writing output data multiple outputs p case one writing additional outputs job default output each additional output named output may configured code output format code key value p case two write data different files provided user p p multiple outputs supports counters default disabled the counters group link multiple outputs name the names counters output name these count number records written output name p usage pattern job submission pre job job new job file input format set input path job dir file output format set output path job dir job set mapper class mo map job set reducer class mo reduce defines additional single text based output text job multiple outputs add named output job text text output format long writable text defines additional sequence file based output sequence job multiple outputs add named output job seq sequence file output format long writable text job wait for completion true pre p usage reducer pre k v string generate file name k k v v return k string v string mo reduce extends reducer lt writable comparable writable writable comparable writable gt multiple outputs mos setup context context mos new multiple outputs context reduce writable comparable key iterator lt writable gt values context context throws io exception mos write text key new text hello mos write seq long writable new text bye seq mos write seq long writable key new text chau seq b mos write key new text value generate file name key new text value cleanup context throws io exception mos close pre multiple outputs keyout valueout string multiple outputs mapreduce multipleoutputs string mo prefix string format format string key key string value value string counters enabled string counters group multiple outputs get name map string task attempt context task contexts new hash map string task attempt context check token name string named output check base output path string output path check named output name job context job returns list channel names list string get named outputs list job context job returns named output output format class extends output format get named output format class returns key named output class get named output key class job context job returns value named output class get named output value class add named output job job string named output set counters enabled job job boolean enabled boolean get counters enabled job context job record writer with counter extends record writer instance code used mapper reducer code task input output context keyout valueout context set string named outputs map string record writer record writers boolean counters enabled multiple outputs k v write string named output k key v value k v write string named output k key v value write keyout key valueout value string base output path synchronized multiple output task use multithreaded mapper synchronized record writer get record writer task attempt context get context string name output throws io exception wrapped status reporter extends status reporter close throws io exception interrupted exception
1564	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\NullOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output consume outputs put dev null null output format k v extends output format k v record writer k v check output specs job context context output committer get output committer task attempt context context
1565	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\SequenceFileAsBinaryOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link org apache hadoop mapreduce output format writes keys values link sequence file binary raw format sequence file as binary output format extends sequence file output format bytes writable bytes writable string key class mapreduce output seqbinaryoutputformat key string value class mapreduce output seqbinaryoutputformat value inner used append raw writable value bytes implements value bytes bytes writable value writable value bytes writable value bytes bytes writable value reset bytes writable value write uncompressed bytes data output stream stream write compressed bytes data output stream stream get size set key link sequence file p this allows user specify key different actual link bytes writable used writing p set sequence file output key class job job job get configuration set class key class set value link sequence file p this allows user specify value different actual link bytes writable used writing p set sequence file output value class job job job get configuration set class value class get key link sequence file class extends writable comparable return job get configuration get class key class get value link sequence file class extends writable get sequence file output value class return job get configuration get class value class record writer bytes writable bytes writable get record writer sequence file writer get sequence writer context return new record writer bytes writable bytes writable check output specs job context job throws io exception super check output specs job get compress output job
1566	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\SequenceFileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link output format writes link sequence file sequence file output format k v extends file output format k v protected sequence file writer get sequence writer task attempt context context record writer k v get link compression type output link sequence file defaulting link compression type record compression type get output compression type job context job set link compression type output link sequence file link sequence file set output compression type job job
1567	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\TextOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link output format writes plain text files text output format k v extends file output format k v string seperator mapreduce output textoutputformat separator protected line record writer k v extends record writer k v string utf utf byte newline protected data output stream byte key value separator line record writer data output stream string key value separator line record writer data output stream write object byte stream handling text special case write object object throws io exception synchronized write k key v value synchronized close task attempt context context throws io exception record writer k v configuration conf job get configuration boolean compressed get compress output job string key value separator conf get seperator compression codec codec null string extension compressed path file get default work file job extension file system fs file get file system conf compressed else
1568	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\BinaryPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition p partition link binary comparable keys using configurable part bytes array returned link binary comparable get bytes p p the subarray used partitioning defined means following properties ul li mapreduce partition binarypartitioner left offset left offset array default li li mapreduce partition binarypartitioner right offset right offset array default li ul like python negative positive offsets allowed meaning slightly different in case array length instance possible offsets pre code b b b b b code pre the first row numbers gives position offsets array second row gives corresponding negative offsets contrary python specified subarray byte code code code j code first last element repectively code code code j code left right offset p for hadoop programs written java advisable use one following convenience methods setting offsets ul li link set offsets li li link set left offset li li link set right offset li ul p binary partitioner v extends partitioner binary comparable v implements configurable string left offset property name string right offset property name set subarray used partitioning code bytes left right code python syntax set offsets configuration conf left right set subarray used partitioning code bytes offset code python syntax set left offset configuration conf offset set subarray used partitioning code bytes offset code python syntax set right offset configuration conf offset configuration conf left offset right offset set conf configuration conf configuration get conf use specified slice array returned link binary comparable get bytes partition get partition binary comparable key v value num partitions
1569	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\HashPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition partition keys link object hash code hash partitioner k v extends partitioner k v use link object hash code partition get partition k key v value
1570	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\InputSampler.java	unrelated	package org apache hadoop mapreduce lib partition utility collecting samples writing partition file link total order partitioner input sampler k v extends configured implements tool log log log factory get log input sampler print usage system println sampler r reduces n system println default sampler split random tool runner print generic command usage system return input sampler configuration conf set conf conf interface sample using link org apache hadoop mapreduce input format sampler k v for given job collect return subset keys input data k get sample input format k v inf job job throws io exception interrupted exception samples first n records splits inexpensive way sample random data split sampler k v implements sampler k v num samples max splits sampled create split sampler sampling em em splits takes first num samples num splits records split splits split sampler num samples create new split sampler splits split sampler num samples max splits sampled from split sampled take first num samples num splits records k get sample input format k v inf job job sample random points input general purpose sampler takes num samples max splits sampled inputs split random sampler k v implements sampler k v freq num samples max splits sampled create new random sampler sampling em em splits this read every split client expensive splits random sampler freq num samples create new random sampler splits random sampler freq num samples max splits sampled randomize split order take specified number keys split sampled key selected specified probability possibly replaced subsequently selected key quota keys split satisfied k get sample input format k v inf job job sample splits regular intervals useful sorted data interval sampler k v implements sampler k v freq max splits sampled create new interval sampler sampling em em splits interval sampler freq create new interval sampler interval sampler freq max splits sampled for split sampled emit ratio number records retained total record count less specified frequency k get sample input format k v inf job job write partition file given job using sampler provided queries sampler sample keyset sorts output key comparator selects keys rank writes destination returned link total order partitioner get partition file k v write partition file job job sampler k v sampler configuration conf job get configuration input format inf num partitions job get num reduce tasks k samples sampler get sample inf job log info using samples length samples raw comparator k comparator arrays sort samples comparator path dst new path total order partitioner get partition file conf file system fs dst get file system conf fs exists dst sequence file writer writer sequence file create writer fs null writable null value null writable get step size samples length num partitions last num partitions writer close driver input sampler command line configures job conf instance calls link write partition file run string args throws exception job job new job get conf array list string args new array list string sampler k v sampler null args length job get num reduce tasks args size null sampler path outf new path args remove
1571	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldBasedComparator.java	unrelated	package org apache hadoop mapreduce lib partition this comparator implementation provides subset features provided unix gnu sort in particular supported features n sort numerically r reverse result comparison k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options nr described we assume fields key separated link job context map output key field seperator key field based comparator k v extends writable comparator key field helper key field helper new key field helper string comparator options mapreduce partition keycomparator options byte negative byte byte zero byte byte decimal byte configuration conf set conf configuration conf configuration get conf key field based comparator compare byte b compare byte sequence byte first start end numerical compare byte start end boolean isdigit byte b decimal compare byte end decimal compare byte end one negative compare byte start end boolean zero byte start end set link key field based comparator options used compare keys pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options the supported options n sort numerically r reverse result comparison set key field comparator options job job string key spec get link key field based comparator options string get key field comparator option job context job
1572	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldBasedPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition defines way partition keys based certain key fields also see link key field based comparator the key specification supported form k pos pos pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field key field based partitioner k v extends partitioner k v log log log factory get log string partitioner options num of partition fields key field helper key field helper new key field helper configuration conf set conf configuration conf configuration get conf get partition k key v value num reduce tasks protected hash code byte b start end current hash protected get partition hash num reduce tasks set link key field based partitioner options used link partitioner pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field set key field partitioner options job job string key spec get link key field based partitioner options string get key field partitioner option job context job
1573	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldHelper.java	unrelated	package org apache hadoop mapreduce lib partition this used link key field based comparator link key field based partitioner defines methods parsing key specifications the key specification form k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options supported options nr key field helper protected key description begin field idx begin char end field idx end char boolean numeric boolean reverse string string list key description key specs new array list key description byte key field separator boolean key spec seen false set key field separator string key field separator try catch unsupported encoding exception e required backcompatibility num key fields partition link key field based partitioner set key field spec start end end start list key description key specs return key specs get word lengths byte b start end given like hello returns array like first element number fields key spec seen lengths new curr len lengths lengths length idx pos pos utf byte array utils find bytes b start end start end lengths idx number words first element return lengths get start offset byte b start end k keyspec start char length indices note th element number fields key length indices k begin field idx return get end offset byte b start end k keyspec end char length indices note th element number fields key k end field idx length indices k end field idx return end parse option string option option null option equals string tokenizer args new string tokenizer option key description global new key description args more tokens key description key key specs key specs size key description parse key string arg string tokenizer args allow k arg k arg string key args null arg length else key args null key args length string tokenizer st new string tokenizer key args nr true key description key new key description string token key form nr nr st more tokens st more tokens return key print key key description key system println key begin field idx key begin field idx system println key begin char key begin char system println key end field idx key end field idx system println key end char key end char system println key numeric key numeric system println key reverse key reverse system println parse key
1574	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\TotalOrderPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition partitioner effecting total order reading split points externally generated source total order partitioner k extends writable comparable v extends partitioner k v implements configurable node partitions string default path partition lst string partitioner path mapreduce totalorderpartitioner path string max trie depth mapreduce totalorderpartitioner trie maxdepth string natural order mapreduce totalorderpartitioner naturalorder configuration conf total order partitioner read partition file build indexing data structures if keytype link org apache hadoop io binary comparable tt total order partitioner natural order tt false trie first tt total order partitioner max trie depth tt bytes built otherwise keys located using binary search partition keyset using link org apache hadoop io raw comparator defined job the input file must sorted comparator contain link job get num reduce tasks keys set conf configuration conf try catch io exception e configuration get conf return conf construction know keytype get partition k key v value num partitions return partitions find partition key set path sequence file storing sorted partition keyset it must case tt r tt reduces tt r tt keys sequence file set partition file configuration conf path p conf set partitioner path p string get path sequence file storing sorted partition keyset string get partition file configuration conf return conf get partitioner path default path interface partitioner locate key partition keyset node t locate partition keyset k st ki ki defines partition implicit k inf kn inf k partitions find partition t key base trie nodes if keytype memcomp able builds tries first tt total order partitioner max trie depth tt bytes trie node implements node binary comparable level trie node level get level for types link org apache hadoop io binary comparable disabled tt total order partitioner natural order tt search partition keyset binary search binary search node implements node k k split points raw comparator k comparator binary search node k split points raw comparator k comparator find partition k key an inner trie node contains children based next character inner trie node extends trie node trie node child new trie node inner trie node level find partition binary comparable key split point points leaf node contain find parttion returns canned index one split point compare single comparand one binary search the last case rare trie node leaf trie node factory a leaf trie node scans key lower upper we generate many since usually continue trie ing one split point remains level make different objects nodes split point leaf trie node extends trie node lower upper binary comparable split points leaf trie node level binary comparable split points lower upper find partition binary comparable key unsplit trie node extends trie node singly split trie node extends trie node read cut points given i file k read partitions file system fs path p class k key class sequence file reader reader new sequence file reader fs p conf array list k parts new array list k k key reflection utils new instance key class conf null writable value null writable get reader next key value reader close return parts array
1575	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\IntSumReducer.java	unrelated	package org apache hadoop mapreduce lib reduce int sum reducer key extends reducer key int writable int writable result new int writable reduce key key iterable int writable values
1576	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\LongSumReducer.java	unrelated	package org apache hadoop mapreduce lib reduce long sum reducer key extends reducer key long writable long writable result new long writable reduce key key iterable long writable values
1577	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\WrappedReducer.java	unrelated	package org apache hadoop mapreduce lib reduce a link reducer wraps given one allow custom link reducer context implementations wrapped reducer keyin valuein keyout valueout extends reducer keyin valuein keyout valueout a wrapped link reducer context custom implementations reducer keyin valuein keyout valueout context get reducer context reduce context keyin valuein keyout valueout reduce context return new context reduce context context protected reduce context keyin valuein keyout valueout reduce context context reduce context keyin valuein keyout valueout reduce context keyin get current key throws io exception interrupted exception valuein get current value throws io exception interrupted exception boolean next key value throws io exception interrupted exception counter get counter enum counter name counter get counter string group name string counter name output committer get output committer write keyout key valueout value throws io exception string get status task attempt id get task attempt id set status string msg path get archive class paths string get archive timestamps uri get cache archives throws io exception uri get cache files throws io exception class extends reducer get combiner class configuration get configuration path get file class paths string get file timestamps raw comparator get grouping comparator class extends input format get input format class string get jar job id get job id string get job name boolean get job setup cleanup needed boolean get task cleanup needed path get local cache archives throws io exception path get local cache files throws io exception class get map output key class class get map output value class class extends mapper get mapper class get max map attempts get max reduce attempts get num reduce tasks class extends output format get output format class class get output key class class get output value class class extends partitioner get partitioner class class extends reducer get reducer class raw comparator get sort comparator boolean get symlink path get working directory throws io exception progress iterable valuein get values throws io exception boolean next key throws io exception interrupted exception boolean get profile enabled string get profile params integer ranges get profile task range boolean map string get user credentials get credentials get progress
1578	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\ClientProtocol.java	scheduler	package org apache hadoop mapreduce protocol protocol job client central job tracker use communicate the job client use methods submit job execution learn current system status server principal jt config jt user name client protocol extends versioned protocol changing version id l since get task completion events method changed changed since kill task string boolean added version added jobtracker state cluster status version max tasks cluster status replaced max map tasks max reduce tasks hadoop version change counters representation hadoop version added get all jobs hadoop version change job task id use corresponding objects rather strings version change counter representation hadoop version added get system dir hadoop version changed job profile queue name hadoop version added get cleanup task reports cleanup progress job status part hadoop version added get job queue infos get job queue info queue name get all jobs queue part hadoop version added set priority hadoop version added killed status job status part hadoop version added get setup task reports setup progress job status part hadoop version get cluster status returns amount memory used server hadoop version added blacklisted trackers cluster status hadoop version modified task report tip status modified method get cluster status take boolean argument hadoop version modified cluster status tasktracker expiry interval hadoop version modified task id aware new task types version added method get queue acls for current user get queue acls info user version modified job queue info inlucde queue state part hadoop version modified cluster status black list info encapsulates reasons report blacklisted node version added fields job status hadoop version added properties job queue info part mapreduce added new api get root queues get child queues string queue name version changed protocol use new api objects and protocol renamed job submission protocol client protocol version added get job history dir part mapreduce version added reserved slots running tasks total job submissions cluster metrics part mapreduce version job submission files uploaded staging area user home dir job tracker reads required files staging area using user credentials passed via rpc version added token storage submit job version added delegation tokens add renew cancel version added job ac ls job status part mapreduce version modified submit job use credentials instead token storage version added method get queue admins queue name part mapreduce version added method get job tracker status part mapreduce version id l allocate name job job id get new job id throws io exception interrupted exception submit job execution returns latest profile job job status submit job job id job id string job submit dir credentials ts get current status cluster cluster metrics get cluster metrics throws io exception interrupted exception get job tracker state state get job tracker state throws io exception interrupted exception get job tracker status job tracker status get job tracker status throws io exception interrupted exception get task tracker expiry interval throws io exception get administrators given job queue this method hadoop internal use submitted access control list get queue admins string queue name throws io exception kill indicated job kill job job id jobid throws io exception
1579	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\ClientProtocolProvider.java	unrelated	package org apache hadoop mapreduce protocol client protocol provider client protocol create configuration conf throws io exception client protocol create inet socket address addr close client protocol client protocol throws io exception
1580	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\package-info.java	unrelated	package org apache hadoop mapreduce protocol
1581	mapreduce\src\java\org\apache\hadoop\mapreduce\security\SecureShuffleUtils.java	unrelated	package org apache hadoop mapreduce security utilities generating kyes hashes verifying shuffle secure shuffle utils string http header url hash url hash string http header reply url hash reply hash base encoded hash msg string generate hash byte msg secret key key calculate hash msg byte generate byte hash byte msg secret key key verify hash equals h mac hash msg boolean verify hash byte hash byte msg secret key key aux util calculate hash string string hash from string string enc str secret key key throws io exception verify base hash h mac hash msg verify reply string base hash string msg secret key key throws io exception shuffle specific utils build encoding url string build msg from url url shuffle specific utils build encoding url string build msg from http servlet request request shuffle specific utils build encoding url string build msg from string uri path string uri query port byte array hex string string hex byte ba
1582	mapreduce\src\java\org\apache\hadoop\mapreduce\security\TokenCache.java	unrelated	package org apache hadoop mapreduce security this provides user facing ap is transferring secrets job client tasks the secrets stored submission jobs read task execution token cache log log log factory get log token cache byte get secret key credentials credentials text alias obtain tokens for namenodes credentials credentials obtain tokens for namenodes internal credentials credentials obtain tokens for namenodes internal file system fs string job token hdfs file job token string job tokens filename mapreduce job job token file text job token new text shuffle and job token token delegation token identifier get delegation token credentials load tokens string job token file job conf conf throws io exception set job token token extends token identifier token job token identifier get job token credentials credentials
1583	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\DelegationTokenRenewal.java	unrelated	package org apache hadoop mapreduce security token delegation token renewal log log log factory get log delegation token renewal string scheme hdfs used keeping tracks dt renew delegation token to renew token delegation token identifier token job id job id configuration conf expiration date timer task timer task delegation token to renew set timer task timer task task string string boolean equals object obj hash code global single timer daemon timer renewal timer new timer true delegation token canceler thread delegation token cancel thread dt cancel thread new delegation token cancel thread dt cancel thread start managing list tokens using map job id list tokens set delegation token to renew delegation tokens collections synchronized set new hash set delegation token to renew delegation token cancel thread extends thread token with conf linked blocking queue token with conf queue delegation token cancel thread cancel token token delegation token identifier token run adding token add token to list delegation token to renew delegation tokens add kind tokens currently renew text kind hdfs delegation token identifier hdfs delegation kind synchronized register delegation tokens for renewal ts null collection token extends token identifier tokens ts get all tokens system current time millis token extends token identifier tokens string get http address for token throws io exception string ipaddr token get service string split inet address iaddr inet address get by name ipaddr string dns name iaddr get canonical host name case different cluster may different port string https port conf get dfs hftp https port https port null always use https security return https dns name https port protected renew delegation token over https throws interrupted exception io exception string http address get http address for token token conf chaged debug log info address renew http address tok token get service long exp date long user group information get login user as log info renew http done addr http address res exp date return exp date renew delegation token delegation token to renew dttr throws exception new expiration date system current time millis token delegation token identifier token dttr token configuration conf dttr conf token get kind equals kind hdfs else return new expiration date task renew token renewal timer task extends timer task delegation token to renew dttr renewal timer task delegation token to renew dttr run distributed file system get dfs for token throws exception distributed file system dfs null try catch exception e return dfs find soonest expiring token set renew set timer for token renewal calculate timer time system current time millis renew in first time else try catch exception e removing tokens renewals close renewal timer cancel delegation tokens clear protected cancel delegation token over https throws interrupted exception io exception string http address get http address for token token conf chaged debug log info address cancel http address tok token get service user group information get login user as log info cancel http done addr http address cancel token cancel token delegation token to renew token delegation token identifier token token configuration conf conf token get kind equals kind hdfs
1584	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenIdentifier.java	unrelated	package org apache hadoop mapreduce security token the token identifier job token job token identifier extends token identifier text jobid text kind name new text mapreduce job default constructor job token identifier create job token identifier jobid job token identifier text jobid inherit doc text get kind inherit doc user group information get user get jobid text get job id inherit doc read fields data input throws io exception inherit doc write data output throws io exception
1585	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenSecretManager.java	unrelated	package org apache hadoop mapreduce security token secret manager job token it used cache generated job tokens job token secret manager extends secret manager job token identifier secret key master key map string secret key current job tokens convert byte secret key secret key create secret key byte key compute hmac hash message using key byte compute hash byte msg secret key key default constructor job token secret manager create new password secret given job token identifier byte create password job token identifier identifier add job token job cache add token for job string job id token job token identifier token remove cached job token job cache remove token for job string job id look token password secret given job id secret key retrieve token secret string job id throws invalid token look token password secret given job token identifier byte retrieve password job token identifier identifier create empty job token identifier job token identifier create identifier
1586	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenSelector.java	unrelated	package org apache hadoop mapreduce security token look tokens find first job token matches service return job token selector implements token selector job token identifier token job token identifier select token text service
1587	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\package-info.java	unrelated	package org apache hadoop mapreduce security token
1588	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenIdentifier.java	unrelated	package org apache hadoop mapreduce security token delegation a delegation token identifier specific map reduce delegation token identifier text mapreduce delegation kind create empty delegation token identifier reading delegation token identifier create new delegation token identifier delegation token identifier text owner text renewer text real user text get kind
1589	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenSecretManager.java	unrelated	package org apache hadoop mapreduce security token delegation a map reduce specific delegation token secret manager the secret manager responsible generating accepting password token delegation token secret manager create secret manager secret keys tokens expired tokens delegation token secret manager delegation key update interval delegation token identifier create identifier
1590	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenSelector.java	unrelated	package org apache hadoop mapreduce security token delegation a delegation token specialized map reduce delegation token selector delegation token selector
1591	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\package-info.java	unrelated	package org apache hadoop mapreduce security token delegation
1592	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\JobTrackerJspHelper.java	unrelated	package org apache hadoop mapreduce server jobtracker methods help format output job tracker xml jspx job tracker jsp helper job tracker jsp helper decimal format percent format returns xml formatted table jobs list this called repeatedly different lists jobs e g running completed failed generate job table jsp writer string label list job in progress jobs generates xml formatted block summarizes state job tracker generate summary table jsp writer
1593	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\JTConfig.java	heartbeat	package org apache hadoop mapreduce server jobtracker place holder job tracker server level configuration the keys mapreduce jobtracker prefix jt config extends mr config job tracker configuration parameters string jt ipc address mapreduce jobtracker address string jt http address string jt ipc handler count string jt restart enabled string jt task scheduler string jt instrumentation string jt tasks per job string jt heartbeats in second string jt heartbeats scaling factor string jt heartbeat interval min jt heartbeat interval min default string jt persist jobstatus string jt persist jobstatus hours string jt persist jobstatus dir string jt supergroup string jt retirejobs string jt retirejob cache size string jt taskcache levels string jt task alloc pad fraction string jt jobinit threads string jt tracker expiry interval string jt runningtasks per job string jt hosts filename string jt hosts exclude filename string jt jobhistory cache size string jt jobhistory block size string jt jobhistory completed location string jt jobhistory location string jt avg blacklist threshold string jt system dir mapreduce jobtracker system dir string jt staging area root string jt max tracker blacklists string jt jobhistory maxage string jt max mapmemory mb string jt max reducememory mb string jt max job split metainfo size mapreduce jobtracker split metainfo maxsize string jt user name mapreduce jobtracker kerberos principal string jt keytab file string private actions key string jt plugins string shuffle exception stack regex string shuffle exception msg regex
1594	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\State.java	unrelated	package org apache hadoop mapreduce server jobtracker describes state job tracker enum state initializing running
1595	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\TaskTracker.java	unrelated	package org apache hadoop mapreduce server jobtracker the representation single code task tracker code seen link job tracker task tracker log log log factory get log task tracker string tracker name task tracker status status job in progress job for fallow map slot job in progress job for fallow reduce slot create new link task tracker task tracker string tracker name get unique identifier link task tracker string get tracker name get current link task tracker status code task tracker code code task tracker code task tracker status get status set current link task tracker status code task tracker code code task tracker code set status task tracker status status get number currently available slots tasktracker given type task code task type code get available slots task type task type get link job in progress fallow slot held code null code fallow slots job in progress get job for fallow slot task type task type reserve specified number slots given code job code reserved reserve slots task type task type job in progress job num slots free map slots code task tracker code reserved code task type code unreserve slots task type task type job in progress job cleanup link task tracker declared lost blacklisted job tracker the method assumes lock link job tracker obtained caller cancel all reservations
1596	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\Localizer.java	unrelated	package org apache hadoop mapreduce server tasktracker localizer log log log factory get log localizer file system fs string local dirs task controller task controller create localizer instance localizer file system file sys string dirs task controller tc data structure synchronizing localization user directories map string atomic boolean localized users initialize local directories particular user tt this involves creation setting permissions following directories ul li mapreduce cluster local dir task tracker user li li mapreduce cluster local dir task tracker user jobcache li li mapreduce cluster local dir task tracker user distcache li ul initialize user dirs string user prepare job directories given job to called job localization code job already localized br here set permissions job directories created disks this avoid misuse users till time link task controller initialize job job initialization context run later time set proper permissions job directories br initialize job dirs string user job id job id create task dirs disks otherwise cases like linux task controller use child might wish balance load across disks cannot create attempt directory fact job directory writable tt initialize attempt dirs string user string job id create job log directory set appropriate permissions directory initialize job log dir job id job id throws io exception
1597	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\package-info.java	unrelated	package org apache hadoop mapreduce server tasktracker
1598	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\TTConfig.java	heartbeat	package org apache hadoop mapreduce server tasktracker place holder task tracker server level configuration the keys mapreduce tasktracker prefix tt config extends mr config task tracker configuration properties string tt health checker interval string tt health checker script args string tt health checker script path string tt health checker script timeout string tt local dir minspace kill string tt local dir minspace start string tt http address string tt report address string tt task controller string tt contention tracking string tt static resolutions string tt http threads string tt host name mapreduce tasktracker host name string tt sleep time before sig kill string tt dns interface string tt dns nameserver string tt max task completion events to poll string tt index cache string tt instrumentation string tt map slots string tt memory calculator plugin string tt resource calculator plugin string tt reduce slots string tt memory manager monitoring interval string tt local cache size string tt local cache subdirs limit string tt outofband hearbeat string tt reserved physcialmemory mb string tt user name mapreduce tasktracker kerberos principal string tt keytab file string tt group string tt userlogcleanup sleeptime string tt distributed cache check period percentage local distributed cache kept garbage collection string tt local cache keep around pct
1599	mapreduce\src\java\org\apache\hadoop\mapreduce\split\JobSplit.java	unrelated	package org apache hadoop mapreduce split this groups fundamental associated reading writing splits the split information divided two parts based consumer information the two parts split meta information raw split information the first part consumed job tracker create tasks locality data structures the second part used maps runtime know these pieces information written two separate files the metainformation file slurped job tracker job initialization a map task gets meta information launch reads raw split bytes directly file job split meta split version byte meta split file header task split meta info empty task split split meta info implements writable task split meta info task split index
1600	mapreduce\src\java\org\apache\hadoop\mapreduce\split\JobSplitWriter.java	unrelated	package org apache hadoop mapreduce split the used job clients write splits meta raw bytes parts job split writer split version job split meta split version byte split file header t extends input split create split files path job submit dir throws io exception interrupted exception t extends input split create split files path job submit dir throws io exception interrupted exception create split files path job submit dir throws io exception fs data output stream create file file system fs path split file write split header fs data output stream throws io exception t extends input split split meta info write new splits configuration conf throws io exception interrupted exception split meta info write old splits write job split meta info file system fs path filename throws io exception
1601	mapreduce\src\java\org\apache\hadoop\mapreduce\split\package-info.java	unrelated	package org apache hadoop mapreduce split
1602	mapreduce\src\java\org\apache\hadoop\mapreduce\split\SplitMetaInfoReader.java	unrelated	package org apache hadoop mapreduce split a utility reads split meta info creates split meta info objects split meta info reader job split task split meta info read split meta info throws io exception
1603	mapreduce\src\java\org\apache\hadoop\mapreduce\task\JobContextImpl.java	unrelated	package org apache hadoop mapreduce task a read view job provided tasks running job context impl implements job context protected org apache hadoop mapred job conf conf job id job id the user group information object reference current user protected user group information ugi protected credentials credentials job context impl configuration conf job id job id conf new org apache hadoop mapred job conf conf job id job id credentials conf get credentials try catch io exception e return configuration job configuration get configuration return conf get unique id job job id get job id return job id set job id set job id job id job id job id job id get configured number reduce tasks job defaults code code get num reduce tasks return conf get num reduce tasks get current working directory default file system path get working directory throws io exception return conf get working directory get key job output data class get output key class return conf get output key class get value job outputs class get output value class return conf get output value class get key map output data if set use output key this allows map output key different output key class get map output key class return conf get map output key class get value map output data if set use output value this allows map output value different output value class get map output value class return conf get map output value class get user specified job name this used identify job user string get job name return conf get job name get link input format job class extends input format get input format class return class extends input format get link mapper job class extends mapper get mapper class return class extends mapper get combiner job class extends reducer get combiner class return class extends reducer get link reducer job class extends reducer get reducer class return class extends reducer get link output format job class extends output format get output format class return class extends output format get link partitioner job class extends partitioner get partitioner class return class extends partitioner get link raw comparator comparator used compare keys raw comparator get sort comparator return conf get output key comparator get pathname job jar string get jar return conf get jar get user defined link raw comparator comparator grouping keys inputs reduce raw comparator get grouping comparator return conf get output value grouping comparator get whether job setup job cleanup needed job boolean get job setup cleanup needed return conf get boolean mr job config setup cleanup needed true get whether task cleanup needed job boolean get task cleanup needed return conf get boolean mr job config task cleanup needed true this method checks see symlinks create localized cache files current working directory boolean get symlink return distributed cache get symlink conf get archive entries classpath array path path get archive class paths return distributed cache get archive class paths conf get cache archives set configuration uri get cache archives throws io exception return distributed cache get cache archives conf
1604	mapreduce\src\java\org\apache\hadoop\mapreduce\task\MapContextImpl.java	unrelated	package org apache hadoop mapreduce task the context given link mapper map context impl keyin valuein keyout valueout record reader keyin valuein reader input split split map context impl configuration conf task attempt id taskid get input split map input split get input split keyin get current key throws io exception interrupted exception valuein get current value throws io exception interrupted exception boolean next key value throws io exception interrupted exception
1605	mapreduce\src\java\org\apache\hadoop\mapreduce\task\package-info.java	unrelated	package org apache hadoop mapreduce task
1606	mapreduce\src\java\org\apache\hadoop\mapreduce\task\ReduceContextImpl.java	unrelated	package org apache hadoop mapreduce task the context passed link reducer reduce context impl keyin valuein keyout valueout raw key value iterator input counter input value counter counter input key counter raw comparator keyin comparator keyin key current key valuein value current value boolean first value false first value key boolean next key is same false w key boolean more file protected progressable reporter deserializer keyin key deserializer deserializer valuein value deserializer data input buffer buffer new data input buffer bytes writable current raw key new bytes writable value iterable iterable new value iterable boolean marked false backup store keyin valuein backup store serialization factory serialization factory class keyin key class class valuein value class configuration conf task attempt id taskid current key length current value length reduce context impl configuration conf task attempt id taskid start processing next unique key boolean next key throws io exception interrupted exception boolean next key value throws io exception interrupted exception keyin get current key valuein get current value protected value iterator implements reduce context value iterator valuein protected value iterable implements iterable valuein iterable valuein get values throws io exception interrupted exception
1607	mapreduce\src\java\org\apache\hadoop\mapreduce\task\TaskAttemptContextImpl.java	unrelated	package org apache hadoop mapreduce task the context task attempts task attempt context impl extends job context impl implements task attempt context task attempt id task id string status status reporter reporter task attempt context impl configuration conf conf task id new dummy reporter task attempt context impl configuration conf super conf task id get job id task id task id reporter reporter get unique name task attempt task attempt id get task attempt id return task id get last set status message string get status return status counter get counter enum counter name return reporter get counter counter name counter get counter string group name string counter name return reporter get counter group name counter name report progress progress reporter progress protected set status string string status status status set current status task given set status string status set status string status reporter set status status dummy reporter extends status reporter set status string progress counter get counter enum name counter get counter string group string name get progress get progress return reporter get progress
1608	mapreduce\src\java\org\apache\hadoop\mapreduce\task\TaskInputOutputContextImpl.java	unrelated	package org apache hadoop mapreduce task a context object allows input output task it supplied link mapper link reducer task input output context impl keyin valuein keyout valueout record writer keyout valueout output output committer committer task input output context impl configuration conf task attempt id taskid advance next key value pair returning null end boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception generate output key value pair write keyout key valueout value output committer get output committer
1609	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\EventFetcher.java	scheduler	package org apache hadoop mapreduce task reduce event fetcher k v extends thread sleep time max events to fetch max retries retry period log log log factory get log event fetcher task attempt id reduce task umbilical protocol umbilical shuffle scheduler k v scheduler event id exception reporter exception reporter null max map runtime event fetcher task attempt id reduce run queries link task tracker set map completion events given event id get map completion events throws io exception uri get base uri string url
1610	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ExceptionReporter.java	unrelated	package org apache hadoop mapreduce task reduce an reporting exceptions threads exception reporter report exception throwable
1611	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\Fetcher.java	pooling	package org apache hadoop mapreduce task reduce fetcher k v extends thread log log log factory get log fetcher number ms timing copy default stalled copy timeout basic unit connection timeout milliseconds unit connect timeout default read timeout milliseconds default read timeout progressable reporter enum shuffle errors io error wrong length bad id wrong map string shuffle err grp name shuffle errors counters counter connection errs counters counter io errs counters counter wrong length errs counters counter bad id errs counters counter wrong map errs counters counter wrong reduce errs merge manager k v merger shuffle scheduler k v scheduler shuffle client metrics metrics exception reporter exception reporter id next id reduce connection timeout read timeout decompression map outputs compression codec codec decompressor decompressor secret key job token secret fetcher job conf job task attempt id reduce id run the crux matter shuffle available map outputs copy from host map host host throws io exception boolean copy map output map host host do basic verification input received being defensive boolean verify sanity compressed length decompressed length create map output url this contain map ids separated commas url get map output url map host host list task attempt id maps the connection establishment attempted multiple times given last failure instead connecting timeout x try connecting timeout x x multiple times connect url connection connection connection timeout throws io exception shuffle to memory map host host map output k v map output shuffle to disk map host host map output k v map output throws io exception
1612	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\InMemoryReader.java	unrelated	package org apache hadoop mapreduce task reduce code i file in memory reader code read map outputs present memory in memory reader k v extends reader k v task attempt id task attempt id merge manager k v merger data input buffer mem data in new data input buffer start length in memory reader merge manager k v merger task attempt id task attempt id throws io exception reset offset get position throws io exception get length dump on error boolean next raw key data input buffer key throws io exception next raw value data input buffer value throws io exception close
1613	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\InMemoryWriter.java	unrelated	package org apache hadoop mapreduce task reduce in memory writer k v extends writer k v data output stream in memory writer bounded byte array output stream array stream append k key v value throws io exception append data input buffer key data input buffer value throws io exception close throws io exception
1614	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MapHost.java	unrelated	package org apache hadoop mapreduce task reduce map host enum state state state state idle string host name string base url list task attempt id maps new array list task attempt id map host string host name string base url state get state string get host name string get base url synchronized add known map task attempt id map id synchronized list task attempt id get and clear known maps synchronized mark busy synchronized mark penalized synchronized get num known map outputs called node done penalty done copying synchronized state mark available string string mark host penalized synchronized penalize
1615	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MapOutput.java	unrelated	package org apache hadoop mapreduce task reduce map output k v log log log factory get log map output atomic integer id new atomic integer enum type wait memory disk id merge manager k v merger task attempt id map id size byte memory bounded byte array output stream byte stream file system local fs path tmp output path path output path output stream disk type type boolean primary map output map output task attempt id map id merge manager k v merger size id id increment and get map id map id merger merger type type disk memory null byte stream null size size local fs file system get local conf string filename map map id get task id get id string tmp output path separator tmp output path output path new path tmp output path get parent filename disk local fs create tmp output path primary map output primary map output map output task attempt id map id merge manager k v merger size id id increment and get map id map id merger merger type type memory byte stream new bounded byte array output stream size memory byte stream get buffer size size local fs null disk null output path null tmp output path null primary map output primary map output map output task attempt id map id id id increment and get map id map id type type wait merger null memory null byte stream null size local fs null disk null output path null tmp output path null primary map output false boolean primary map output return primary map output boolean equals object obj obj instanceof map output return false hash code return id path get output path return output path byte get memory return memory bounded byte array output stream get array stream return byte stream output stream get disk return disk task attempt id get map id return map id type get type return type get size return size commit throws io exception type type memory else type type disk else abort type type memory else type type disk else string string return map output map id type map output comparator k v implements comparator map output k v compare map output k v map output k v
1616	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MergeManager.java	unrelated	package org apache hadoop mapreduce task reduce merge manager k v log log log factory get log merge manager maximum percentage memory limit single shuffle consume max single shuffle segment fraction f task attempt id reduce id job conf job conf file system local fs file system rfs local dir allocator local dir allocator protected map output file map output file set map output k v memory merged map outputs new tree set map output k v new map output comparator k v intermediate memory to memory merger mem to mem merger set map output k v memory map outputs new tree set map output k v new map output comparator k v in memory merger memory merger set path disk map outputs new tree set path on disk merger disk merger memory limit used memory max single shuffle limit mem to mem merge outputs threshold merge threshold io sort factor reporter reporter exception reporter exception reporter combiner run memory merge defined class extends reducer combiner class resettable collector used combine combine output collector k v combine collector counters counter spilled records counter counters counter reduce combine input counter counters counter merged map outputs counter compression codec codec progress merge phase merge manager task attempt id reduce id job conf job conf reduce id reduce id job conf job conf local dir allocator local dir allocator exception reporter exception reporter reporter reporter codec codec combiner class combiner class combine collector combine collector reduce combine input counter reduce combine input counter spilled records counter spilled records counter merged map outputs counter merged map outputs counter map output file new map output file map output file set conf job conf local fs local fs rfs local file system local fs get raw max in mem copy use max in mem copy use max in mem copy use allow unit tests fix runtime memory memory limit io sort factor job conf get int mr job config io sort factor max single shuffle limit mem to mem merge outputs threshold merge threshold memory limit log info merger manager memory limit memory limit boolean allow mem to mem merge allow mem to mem merge else memory merger new in memory merger memory merger start disk merger new on disk merger disk merger start merge phase merge phase task attempt id get reduce id return reduce id wait for in memory merge throws interrupted exception memory merger wait for merge boolean shuffle to memory requested size return requested size max single shuffle limit map output k v stall shuffle new map output k v null synchronized map output k v reserve task attempt id map id shuffle to memory requested size stall shuffle memory limit it possible threads could stalling make progress this could happen requested size causing used memory go limit requested size single shuffle limit current used size merge threshold merge get triggered to avoid happening allow exactly one thread go past memory limit we check used memory memory limit used memory requested size memory limit when thread done fetching automatically trigger merge thereby unlocking stalled
1617	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MergeThread.java	unrelated	package org apache hadoop mapreduce task reduce merge thread t k v extends thread log log log factory get log merge thread volatile boolean progress false list t inputs new array list t protected merge manager k v manager exception reporter reporter boolean closed false merge factor merge thread merge manager k v manager merge factor synchronized close throws interrupted exception synchronized boolean in progress synchronized start merge set t inputs synchronized wait for merge throws interrupted exception run merge list t inputs throws io exception
1618	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\package-info.java	unrelated	package org apache hadoop mapreduce task reduce
1619	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\Shuffle.java	scheduler	package org apache hadoop mapreduce task reduce shuffle k v implements exception reporter log log log factory get log shuffle progress frequency task attempt id reduce id job conf job conf reporter reporter shuffle client metrics metrics task umbilical protocol umbilical shuffle scheduler k v scheduler merge manager k v merger throwable throwable null string throwing thread name null progress copy phase task status task status task reduce task used status updates shuffle task attempt id reduce id job conf job conf file system local fs reduce id reduce id job conf job conf umbilical umbilical reporter reporter metrics new shuffle client metrics reduce id job conf copy phase copy phase task status status reduce task reduce task scheduler merger new merge manager k v reduce id job conf local fs raw key value iterator run throws io exception interrupted exception start map completion events fetcher thread event fetcher k v event fetcher event fetcher start start map output fetcher threads num fetchers job conf get int mr job config shuffle parallel copies fetcher k v fetchers new fetcher num fetchers num fetchers wait shuffle complete successfully scheduler wait until done progress frequency stop event fetcher thread event fetcher interrupt try catch throwable stop map output fetcher threads fetcher k v fetcher fetchers fetcher k v fetcher fetchers fetchers null stop scheduler scheduler close copy phase complete copy already complete task status set phase task status phase sort reduce task status update umbilical finish going merges raw key value iterator kv iter null try catch throwable e sanity check synchronized return kv iter synchronized report exception throwable throwable null shuffle error extends io exception serial version uid l shuffle error string msg throwable
1620	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleClientMetrics.java	unrelated	package org apache hadoop mapreduce task reduce shuffle client metrics implements updater metrics record shuffle metrics null num failed fetches num success fetches num bytes num threads busy num copiers shuffle client metrics task attempt id reduce id job conf job conf synchronized input bytes num bytes synchronized failed fetch synchronized success fetch synchronized thread busy synchronized thread free updates metrics context unused
1621	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleHeader.java	unrelated	package org apache hadoop mapreduce task reduce shuffle header information sent task tracker deciphered fetcher thread reduce task shuffle header implements writable the longest possible length task attempt id accept max id length string map id uncompressed length compressed length reduce shuffle header shuffle header string map id compressed length read fields data input throws io exception write data output throws io exception
1622	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleScheduler.java	scheduler	package org apache hadoop mapreduce task reduce shuffle scheduler k v thread local long shuffle start new thread local long protected long initial value log log log factory get log shuffle scheduler max maps at once initial penalty penalty growth rate f report failure limit boolean finished maps total maps remaining maps map string map host map locations new hash map string map host set map host pending hosts new hash set map host set task attempt id obsolete maps new hash set task attempt id random random new random system current time millis delay queue penalty penalties new delay queue penalty referee referee new referee map task attempt id int writable failure counts new hash map task attempt id int writable map string int writable host failures new hash map string int writable task status status exception reporter reporter abort failure limit progress progress counters counter shuffled maps counter counters counter reduce shuffle bytes counters counter failed shuffle counter start time last progress time max map runtime max failed unique fetches max fetch failures before reporting total bytes shuffled till now decimal format mbps format new decimal format boolean report read error immediately true shuffle scheduler job conf job task status status total maps job get num map tasks abort failure limit math max total maps remaining maps total maps finished maps new boolean remaining maps reporter reporter status status progress progress shuffled maps counter shuffled maps counter reduce shuffle bytes reduce shuffle bytes failed shuffle counter failed shuffle counter start time system current time millis last progress time start time referee start max failed unique fetches math min total maps max fetch failures before reporting job get int report read error immediately job get boolean synchronized copy succeeded task attempt id map id failure counts remove map id host failures remove host get host name map index map id get task id get id finished maps map index synchronized copy failed task attempt id map id map host host host penalize failures failure counts contains key map id else string hostname host get host name host failures contains key hostname else failures abort failure limit check and inform job tracker failures map id read error check reducer health delay initial penalty penalties add new penalty host delay failed shuffle counter increment notify job tracker every read error report read error immediately true every max fetch failures before reporting failures check and inform job tracker report read error immediately read error check reducer health max allowed failed fetch attempt percent f min required progress percent f max allowed stall time percent f total failures failed shuffle counter get value done maps total maps remaining maps boolean reducer healthy check reducer progressed enough boolean reducer progressed enough check reducer stalled time duration reducer stalled stall duration duration reducer ran progress shuffle progress duration min time reducer run without getting killed min shuffle run duration boolean reducer stalled kill healthy insufficient progress failure counts size max failed unique fetches synchronized tip failed task id task id finished maps task id get id true
1623	mapreduce\src\java\org\apache\hadoop\mapreduce\tools\CLI.java	scheduler	package org apache hadoop mapreduce tools interprets map reduce cli options cli extends configured implements tool log log log factory get log cli cli cli configuration conf run string argv throws exception string get job priority names string get task typess display usage command line tool terminate execution display usage string cmd view history string history file boolean protected get counter counters counters string counter group name list events given job list events job job event id num events protected string get task log url task attempt id task id string base url dump list currently running jobs list jobs cluster cluster dump list jobs submitted list all jobs cluster cluster display list active trackers list active trackers cluster cluster display list blacklisted trackers list blacklisted trackers cluster cluster print task attempts task report report display information job tasks particular type particular state pending running completed failed killed protected display tasks job job string type string state throws io exception interrupted exception protected display job list job status jobs main string argv throws exception
1624	mapreduce\src\java\org\apache\hadoop\mapreduce\tools\package-info.java	unrelated	package org apache hadoop mapreduce tools
1625	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ConfigUtil.java	heartbeat	package org apache hadoop mapreduce util place holder deprecated keys framework config util adds deprecated keys loads mapred default xml mapred site xml load resources adds deprecated keys corresponding new keys configuration add deprecated keys
1626	mapreduce\src\java\org\apache\hadoop\mapreduce\util\LinuxMemoryCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate virtual physical memories linux systems use link org apache hadoop mapreduce util linux resource calculator plugin instead linux memory calculator plugin extends memory calculator plugin linux resource calculator plugin resource calculator plugin use everything linux resource calculator plugin linux memory calculator plugin inherit doc get physical memory size inherit doc get virtual memory size
1627	mapreduce\src\java\org\apache\hadoop\mapreduce\util\LinuxResourceCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate resource information linux systems linux resource calculator plugin extends resource calculator plugin log log proc meminfo virtual file keys values format key value k b string procfs memfile proc meminfo pattern procfs memfile format we need values following keys meminfo string memtotal string mem total string swaptotal string swap total string memfree string mem free string swapfree string swap free string inactive string inactive patterns parsing proc cpuinfo string procfs cpuinfo proc cpuinfo pattern processor format pattern frequency format pattern parsing proc stat string procfs stat proc stat pattern cpu time format string procfs mem file string procfs cpu file string procfs stat file jiffy length in millis ram size swap size ram size free free ram space machine k b swap size free free swap space machine k b inactive size inactive cache memory k b num processors number processors system cpu frequency l cpu frequency system k hz cumulative cpu time l cpu used time since system ms last cumulative cpu time l cpu used time read last time ms unix timestamp reading cpu time ms cpu usage task tracker status unavailable sample time task tracker status unavailable last sample time task tracker status unavailable procfs based process tree p tree null boolean read mem info file false boolean read cpu info file false get current time get current time linux resource calculator plugin constructor allows assigning proc directories this used unit tests linux resource calculator plugin string procfs mem file read proc meminfo parse compute memory information read proc mem info file read proc meminfo parse compute memory information read proc mem info file boolean read again read proc cpuinfo parse calculate cpu information read proc cpu info file read proc stat file parse calculate cumulative cpu read proc stat file inherit doc get physical memory size inherit doc get virtual memory size inherit doc get available physical memory size inherit doc get available virtual memory size inherit doc get num processors inherit doc get cpu frequency inherit doc get cumulative cpu time inherit doc get cpu usage test link linux resource calculator plugin main string args proc resource values get proc resource values
1628	mapreduce\src\java\org\apache\hadoop\mapreduce\util\MemoryCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate virtual physical memories system link org apache hadoop mapreduce util resource calculator plugin instead memory calculator plugin extends configured obtain total size virtual memory present system get virtual memory size obtain total size physical memory present system get physical memory size get memory calculator plugin name configure if name null method try return memory calculator plugin available system memory calculator plugin get memory calculator plugin clazz null no given try os specific try catch security exception se not supported system return null
1629	mapreduce\src\java\org\apache\hadoop\mapreduce\util\MRAsyncDiskService.java	pooling	package org apache hadoop mapreduce util this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion files we move files be deleted folder asychronously deleting make sure caller run faster users write files be deleted folder otherwise files gone time restart mr async disk service this also contains operations performed thread pools mr async disk service log log log factory get log mr async disk service async disk service async disk service string tobedeleted be deleted mr async disk service file system local file system mr async disk service job conf conf throws io exception synchronized execute string root runnable task synchronized shutdown synchronized list runnable shutdown now synchronized boolean await termination milliseconds simple date format format new simple date format yyyy mm dd hh mm ss sss file system local file system string volumes atomic long unique id new atomic long a task deleting path name volume delete task implements runnable boolean move and delete relative path string volume string path name boolean move and delete from each volume string path name throws io exception cleanup all volumes throws io exception string normalize path string path string get relative path name string absolute path name boolean move and delete absolute path string absolute path name
1630	mapreduce\src\java\org\apache\hadoop\mapreduce\util\package-info.java	unrelated	package org apache hadoop mapreduce util
1631	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ProcessTree.java	unrelated	package org apache hadoop mapreduce util process tree related operations process tree log log log factory get log process tree default sleeptime before sigkill l sigquit sigterm sigkill string sigquit str sigquit string sigterm str sigterm string sigkill str sigkill boolean setsid available setsid supported boolean setsid supported shell command executor shexec null boolean setsid supported true try catch io exception ioe finally handle exit code return setsid supported destroy process tree killed sending sigterm separate thread destroy string pid sleeptime before sigkill process group else destroy process sending sigterm separate thread protected destroy process string pid sleeptime before sigkill terminate process pid sig kill pid false sleeptime before sigkill background destroy process group sending sigterm separate thread protected destroy process group string pgrp id terminate process group pgrp id sig kill pgrp id true sleeptime before sigkill background send specified signal specified pid logging send signal string pid signal num string signal name shell command executor shexec null try catch io exception ioe finally send specified signal process alive logging maybe signal process string pid signal num if process tree alive signal unless always signal forces always signal process tree alive pid maybe signal process group string pgrp id signal num always signal process tree process group alive pgrp id sends terminate signal process allowing gracefully exit terminate process string pid maybe signal process pid sigterm sigterm str true sends terminate signal process belonging passed process group allowing group gracefully exit terminate process group string pgrp id maybe signal process group pgrp id sigterm sigterm str true kills process or process group sending signal sigkill current thread sending sigterm sig kill in current thread string pid boolean process group kill subprocesses root process even root process alive process group killed process group process tree alive pid kills process or process group sending signal sigkill sending sigterm separate thread sig kill string pid boolean process group background use separate thread killing else sends kill signal process forcefully terminating process kill process string pid maybe signal process pid sigkill sigkill str false sends sigquit process java programs dump stack stdout sig quit process string pid maybe signal process pid sigquit sigquit str false sends kill signal process belonging process group forcefully terminating process group kill process group string pgrp id maybe signal process group pgrp id sigkill sigkill str false sends sigquit processes belonging process group ordering processes group send stack dump stdout sig quit process group string pgrp id maybe signal process group pgrp id sigquit sigquit str false is process pid pid still alive this method assumes alive called pid alive ago hence assumes chance pid wrapping around boolean alive string pid shell command executor shexec null try catch exit code exception ee catch io exception ioe return shexec get exit code true false is process group still alive this method assumes alive called pid alive ago hence assumes chance pid wrapping around boolean process group alive string pgrp id shell command executor shexec null try catch exit code exception ee catch io exception ioe return shexec get exit code true
1632	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ProcfsBasedProcessTree.java	unrelated	package org apache hadoop mapreduce util a proc file system based process tree works linux procfs based process tree extends process tree log log log factory string procfs proc pattern procfs stat file format pattern compile string procfs stat file stat string procfs cmdline file cmdline page size shell command executor shell executor page size try catch io exception e finally jiffy length in millis millisecond shell command executor shell executor jiffies per second try catch io exception e finally enable testing using variable configured test directory string procfs dir integer pid long cpu time l boolean setsid used false sleeptime before sigkill default sleeptime before sigkill map integer process info process tree new hash map integer process info procfs based process tree string pid pid false default sleeptime before sigkill procfs based process tree string pid boolean setsid used pid setsid used sigkill interval procfs build new process tree rooted pid this method provided mainly testing purposes root proc file system adjusted killing process tree procfs based process tree string pid boolean setsid used pid get valid pid pid setsid used setsid used sleeptime before sigkill sigkill interval procfs dir procfs dir sets sigkill interval string boolean instead sending sigterm set sig kill interval interval sleeptime before sigkill interval checks procfs based process tree available system boolean available try catch security exception se return true get process tree latest state if root process alive empty tree returned procfs based process tree get process tree pid return is root process alive boolean alive pid else is subprocesses process tree alive alive false otherwise boolean any process in tree alive integer p id process tree key set return false verify given process id process group id boolean check pid pgrpid for match string pid str string procfs dir integer p id integer parse int pid str get information process process info p info new process info p id p info construct process info p info procfs dir p info null make sure p id pgrp id match p info get pgrp id equals p id log debug enabled return true make sure given pid process group leader destroy process group sending sigterm separate thread assert and destroy process group string pgrp id interval make sure pid given process group leader check pid pgrpid for match pgrp id procfs destroy process group pgrp id interval background destroy process tree destroy destroy true destroy process tree separate thread destroy boolean background log debug killing procfs based process tree pid pid alive pid string string processtree dump format get dump process tree process tree string get process tree dump string builder ret new string builder the header ret append string format pid ppid pgrpid sessid cmd name process info p process tree values return ret string get cumulative virtual memory used processes process tree get cumulative vmem processes processes older return get cumulative vmem get cumulative resident set size rss memory used processes process tree cannot calculated get cumulative rssmem processes processes older return get cumulative rssmem get cumulative virtual memory used processes process tree older
1633	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ResourceCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate resource information system resource calculator plugin extends configured obtain total size virtual memory present system get virtual memory size obtain total size physical memory present system get physical memory size obtain total size available virtual memory present system get available virtual memory size obtain total size available physical memory present system get available physical memory size obtain total number processors present system get num processors obtain cpu frequency system get cpu frequency obtain cumulative cpu time since system get cumulative cpu time obtain cpu usage machine return unavailable get cpu usage obtain resource status used current process tree proc resource values get proc resource values proc resource values cumulative cpu time physical memory size virtual memory size proc resource values cumulative cpu time physical memory size obtain physical memory size used current process tree get physical memory size obtain virtual memory size used current process tree get virtual memory size obtain cumulative cpu time used current process tree get cumulative cpu time get resource calculator plugin name configure if name null method try return memory calculator plugin available system resource calculator plugin get resource calculator plugin clazz null no given try os specific try catch security exception se not supported system return null
1634	mapreduce\src\java\org\apache\hadoop\util\LinuxMemoryCalculatorPlugin.java	unrelated	package org apache hadoop util plugin calculate virtual physical memories linux systems link org apache hadoop mapreduce util linux memory calculator plugin instead linux memory calculator plugin extends inherits everything super
1635	mapreduce\src\java\org\apache\hadoop\util\MemoryCalculatorPlugin.java	unrelated	package org apache hadoop util plugin calculate virtual physical memories system link org apache hadoop mapreduce util memory calculator plugin instead memory calculator plugin extends org apache hadoop mapreduce util memory calculator plugin
1636	mapreduce\src\java\org\apache\hadoop\util\package-info.java	unrelated	package org apache hadoop util
1637	mapreduce\src\java\org\apache\hadoop\util\ProcessTree.java	unrelated	package org apache hadoop util process tree related operations process tree extends org apache hadoop mapreduce util process tree inherits everything super
1638	mapreduce\src\java\org\apache\hadoop\util\ProcfsBasedProcessTree.java	unrelated	package org apache hadoop util a proc file system based process tree works linux link org apache hadoop mapreduce util procfs based process tree instead procfs based process tree extends procfs based process tree string pid procfs based process tree string pid boolean setsid used procfs based process tree string pid boolean setsid used procfs based process tree get process tree
1639	mapreduce\src\tools\org\apache\hadoop\fs\HarFileSystem.java	unrelated	package org apache hadoop fs this implementation hadoop archive filesystem this archive filesystem index files form index contents form part the index files store indexes real files the index files form masterindex index the master index level indirection index file make look ups faster index file sorted hash code paths contains master index contains pointers positions index ranges hashcodes har file system extends filter file system version map uri har meta data har meta cache new hash map uri har meta data uri representation har filesystem uri uri top level path archive underlying file system path archive path har auth string har auth pointer metadata cache har meta data metadata construction harfilesystem har file system constructor create har file system underlying filesystem har file system file system fs super fs initialize har filesystem per har archive the archive home directory top level directory filesystem contains har archive be careful method want go creating new filesystem instances per call path get file system uri har har underlyingfsscheme host port archivepath har archivepath this assumes underlying filesystem used case specified initialize uri name configuration conf throws io exception decode name uri lying uri decode har uri name conf got right har path check truly har filesystem path har path archive path har path null fs null uri har path uri archive path new path uri get path har auth get har auth lying uri check underlying fs containing index file path master index path new path archive path masterindex path archive index path new path archive path index fs exists master index path fs exists archive index path metadata har meta cache get uri metadata null metadata null get version filesystem masterindex file version currently useful since first version archives get har version throws io exception metadata null else find parent path archive path path the last path segment ends har path returned path archive path path p path ret path null path tmp p p depth return ret path decode raw uri get underlying uri uri decode har uri uri raw uri configuration conf throws io exception string tmp auth raw uri get authority using default file system config create underlying uri return tmp auth null string host raw uri get host host null host index of string lying scheme host substring string lying host host length null host substring lying port raw uri get port string auth lying host null lying port uri tmp null raw uri get query null try catch uri syntax exception e return tmp string decode string string str throws unsupported encoding exception return url decoder decode str utf string decode file name string fname throws unsupported encoding exception version metadata get version version version return fname return top level archive path get working directory return new path uri string create har specific auth har underlyingfs port filesystem string get har auth uri lying uri string auth lying uri get scheme lying uri get host null else return auth returns uri filesystem the uri form har underlyingfsschema host port pathintheunderlyingfs uri get uri return uri method returns path
1640	mapreduce\src\tools\org\apache\hadoop\fs\package-info.java	unrelated	package org apache hadoop fs
1641	mapreduce\src\tools\org\apache\hadoop\tools\DistCh.java	unrelated	package org apache hadoop tools a map reduce program recursively change files properties owner group permission dist ch extends dist tool string name distch string job dir label name job dir string op list label name op list string op count label name op count string usage java dist ch get name op per map max maps per node sync file max enum counter succeed fail enum option ignore failures name ignore failures string cmd propertyname option string cmd string propertyname dist ch configuration conf super create job conf conf job conf create job conf configuration conf job conf jobconf new job conf conf dist ch jobconf set job name name jobconf set map speculative execution false jobconf set input format change input format jobconf set output key class text jobconf set output value class text jobconf set mapper class change files mapper jobconf set num reduce tasks return jobconf file operations file operation implements writable path src string owner string group fs permission permission file operation file operation path src file operation path owner group permission e g user foo foo bar file operation string line check state throws illegal state exception fs permission file umask boolean different file status original run configuration conf throws io exception inherit doc read fields data input throws io exception inherit doc write data output throws io exception inherit doc string string responsible generating splits src file list change input format implements input format text file operation do nothing validate input job conf job produce splits greater quotient total size number splits requested input split get splits job conf job num splits inherit doc record reader text file operation get record reader input split split the mapper changing files change files mapper job conf jobconf boolean ignore failures failcount succeedcount string get count string inherit doc configure job conf job run file operation map text key file operation value inherit doc close throws io exception check configuration conf list file operation ops list path srcs new array list path file operation op ops dist tool check source conf srcs list file operation fetch list configuration conf path inputfile list file operation result new array list file operation string line read file conf inputfile return result this main driver recursively changing files properties run string args throws exception list file operation ops new array list file operation path logpath null boolean ignore failures false try catch duplication exception e catch exception e return calculate many maps run get map count src count num nodes num maps src count op per map num maps math min num maps num nodes max maps per node return math max num maps boolean setup list file operation ops path log throws io exception string random id get random id job client j client new job client jobconf path staging area try catch interrupted exception ie path jobdir new path staging area name random id fs permission mapred sys perms file system mkdirs j client get fs jobdir mapred sys perms log info job dir label jobdir log null file output
1642	mapreduce\src\tools\org\apache\hadoop\tools\DistCp.java	unrelated	package org apache hadoop tools a map reduce program recursively copy directories different file systems dist cp implements tool log log log factory get log dist cp string name distcp string usage name options srcurl desturl n n options n p rbugpt preserve status n r replication number n b block size n u user n g group n p permission n modification access times n p alone equivalent prbugpt n ignore failures n basedir basedir use basedir base directory copying files srcurl n log logdir write logs logdir n num maps maximum number simultaneous copies n overwrite overwrite destination n update overwrite src size different dst size n skipcrccheck do use crc check determine src n different dest relevant update n specified n f urilist uri use list urilist uri src list n filelimit n limit total number files n n sizelimit n limit total size n bytes n delete delete files existing dst src n dryrun display count files total size files n src exit copy done n desturl speicified update n mapred ssl conf f filename ssl configuration mapper task n n note overwrite update set source uri n interpreted isomorphic update existing directory n for example nhadoop name p update hdfs a user foo bar hdfs b user foo baz n n would update descendants baz also bar would n update user foo baz bar n n note the parameter n filelimit sizelimit n specified symbolic representation for examples n k n g n bytes per map max maps per node sync file max default file retries enum counter copy skip fail bytescopied bytesexpected enum options delete delete name delete file limit filelimit name limit file size limit sizelimit name limit size ignore read failures name ignore read failures preserve status p name preserve status overwrite overwrite name overwrite always update update name overwrite ifnewer skipcrc skipcrccheck name skip crc check string cmd propertyname options string cmd string propertyname parse long string args offset enum file attribute block size replication user group permission times char symbol file attribute symbol string lower case char at enum set file attribute parse string string tmp dir label name tmp dir string dst dir label name dest path string job dir label name job dir string max maps label name max map tasks string src list label name src list string src count label name src count string total size label name total size string dst dir list label name dst dir list string bytes per map label name bytes per map string preserve status label string file retries label name file retries job conf conf set conf configuration conf conf instanceof job conf else configuration get conf return conf dist cp configuration conf set conf conf an input output pair filenames file pair implements writable file status input new file status string output file pair file pair file status input string output read fields data input throws io exception write data output throws io exception string string input format distcp job responsible generating splits src file list copy input format implements
1643	mapreduce\src\tools\org\apache\hadoop\tools\DistTool.java	unrelated	package org apache hadoop tools an distributed tool file related operations dist tool implements org apache hadoop util tool protected log log log factory get log dist tool protected job conf jobconf inherit doc set conf configuration conf inherit doc job conf get conf return jobconf protected dist tool configuration conf set conf conf random random new random protected string get random id sanity check source protected check source configuration conf list path srcs protected string read string data input throws io exception protected write string data output string protected list string read file configuration conf path inputfile an exception duplicated source files duplication exception extends io exception
1644	mapreduce\src\tools\org\apache\hadoop\tools\HadoopArchives.java	unrelated	package org apache hadoop tools archive creation utility this provides methods used create hadoop archives for understanding hadoop archives look link har file system hadoop archives implements tool version log log log factory get log hadoop archives string name har string src list label name src list string dst dir label name dest path string tmp dir label name tmp dir string job dir label name job dir string src count label name src count string total size label name total size string dst har label name archive name string src parent label name parent path size blocks created archiving string har blocksize label name block size size part files created archiving string har partsize label name partfile size size part file size part size size blocks hadoop archives block size string usage archive archive name name p parent path src dest n job conf conf set conf configuration conf configuration get conf hadoop archives configuration conf check src paths check paths configuration conf list path paths throws io exception recursivels file system fs file status dir fdir list file status dir throws io exception har entry used link h archives mapper input value har entry implements writable h archive input format implements input format long writable har entry boolean check valid name string name path largest depth list path paths path rel path to root path full path path root write top level dirs sequence file writer src writer append sequence file writer src writer len file status dir archive given source paths archive path parent path list path src paths h archives mapper implements mapper long writable har entry int writable text reduce creating index master index h archives reducer implements reducer int writable text text text main driver creating archives run string args throws exception main functions main string args
1645	mapreduce\src\tools\org\apache\hadoop\tools\Logalyzer.java	unrelated	package org apache hadoop tools logalyzer a utility tool archiving analyzing hadoop logs p this tool supports archiving anaylzing sort grep log files it takes input input uri serve uris logs archived b output directory mandatory b directory dfs archive logs c the sort grep patterns analyzing files separator boundaries usage logalyzer archive archive dir directory archive logs analysis directory logs log list uri grep pattern sort col col separator separator p logalyzer constants configuration fs config new configuration string sort columns logalizer logcomparator sort columns string column separator logalizer logcomparator column separator configuration add deprecation mapred reducer sort configuration add deprecation mapred reducer separator a link mapper extracts text matching regular expression log regex mapper k extends writable comparable extends map reduce base implements mapper k text text long writable pattern pattern configure job conf job map k key text value a writable comparator optimized utf keys logs log comparator extends text comparator implements configurable log log log factory get log logalyzer job conf conf null string sort spec null string column separator null set conf configuration conf configuration get conf compare byte b archive workhorse function archive log files archive string log list uri string archive directory throws io exception string dest url file system get default uri fs config archive directory dist cp copy new job conf fs config log list uri dest url null true false analyze analyze string input files directory string output directory throws io exception path grep input new path input files directory path analysis output null output directory equals else job conf grep job new job conf fs config grep job set job name logalyzer grep sort file input format set input paths grep job grep input grep job set input format text input format grep job set mapper class log regex mapper grep job set regex mapper pattern grep pattern grep job set sort columns sort columns grep job set column separator column separator grep job set combiner class long sum reducer grep job set reducer class long sum reducer file output format set output path grep job analysis output grep job set output format text output format grep job set output key class text grep job set output value class long writable grep job set output key comparator class log comparator grep job set num reduce tasks write single file job client run job grep job main string args log log log factory get log logalyzer string version logalyzer string usage usage logalyzer archive logs urls file system println version args length command line arguments boolean archive false boolean grep false boolean sort false string archive dir string log list uri string grep pattern string sort columns string column separator string output directory args length parse command line log info analysis dir output directory log info archive dir archive dir log info log list uri log list uri log info grep pattern grep pattern log info sort columns sort columns log info separator column separator try catch io exception ioe main logalyzer
1646	mapreduce\src\tools\org\apache\hadoop\tools\package-info.java	unrelated	package org apache hadoop tools
1647	mapreduce\src\tools\org\apache\hadoop\tools\rumen\AbstractClusterStory.java	unrelated	package org apache hadoop tools rumen link abstract cluster story provides partial implementation link cluster story parsing topology tree abstract cluster story implements cluster story protected set machine node machine nodes protected set rack node rack nodes protected machine node nodes flattened protected map string machine node node map protected map string rack node r node map protected maximum distance set machine node get machines synchronized set rack node get racks synchronized machine node get random machines expected protected synchronized build machine node map machine node get machine by name string name distance node node b protected synchronized build rack node map rack node get rack by name string name get maximum distance protected synchronized parse topology tree
1648	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CDFPiecewiseLinearRandomGenerator.java	unrelated	package org apache hadoop tools rumen cdf piecewise linear random generator extends cdf random generator builds cdf random value engine around link logged discrete cdf defaultly seeded rng cdf piecewise linear random generator logged discrete cdf cdf builds cdf random value engine around link logged discrete cdf explicitly seeded rng random number generator seed cdf piecewise linear random generator logged discrete cdf cdf seed todo this code assumes empirical minimum resp maximum epistomological minimum resp maximum this probably okay minimum likely represents task everything went well maximum may want develop way extrapolating past maximum value at probability
1649	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CDFRandomGenerator.java	unrelated	package org apache hadoop tools rumen an instance generates random values confirm embedded link logged discrete cdf the discrete cdf pointwise approximation real cdf we therefore choice interpolation rules a concrete subclass implement value at using dependent interpolation rule cdf random generator rankings values random random cdf random generator logged discrete cdf cdf cdf random generator logged discrete cdf cdf seed cdf random generator logged discrete cdf cdf random random protected initialize tables logged discrete cdf cdf protected floor index probe protected get ranking at index protected get datum at index random value value at probability
1650	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ClusterStory.java	unrelated	package org apache hadoop tools rumen link cluster story represents configurations map reduce cluster including nodes network topology slot configurations cluster story get machines cluster set machine node get machines get racks cluster set rack node get racks get cluster topology tree node get cluster topology select random set machines machine node get random machines expected random random get link machine node host name machine node get machine by name string name get link rack node name rack node get rack by name string name determine distance two link node currently distance loosely defined length longer path either b reach common ancestor distance node node b get maximum distance possible two nodes get maximum distance
1651	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ClusterTopologyReader.java	unrelated	package org apache hadoop tools rumen reading json encoded cluster topology produce parsed link logged network topology object cluster topology reader logged network topology topology read topology json object mapper parser logged network topology parser constructor path json encoded topology file possibly compressed cluster topology reader path path configuration conf constructor the input stream json encoded topology data cluster topology reader input stream input throws io exception get link logged network topology object logged network topology get
1652	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CurrentJHParser.java	unrelated	package org apache hadoop tools rumen link job history parser parses link job history files produced link org apache hadoop mapreduce jobhistory job history source code tree rumen current jh parser implements job history parser event reader reader forked data input stream extends data input stream forked data input stream input stream input close can parser parse input boolean parse input stream input throws io exception data input stream new forked data input stream input try catch io exception e return true current jh parser input stream input throws io exception reader new event reader new data input stream input history event next event throws io exception return reader get next event close throws io exception reader close
1653	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeepCompare.java	unrelated	package org apache hadoop tools rumen classes implement deep compare equality order another instance they deep compare if semantically significant difference implementer throws exception thrown chain causes describing chain field references indices get miscompared point deep compare comparand compared path got in root location null to process scalar code foo field root make recursive call link tree path whose code field name code bar whose code index whose code parent code null to process plural code bar field root make recursive call link tree path whose field name code foo whose code index whose code parent also code null deep compare deep compare tree path location
1654	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeepInequalityException.java	unrelated	package org apache hadoop tools rumen we use exception unit test deep comparison run deep inequality exception extends exception serial version uid tree path path deep inequality exception string message tree path path deep inequality exception string message tree path path
1655	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DefaultInputDemuxer.java	unrelated	package org apache hadoop tools rumen link default input demuxer acts pass demuxer it opens file returns back input stream if input compressed would return decompression stream default input demuxer implements input demuxer string name input stream input bind to path path configuration conf throws io exception pair string input stream get next throws io exception close throws io exception
1656	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DefaultOutputter.java	pooling	package org apache hadoop tools rumen the default link outputter outputs plain file compression applied path right suffix default outputter t implements outputter t json object mapper writer t writer compressor compressor init path path configuration conf throws io exception output t object throws io exception close throws io exception
1657	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeskewedJobTraceReader.java	unrelated	package org apache hadoop tools rumen deskewed job trace reader implements closeable underlying engine job trace reader reader configuration variables skew buffer length boolean abort on unfixable skew state variables skew measurement latest submit time long min value returned latest submit time long min value max skew buffer needed submit time not counted repeated submit times so far occurs this situation represented time submit times so far a submit time occurs twice appears counted repeated submit times so far appropriate range value and submit times so far tree map long integer counted repeated submit times so far tree set long submit times so far new tree set long priority queue logged job skew buffer log log job comparator implements comparator logged job compare logged job j logged job j constructor link job trace reader protected number late jobs preced later order earlier job deskewed job trace reader job trace reader reader skew buffer length reader reader skew buffer length skew buffer length abort on unfixable skew abort on unfixable skew skew buffer fill skew buffer deskewed job trace reader job trace reader reader throws io exception reader true logged job raw next job throws io exception logged job result reader get next abort on unfixable skew skew buffer length result null return result out of order exception extends runtime exception serial version uid l out of order exception string text logged job next job throws io exception out of order exception logged job new job raw next job new job null logged job result skew buffer poll result null result get submit time returned latest submit time result null return result fill skew buffer throws io exception skew buffer length needed skew buffer size return max skew buffer needed close throws io exception reader close
1658	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Folder.java	pooling	package org apache hadoop tools rumen folder extends configured implements tool output duration input cycle concentration random seed irrelevant seeded false boolean seeded false boolean debug false boolean allow missorting false skew buffer length starts after log log log factory get log folder deskewed job trace reader reader null json generator gen null list path temp paths new linked list path path temp dir null first job submit time time dilation transcription rate fraction transcription rate integer random random ticks per second l error return codes non existent files no input cycle length empty job trace out of order jobs all jobs simultaneous io error other error set closeable closees new hash set closeable set path deletees new hash set path parse duration string duration string string numeral duration string substring duration string length char duration code duration string char at duration string length result integer parse int numeral result throw new illegal argument exception negative durations allowed switch duration code case d case return l l l ticks per second result case h case return l l ticks per second result case m case return l ticks per second result case s case return ticks per second result default throw new illegal argument exception missing invalid duration code initialize string args throws illegal argument exception string temp dir name null string input path name null string output path name null args length string arg args arg equals ignore case starts else arg equals ignore case output duration else arg equals ignore case input cycle else arg equals ignore case concentration else arg equals ignore case debug else arg equals ignore case allow missorting else arg equals ignore case seed else arg equals ignore case skew buffer length else arg equals ignore case temp directory else arg equals arg starts with else try configuration conf get conf path path new path input path name reader path path new path output path name object mapper mapper new object mapper mapper configure json factory factory mapper get json factory file system fs path get file system conf compression codec codec output stream output compressor compressor null codec null else gen factory create json generator output json encoding utf gen use default pretty printer temp dir file system fs temp dir get file system get conf fs get file status temp dir directory input cycle output duration input cycle time dilation output duration input cycle random seeded new random random seed new random debug catch io exception e e print stack trace system err return non existent files return run string args throws io exception result initialize args result return result return run run throws io exception job entry comparator implements compare pair logged job job trace reader p object mapper mapper new object mapper mapper configure json factory factory mapper get json factory initialize empty heap take error establishing real one finally code goes queue pair logged job job trace reader heap try logged job job reader next job job null if starts time specified skip number jobs till reach starting time limit starts
1659	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Hadoop20JHParser.java	unrelated	package org apache hadoop tools rumen link job history parser parse job histories hadoop meta hadoop jh parser implements job history parser line reader reader string end line string internal version can parser parse input we deem stream good job history stream first line exactly meta version boolean parse input stream input throws io exception hadoop jh parser input stream input throws io exception map string history event emitter live emitters queue history event remaining events new linked list history event enum line type history event next event history event emitter find or make emitter string name line type type string get one line throws io exception string get full line throws io exception close throws io exception
1660	mapreduce\src\tools\org\apache\hadoop\tools\rumen\HadoopLogsAnalyzer.java	pooling	package org apache hadoop tools rumen this main rumen log mining functionality it reads directory job tracker logs computes various information see code usage hadoop logs analyzer extends configured implements tool output streams print stream status output system print stream statistical output system print stream debug output system err maximum preferred locations small spread compensation threshold l maximum clock skew l pattern task attempt id pattern pattern xml file prefix pattern compile pattern conf file header pattern compile conf xml map string pattern counter patterns parsed config file jobconf null boolean omit task details false json generator job trace gen null boolean prettyprint trace true logged job job being traced null map string logged task tasks in current job map string logged task attempt attempts in current job histogram successful map attempt times histogram successful reduce attempt times histogram failed map attempt times histogram failed reduce attempt times histogram successful nth mapper attempts histogram successful nth reducer attempts histogram mapper locality log log log factory get log hadoop logs analyzer attempt times percentiles json generator topology gen null hash set parsed host hosts new hash set parsed host number ticks per second boolean collecting false line number string rereadable line null string input filename boolean input is directory false path input directory path null string input directory files null input directory cursor line reader input null compression codec input codec null decompressor input decompressor null text input line text new text boolean debug false version number buckets spread min spread max boolean spreading false boolean delays false boolean runtimes false boolean collect task times false log record type canonical job log record type intern job log record type canonical map attempt log record type canonical reduce attempt log record type canonical task log record type intern task pattern streaming jobname pattern hash set string host names new hash set string boolean file first line true string current file name null here cumulative statistics enum job outcome histogram run time dists histogram delay time dists histogram map time spread dists histogram shuffle time spread dists histogram sort time spread dists histogram reduce time spread dists histogram map time dists histogram shuffle time dists histogram sort time dists histogram reduce time dists map string long task attempt start times map string long task reduce attempt shuffle end times map string long task reduce attempt sort end times map string long task map attempt finish times map string long task reduce attempt finish times submit time current job launch time current job string current job id todo currently set correctly we fix that matters statistics extraction logged job job type job type histogram new distribution block histogram new distribution block string blockname histogram get distribution histogram block job outcome outcome usage hadoop logs analyzer boolean path is directory path p throws io exception initialize hadoop logs analyzer string args line reader maybe uncompressed path path p boolean set next directory input stream throws file not found exception string read input line throws io exception string read counted line throws io exception unread counted line string
1661	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Histogram.java	unrelated	package org apache hadoop tools rumen link histogram represents ordered summary sequence code queried produce discrete approximation cumulative distribution function histogram implements iterable map entry long long tree map long long content new tree map long long string name total count histogram histogram string name dump print stream stream iterator map entry long long iterator get key get total count enter value produces discrete approximation cdf the user provides points code y axis wants give corresponding points code x axis plus minimum maximum data denominator applied every element buckets for example code scale code code buckets element specify median output slot array less scale strictly greater predecessor we check requirements the first resp last element minimum resp maximum value ever code enter ed the rest elements correspond elements code buckets carry first element whose rank less code content elements scale bucket get cdf scale buckets
1662	mapreduce\src\tools\org\apache\hadoop\tools\rumen\HistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen history event emitter log log log factory get log history event emitter list single event emitter non final se es list single event emitter se es protected history event emitter enum post emit action pair queue history event post emit action emitter core parsed line line protected counters maybe parse counters string counters protected counters parse counters string counters
1663	mapreduce\src\tools\org\apache\hadoop\tools\rumen\InputDemuxer.java	unrelated	package org apache hadoop tools rumen link input demuxer dem ultiplexes input files individual input streams input demuxer extends closeable bind link input demuxer particular file the path file bind configuration returns true binding succeeds if file read wrong format returns false io exception reserved read errors bind to path path configuration conf throws io exception get next name input pair the name preserve original job history file job conf file name the input object closed calling get next the old input object would invalid calling get next pair string input stream get next throws io exception
1664	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Job20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen job line history event emitter extends history event emitter list single event emitter non finals list single event emitter finals new linked list single event emitter long original submit time null non finals add new job submitted event emitter non finals add new job priority change event emitter non finals add new job status changed event emitter non finals add new job inited event emitter non finals add new job info change event emitter finals add new job unsuccessful completion event emitter finals add new job finished event emitter job line history event emitter super job submitted event emitter extends single event emitter history event maybe emit event parsed line line string job id name job priority change event emitter extends single event emitter history event maybe emit event parsed line line string job id name job inited event emitter extends single event emitter history event maybe emit event parsed line line string job id name job status changed event emitter extends single event emitter history event maybe emit event parsed line line string job id name job info change event emitter extends single event emitter history event maybe emit event parsed line line string job id name job unsuccessful completion event emitter extends history event maybe emit event parsed line line string job id name job finished event emitter extends single event emitter history event maybe emit event parsed line line string job id name list single event emitter se es return finals list single event emitter non final se es return non finals
1665	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobBuilder.java	scheduler	package org apache hadoop tools rumen link job builder builds one job it processes sequence link history event job builder bytes in meg string utils traditional binary prefix string job id boolean finalized false logged job result new logged job map string logged task map tasks new hash map string logged task map string logged task reduce tasks new hash map string logged task map string logged task tasks new hash map string logged task map string logged task attempt attempts new hash map string logged task attempt map parsed host parsed host hosts new hash map parsed host parsed host the number splits task ignore maximum preferred locations the regular expression used parse task attempt i ds job tracker logs pattern task attempt id pattern pattern compile attempt times percentiles null use search within java options get heap sizes the heap size number capturing group the heap size order magnitude suffix capturing group pattern heap pattern pattern compile xmx k km mg gt t properties job configuration parameters null job builder string job id job id job id string get job id return job id attempt times percentiles null attempt times percentiles new process one link history event the link history event processed process history event event finalized throw new illegal state exception lexicographical order name event instanceof job finished event process job finished event job finished event event else event instanceof job info change event process job info change event job info change event event else event instanceof job inited event process job inited event job inited event event else event instanceof job priority change event process job priority change event job priority change event event else event instanceof job status changed event process job status changed event job status changed event event else event instanceof job submitted event process job submitted event job submitted event event else event instanceof job unsuccessful completion event process job unsuccessful completion event job unsuccessful completion event event else event instanceof map attempt finished event process map attempt finished event map attempt finished event event else event instanceof reduce attempt finished event process reduce attempt finished event reduce attempt finished event event else event instanceof task attempt finished event process task attempt finished event task attempt finished event event else event instanceof task attempt started event process task attempt started event task attempt started event event else event instanceof task attempt unsuccessful completion event process task attempt unsuccessful completion event task attempt unsuccessful completion event event else event instanceof task failed event process task failed event task failed event event else event instanceof task finished event process task finished event task finished event event else event instanceof task started event process task started event task started event event else event instanceof task updated event process task updated event task updated event event else throw new illegal argument exception string extract properties conf string names string default value string name names string result conf get property name result null return default value integer extract megabytes properties conf string names string java options extract conf names
1666	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobConfigurationParser.java	unrelated	package org apache hadoop tools rumen link job configuration parser parses job configuration xml file extracts configuration properties it parses file using stream parser thus memory efficient this optimization may postponed future release job configuration parser parse job configuration file input stream return link properties collection the input stream closed return call the input data configuration xml properties parse input stream input throws io exception
1667	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobConfPropertyNames.java	unrelated	package org apache hadoop tools rumen enum job conf property names queue names mapred job queue name mr job config queue name job names mapred job name mr job config job name task java opts s mapred child java opts map java opts s mapred child java opts mr job config map java opts reduce java opts s mapred child java opts mr job config reduce java opts string candidates job conf property names string candidates string get candidates
1668	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobHistoryParser.java	unrelated	package org apache hadoop tools rumen link job history parser defines job history file parser job history parser extends closeable get next link history event history event next event throws io exception
1669	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobHistoryParserFactory.java	unrelated	package org apache hadoop tools rumen link job history parser factory singleton attempts determine version job history return proper parser job history parser factory job history parser get parser rewindable input stream ris enum version detector
1670	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobStory.java	unrelated	package org apache hadoop tools rumen link job story represents runtime information available completed map reduce job job story get link job conf job job conf get job conf get job name string get name get job id job id get job id get user ran job string get user get job submission time get submission time get number maps link job story get number maps get number reduce link job story get number reduces get input splits job input split get input splits get link task info given task task info get task info task type task type task number get link task attempt info given task attempt without regard impact locality e g needed make scheduling decisions task attempt info get task attempt info task type task type get link task attempt info given task attempt considering impact locality task attempt info get outcome job execution values get outcome get queue job submitted string get queue name
1671	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobStoryProducer.java	unrelated	package org apache hadoop tools rumen link job story producer produces sequence link job story job story producer extends closeable get next job job story get next job throws io exception
1672	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobTraceReader.java	unrelated	package org apache hadoop tools rumen reading json encoded job traces produce link logged job instances job trace reader extends json object mapper parser logged job constructor path json trace file possibly compressed job trace reader path path configuration conf throws io exception constructor the input stream json trace job trace reader input stream input throws io exception
1673	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JsonObjectMapperParser.java	unrelated	package org apache hadoop tools rumen a simple wrapper parsing json encoded data using object mapper the base type object parsed parser json object mapper parser t implements closeable object mapper mapper class extends t clazz json parser json parser constructor path json data file possibly compressed json object mapper parser path path class extends t clazz constructor the input stream json data json object mapper parser input stream input class extends t clazz get next object trace stream t get next throws io exception close throws io exception
1674	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JsonObjectMapperWriter.java	unrelated	package org apache hadoop tools rumen simple wrapper around link json generator write objects json format json object mapper writer t implements closeable json generator writer json object mapper writer output stream output boolean pretty print throws io exception write t object throws io exception close throws io exception
1675	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedDiscreteCDF.java	unrelated	package org apache hadoop tools rumen a link logged discrete cdf discrete approximation cumulative distribution function set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged discrete cdf implements deep compare number values l minimum long min value list logged single relative ranking rankings new array list logged single relative ranking maximum long max value set cdf histogram data steps modulus get minimum set minimum minimum list logged single relative ranking get rankings set rankings list logged single relative ranking rankings get maximum set maximum maximum get number values set number values number values compare c c tree path loc string eltname compare list logged single relative ranking c deep compare deep compare comparand tree path loc
1676	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedJob.java	unrelated	package org apache hadoop tools rumen a link logged discrete cdf representation hadoop job details set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged job implements deep compare enum job type enum job priority set string already seen any setter attributes string job id string user computons per map input byte l computons per map output byte l computons per reduce input byte l computons per reduce output byte l submit time l launch time l finish time l heap megabytes total maps total reduces pre job history constants values outcome null job type jobtype job type java job priority priority job priority normal list string direct dependant jobs new array list string list logged task map tasks new array list logged task list logged task reduce tasks new array list logged task list logged task tasks new array list logged task there cd fs level locality local first array list logged discrete cdf successful map attempt cd fs there cd fs level locality local first array list logged discrete cdf failed map attempt cd fs logged discrete cdf successful reduce attempt cdf logged discrete cdf failed reduce attempt cdf string queue null string job name null cluster map mb cluster reduce mb job map mb job reduce mb relative time mapper tries to succeed failed mapper fraction properties job properties new properties logged job logged job string job id set job properties properties conf properties get job properties adjust times adjustment input parameter ignored set unknown attribute string attribute name object ignored string get user set user string user string get job id set job id string job id job priority get priority set priority job priority priority get computons per map input byte set computons per map input byte computons per map input byte get computons per map output byte set computons per map output byte computons per map output byte get computons per reduce input byte set computons per reduce input byte computons per reduce input byte get computons per reduce output byte set computons per reduce output byte computons per reduce output byte get submit time set submit time submit time get launch time set launch time start time get finish time set finish time finish time get heap megabytes set heap megabytes heap megabytes get total maps set total maps total maps get total reduces set total reduces total reduces pre job history constants values get outcome set outcome pre job history constants values outcome job type get jobtype set jobtype job type jobtype list string get direct dependant jobs set direct dependant jobs list string direct dependant jobs list logged task get map tasks set map tasks list logged task map tasks list logged task get reduce tasks set reduce tasks list logged task reduce tasks list logged task get other tasks set other tasks list logged task tasks array list logged discrete cdf get successful map attempt cd fs set successful map attempt cd fs array list logged discrete cdf get failed map attempt cd fs
1677	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedLocation.java	unrelated	package org apache hadoop tools rumen a link logged location representation point hierarchical network represented series membership names broadest first for example network hosts grouped racks onecluster might node code node rack code rack this would represented array list two layers two link string code rack code node the details set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged location implements deep compare list string layers collections empty list set string already seen any setter attributes list string get layers set layers list string layers input parameter ignored set unknown attribute string attribute name object ignored i treat atomic object type compare strings list string c list string c tree path loc deep compare deep compare comparand tree path loc
1678	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedNetworkTopology.java	unrelated	package org apache hadoop tools rumen a link logged network topology represents tree turn represents hierarchy hosts the current version requires tree leaves level all methods simply accessors instance variables want write json files logged network topology implements deep compare string name list logged network topology children new array list logged network topology set string already seen any setter attributes logged network topology super input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name we need sort code children field that field set valued sort fields ensure comparisons bogusly fail hash table happened enumerate different order topo sort implements comparator logged network topology compare logged network topology logged network topology hash set link parsed host name level host recursive descent level number logged network topology set parsed host hosts string name level name name children null level parsed host number of distances else logged network topology set parsed host hosts hosts root string get name return name set name string name name name list logged network topology get children return children set children list logged network topology children children children compare list logged network topology c c null c null c null c null c size c size collections sort c new topo sort collections sort c new topo sort c size deep compare deep compare comparand tree path loc comparand instanceof logged network topology logged network topology logged network topology comparand compare children children loc children
1679	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedSingleRelativeRanking.java	unrelated	package org apache hadoop tools rumen a link logged single relative ranking represents x y coordinate single point discrete cdf all methods simply accessors instance variables want write json files logged single relative ranking implements deep compare the y coordinate fraction code d d the default value mark unfilled value relative ranking d the x coordinate datum l set string already seen any setter attributes input parameter ignored set unknown attribute string attribute name object ignored get relative ranking set relative ranking relative ranking get datum set datum datum compare c c tree path loc string eltname compare c c tree path loc string eltname deep compare deep compare comparand tree path loc
1680	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedTask.java	unrelated	package org apache hadoop tools rumen a link logged task represents hadoop task part hadoop job it knows pssibly empty sequence attempts i o footprint runtime all methods simply accessors instance variables want write json files logged task implements deep compare input bytes l input records l output bytes l output records l string task id start time l finish time l pre job history constants values task type pre job history constants values task status list logged task attempt attempts new array list logged task attempt list logged location preferred locations collections empty list set string already seen any setter attributes input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name logged task super adjust times adjustment start time adjustment finish time adjustment logged task attempt attempt attempts get input bytes return input bytes set input bytes input bytes input bytes input bytes get input records return input records set input records input records input records input records get output bytes return output bytes set output bytes output bytes output bytes output bytes get output records return output records set output records output records output records output records string get task id return task id set task id string task id task id task id get start time return start time set start time start time start time start time get finish time return finish time set finish time finish time finish time finish time list logged task attempt get attempts return attempts set attempts list logged task attempt attempts attempts null else list logged location get preferred locations return preferred locations set preferred locations list logged location preferred locations preferred locations null preferred locations empty else pre job history constants values get task status return task status set task status pre job history constants values task status task status task status pre job history constants values get task type return task type set task type pre job history constants values task type task type task type incorporate map counters jh counters counters incorporate counter new set field counters hdfs bytes read incorporate counter new set field counters file bytes written incorporate counter new set field counters map input records incorporate counter new set field counters map output records incorporate reduce counters jh counters counters incorporate counter new set field counters reduce shuffle bytes incorporate counter new set field counters hdfs bytes written incorporate counter new set field counters reduce input records incorporate counter new set field counters reduce output records incorporate event counters logged task must know its type before this call incorporate counters jh counters counters switch task type case map case reduce string canonicalize counter name string non canonical name string result non canonical name lower case result result replace result result replace result result replace result result replace return result set field logged task task set field logged task task set value incorporate counter set field thunk jh counters counters counter name canonicalize counter name counter name jh counter group group counters groups compare c c tree path
1681	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedTaskAttempt.java	unrelated	package org apache hadoop tools rumen hack alert this two subclasses might called logged map task attempt logged reduce task attempt jackson implementation json handle superclass valued field a link logged task attempt represents attempt run hadoop task hadoop job note task several attempts all methods simply accessors instance variables want write json files logged task attempt implements deep compare string attempt id pre job history constants values result start time l finish time l string host name hdfs bytes read l hdfs bytes written l file bytes read l file bytes written l map input records l map input bytes l map output bytes l map output records l combine input records l reduce input groups l reduce input records l reduce shuffle bytes l reduce output records l spilled records l shuffle finished l sort finished l logged location location initialize default object backward compatibility resource usage metrics metrics new resource usage metrics logged task attempt super set string already seen any setter attributes input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name adjust times adjustment start time adjustment finish time adjustment get shuffle finished return shuffle finished set shuffle finished shuffle finished shuffle finished shuffle finished get sort finished return sort finished set sort finished sort finished sort finished sort finished string get attempt id return attempt id set attempt id string attempt id attempt id attempt id pre job history constants values get result return result set result pre job history constants values result result result get start time return start time set start time start time start time start time get finish time return finish time set finish time finish time finish time finish time string get host name return host name set host name string host name host name host name null null host name intern get hdfs bytes read return hdfs bytes read set hdfs bytes read hdfs bytes read hdfs bytes read hdfs bytes read get hdfs bytes written return hdfs bytes written set hdfs bytes written hdfs bytes written hdfs bytes written hdfs bytes written get file bytes read return file bytes read set file bytes read file bytes read file bytes read file bytes read get file bytes written return file bytes written set file bytes written file bytes written file bytes written file bytes written get map input records return map input records set map input records map input records map input records map input records get map output bytes return map output bytes set map output bytes map output bytes map output bytes map output bytes get map output records return map output records set map output records map output records map output records map output records get combine input records return combine input records set combine input records combine input records combine input records combine input records get reduce input groups return reduce input groups set reduce input groups reduce input groups reduce input groups reduce input groups get reduce input records return reduce input records set reduce input
1682	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LogRecordType.java	unrelated	package org apache hadoop tools rumen log record type map string log record type internees new hash map string log record type string name index log record type string name log record type intern string type name log record type intern soft string type name string string string line types
1683	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MachineNode.java	unrelated	package org apache hadoop tools rumen link machine node represents configuration cluster node link machine node constructed link machine node builder machine node extends node memory kb map slots reduce slots memory per map slot kb memory per reduce slot kb num cores machine node string name level super name level boolean equals object obj name level sufficient return super equals obj hash code match equals return super hash code get available physical ram node get memory return memory get number map slots node get map slots return map slots get number reduce slots node get reduce slots return reduce slots get amount ram reserved map slot get memory per map slot return memory per map slot get amount ram reserved reduce slot get memory per reduce slot return memory per reduce slot get number cores node get num cores return num cores get rack node machine belongs machine belong rack rack node get rack node return rack node get parent synchronized boolean add child node child throw new illegal state exception cannot add child machine node builder node info object builder machine node node start building new node info object unique name node typically fully qualified domain name builder string name level set physical memory node builder set memory memory set number map slot node builder set map slots map slots set number reduce slot node builder set reduce slots reduce slots set amount ram reserved map slot builder set memory per map slot memory per map slot set amount ram reserved reduce slot builder set memory per reduce slot memory per reduce slot set number cores node builder set num cores num cores clone settings reference link machine node object builder clone from machine node ref build link machine node object machine node build
1684	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MapAttempt20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen map attempt line history event emitter extends task attempt line event emitter list single event emitter non finals list single event emitter finals new linked list single event emitter non finals add all task event non final se es finals add new map attempt finished event emitter protected map attempt line history event emitter super map attempt finished event emitter extends history event maybe emit event parsed line line string task attempt id name list single event emitter se es return finals list single event emitter non final se es return non finals
1685	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MapTaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link map task attempt info represents information regard map task attempt map task attempt info extends task attempt info runtime map task attempt info state state task info task info runtime get runtime get runtime b map b phase map task attempt get map runtime
1686	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Node.java	unrelated	package org apache hadoop tools rumen link node represents node cluster topology a node link machine node link rack node etc node implements comparable node sorted set node empty set node parent string name level sorted set node children a unique name identify node cluster the level node cluster node string name level get name node string get name get level node get level check children add child node node synchronized boolean add child node child does node children synchronized boolean children get children node returned the returned set read synchronized set node get children get parent node node get parent hash code boolean equals object obj string string compare to node
1687	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Outputter.java	unrelated	package org apache hadoop tools rumen interface output sequence objects type t outputter t extends closeable initialize link outputter specific path init path path configuration conf throws io exception output object output t object throws io exception
1688	mapreduce\src\tools\org\apache\hadoop\tools\rumen\package-info.java	unrelated	package org apache hadoop tools rumen
1689	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Pair.java	unrelated	package org apache hadoop tools rumen pair car type cdr type car type car cdr type cdr pair car type car cdr type cdr car type first cdr type second
1690	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedConfigFile.java	unrelated	package org apache hadoop tools rumen parsed config file pattern job id pattern pattern heap pattern heap megabytes string queue string job name cluster map mb cluster reduce mb job map mb job reduce mb string job id boolean valid properties properties new properties maybe get int value string prop name string attr string value parsed config file string filename line string xml string
1691	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedHost.java	unrelated	package org apache hadoop tools rumen parsed host string rack name string node name todo following works rack host format change support arbitrary level network names pattern split pattern pattern todo handle arbitrary level network names number of distances string name component throws illegal argument exception hash code parsed host parse string name parsed host logged location loc logged location make logged location string get node name string get rack name expects broadest name first parsed host string rack name string node name boolean equals object distance parsed host
1692	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedLine.java	unrelated	package org apache hadoop tools rumen parsed line properties content log record type type string key w the value enclosed quotation marks occurrences escaped so escaped value essentially escaped sequence followed character character the straightforward regex capture unfortunately java regex implementation broken perform nfa dfa conversion expressions would lead backtracking stack overflow matching strings the following manual unfolding regex get rid backtracking string value regex match key value pairs input line capture group matches key capture group matches value without quotation marks pattern key val pair pattern compile key value parsed line string full line version protected log record type get type protected string get string key protected get long string key
1693	mapreduce\src\tools\org\apache\hadoop\tools\rumen\PossiblyDecompressedInputStream.java	pooling	package org apache hadoop tools rumen possibly decompressed input stream extends input stream decompressor decompressor input stream core input stream possibly decompressed input stream path input path configuration conf read throws io exception read byte buffer offset length throws io exception close throws io exception
1694	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Pre21JobHistoryConstants.java	scheduler	package org apache hadoop tools rumen pre job history constants job history files contain key value pairs keys belong enum it acts global namespace keys enum keys this enum contains values commonly used history log events since values history strings values name used places history file enum values pre regex jobhistory filename e jt identifier job id user name job name pattern jobhistory filename regex pre regex jobhistory conf filename e jt identifier job id conf xml pattern conf filename regex
1695	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RackNode.java	unrelated	package org apache hadoop tools rumen link rack node represents rack node cluster topology rack node extends node rack node string name level synchronized boolean add child node child get machine nodes belong rack set machine node get machines in rack
1696	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RandomSeedGenerator.java	unrelated	package org apache hadoop tools rumen the purpose generate new random seeds master seed this needed make random next calls rumen mumak deterministic mumak simulations become deterministically replayable in tools need many independent streams random numbers created dynamically we seed streams sub seeds returned random seed generator for slightly complicated approach generating multiple streams random numbers better theoretical guarantees see p l ecuyer r simard e j chen w d kelton an objected oriented random number package many long streams substreams operations research http www iro umontreal ca lecuyer papers html http www iro umontreal ca lecuyer myftp streams random seed generator log log log factory get log random seed generator md algorithm instance one thread thread local message digest md holder get seed string stream id master seed
1697	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ReduceAttempt20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen reduce attempt line history event emitter extends task attempt line event emitter list single event emitter non finals list single event emitter finals new linked list single event emitter non finals add all task event non final se es finals add new reduce attempt finished event emitter reduce attempt line history event emitter super reduce attempt finished event emitter extends history event maybe emit event parsed line line string task attempt id name list single event emitter se es return finals list single event emitter non final se es return non finals
1698	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ReduceTaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link reduce task attempt info represents information regard reduce task attempt reduce task attempt info extends task attempt info shuffle time merge time reduce time reduce task attempt info state state task info task info shuffle time get runtime b reduce b phase reduce task attempt get reduce runtime get runtime b shuffle b phase reduce task attempt get shuffle runtime get runtime b merge b phase reduce task attempt get merge runtime get runtime
1699	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ResourceUsageMetrics.java	unrelated	package org apache hadoop tools rumen captures resource usage metrics resource usage metrics implements writable deep compare cumulative cpu usage virtual memory usage physical memory usage heap usage resource usage metrics get cumulative cpu usage get cumulative cpu usage set cumulative cpu usage set cumulative cpu usage usage get virtual memory usage get virtual memory usage set virtual memory usage set virtual memory usage usage get physical memory usage get physical memory usage set physical memory usage set physical memory usage usage get total heap usage get heap usage set total heap usage set heap usage usage returns size serialized data size read fields data input throws io exception write data output throws io exception compare metric tree path loc throws deep inequality exception compare size resource usage metrics throws deep inequality exception deep compare deep compare tree path loc
1700	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RewindableInputStream.java	unrelated	package org apache hadoop tools rumen a simple wrapper make input stream rewindable it could made memory efficient grow internal buffer adaptively rewindable input stream extends input stream input stream input rewindable input stream input stream input rewindable input stream input stream input max bytes to remember read throws io exception read byte buffer offset length throws io exception close throws io exception input stream rewind throws io exception
1701	mapreduce\src\tools\org\apache\hadoop\tools\rumen\SingleEventEmitter.java	unrelated	package org apache hadoop tools rumen single event emitter history event maybe emit event parsed line line string name
1702	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Task20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen task line history event emitter extends history event emitter list single event emitter non finals list single event emitter finals new linked list single event emitter long original start time null task type original task type null non finals add new task started event emitter non finals add new task updated event emitter finals add new task finished event emitter finals add new task failed event emitter protected task line history event emitter super task started event emitter extends single event emitter history event maybe emit event parsed line line string task id name task updated event emitter extends single event emitter history event maybe emit event parsed line line string task id name task finished event emitter extends single event emitter history event maybe emit event parsed line line string task id name task failed event emitter extends single event emitter history event maybe emit event parsed line line string task id name list single event emitter se es return finals list single event emitter non final se es return non finals
1703	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskAttempt20LineEventEmitter.java	unrelated	package org apache hadoop tools rumen task attempt line event emitter extends history event emitter list single event emitter task event non final se es list single event emitter task event final se es default http port long original start time null org apache hadoop mapreduce task type original task type null task event non final se es add new task attempt started event emitter task event non final se es add new task attempt finished event emitter task event non final se es protected task attempt line event emitter super task attempt started event emitter extends history event maybe emit event parsed line line string task attempt id name task attempt finished event emitter extends history event maybe emit event parsed line line string task attempt id name task attempt unsuccessful completion event emitter extends history event maybe emit event parsed line line string task attempt id name
1704	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link task attempt info collection statistics particular task attempt gleaned job history job task attempt info protected state state protected task info task info protected task attempt info state state task info task info get link state task attempt state get run state get total runtime task attempt get runtime get link task info given task attempt task info get task info
1705	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskInfo.java	unrelated	package org apache hadoop tools rumen task info bytes in recs in bytes out recs out max memory resource usage metrics metrics task info bytes in recs in bytes out recs out task info bytes in recs in bytes out recs out may always match input bytes task get input bytes get input records match output bytes get output bytes get output records get task memory resource usage metrics get resource usage metrics
1706	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TopologyBuilder.java	unrelated	package org apache hadoop tools rumen building cluster topology topology builder set parsed host hosts new hash set parsed host process one link history event the link history event processed process history event event process collection job conf link properties we restrict called the job conf properties added process properties conf request builder build object once called link topology builder would accept events job conf properties logged network topology build process task started event task started event event process task attempt unsuccessful completion event process task attempt finished event task attempt finished event event record parsed host string host name preferred location for splits string splits
1707	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TraceBuilder.java	unrelated	package org apache hadoop tools rumen the main driver rumen parser trace builder extends configured implements tool log log log factory get log trace builder run method failed exit code topology builder topology builder new topology builder outputter logged job trace writer outputter logged network topology topology writer my options class extends input demuxer input demuxer class default input demuxer class extends outputter clazz trace outputter default outputter path trace output path topology output list path inputs new linked list path my options string args configuration conf throws file not found exception compare history file names full paths job history file name format lexicographic sort history file names result order jobs submission times history logs comparator processes input file folder argument if input file directly considered processing trace builder if input folder history logs input folder considered processing if recursive true input path recursively scanned job history logs processing trace builder note if input represents globbed path first flattened individual paths represented globbed input path considered processing find history logs list path process input argument string input configuration conf main string args trace builder builder new trace builder result run method failed exit code try catch throwable finally string apply parser string file name pattern pattern matcher matcher pattern matcher file name matcher matches return matcher group string either history log file config file otherwise especially crc files return null string extract job id string file name string job id apply parser file name job history jobhistory filename regex job id null return job id boolean job conf xml string file name input stream input string job id apply parser file name job history conf filename regex job id null return job id null run string args throws exception my options options new my options args get conf trace writer options clazz trace outputter new instance trace writer init options trace output get conf topology writer new default outputter logged network topology topology writer init options topology output get conf try finally return process job conf properties properties job builder job builder job builder process properties topology builder process properties process job history job history parser parser job builder job builder history event e e parser next event null parser close finish io utils cleanup log trace writer topology writer
1708	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TreePath.java	unrelated	package org apache hadoop tools rumen this describes path node root we use compare two trees rumen unit tests if trees identical chain converted describes path root fields compare tree path tree path parent string field name index tree path tree path parent string field name tree path tree path parent string field name index string string
1709	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Version20LogInterfaceUtils.java	unrelated	package org apache hadoop tools rumen this exists hold bunch utils it never instantiated version log interface utils task type get task type string task type
1710	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieCluster.java	unrelated	package org apache hadoop tools rumen link zombie cluster rebuilds cluster topology using information obtained job history logs zombie cluster extends abstract cluster story node root construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the network topology the default node setting zombie cluster logged network topology topology machine node default node construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the default node setting zombie cluster path path machine node default node configuration conf throws io exception construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the default node setting zombie cluster input stream input machine node default node throws io exception node get cluster topology build cluster logged network topology topology
1711	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieJob.java	unrelated	package org apache hadoop tools rumen link zombie job layer link logged job raw json objects each link zombie job object represents job job history for everything exists job history contents returned unchanged faithfully to get input splits non exist task non exist task attempt ill formed task attempt proper objects made statistical sketches zombie job implements job story log log log factory get log zombie job logged job job map task id logged task logged task map map task attempt id logged task attempt logged task attempt map random random input split splits cluster story cluster job conf job conf seed num random seeds boolean random seed false map logged discrete cdf cdf random generator interpolator map todo fix zombie job initialize correctly observed data rack local over node local rack remote over node local this constructor creates link zombie job semantics link logged job passed parameter the dead job zombie job instance based the cluster topology dead job ran this argument null knowledge cluster topology seed random number generator filling information available zombie job zombie job logged job job cluster story cluster seed job null job job cluster cluster random new random seed seed seed random seed true this constructor creates link zombie job semantics link logged job passed parameter the dead job zombie job instance based the cluster topology dead job ran this argument null knowledge cluster topology zombie job logged job job cluster story cluster job cluster system nano time state convert state values status status values success else status values failed else status values killed else synchronized job conf get job conf job conf null return job conf input split get input splits splits null return splits string get name string job name job get job name job name null else job id get job id return job id name get logged job get job id sanitize value old val default val string name string id old val return old val get number maps return sanitize value job get total maps number maps job get job id get number reduces return sanitize value job get total reduces number reduces job get job id values get outcome return job get outcome get submission time return job get submit time job get relative time string get queue name string queue job get queue return queue null job conf default queue name queue getting number map tasks actually logged trace get num logged maps return job get map tasks size getting number reduce tasks actually logged trace get num logged reduces return job get reduce tasks size mask job id part link task id raw link task id read trace task id mask task id task id task id job id job id new job id task type task type task id get task type return new task id job id task type task id get id mask job id part link task attempt id raw link task attempt id read trace task attempt id mask attempt id task attempt id attempt id job id job id new job id
1712	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieJobProducer.java	unrelated	package org apache hadoop tools rumen producing link job story job trace zombie job producer implements job story producer job trace reader reader zombie cluster cluster boolean random seed false random seed zombie job producer job trace reader reader zombie cluster cluster constructor path json trace file possibly compressed the topology cluster corresponds jobs trace the argument null knowledge cluster topology zombie job producer path path zombie cluster cluster configuration conf constructor path json trace file possibly compressed the topology cluster corresponds jobs trace the argument null knowledge cluster topology use deterministic seed zombie job producer path path zombie cluster cluster constructor the input stream json trace the topology cluster corresponds jobs trace the argument null knowledge cluster topology zombie job producer input stream input zombie cluster cluster constructor the input stream json trace the topology cluster corresponds jobs trace the argument null knowledge cluster topology use deterministic seed zombie job producer input stream input zombie cluster cluster zombie job get next job throws io exception close throws io exception
