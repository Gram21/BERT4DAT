	file	label	text
0	common\src\java\org\apache\hadoop\HadoopIllegalArgumentException.java	unrelated	package org apache hadoop indicates method passed illegal invalid argument this exception thrown instead illegal argument exception differentiate exception thrown hadoop implementation one thrown jdk hadoop illegal argument exception extends illegal argument exception serial version uid l constructs exception specified detail message hadoop illegal argument exception string message super message
1	common\src\java\org\apache\hadoop\HadoopVersionAnnotation.java	unrelated	package org apache hadoop a package attribute captures version hadoop compiled hadoop version annotation get hadoop version string version get username compiled hadoop string user get date hadoop compiled string date get url subversion repository string url get subversion revision string revision get branch compiled string branch get checksum source files hadoop compiled string src checksum
2	common\src\java\org\apache\hadoop\classification\InterfaceAudience.java	unrelated	package org apache hadoop classification annotation inform users package method intended audience interface audience intended use project application intended project specified annotation for example common hdfs map reduce zoo keeper h base string value intended use within hadoop interface audience audience exist
3	common\src\java\org\apache\hadoop\classification\InterfaceStability.java	unrelated	package org apache hadoop classification annotation inform users much rely particular package method changing time interface stability can evolve retaining compatibility minor release boundaries break compatibility major release ie stable evolving break compatibility minor release e x evolving no guarantee provided reliability stability across level release granularity unstable
4	common\src\java\org\apache\hadoop\classification\tools\ExcludePrivateAnnotationsJDiffDoclet.java	unrelated	package org apache hadoop classification tools a href http java sun com javase docs jdk api javadoc doclet doclet excluding elements annotated link org apache hadoop classification interface audience private link org apache hadoop classification interface audience limited private it delegates j diff doclet takes options exclude private annotations j diff doclet language version language version return language version java boolean start root doc root system println exclude private annotations j diff doclet get simple name return j diff start root doc processor process root option length string option integer length stability options option length option length null return length return j diff option length option boolean valid options string options doc error reporter reporter stability options valid options options reporter string filtered options stability options filter options options return j diff valid options filtered options reporter
5	common\src\java\org\apache\hadoop\classification\tools\ExcludePrivateAnnotationsStandardDoclet.java	unrelated	package org apache hadoop classification tools a href http java sun com javase docs jdk api javadoc doclet doclet excluding elements annotated link org apache hadoop classification interface audience private link org apache hadoop classification interface audience limited private it delegates standard doclet takes options exclude private annotations standard doclet language version language version return language version java boolean start root doc root system println exclude private annotations standard doclet get simple name return standard start root doc processor process root option length string option integer length stability options option length option length null return length return standard option length option boolean valid options string options doc error reporter reporter stability options valid options options reporter string filtered options stability options filter options options return standard valid options filtered options reporter
6	common\src\java\org\apache\hadoop\classification\tools\package-info.java	unrelated	package org apache hadoop classification tools
7	common\src\java\org\apache\hadoop\classification\tools\RootDocProcessor.java	unrelated	package org apache hadoop classification tools process link root doc substituting nested proxy objects exclude elements private limited private annotations p based code http www sixlegs com blog java exclude javadoc tag html root doc processor string stability stability options unstable option root doc process root doc root return root doc process root root doc object process object obj class type obj null return null class cls obj get class cls get name starts with com sun return get proxy obj else obj instanceof object class component type type array type get component type cls get component type object array object obj object new array object array new instance component type array length array length new array process array component type return new array return obj map object object proxies new weak hash map object object object get proxy object obj object proxy proxies get obj proxy null proxy proxy new proxy instance obj get class get class loader obj get class get interfaces new exclude handler obj proxies put obj proxy return proxy exclude handler implements invocation handler object target exclude handler object target target target object invoke object proxy method method object args throws throwable string method name method get name target instanceof doc method name equals included doc doc doc target return exclude doc doc included target instanceof root doc method name equals return filter root doc target class doc else method name equals specified classes return filter root doc target specified classes class doc else method name equals specified packages return filter root doc target specified packages package doc else target instanceof class doc filtered args method name equals methods return filter class doc target methods true method doc else method name equals fields return filter class doc target fields true field doc else method name equals inner classes return filter class doc target inner classes true class doc else method name equals constructors return filter class doc target constructors true constructor doc else target instanceof package doc method name equals classes filtered args return filter package doc target classes true class doc else return filter package doc target classes class doc else method name equals annotation types return filter package doc target annotation types annotation type doc else method name equals enums return filter package doc target enums class doc else method name equals errors return filter package doc target errors class doc else method name equals exceptions return filter package doc target exceptions class doc else method name equals interfaces return filter package doc target interfaces class doc else method name equals ordinary classes return filter package doc target ordinary classes class doc args null method name equals compare to method name equals equals method name equals overrides method name equals subclass of args unwrap args try return process method invoke target args method get return type catch invocation target exception e throw e get target exception boolean exclude doc doc annotation desc annotations null doc instanceof program element doc annotations program element doc doc annotations else doc instanceof package doc annotations package doc doc annotations
8	common\src\java\org\apache\hadoop\classification\tools\StabilityOptions.java	unrelated	package org apache hadoop classification tools stability options string stable option stable string evolving option evolving string unstable option unstable integer option length string option string opt option lower case opt equals unstable option return opt equals evolving option return opt equals stable option return return null valid options string options doc error reporter reporter options length string opt options lower case opt equals unstable option root doc processor stability unstable option else opt equals evolving option root doc processor stability evolving option else opt equals stable option root doc processor stability stable option string filter options string options list string options list new array list string options length options equals ignore case unstable option options equals ignore case evolving option options equals ignore case stable option options list add options string filtered options new string options list size string option options list filtered options option return filtered options
9	common\src\java\org\apache\hadoop\conf\Configurable.java	unrelated	package org apache hadoop conf something may configured link configuration configurable set configuration used object set conf configuration conf return configuration used object configuration get conf
10	common\src\java\org\apache\hadoop\conf\Configuration.java	unrelated	package org apache hadoop conf provides access configuration parameters id resources resources p configurations specified resources a resource contains set name value pairs xml data each resource named either code string code link path if named code string code classpath examined file name if named code path code local filesystem examined directly without referring classpath p unless explicitly turned hadoop default specifies two resources loaded order classpath ol li tt href doc root core default html core default xml tt read defaults hadoop li li tt core site xml tt site specific configuration given hadoop installation li ol applications may add additional resources loaded subsequent resources order added id final params final parameters p configuration parameters may declared once resource declares value subsequently loaded resource alter value for example one might define parameter tt pre lt property gt lt name gt dfs client buffer dir lt name gt lt value gt tmp hadoop dfs client lt value gt b lt gt true lt gt b lt property gt pre tt administrators typically define parameters tt core site xml tt values user applications may alter id variable expansion variable expansion p value strings first processed variable expansion the available properties ol li other properties defined configuration name undefined li li properties link system get properties li ol p for example configuration resource contains following property definitions tt pre lt property gt lt name gt basedir lt name gt lt value gt user user name lt value gt lt property gt lt property gt lt name gt tempdir lt name gt lt value gt basedir tmp lt value gt lt property gt pre tt when tt conf get tempdir tt called tt basedir tt resolved another property configuration tt user name tt would ordinarily resolved value system property name configuration implements iterable map entry string string writable log log log factory get log configuration boolean quietmode true list configuration resources array list object resources new array list object the value reported setting resource key set code rather file resource string unknown resource unknown list configuration parameters marked b b set string parameters new hash set string boolean load defaults true configuration objects weak hash map configuration object registry new weak hash map configuration object list default resources resources loaded order list entries copy on write array list string default resources new copy on write array list string map class loader map string class cache classes new weak hash map class loader map string class stores mapping key resource modifies loads key recently hash map string string updating resource class keep information keys replace deprecated ones this stores new keys replace deprecated keys also gives provision custom message deprecated key replaced it also provides method get appropriate warning message logged whenever deprecated key used deprecated key info string new keys string custom message boolean accessed deprecated key info string new keys string custom message new keys new keys custom message custom message accessed false method provide warning message it gives custom message non null default message otherwise string get warning message string key string warning
11	common\src\java\org\apache\hadoop\conf\Configured.java	unrelated	package org apache hadoop conf base things may configured link configuration configured implements configurable configuration conf construct configured configured null construct configured configured configuration conf set conf conf inherit javadoc set conf configuration conf conf conf inherit javadoc configuration get conf return conf
12	common\src\java\org\apache\hadoop\conf\ConfServlet.java	unrelated	package org apache hadoop conf a servlet print running configuration data conf servlet extends http servlet serial version uid l string format json json string format xml xml string format param format return configuration daemon hosting servlet this populated http server starts configuration get conf from context configuration conf configuration get servlet context get attribute http server conf context attribute assert conf null return conf get http servlet request request http servlet response response throws servlet exception io exception do authorization http server administrator access get servlet context request response return string format request get parameter format param null format format format xml format xml equals format response set content type text xml charset utf else format json equals format response set content type application json charset utf writer response get writer try write response get conf from context format catch bad format exception bfe response send error http servlet response sc bad request bfe get message close guts servlet extracted easy testing write response configuration conf writer string format throws io exception bad format exception format json equals format configuration dump configuration conf else format xml equals format conf write xml else throw new bad format exception bad format format bad format exception extends exception serial version uid l bad format exception string msg super msg
13	common\src\java\org\apache\hadoop\conf\Reconfigurable.java	unrelated	package org apache hadoop conf something whose link configuration changed run time reconfigurable extends configurable change configuration property object value specified change configuration property object value specified return previous value configuration property set null previously set if new val null set property default value if property cannot changed throw link reconfiguration exception string reconfigure property string property string new val throws reconfiguration exception return whether given property changeable run time if property reconfigurable returns true property change conf throw exception changing property boolean property reconfigurable string property return properties changed run time collection string get reconfigurable properties
14	common\src\java\org\apache\hadoop\conf\ReconfigurableBase.java	unrelated	package org apache hadoop conf utility base implementing reconfigurable subclasses reconfigure property impl change individual properties get reconfigurable properties get properties changed run time reconfigurable base extends configured implements reconfigurable log log log factory get log reconfigurable base construct reconfigurable base reconfigurable base super new configuration construct reconfigurable base link configuration conf reconfigurable base configuration conf super conf null new configuration conf inherit doc this method makes change objects link configuration calls reconfigure property impl update internal data structures this method cannot overridden subclasses instead reconfigure property string reconfigure property string property string new val throws reconfiguration exception property reconfigurable property log info changing property property new val string old val synchronized get conf old val get conf get property reconfigure property impl property new val new val null get conf set property new val else get conf unset property return old val else throw new reconfiguration exception property new val get conf get property inherit doc subclasses must collection string get reconfigurable properties inherit doc subclasses may wish efficient implementation boolean property reconfigurable string property return get reconfigurable properties contains property change configuration property subclasses must this method applies change internal data structures derived configuration property changed if object owns reconfigurable objects reconfigure property called recursively make sure make sure configuration objects updated protected reconfigure property impl string property string new val throws reconfiguration exception
15	common\src\java\org\apache\hadoop\conf\ReconfigurationException.java	unrelated	package org apache hadoop conf exception indicating configuration property cannot changed run time reconfiguration exception extends exception serial version uid l string property string new val string old val construct exception message string construct message string property string new val string old val string message could change property property old val null message old val new val null message new val return message create new instance link reconfiguration exception reconfiguration exception super could change configuration property null new val null old val null create new instance link reconfiguration exception reconfiguration exception string property string new val string old val throwable cause super construct message property new val old val cause property property new val new val old val old val create new instance link reconfiguration exception reconfiguration exception string property string new val string old val super construct message property new val old val property property new val new val old val old val get property cannot changed string get property return property get value property supposed changed string get new value return new val get old value property cannot changed string get old value return old val
16	common\src\java\org\apache\hadoop\conf\ReconfigurationServlet.java	unrelated	package org apache hadoop conf a servlet changing node configuration reloads configuration file verifies whether changes possible asks admin approve change reconfiguration servlet extends http servlet serial version uid l log log log factory get log reconfiguration servlet prefix used fing attribute holding reconfigurable given request get attribute prefix servlet path string conf servlet reconfigurable prefix conf servlet reconfigurable inherit doc init throws servlet exception super init reconfigurable get reconfigurable http servlet request req log info servlet path req get servlet path log info getting attribute conf servlet reconfigurable prefix req get servlet path return reconfigurable get servlet context get attribute conf servlet reconfigurable prefix req get servlet path print header print writer string node name print html head printf title reconfiguration utility title n string escape utils escape html node name print head body n printf reconfiguration utility n string escape utils escape html node name print footer print writer print body html n print configuration options changed print conf print writer reconfigurable reconf configuration old conf reconf get conf configuration new conf new configuration collection reconfiguration util property change changes reconfiguration util get changed properties new conf old conf boolean change ok true println form action method post println table border println tr th property th th old value th println th new value th th th tr reconfiguration util property change c changes print tr td reconf property reconfigurable c prop print font color red string escape utils escape html c prop font change ok false else print string escape utils escape html c prop print input type hidden name string escape utils escape html c prop value string escape utils escape html c new val print td td c old val null default string escape utils escape html c old val td td c new val null default string escape utils escape html c new val td print tr n println table change ok println p font color red warning properties marked red changed next restart font p println input type submit value apply println form enumeration string get params http servlet request req return enumeration string req get parameter names apply configuratio changes admin approved apply changes print writer reconfigurable reconf http servlet request req throws io exception reconfiguration exception configuration old conf reconf get conf configuration new conf new configuration enumeration string params get params req synchronized old conf params more elements string raw param params next element string param string escape utils unescape html raw param string value string escape utils unescape html req get parameter raw param value null value equals new conf get raw param value equals default value equals null value equals value equals default value equals null value equals old conf get raw param null println p changed string escape utils escape html param string escape utils escape html old conf get raw param default p reconf reconfigure property param null else value equals default value equals null value equals old conf get raw param null old conf get raw param equals value change default value different value old conf get
17	common\src\java\org\apache\hadoop\conf\ReconfigurationUtil.java	unrelated	package org apache hadoop conf reconfiguration util property change string prop string old val string new val property change string prop string new val string old val prop prop new val new val old val old val collection property change get changed properties configuration new conf configuration old conf map string property change changes new hash map string property change iterate old configuration map entry string string old entry old conf string prop old entry get key string old val old entry get value string new val new conf get raw prop new val null new val equals old val changes put prop new property change prop new val old val iterate new configuration look properties present old conf map entry string string new entry new conf string prop new entry get key string new val new entry get value old conf get prop null changes put prop new property change prop new val null return changes values
18	common\src\java\org\apache\hadoop\fs\AbstractFileSystem.java	unrelated	package org apache hadoop fs this provides implementors hadoop file system analogous vfs unix applications access instead access files across file systems using link file context pathnames passed abstract file system fully qualified uri matches file system ie scheme authority slash relative name assumed relative root file system abstract file system log log log factory get log abstract file system recording statistics per file system map uri statistics statistics table new hash map uri statistics cache constructors file system map class constructor constructor cache new concurrent hash map class constructor class uri config args new class uri configuration the statistics file system protected statistics statistics uri uri statistics get statistics return statistics prohibits names contain boolean valid name string src check string tokenizer tokens new string tokenizer src path separator tokens more tokens string element tokens next token element equals element equals element index of return false return true create object given initialize conf t t new instance class t class uri uri configuration conf t result try constructor t meth constructor t constructor cache get class meth null meth class get declared constructor uri config args meth set accessible true constructor cache put class meth result meth new instance uri conf catch exception e throw new runtime exception e return result create file system instance specified uri using conf the conf used find name implements file system the conf also passed file system configuration found abstract file system create file system uri uri configuration conf throws unsupported file system exception class clazz conf get class fs abstract file system uri get scheme impl null clazz null throw new unsupported file system exception no abstract file system scheme uri get scheme return abstract file system new instance clazz uri conf get statistics particular file system used key lookup statistics table only scheme authority part uri used protected synchronized statistics get statistics uri uri string scheme uri get scheme scheme null throw new illegal argument exception scheme defined uri uri uri base uri get base uri uri statistics result statistics table get base uri result null result new statistics scheme statistics table put base uri result return result uri get base uri uri uri string scheme uri get scheme string authority uri get authority string base uri string scheme authority null base uri string base uri string authority else base uri string base uri string return uri create base uri string synchronized clear statistics statistics stat statistics table values stat reset prints statistics file systems synchronized print statistics map entry uri statistics pair statistics table entry set system println file system pair get key get scheme pair get key get authority pair get value protected synchronized map uri statistics get all statistics map uri statistics stats map new hash map uri statistics statistics table size map entry uri statistics pair statistics table entry set uri key pair get key statistics value pair get value statistics new stats obj new statistics value stats map put uri create key string new stats obj return stats map the main factory method creating file system get file
19	common\src\java\org\apache\hadoop\fs\AvroFSInput.java	unrelated	package org apache hadoop fs adapts link fs data input stream avro seekable input avro fs input implements closeable seekable input fs data input stream stream len construct given link fs data input stream length avro fs input fs data input stream len stream len len construct given link file context link path avro fs input file context fc path p throws io exception file status status fc get file status p len status get len stream fc open p length return len read byte b len throws io exception return stream read b len seek p throws io exception stream seek p tell throws io exception return stream get pos close throws io exception stream close
20	common\src\java\org\apache\hadoop\fs\BlockLocation.java	unrelated	package org apache hadoop fs a block location lists hosts offset length block block location implements writable register ctor writable factories set factory block location new writable factory writable new instance return new block location string hosts hostnames datanodes string names hostname port number datanodes string topology paths full path name network topology offset offset block file length boolean corrupt default constructor block location new string new string l l constructor host name offset length block location string names string hosts offset length names hosts offset length false constructor host name offset length corrupt flag block location string names string hosts offset length boolean corrupt names null names new string else names names hosts null hosts new string else hosts hosts offset offset length length topology paths new string corrupt corrupt constructor host name network topology offset length block location string names string hosts string topology paths offset length names hosts topology paths offset length false constructor host name network topology offset length corrupt flag block location string names string hosts string topology paths offset length boolean corrupt names hosts offset length corrupt topology paths null topology paths new string else topology paths topology paths get list hosts hostname hosting block string get hosts throws io exception hosts null hosts length return new string else return hosts get list names hostname port hosting block string get names throws io exception names null names length return new string else return names get list network topology paths hosts the last component path host string get topology paths throws io exception topology paths null topology paths length return new string else return topology paths get start offset file associated block get offset return offset get length block get length return length get corrupt flag boolean corrupt return corrupt set start offset file associated block set offset offset offset offset set length block set length length length length set corrupt flag set corrupt boolean corrupt corrupt corrupt set hosts hosting block set hosts string hosts throws io exception hosts null hosts new string else hosts hosts set names host port hosting block set names string names throws io exception names null names new string else names names set network topology paths hosts set topology paths string topology paths throws io exception topology paths null topology paths new string else topology paths topology paths implement write writable write data output throws io exception write long offset write long length write boolean corrupt write int names length names length text name new text names name write write int hosts length hosts length text host new text hosts host write write int topology paths length topology paths length text host new text topology paths host write implement read fields writable read fields data input throws io exception offset read long length read long corrupt read boolean num names read int names new string num names num names text name new text name read fields names name string num hosts read int hosts new string num hosts num hosts text host new text host read fields hosts host string num
21	common\src\java\org\apache\hadoop\fs\BufferedFSInputStream.java	unrelated	package org apache hadoop fs a optimizes reading fs input stream bufferring buffered fs input stream extends buffered input stream implements seekable positioned readable creates code buffered fs input stream code specified buffer size saves argument input stream code code later use an internal buffer array length code size code created stored code buf code buffered fs input stream fs input stream size super size get pos throws io exception return fs input stream get pos count pos skip n throws io exception n return seek get pos n return n seek pos throws io exception pos return optimize check pos buffer end fs input stream get pos start end count pos start pos end pos pos start return invalidate buffer pos count fs input stream seek pos boolean seek to new source target pos throws io exception pos count return fs input stream seek to new source target pos read position byte buffer offset length throws io exception return fs input stream read position buffer offset length read fully position byte buffer offset length throws io exception fs input stream read fully position buffer offset length read fully position byte buffer throws io exception fs input stream read fully position buffer
22	common\src\java\org\apache\hadoop\fs\ChecksumException.java	unrelated	package org apache hadoop fs thrown checksum errors checksum exception extends io exception serial version uid l pos checksum exception string description pos super description pos pos get pos return pos
23	common\src\java\org\apache\hadoop\fs\ChecksumFileSystem.java	unrelated	package org apache hadoop fs abstract checksumed file system it provide basice implementation checksumed file system creates checksum file raw file it generates verifies checksums client side checksum file system extends filter file system byte checksum version new byte c r c bytes per checksum boolean verify checksum true get approx chk sum length size return checksum fs output summer chksum as fraction size checksum file system file system fs super fs set conf configuration conf super set conf conf conf null bytes per checksum conf get int local file system config keys local fs bytes per checksum key local file system config keys local fs bytes per checksum default set whether verify checksum set verify checksum boolean verify checksum verify checksum verify checksum get raw file system file system get raw file system return fs return name checksum file associated file path get checksum file path file return new path file get parent file get name crc return true iff file checksum file name boolean checksum file path file string name file get name return name starts with name ends with crc return length checksum file given size actual file get checksum file length path file file size return get checksum length file size get bytes per sum return bytes per checksum get bytes per sum return bytes per checksum get sum buffer size bytes per sum buffer size default buffer size get conf get int local file system config keys local fs stream buffer size key local file system config keys local fs stream buffer size default proportional buffer size buffer size bytes per sum return math max bytes per sum math max proportional buffer size default buffer size for open fs input stream it verifies data matches checksums checksum fs input checker extends fs input checker log log log factory get log fs input checker checksum file system fs fs data input stream datas fs data input stream sums header length bytes per sum file len l checksum fs input checker checksum file system fs path file throws io exception fs file fs get conf get int local file system config keys local fs stream buffer size key local file system config keys local fs stream buffer size default checksum fs input checker checksum file system fs path file buffer size throws io exception super file fs get file status file get replication datas fs get raw file system open file buffer size fs fs path sum file fs get checksum file file try sum buffer size fs get sum buffer size fs get bytes per sum buffer size sums fs get raw file system open sum file sum buffer size byte version new byte checksum version length sums read fully version arrays equals version checksum version throw new io exception not checksum file sum file bytes per sum sums read int set fs verify checksum new pure java crc bytes per sum catch file not found exception e quietly ignore set fs verify checksum null catch io exception e loudly ignore log warn problem opening checksum file file
24	common\src\java\org\apache\hadoop\fs\ChecksumFs.java	unrelated	package org apache hadoop fs abstract checksumed fs it provide basic implementation checksumed fs creates checksum file raw file it generates verifies checksums client side checksum fs extends filter fs byte checksum version new byte c r c default bytes per checksum boolean verify checksum true get approx chk sum length size return checksum fs output summer chksum as fraction size checksum fs abstract file system fs throws io exception uri syntax exception super fs default bytes per checksum get my fs get server defaults get bytes per checksum set whether verify checksum set verify checksum boolean verify checksum verify checksum verify checksum get raw file system abstract file system get raw fs return get my fs return name checksum file associated file path get checksum file path file return new path file get parent file get name crc return true iff file checksum file name boolean checksum file path file string name file get name return name starts with name ends with crc return length checksum file given size actual file get checksum file length path file file size return get checksum length file size get bytes per sum return bytes per checksum get bytes per sum return default bytes per checksum get sum buffer size bytes per sum buffer size throws io exception default buffer size get my fs get server defaults get file buffer size proportional buffer size buffer size bytes per sum return math max bytes per sum math max proportional buffer size default buffer size for open fs input stream it verifies data matches checksums checksum fs input checker extends fs input checker log log log factory get log fs input checker header length checksum fs fs fs data input stream datas fs data input stream sums bytes per sum file len l checksum fs input checker checksum fs fs path file throws io exception unresolved link exception fs file fs get server defaults get file buffer size checksum fs input checker checksum fs fs path file buffer size throws io exception unresolved link exception super file fs get file status file get replication datas fs get raw fs open file buffer size fs fs path sum file fs get checksum file file try sum buffer size fs get sum buffer size fs get bytes per sum buffer size sums fs get raw fs open sum file sum buffer size byte version new byte checksum version length sums read fully version arrays equals version checksum version throw new io exception not checksum file sum file bytes per sum sums read int set fs verify checksum new pure java crc bytes per sum catch file not found exception e quietly ignore set fs verify checksum null catch io exception e loudly ignore log warn problem opening checksum file file ignoring exception e set fs verify checksum null get checksum file pos data pos return header length data pos bytes per sum protected get chunk position data pos return data pos bytes per sum bytes per sum available throws io exception return datas available super available read position byte
25	common\src\java\org\apache\hadoop\fs\CommonConfigurationKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used common code it inherits publicly documented configuration keys adds unsupported keys common configuration keys extends common configuration keys public default location user home directories string fs home dir key fs home dir default value fs home dir key string fs home dir default user default umask files created hdfs string fs permissions umask key fs permissions umask mode default value fs permissions umask key fs permissions umask default https issues apache org jira browse hadoop string fs client buffer dir key fs client buffer dir how often rpc client send pings rpc server string ipc ping interval key ipc ping interval default value ipc ping interval key ipc ping interval default enables pings rpc client server string ipc client ping key ipc client ping default value ipc client ping key boolean ipc client ping default true responses larger logged string ipc server rpc max response size key ipc server max response size default value ipc server rpc max response size key ipc server rpc max response size default number threads rpc server reading socket string ipc server rpc read threads key ipc server read threadpool size default value ipc server rpc read threads key ipc server rpc read threads default how many calls per handler allowed queue string ipc server handler queue size key ipc server handler queue size default value ipc server handler queue size key ipc server handler queue size default internal buffer size lzo compressor decompressors string io compression codec lzo buffersize key io compression codec lzo buffersize default value io compression codec lzo buffersize key io compression codec lzo buffersize default this specifying implementation mappings hostnames racks belong string net topology configured node mapping key net topology configured node mapping internal buffer size snappy compressor decompressors string io compression codec snappy buffersize key io compression codec snappy buffersize default value io compression codec snappy buffersize key io compression codec snappy buffersize default
26	common\src\java\org\apache\hadoop\fs\CommonConfigurationKeysPublic.java	unrelated	package org apache hadoop fs this contains constants configuration keys used common code it includes publicly documented configuration keys in general used directly use common configuration keys instead common configuration keys public the keys see href doc root core default html core default xml string io native lib available key io native lib available default value io native lib available key boolean io native lib available default true see href doc root core default html core default xml string net topology script number args key net topology script number args default value net topology script number args key net topology script number args default fs keys see href doc root core default html core default xml string fs default name key fs default fs default value fs default name key string fs default name default file see href doc root core default html core default xml string fs df interval key fs df interval default value fs df interval key fs df interval default defaults specified following keys see href doc root core default html core default xml string net topology script file name key net topology script file name see href doc root core default html core default xml string net topology node switch mapping impl key net topology node switch mapping impl see href doc root core default html core default xml string fs trash checkpoint interval key fs trash checkpoint interval default value fs trash checkpoint interval key fs trash checkpoint interval default tbd code still using hardcoded values e g fs automatic close instead constant e g fs automatic close key not used anywhere looks like default value fs local block size fs local block size default see href doc root core default html core default xml string fs automatic close key fs automatic close default value fs automatic close key boolean fs automatic close default true see href doc root core default html core default xml string fs file impl key fs file impl see href doc root core default html core default xml string fs ftp host key fs ftp host see href doc root core default html core default xml string fs ftp host port key fs ftp host port see href doc root core default html core default xml string fs trash interval key fs trash interval default value fs trash interval key fs trash interval default see href doc root core default html core default xml string io mapfile bloom size key io mapfile bloom size default value io mapfile bloom size key io mapfile bloom size default see href doc root core default html core default xml string io mapfile bloom error rate key io mapfile bloom error rate default value io mapfile bloom error rate key io mapfile bloom error rate default f codec implements lzo compression algorithm string io compression codec lzo class key io compression codec lzo see href doc root core default html core default xml string io map index interval key io map index interval default value io map index interval default io map index interval default
27	common\src\java\org\apache\hadoop\fs\ContentSummary.java	unrelated	package org apache hadoop fs store summary content directory file content summary implements writable length file count directory count quota space consumed space quota constructor content summary constructor content summary length file count directory count length file count directory count l length l constructor content summary length file count directory count quota space consumed space quota length length file count file count directory count directory count quota quota space consumed space consumed space quota space quota get length return length get directory count return directory count get file count return file count return directory quota get quota return quota retuns disk space consumed get space consumed return space consumed returns disk space quota get space quota return space quota inherit doc write data output throws io exception write long length write long file count write long directory count write long quota write long space consumed write long space quota inherit doc read fields data input throws io exception length read long file count read long directory count read long quota read long space consumed read long space quota read long output format dir count file count content size file name string string format output format quota remaining quata space quota space quota rem dir count file count content size file name string quota string format string space quota string format the header string header string format string format replace directories files bytes string quota header string format quota string format space quota string format quota remaining quota space quota reamaining quota header return header output q option false output directory count file count content size q option true output quota remaining quota well string get header boolean q option return q option quota header header inherit doc string string return string true return representation object output format q option false output directory count file count content size q option true output quota remaining quota well string string boolean q option string prefix q option string quota str none string quota rem inf string space quota str none string space quota rem inf quota quota str long string quota quota rem long string quota directory count file count space quota space quota str long string space quota space quota rem long string space quota space consumed prefix string format quota string format space quota string format quota str quota rem space quota str space quota rem return prefix string format string format directory count file count length
28	common\src\java\org\apache\hadoop\fs\CreateFlag.java	unrelated	package org apache hadoop fs create flag specifies file create semantic users combine flags like br code enum set create flag create create flag append code p use create flag follows ol li create create file exist else throw file already exists li li append append file exists else throw file not found exception li li overwrite truncate file exists else throw file not found exception li li create append create file exist else append existing file li li create overwrite create file exist else overwrite existing file li ol following combination valid result link hadoop illegal argument exception ol li append overwrite li li create append overwrite li ol enum create flag create file see javadoc description already exists create short x truncate overwrite file same posix o trunc see javadoc description overwrite short x append file see javadoc description append short x short mode create flag short mode mode mode short get mode return mode validate create flag throw exception invalid validate enum set create flag flag flag null flag empty throw new hadoop illegal argument exception flag specify options boolean append flag contains append boolean overwrite flag contains overwrite both append overwrite error append overwrite throw new hadoop illegal argument exception flag both append overwrite options cannot enabled validate create flag create operation validate object path boolean path exists enum set create flag flag throws io exception validate flag boolean append flag contains append boolean overwrite flag contains overwrite path exists append overwrite throw new file already exists exception file already exists path string append overwrite option must specified flag else flag contains create throw new file not found exception non existing file path string create option specified flag
29	common\src\java\org\apache\hadoop\fs\DelegateToFileSystem.java	unrelated	package org apache hadoop fs implementation abstract file system based existing implementation link file system delegate to file system extends abstract file system protected file system fs impl protected delegate to file system uri uri file system fs impl configuration conf string supported scheme boolean authority required throws io exception uri syntax exception super uri supported scheme authority required file system get default uri conf get port fs impl fs impl fs impl initialize uri conf fs impl statistics get statistics path get initial working directory return fs impl get initial working directory fs data output stream create internal path f enum set create flag flag fs permission absolute permission buffer size short replication block size progressable progress bytes per checksum boolean create parent throws io exception check path f default impl assumes permissions matter calling regular create good enough f ss implement permissions create parent parent must exist since create makes parent dirs automatically must throw exception parent exist file status stat get file status f get parent stat null throw new file not found exception missing parent f stat directory throw new parent not directory exception parent dir f parent exist go ahead create file return fs impl primitive create f absolute permission flag buffer size replication block size progress bytes per checksum boolean delete path f boolean recursive throws io exception check path f return fs impl delete f recursive block location get file block locations path f start len throws io exception check path f return fs impl get file block locations f start len file checksum get file checksum path f throws io exception check path f return fs impl get file checksum f file status get file status path f throws io exception check path f return fs impl get file status f file status get file link status path f throws io exception return get file status f fs status get fs status throws io exception return fs impl get status fs server defaults get server defaults throws io exception return fs impl get server defaults get uri default port return file status list status path f throws io exception check path f return fs impl list status f mkdir path dir fs permission permission boolean create parent throws io exception check path dir fs impl primitive mkdir dir permission create parent fs data input stream open path f buffer size throws io exception check path f return fs impl open f buffer size rename internal path src path dst throws io exception check path src check path dst fs impl rename src dst options rename none set owner path f string username string groupname throws io exception check path f fs impl set owner f username groupname set permission path f fs permission permission throws io exception check path f fs impl set permission f permission boolean set replication path f short replication throws io exception check path f return fs impl set replication f replication set times path f mtime atime throws io exception check path f fs impl set times f mtime
30	common\src\java\org\apache\hadoop\fs\DF.java	unrelated	package org apache hadoop fs filesystem disk space usage statistics uses unix df program get mount points java io file space utilization tested linux free bsd cygwin df extends shell default df refresh interval df interval default string dir path file dir file string filesystem string mount enum os type os type unix unix os type win windows os type solaris sun os os type mac mac os type aix aix string id os type string id id id boolean match string os str return os str null os str index of id string get id return id string os name system get property os name os type os type get os type os name protected os type get os type string os name os type ost enum set of os type ost match os name return ost return os type os type unix df file path configuration conf throws io exception path conf get long common configuration keys fs df interval key df df interval default df file path df interval throws io exception super df interval dir path path get canonical path dir file new file dir path protected os type get os type return os type accessors string get dir path return dir path string get filesystem throws io exception run return filesystem get capacity return dir file get total space get used return dir file get total space dir file get free space get available return dir file get usable space get percent used cap get capacity used cap get available return used cap string get mount throws io exception run return mount string string return df k mount n filesystem get capacity get used get available get percent used mount protected string get exec string ignoring error since exit code enough return new string bash c exec df k dir path dev null protected parse exec result buffered reader lines throws io exception lines read line skip headings string line lines read line line null throw new io exception expecting line end stream string tokenizer tokens new string tokenizer line n r f filesystem tokens next token tokens more tokens filesystem name line lines read line line null throw new io exception expecting line end stream tokens new string tokenizer line n r f switch get os type case os type aix long parse long tokens next token capacity long parse long tokens next token available integer parse int tokens next token pct used tokens next token tokens next token mount tokens next token break case os type win case os type solaris case os type mac case os type unix default long parse long tokens next token capacity long parse long tokens next token used long parse long tokens next token available integer parse int tokens next token pct used mount tokens next token break main string args throws exception string path args length path args system println new df new file path df interval default string
31	common\src\java\org\apache\hadoop\fs\DU.java	unrelated	package org apache hadoop fs filesystem disk space usage statistics uses unix du program du extends shell string dir path atomic long used new atomic long volatile boolean run true thread refresh used io exception du exception null refresh interval keeps track disk usage du file path interval throws io exception super set shell interval always run command use one set thread sleep interval refresh interval interval dir path path get canonical path populate used variable run keeps track disk usage du file path configuration conf throws io exception path l minutes default refresh interval this thread refreshes used variable future improvements could permanently run thread instead run get used called du refresh thread implements runnable run run try thread sleep refresh interval try update used variable du run catch io exception e synchronized du save latest exception return get used du exception e log warn could get disk usage information e catch interrupted exception e decrease much disk space use dec dfs used value used add and get value increase much disk space use inc dfs used value used add and get value get used throws io exception updating thread started update demand refresh used null run else synchronized du exception thrown last run rethrow du exception null io exception tmp du exception du exception null throw tmp return used value string get dir path return dir path start disk usage checking thread start start thread interval sane refresh interval refresh used new thread new du refresh thread refresh used dir path refresh used set daemon true refresh used start shut refreshing thread shutdown run false refresh used null refresh used interrupt string string return du sk dir path n used dir path protected string get exec string return new string du sk dir path protected parse exec result buffered reader lines throws io exception string line lines read line line null throw new io exception expecting line end stream string tokens line split tokens length throw new io exception illegal du output used set long parse long tokens main string args throws exception string path args length path args system println new du new file path new configuration string
32	common\src\java\org\apache\hadoop\fs\FileAlreadyExistsException.java	unrelated	package org apache hadoop fs used target file already exists operation configured overwritten file already exists exception extends io exception file already exists exception super file already exists exception string msg super msg
33	common\src\java\org\apache\hadoop\fs\FileChecksum.java	unrelated	package org apache hadoop fs an representing file checksums files file checksum implements writable the checksum algorithm name string get algorithm name the length checksum bytes get length the value checksum bytes byte get bytes return true algorithms values boolean equals object return true null instanceof file checksum return false file checksum file checksum return get algorithm name equals get algorithm name arrays equals get bytes get bytes inherit doc hash code return get algorithm name hash code arrays hash code get bytes
34	common\src\java\org\apache\hadoop\fs\FileContext.java	unrelated	package org apache hadoop fs the file context provides application writer using hadoop file system it provides set methods usual operation create open list etc p b path names b p the hadoop file system supports uri name space uri names it offers forest file systems referenced using fully qualified ur is two common hadoop file systems implementations ul li local file system file path li hdfs file system hdfs nn address nn port path ul while uri names flexible requires knowing name address server for convenience one often wants access default system one environment without knowing name address this additional benefit allows one change one default fs e g admin moves application cluster cluster p to facilitate hadoop supports notion default file system the user set default file system although typically set environment via default config a default file system implies default scheme authority slash relative names bar resolved relative default fs similarly user also working directory relative names e names starting slash while working directory generally default fs wd different fs p hence hadoop path names one ul li fully qualified uri scheme authority path li slash relative names path relative default file system li wd relative names path relative working dir ul relative paths scheme scheme foo bar illegal p b the role file context configuration defaults b p the file context provides file namespace context resolving file names also contains umask permissions in sense like per process file related state unix system these two properties ul li default file system e slash li umask ul general obtained default configuration file environment see link configuration no configuration parameters obtained default config far file context layer concerned all file system instances e deployments file systems default properties call server side ss defaults operation like create allow one select many properties either pass explicit parameters use ss properties p the file system related ss defaults ul li home directory default user user name li initial wd local fs li replication factor li block size li buffer size li bytes per checksum used ul p b usage model file context b p example use default config read hadoop config core xml unspecified values come core defaults xml release jar ul li f context file context get file context uses default config default fs li f context create path li f context set working dir path li f context open path ul example get file context specific uri default fs ul li f context file context get file context uri li f context create path ul example file context local file system default ul li f context file context get local fs file context li f context create path li ul example use specific config ignoring hadoop config generally need use config unless ul li config x config some one passed to you li f context get file context config x config x changed passed li f context create path li ul file context log log log factory get log file context fs permission default perm fs permission get default list files deleted
35	common\src\java\org\apache\hadoop\fs\FileStatus.java	unrelated	package org apache hadoop fs interface represents client side information file file status implements writable comparable path path length boolean isdir short block replication blocksize modification time access time fs permission permission string owner string group path symlink file status false null null null null we deprecate soon file status length boolean isdir block replication blocksize modification time path path length isdir block replication blocksize modification time null null null path constructor file systems symbolic links supported file status length boolean isdir block replication blocksize modification time access time fs permission permission string owner string group path path length isdir block replication blocksize modification time access time permission owner group null path file status length boolean isdir block replication blocksize modification time access time fs permission permission string owner string group path symlink path path length length isdir isdir block replication short block replication blocksize blocksize modification time modification time access time access time permission permission null fs permission get default permission owner owner null owner group group null group symlink symlink path path the variables isdir symlink indicate type isdir implies directory case symlink must null isdir implies file symlink symlink null implies symlink otherwise file assert isdir symlink null isdir get length file bytes get len return length is file boolean file return isdir symlink is directory boolean directory return isdir old instead use explicit link file status file link file status directory link file status symlink link file status directory link file status symlink instead boolean dir return isdir is symbolic link boolean symlink return symlink null get block size file get block size return blocksize get replication factor file short get replication return block replication get modification time file get modification time return modification time get access time file get access time return access time get fs permission associated file permissions could determined default permissions equivalent rwxrwxrwx returned fs permission get permission return permission get owner file notion owner file filesystem could determined rare string get owner return owner get group associated file notion group file filesystem could determined rare string get group return group path get path return path set path path p path p these provided values could loaded lazily filesystem e g local file system sets permission protected set permission fs permission permission permission permission null fs permission get default permission sets owner protected set owner string owner owner owner null owner sets group protected set group string group group group null group path get symlink throws io exception symlink throw new io exception path path symbolic link return symlink set symlink path p symlink p writable write data output throws io exception text write string get path string write long get len write boolean directory write short get replication write long get block size write long get modification time write long get access time get permission write text write string get owner text write string get group write boolean symlink symlink text write string get symlink string read fields data input throws io exception string str path text read string path new path str
36	common\src\java\org\apache\hadoop\fs\FileSystem.java	unrelated	package org apache hadoop fs an base fairly generic filesystem it may implemented distributed filesystem local one reflects locally connected disk the local version exists small hadoop instances testing p all user code may potentially use hadoop distributed file system written use file system object the hadoop dfs multi machine system appears single disk it useful fault tolerance potentially large capacity p the local implementation link local file system distributed implementation distributed file system file system extends configured implements closeable string fs default name key common configuration keys fs default name key string default fs common configuration keys fs default name default log log log factory get log file system file system cache cache cache new cache the key instance stored cache cache key key recording statistics per file system map class extends file system statistics statistics table new identity hash map class extends file system statistics the statistics file system protected statistics statistics a cache files deleted filsystem closed jvm exited set path delete on exit new tree set path this method adds file system testing find later it testing add file system for testing uri uri configuration conf file system fs throws io exception cache map put new cache key uri conf fs get filesystem instance based uri passed configuration user file system get uri uri configuration conf string user throws io exception interrupted exception user group information ugi user null ugi user group information get current user else ugi user group information create remote user user return ugi as new privileged exception action file system file system run throws io exception return get uri conf returns configured filesystem implementation file system get configuration conf throws io exception return get get default uri conf conf get default filesystem uri configuration uri get default uri configuration conf return uri create fix name conf get fs default name key default fs set default filesystem uri configuration set default uri configuration conf uri uri conf set fs default name key uri string set default filesystem uri configuration set default uri configuration conf string uri set default uri conf uri create fix name uri called new file system instance constructed file system initialize uri name configuration conf throws io exception statistics get statistics name get scheme get class returns uri whose scheme authority identify file system uri get uri get default port file system protected get default port return get canonical name file system string get canonical service name return security util build dt service name get uri get default port string get name return get uri string file system get named string name configuration conf throws io exception return get uri create fix name name conf update old format filesystem names back compatibility this eventually replaced check name method throws exception old format names string fix name string name convert old format name new format name name equals local local file log warn local deprecated filesystem name use file instead name file else name index of unqualified hdfs log warn name deprecated filesystem name use hdfs name instead name hdfs name return name
37	common\src\java\org\apache\hadoop\fs\FileUtil.java	unrelated	package org apache hadoop fs a collection file processing util methods file util log log log factory get log file util convert array file status array path array file status objects path stat paths file status stats stats null return null path ret new path stats length stats length ret stats get path return ret convert array file status array path if stats null return path array file status objects default path return stats null path stat paths file status stats path path stats null return new path path else return stat paths stats delete directory contents if return false directory may partially deleted if dir symlink file symlink deleted the file pointed symlink deleted if dir symlink directory symlink deleted the directory pointed symlink deleted if dir normal file deleted if dir normal directory dir contents recursively deleted boolean fully delete file dir throws io exception dir delete dir normal file b symlink file c empty directory symlink directory return true handle nonempty directory deletion fully delete contents dir return false return dir delete delete contents directory directory if return false directory may partially deleted if dir symlink directory contents actual directory pointed dir deleted boolean fully delete contents file dir throws io exception boolean deletion succeeded true file contents dir list files contents null contents length contents file contents delete normal file symlink another file deletion succeeded false continue continue deletion files dirs dir else either directory symlink another directory try deleting directory might symlink boolean b false b contents delete b indeed symlink empty directory continue empty directory symlink let fullydelete handle fully delete contents deletion succeeded false continue continue deletion files dirs dir return deletion succeeded recursively delete directory fully delete file system fs path dir throws io exception fs delete dir true if destination subdirectory source generate exception check dependencies file system src fs path src file system dst fs path dst throws io exception src fs dst fs string srcq src make qualified src fs string path separator string dstq dst make qualified dst fs string path separator dstq starts with srcq srcq length dstq length throw new io exception cannot copy src else throw new io exception cannot copy src subdirectory dst copy files file systems boolean copy file system src fs path src file system dst fs path dst boolean delete source configuration conf throws io exception return copy src fs src dst fs dst delete source true conf boolean copy file system src fs path srcs file system dst fs path dst boolean delete source boolean overwrite configuration conf throws io exception boolean got exception false boolean return val true string builder exceptions new string builder srcs length return copy src fs srcs dst fs dst delete source overwrite conf check dest directory dst fs exists dst throw new io exception dst specified destination directory doest exist else file status sdst dst fs get file status dst sdst directory throw new io exception copying multiple files last argument dst directory path src srcs try copy src fs src dst fs dst delete source
38	common\src\java\org\apache\hadoop\fs\FilterFileSystem.java	unrelated	package org apache hadoop fs a code filter file system code contains file system uses basic file system possibly transforming data along way providing additional functionality the code filter file system code simply overrides methods code file system code versions pass requests contained file system subclasses code filter file system code may methods may also provide additional methods fields filter file system extends file system protected file system fs extending define filter file system filter file system file system fs fs fs statistics fs statistics called new file system instance constructed file system initialize uri name configuration conf throws io exception fs initialize name conf returns uri whose scheme authority identify file system uri get uri return fs get uri make sure path specifies file system path make qualified path path return fs make qualified path file system check path belongs file system protected check path path path fs check path path block location get file block locations file status file start len throws io exception return fs get file block locations file start len path resolve path path p throws io exception return fs resolve path p opens fs data input stream indicated path fs data input stream open path f buffer size throws io exception return fs open f buffer size inherit doc fs data output stream append path f buffer size progressable progress throws io exception return fs append f buffer size progress inherit doc fs data output stream create path f fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception return fs create f permission overwrite buffer size replication block size progress set replication existing file false file exist directory boolean set replication path src short replication throws io exception return fs set replication src replication renames path src path dst can take place local fs remote dfs boolean rename path src path dst throws io exception return fs rename src dst delete file boolean delete path f boolean recursive throws io exception return fs delete f recursive mark path deleted file system closed when jvm shuts file system objects closed automatically then marked path deleted result closing file system the path exist file system boolean delete on exit path f throws io exception return fs delete on exit f list files directory file status list status path f throws io exception return fs list status f inherit doc remote iterator path list corrupt file blocks path path throws io exception return fs list corrupt file blocks path list files block locations directory remote iterator located file status list located status path f throws io exception return fs list located status f path get home directory return fs get home directory set current working directory given file system all relative paths resolved relative set working directory path new dir fs set working directory new dir get current working directory given file system path get working directory return fs get working directory protected path get initial working directory return fs get initial working directory inherit doc fs status get status path p
39	common\src\java\org\apache\hadoop\fs\FilterFs.java	unrelated	package org apache hadoop fs licensed apache software foundation asf one contributor license agreements see notice file distributed work additional information regarding copyright ownership the asf licenses file apache license version license may use file except compliance license you may obtain copy license http www apache org licenses license unless required applicable law agreed writing software distributed license distributed as is basis without warranties or conditions of any kind either express implied see license specific language governing permissions limitations license a code filter fs code contains file system uses basic file system possibly transforming data along way providing additional functionality the code filter fs code simply overrides methods code abstract file system code versions pass requests contained file system subclasses code filter fs code may methods may also provide additional methods fields filter fs extends abstract file system abstract file system fs protected abstract file system get my fs return fs protected filter fs abstract file system fs throws io exception uri syntax exception super fs get uri fs get uri get scheme fs get uri get authority null fs get uri default port fs fs statistics get statistics return fs get statistics path make qualified path path return fs make qualified path path get initial working directory return fs get initial working directory path get home directory return fs get home directory fs data output stream create internal path f enum set create flag flag fs permission absolute permission buffer size short replication block size progressable progress bytes per checksum boolean create parent throws io exception unresolved link exception check path f return fs create internal f flag absolute permission buffer size replication block size progress bytes per checksum create parent boolean delete path f boolean recursive throws io exception unresolved link exception check path f return fs delete f recursive block location get file block locations path f start len throws io exception unresolved link exception check path f return fs get file block locations f start len file checksum get file checksum path f throws io exception unresolved link exception check path f return fs get file checksum f file status get file status path f throws io exception unresolved link exception check path f return fs get file status f file status get file link status path f throws io exception unresolved link exception check path f return fs get file link status f fs status get fs status path f throws access control exception file not found exception unresolved link exception io exception return fs get fs status f fs status get fs status throws io exception return fs get fs status fs server defaults get server defaults throws io exception return fs get server defaults path resolve path path p throws file not found exception unresolved link exception access control exception io exception return fs resolve path p get uri default port return fs get uri default port uri get uri return fs get uri check path path path fs check path path string get uri path path p return fs get uri path p file
40	common\src\java\org\apache\hadoop\fs\FsConfig.java	unrelated	package org apache hadoop fs this thin layer manage fs related keys configuration object it provides convenience method set get keys configuration fs config fs config configuration keys default values config file tbd note deprecate keys constants elsewhere the keys string fs replication factor key dfs replication string fs block size key dfs block size the default values default values server default implies use ones target file system files created short fs default replication factor fs default block size string get default fs uri configuration conf return conf get fs default name key fs default name default string get home dir configuration conf return conf get fs home dir key fs home dir default short get default replication factor configuration conf return short conf get int fs replication factor key fs default replication factor get default block size configuration conf return conf get long fs block size key fs default block size get default io buffersize configuration conf return conf get int io file buffer size key io file buffer size default class get impl class uri uri configuration conf string scheme uri get scheme scheme null throw new illegal argument exception no scheme return conf get class fs uri get scheme impl null the setters see note javdoc set default fs configuration conf string uri conf set fs default name key uri set home dir configuration conf string path conf set fs home dir key path set default replication factor configuration conf short rf conf set int fs replication factor key rf set default block size configuration conf bs conf set long fs block size key bs set default io buffersize configuration conf bs conf set int io file buffer size key bs
41	common\src\java\org\apache\hadoop\fs\FsConstants.java	unrelated	package org apache hadoop fs file system related constants fs constants uri local filesystem uri local fs uri uri create file uri scheme ftp string ftp scheme ftp view fs view fs file system ie mount file system client side uri viewfs uri uri create viewfs string viewfs scheme viewfs
42	common\src\java\org\apache\hadoop\fs\FSDataInputStream.java	unrelated	package org apache hadoop fs utility wraps link fs input stream link data input stream buffers input link buffered input stream fs data input stream extends data input stream implements seekable positioned readable closeable fs data input stream input stream throws io exception super instanceof seekable instanceof positioned readable throw new illegal argument exception in instance seekable positioned readable seek given offset synchronized seek desired throws io exception seekable seek desired get current position input stream get pos throws io exception return seekable get pos read bytes given position stream given buffer data end stream reached read position byte buffer offset length throws io exception return positioned readable read position buffer offset length read bytes given position stream given buffer continues read code length code bytes read if exception thrown undetermined number bytes buffer may written read fully position byte buffer offset length throws io exception positioned readable read fully position buffer offset length see link read fully byte read fully position byte buffer throws io exception positioned readable read fully position buffer buffer length seek given position alternate copy data boolean seek to new source target pos throws io exception return seekable seek to new source target pos get reference wrapped input stream used unit tests input stream get wrapped stream return
43	common\src\java\org\apache\hadoop\fs\FSDataOutputStream.java	unrelated	package org apache hadoop fs utility wraps link output stream link data output stream buffers output link buffered output stream creates checksum file fs data output stream extends data output stream implements syncable output stream wrapped stream position cache extends filter output stream file system statistics statistics position position cache output stream file system statistics stats pos throws io exception super statistics stats position pos write b throws io exception write b position statistics null statistics increment bytes written write byte b len throws io exception write b len position len update position statistics null statistics increment bytes written len get pos throws io exception return position return cached position close throws io exception close fs data output stream output stream throws io exception null fs data output stream output stream file system statistics stats throws io exception stats fs data output stream output stream file system statistics stats start position throws io exception super new position cache stats start position wrapped stream get current position output stream get pos throws io exception return position cache get pos close underlying output stream close throws io exception close this invokes position cache close get reference wrapped output stream used unit tests output stream get wrapped stream return wrapped stream sync throws io exception wrapped stream instanceof syncable syncable wrapped stream sync hflush throws io exception wrapped stream instanceof syncable syncable wrapped stream hflush else wrapped stream flush hsync throws io exception wrapped stream instanceof syncable syncable wrapped stream hsync else wrapped stream flush
44	common\src\java\org\apache\hadoop\fs\FSError.java	unrelated	package org apache hadoop fs thrown unexpected filesystem errors presumed reflect disk errors native filesystem fs error extends error serial version uid l fs error throwable cause super cause
45	common\src\java\org\apache\hadoop\fs\FSInputChecker.java	unrelated	package org apache hadoop fs this generic input stream verifying checksums data read user fs input checker extends fs input stream log log log factory get log fs input checker the file name data read protected path file checksum sum boolean verify checksum true max chunk size data bytes checksum eg byte buf buffer non chunk aligned reading byte checksum int buffer checksum ints wrapper checksum buffer pos position reader inside buf count number bytes currently buf num of retries cached file position always multiple max chunk size chunk pos number checksum chunks read user buffer chosen benchmarks higher values reduce cpu usage the size data reads made underlying stream chunks per read max chunk size chunks per read protected checksum size bit checksum constructor protected fs input checker path file num of retries file file num of retries num of retries constructor protected fs input checker path file num of retries boolean verify checksum checksum sum chunk size checksum size file num of retries set verify checksum sum chunk size checksum size reads checksum chunks code buf code code offset code checksum code checksum code since checksums disabled two cases implementors need worry need checksum return false len positive value checksum null implementors simply pass underlying data stream b need checksum return true len max chunk size checksum length multiple checksum size implementors read integer number data chunks buf the amount read bounded len checksum length checksum size max chunk size note len may value multiple max chunk size case implementation may return less len the method used implementing read therefore optimized sequential reading protected read chunk pos byte buf offset len byte checksum throws io exception return position beginning chunk containing pos protected get chunk position pos return true need checksum verification protected synchronized boolean need checksum return verify checksum sum null read one checksum verified byte stream reached synchronized read throws io exception pos count fill pos count return return buf pos xff read checksum verified bytes byte input stream specified byte array starting given offset p this method implements general contract corresponding code link input stream read byte read code method code link input stream code as additional convenience attempts read many bytes possible repeatedly invoking code read code method underlying stream this iterated code read code continues one following conditions becomes true ul li the specified number bytes read li the code read code method underlying stream returns code code indicating end file ul if first code read code underlying stream returns code code indicate end file method returns code code otherwise method returns number bytes actually read stream reached checksum exception checksum error occurs synchronized read byte b len throws io exception parameter check len len b length len throw new index out of bounds exception else len return n nread read b n len n nread return n nread n n nread n len return n fills buffer chunk data no mark supported this method assumes data buffer already read hence pos count fill throws io exception assert pos count fill internal buffer count read
46	common\src\java\org\apache\hadoop\fs\FSInputStream.java	unrelated	package org apache hadoop fs fs input stream generic old input stream little bit raf style seek ability fs input stream extends input stream implements seekable positioned readable seek given offset start file the next read location can seek past end file seek pos throws io exception return current offset start file get pos throws io exception seeks different copy data returns true found new source false otherwise boolean seek to new source target pos throws io exception read position byte buffer offset length throws io exception synchronized old pos get pos nread try seek position nread read buffer offset length finally seek old pos return nread read fully position byte buffer offset length throws io exception nread nread length nbytes read position nread buffer offset nread length nread nbytes throw new eof exception end file reached reading fully nread nbytes read fully position byte buffer throws io exception read fully position buffer buffer length
47	common\src\java\org\apache\hadoop\fs\FSOutputSummer.java	unrelated	package org apache hadoop fs this generic output stream generating checksums data written underlying stream fs output summer extends output stream data checksum checksum sum internal buffer storing data checksumed byte buf internal buffer storing checksum byte checksum the number valid bytes buffer count protected fs output summer checksum sum max chunk size checksum size sum sum buf new byte max chunk size checksum new byte checksum size count write data chunk code b code staring code offset code length code len code checksum protected write chunk byte b offset len byte checksum throws io exception write one byte synchronized write b throws io exception sum update b buf count byte b count buf length flush buffer writes code len code bytes specified byte array starting offset code code generate checksum data chunk p this method stores bytes given array stream buffer gets checksumed the buffer gets checksumed flushed underlying output stream data checksum chunk buffer if buffer empty requested length least large size next checksum chunk size method checksum write chunk directly underlying output stream thus avoids uneccessary data copy synchronized write byte b len throws io exception len b length len throw new array index out of bounds exception n n len n write b n len n write portion array flushing underlying stream necessary write byte b len throws io exception count len buf length local buffer empty user data one chunk checksum output data length buf length sum update b length write checksum chunk b length false return length copy user data local buffer bytes to copy buf length count bytes to copy len bytes to copy len bytes to copy sum update b bytes to copy system arraycopy b buf count bytes to copy count bytes to copy count buf length local buffer full flush buffer return bytes to copy forces buffered output bytes checksumed written underlying output stream protected synchronized flush buffer throws io exception flush buffer false forces buffered output bytes checksumed written underlying output stream if keep true state object remains intact protected synchronized flush buffer boolean keep throws io exception count chunk len count count write checksum chunk buf chunk len keep keep count chunk len return number valid bytes currently buffer protected synchronized get buffered data size return count generate checksum data chunk output data chunk checksum underlying output stream if keep true keep current checksum intact reset write checksum chunk byte b len boolean keep throws io exception temp checksum sum get value keep sum reset byte temp checksum checksum write chunk b len checksum converts checksum integer value byte stream byte convert to byte stream checksum sum checksum size return byte sum get value new byte checksum size byte byte integer byte bytes bytes byte integer x ff bytes byte integer x ff bytes byte integer x ff bytes byte integer x ff return bytes resets existing buffer new one specified size protected synchronized reset checksum chunk size sum reset buf new byte size count
48	common\src\java\org\apache\hadoop\fs\FsServerDefaults.java	unrelated	package org apache hadoop fs provides server default configuration values clients fs server defaults implements writable register ctor writable factories set factory fs server defaults new writable factory writable new instance return new fs server defaults block size bytes per checksum write packet size short replication file buffer size fs server defaults fs server defaults block size bytes per checksum write packet size short replication file buffer size block size block size bytes per checksum bytes per checksum write packet size write packet size replication replication file buffer size file buffer size get block size return block size get bytes per checksum return bytes per checksum get write packet size return write packet size short get replication return replication get file buffer size return file buffer size writable write data output throws io exception write long block size write int bytes per checksum write int write packet size write short replication write int file buffer size read fields data input throws io exception block size read long bytes per checksum read int write packet size read int replication read short file buffer size read int
49	common\src\java\org\apache\hadoop\fs\FsShell.java	unrelated	package org apache hadoop fs provide command line access file system fs shell extends configured implements tool log log log factory get log fs shell file system fs trash trash protected command factory command factory string usage prefix usage hadoop fs generic options default ctor configuration be sure invoke link set conf configuration valid configuration prior running commands fs shell null construct fs shell given configuration commands executed via link run string fs shell configuration conf super conf protected file system get fs throws io exception fs null fs file system get get conf return fs protected trash get trash throws io exception trash null trash new trash get conf return trash protected init throws io exception get conf set quiet mode true command factory null command factory new command factory get conf command factory add object new help help command factory add object new usage usage register commands command factory protected register commands command factory factory todo dfs admin subclasses fs shell need protect command registration this morph base commands method get class equals fs shell factory register commands fs command returns trash object associated shell path get current trash dir throws io exception return get trash get current trash dir note usage help inner allow access outer methods access command factory display help commands short usage description protected usage extends fs command string name usage string usage cmd string description displays usage given command commands none n specified protected process raw arguments linked list string args args empty print usage system else string arg args print usage system arg displays short usage commands sans description protected help extends fs command string name help string usage cmd string description displays help given command commands none n specified protected process raw arguments linked list string args args empty print help system else string arg args print help system arg the following helper methods get info they defined outside scope help usage run method needs invoke print usages print usage print stream print info null false print one usage print usage print stream string cmd print info cmd false print helps print help print stream print info null true print one help print help print stream string cmd print info cmd true print info print stream string cmd boolean show help cmd null display help usage one command command instance command factory get instance cmd instance null throw new unknown command exception cmd show help print instance help instance else print instance usage instance else display help usage commands println usage prefix display list short usages array list command instances new array list command string name command factory get names command instance command factory get instance name instance deprecated system println instance get usage instances add instance display descriptions command show help command instance instances println print instance help instance println tool runner print generic command usage print instance usage print stream command instance println usage prefix instance get usage todo eventually auto wrap text matches expected output hdfs tests print instance help print stream command instance boolean first line true string line
50	common\src\java\org\apache\hadoop\fs\FsShellPermissions.java	unrelated	package org apache hadoop fs this home file permissions related commands moved separate since fs shell getting large fs shell permissions extends fs command log log fs shell log register permission related commands factory register commands command factory factory factory add class chmod chmod factory add class chown chown factory add class chgrp chgrp the pattern almost flexible mode allowed chmod shell command the main restriction recognize rwx xt to reduce errors also enforce octal mode specifications either digits without sticky bit setting four digits sticky bit setting chmod extends fs shell permissions string name chmod string usage r mode mode octalmode path string description changes permissions file n this works similar shell chmod exceptions n n r tmodifies files recursively this option n tcurrently supported n n mode mode mode used chmod shell command n only letters recognized rwx xt e g r g w rwx r n n octalmode mode specifed digits if digits first may n turn sticky bit respectively unlike shell command possible specify part mode n e g u rwx g rx r n n if none augo specified assumed unlike n tshell command umask applied protected chmod parser pp protected process options linked list string args throws io exception command format cf new command format integer max value r null cf parse args set recursive cf get opt r string mode str args remove first try pp new chmod parser mode str catch illegal argument exception iea todo remove chmod doubled output backwards compatibility throw new illegal argument exception chmod mode mode str match expected pattern protected process path path data item throws io exception short newperms pp apply new permission item stat item stat get permission short newperms try item fs set permission item path new fs permission newperms catch io exception e log debug error changing permissions item e throw new io exception changing permissions item e get message used chown chgrp string allowed chars z a z used change owner group files chown extends fs shell permissions string name chown string usage r owner group path string description changes owner group file n this similar shell chown exceptions n n r tmodifies files recursively this option n tcurrently supported n n if owner group specified owner n tgroup modified n n the owner group names may cosists digits alphabet n tand e z a z the names case n tsensitive n n warning avoid using separate user name group though n linux allows if user names dots n tusing local file system might see surprising results since n tshell command chown used local files allows allowed chars names owner group pattern chown pattern pattern compile allowed chars allowed chars protected string owner null protected string group null protected process options linked list string args throws io exception command format cf new command format integer max value r cf parse args set recursive cf get opt r parse owner group args remove first parse first argument owner group protected parse owner group string owner str matcher matcher chown pattern matcher owner str matcher matches
51	common\src\java\org\apache\hadoop\fs\FsStatus.java	unrelated	package org apache hadoop fs this used represent capacity free used space link file system fs status implements writable capacity used remaining construct fs status object using specified statistics fs status capacity used remaining capacity capacity used used remaining remaining return capacity bytes file system get capacity return capacity return number bytes used file system get used return used return number remaining bytes file system get remaining return remaining writable write data output throws io exception write long capacity write long used write long remaining read fields data input throws io exception capacity read long used read long remaining read long
52	common\src\java\org\apache\hadoop\fs\FsUrlConnection.java	unrelated	package org apache hadoop fs representation url connection open input streams fs url connection extends url connection configuration conf input stream fs url connection configuration conf url url super url conf conf connect throws io exception try file system fs file system get url uri conf fs open new path url get path catch uri syntax exception e throw new io exception e string input stream get input stream throws io exception null connect return
53	common\src\java\org\apache\hadoop\fs\FsUrlStreamHandler.java	unrelated	package org apache hadoop fs url stream handler relying file system given configuration handle url protocols fs url stream handler extends url stream handler configuration conf fs url stream handler configuration conf conf conf fs url stream handler conf new configuration protected fs url connection open connection url url throws io exception return new fs url connection conf url
54	common\src\java\org\apache\hadoop\fs\FsUrlStreamHandlerFactory.java	unrelated	package org apache hadoop fs factory url stream handlers there one handler whose job create url connections a fs url connection relies file system choose appropriate fs implementation before returning handler make sure file system knows implementation requested scheme protocol fs url stream handler factory implements url stream handler factory the configuration holds supported fs implementation names configuration conf this map stores whether protocol know file system map string boolean protocols new hash map string boolean the url stream handler java net url stream handler handler fs url stream handler factory conf new configuration force resolution configuration files required want factory able handle file ur ls conf get class fs file impl null handler new fs url stream handler conf fs url stream handler factory configuration conf conf new configuration conf force resolution configuration files conf get class fs file impl null handler new fs url stream handler conf java net url stream handler create url stream handler string protocol protocols contains key protocol boolean known conf get class fs protocol impl null null protocols put protocol known protocols get protocol return handler else file system know protocol let vm handle return null
55	common\src\java\org\apache\hadoop\fs\GlobExpander.java	unrelated	package org apache hadoop fs glob expander string with offset string offset string with offset string offset super offset offset expand globs given code file pattern code collection file patterns expanded set file pattern slash character curly bracket pair list string expand string file pattern throws io exception list string fully expanded new array list string list string with offset expand new array list string with offset expand add new string with offset file pattern expand empty string with offset path expand remove list string with offset expanded expand leftmost path expanded null fully expanded add path else expand add all expanded return fully expanded expand leftmost outer curly bracket pair containing slash character code file pattern code list string with offset expand leftmost string with offset file pattern with offset throws io exception string file pattern file pattern with offset leftmost leftmost outer curly containing slash file pattern file pattern with offset offset leftmost return null curly open string builder prefix new string builder file pattern substring leftmost string builder suffix new string builder list string alts new array list string string builder alt new string builder string builder cur prefix leftmost file pattern length char c file pattern char at cur suffix cur append c else c file pattern length throw new io exception illegal file pattern an escaped character present glob file pattern c file pattern char at cur append c else c curly open alt set length cur alt else cur append c else c curly open curly open alts add alt string alt set length cur suffix else cur append c else c curly open alts add alt string alt set length else cur append c else cur append c list string with offset exp new array list string with offset string alts exp add new string with offset prefix suffix prefix length return exp finds index leftmost opening curly bracket containing slash character code file pattern code slash character bracket leftmost outer curly containing slash string file pattern offset throws io exception curly open leftmost boolean seen slash false offset file pattern length char c file pattern char at c file pattern length throw new io exception illegal file pattern an escaped character present glob file pattern else c curly open leftmost else c curly open curly open leftmost seen slash return leftmost else c curly open seen slash true return
56	common\src\java\org\apache\hadoop\fs\GlobFilter.java	unrelated	package org apache hadoop fs a could decide matches glob glob filter implements path filter path filter default filter new path filter boolean accept path file return true path filter user filter default filter glob pattern pattern glob filter string file pattern throws io exception init file pattern default filter glob filter string file pattern path filter filter throws io exception init file pattern filter init string file pattern path filter filter throws io exception try user filter filter pattern new glob pattern file pattern catch pattern syntax exception e existing code expects io exception start with illegal file pattern throw new io exception illegal file pattern e get message e boolean pattern return pattern wildcard boolean accept path path return pattern matches path get name user filter accept path
57	common\src\java\org\apache\hadoop\fs\GlobPattern.java	unrelated	package org apache hadoop fs a posix glob pattern brace expansions glob pattern char backslash pattern compiled boolean wildcard false construct glob pattern object glob pattern glob pattern string glob pattern set glob pattern pattern compiled return compiled compile glob pattern pattern compile string glob pattern return new glob pattern glob pattern compiled match input compiled glob pattern boolean matches char sequence return compiled matcher matches set compile glob pattern set string glob string builder regex new string builder set open curly open len glob length wildcard false len char c glob char at switch c case backslash len error missing escaped character glob regex append c append glob char at continue case case case case case case escape regex special chars glob special chars regex append backslash break case regex append wildcard true break case regex append wildcard true continue case start group regex append non capturing curly open wildcard true continue case regex append curly open c continue case curly open end group curly open regex append continue break case set open error unclosed character glob set open wildcard true break case inside unescaped set open regex append backslash break case needs translated regex append set open glob char at continue case many set errors like could easily detected valid posix glob java regex we let regex compiler real work set open break default regex append c set open error unclosed character glob len curly open error unclosed group glob len compiled pattern compile regex string boolean wildcard return wildcard error string message string pattern pos throw new pattern syntax exception message pattern pos
58	common\src\java\org\apache\hadoop\fs\HardLink.java	unrelated	package org apache hadoop fs class creating hardlinks supports unix linux win xp vista via cygwin mac os x the hard link formerly inner fs util methods provided blatantly non thread safe to enable volume parallel update snapshots provide threadsafe methods allocate new buffer arrays upon call we also provide api hardlink files directory single command times efficient minimizes impact extra buffer creations hard link enum os type os type unix os type winxp os type solaris os type mac os type os type hard link command getter get hard link command link stats link stats initialize command getters statically use methods without instantiating hard link object os type get os type os type os type os type winxp windows get hard link command new hard link cg win else unix get hard link command new hard link cg unix get link count command particular unix variant linux already set default stat c null os type os type os type mac string link count cmd template stat f null hard link cg unix set link count cmd template link count cmd template else os type os type os type solaris string link count cmd template ls null hard link cg unix set link count cmd template link count cmd template hard link link stats new link stats os type get os type string os name system get property os name os name contains windows os name contains xp os name contains os name contains vista os name contains windows os name contains windows os name contains windows return os type os type winxp else os name contains sun os os name contains solaris return os type os type solaris else os name contains mac return os type os type mac else return os type os type unix this bridges os dependent implementations needed functionality creating hardlinks querying link counts the particular implementation chosen initialization phase hard link the getter methods construct shell command strings various purposes hard link command getter get command needed hardlink bunch files single source directory target directory the source directory specified command executed using source directory current working directory shell invocation source directory link runtime exec string link mult string file base names file link dir throws io exception get command needed hardlink single file string link one file file file link name throws io exception get command query hardlink count file string link count file file throws io exception calculate total length shell command resulting execution link mult plus length source directory name also provided shell source directory get link mult arg length file file dir string file base names file link dir throws io exception get maximum allowed length shell command os documented minimum guaranteed supported command length aprx kb unix kb windows get max allowed cmd arg length implementation hard link command getter unix hard link cg unix extends hard link command getter string hard link command ln null null string hard link mult prefix ln string hard link mult suffix null string get link count command stat c null unix guarantees least k bytes
59	common\src\java\org\apache\hadoop\fs\InvalidPathException.java	unrelated	package org apache hadoop fs path invalid either invalid characters due file system specific reasons invalid path exception extends hadoop illegal argument exception serial version uid l constructs exception specified detail message invalid path exception string path super invalid path name path constructs exception specified detail message invalid path exception string path string reason super invalid path path reason null reason
60	common\src\java\org\apache\hadoop\fs\LocalDirAllocator.java	unrelated	package org apache hadoop fs an implementation round robin scheme disk allocation creating files the way works kept track disk last allocated file write for current request next disk set disks would allocated free space disk sufficient enough accommodate file considered creation if space requirements cannot met next disk order would tried till disk found sufficient capacity once disk sufficient space identified check done make sure disk writable also api provided take space requirements consideration checks whether disk consideration writable used cases file size known apriori an api provided read path created earlier that api works scan disks input pathname this implementation also provides functionality multiple allocators per jvm one unique functionality context like mapred dfs client etc it ensures one instance allocator per context per jvm note the contexts referred actually configuration items defined configuration like mapred local dir want control dir allocations the context strings exactly configuration items this implementation take consideration cases disk becomes read goes space file written disks shared multiple processes latter situation probable in implementation disk referred dir actually points configured directory disk parent file write read allocations local dir allocator a map config item names like mapred local dir dfs client buffer dir instance allocator per context this object make sure exists exactly one instance per jvm map string allocator per context contexts new tree map string allocator per context string context cfg item name used size file allocated unknown size unknown create allocator object local dir allocator string context cfg item name context cfg item name context cfg item name this method must used obtain dir allocation context particular value context name the context name must item defined configuration object want control dir allocations e g code mapred local dir code the method create context name already exist allocator per context obtain context string context cfg item name synchronized contexts allocator per context contexts get context cfg item name null contexts put context cfg item name new allocator per context context cfg item name return get path local fs this method used size file known apriori we go round robin set disks via configured dirs return first complete path could create parent directory passed path available disk path get local path for write string path str configuration conf throws io exception return get local path for write path str size unknown conf get path local fs pass size size unknown known apriori we round robin set disks via configured dirs return first complete path enough space available disk path get local path for write string path str size configuration conf throws io exception allocator per context context obtain context context cfg item name return context get local path for write path str size conf get path local fs reading we search configured dirs file existence return complete path file find one path get local path to read string path str configuration conf throws io exception allocator per context context obtain context context cfg item name return context get local path to read path str conf creates temporary file local fs pass size known apriori
61	common\src\java\org\apache\hadoop\fs\LocalFileSystem.java	unrelated	package org apache hadoop fs implement file system api checksumed local filesystem local file system extends checksum file system uri name uri create file random rand new random file system rfs local file system new raw local file system file system get raw return rfs local file system file system raw local file system super raw local file system rfs raw local file system convert path file file path to file path path return raw local file system fs path to file path copy from local file boolean del src path src path dst throws io exception file util copy src dst del src get conf copy to local file boolean del src path src path dst throws io exception file util copy src dst del src get conf moves files bad file directory device storage reused boolean report checksum failure path p fs data input stream pos fs data input stream sums sums pos try canonicalize f file f raw local file system fs path to file p get canonical file find highest writable parent dir f device string device new df f get conf get mount file parent f get parent file file dir null parent null parent write parent string starts with device dir parent parent parent get parent file dir null throw new io exception able find highest writable parent dir move file file bad dir new file dir bad files bad dir mkdirs bad dir directory throw new io exception mkdirs failed create bad dir string string suffix rand next int file bad file new file bad dir f get name suffix log warn moving bad file f bad file close close first boolean b f rename to bad file rename b log warn ignoring failure rename to move checksum file file check file raw local file system fs path to file get checksum file p b check file rename to new file bad dir check file get name suffix b log warn ignoring failure rename to catch io exception e log warn error moving bad file p e return false
62	common\src\java\org\apache\hadoop\fs\LocalFileSystemConfigKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used local file system raw local fs checksum fs local file system config keys extends common configuration keys string local fs block size key file blocksize local fs block size default string local fs replication key file replication short local fs replication default string local fs stream buffer size key file stream buffer size local fs stream buffer size default string local fs bytes per checksum key file bytes per checksum local fs bytes per checksum default string local fs client write packet size key file client write packet size local fs client write packet size default
63	common\src\java\org\apache\hadoop\fs\LocatedFileStatus.java	unrelated	package org apache hadoop fs this defines file status includes file block locations located file status extends file status block location locations constructor located file status file status stat block location locations throws io exception stat get len stat directory stat get replication stat get block size stat get modification time stat get access time stat get permission stat get owner stat get group null stat get path locations symlink set symlink stat get symlink constructor located file status length boolean isdir block replication blocksize modification time access time fs permission permission string owner string group path symlink path path block location locations super length isdir block replication blocksize modification time access time permission owner group symlink path locations locations get file block locations block location get block locations return locations compare object another object less equal greater specified object type file status compare to object return super compare to compare object equal another object boolean equals object return super equals returns hash code value object defined hash code path name hash code return super hash code
64	common\src\java\org\apache\hadoop\fs\MD5MD5CRC32FileChecksum.java	unrelated	package org apache hadoop fs md md crc md md crc file checksum extends file checksum length md hash md len integer size long size byte size bytes per crc crc per block md hash md same null md md crc file checksum null create md file checksum md md crc file checksum bytes per crc crc per block md hash md bytes per crc bytes per crc crc per block crc per block md md inherit doc string get algorithm name return md crc per block md bytes per crc crc inherit doc get length return length inherit doc byte get bytes return writable utils byte array inherit doc read fields data input throws io exception bytes per crc read int crc per block read long md md hash read inherit doc write data output throws io exception write int bytes per crc write long crc per block md write write object xml output write xml outputter xml md md crc file checksum throws io exception xml start tag md md crc file checksum get name null xml attribute bytes per crc bytes per crc xml attribute crc per block crc per block xml attribute md md xml end tag return object represented attributes md md crc file checksum value of attributes attrs throws sax exception string bytes per crc attrs get value bytes per crc string crc per block attrs get value crc per block string md attrs get value md bytes per crc null crc per block null md null return null try return new md md crc file checksum integer value of bytes per crc integer value of crc per block new md hash md catch exception e throw new sax exception invalid attributes bytes per crc bytes per crc crc per block crc per block md md e inherit doc string string return get algorithm name md
65	common\src\java\org\apache\hadoop\fs\Options.java	unrelated	package org apache hadoop fs this contains options related file system operations options class support varargs create options create opts create opts block size block size bs return new block size bs buffer size buffer size bs return new buffer size bs replication factor rep fac short rf return new replication factor rf bytes per checksum bytes per checksum short crc return new bytes per checksum crc perms perms fs permission perm return new perms perm create parent create parent return new create parent true create parent donot create parent return new create parent false block size extends create opts block size protected block size bs bs throw new illegal argument exception block size must greater block size bs get value return block size replication factor extends create opts short replication protected replication factor short rf rf throw new illegal argument exception replication must greater replication rf short get value return replication buffer size extends create opts buffer size protected buffer size bs bs throw new illegal argument exception buffer size must greater buffer size bs get value return buffer size bytes per checksum extends create opts bytes per checksum protected bytes per checksum short bpc bpc throw new illegal argument exception bytes per checksum must greater bytes per checksum bpc get value return bytes per checksum perms extends create opts fs permission permissions protected perms fs permission perm perm null throw new illegal argument exception permissions must null permissions perm fs permission get value return permissions progress extends create opts progressable progress protected progress progressable prog prog null throw new illegal argument exception progress must null progress prog progressable get value return progress create parent extends create opts boolean create parent protected create parent boolean create par create parent create par boolean get value return create parent get option desired type returns null protected create opts get opt class extends create opts class create opts opts opts null throw new illegal argument exception null opt create opts result null opts length opts get class class result null throw new illegal argument exception multiple blocksize varargs result opts return result set option protected t extends create opts create opts set opt t new value create opts opts boolean already in opts false opts null opts length opts get class new value get class already in opts throw new illegal argument exception multiple opts varargs already in opts true opts new value create opts result opt opts already in opts new value opt create opts new opts new create opts opts length system arraycopy opts new opts opts length new opts opts length new value result opt new opts return result opt enum support varargs rename options enum rename none byte no options overwrite byte overwrite rename destination byte code rename byte code code code rename value of byte code return code code values length null values code byte value return code
66	common\src\java\org\apache\hadoop\fs\ParentNotDirectoryException.java	unrelated	package org apache hadoop fs indicates parent specified path directory expected parent not directory exception extends io exception serial version uid l parent not directory exception super parent not directory exception string msg super msg
67	common\src\java\org\apache\hadoop\fs\Path.java	unrelated	package org apache hadoop fs names file directory link file system path strings use slash directory separator a path absolute begins slash path implements comparable the directory separator slash string separator char separator char string cur dir boolean windows system get property os name starts with windows uri uri hierarchical uri resolve child path parent path path string parent string child new path parent new path child resolve child path parent path path path parent string child parent new path child resolve child path parent path path string parent path child new path parent child resolve child path parent path path path parent path child add slash parent path resolution compatible uri uri parent uri parent uri string parent path parent uri get path parent path equals parent path equals try parent uri new uri parent uri get scheme parent uri get authority parent uri get path null parent uri get fragment catch uri syntax exception e throw new illegal argument exception e uri resolved parent uri resolve child uri initialize resolved get scheme resolved get authority normalize path resolved get path resolved get fragment check path arg string path disallow construction path empty path null throw new illegal argument exception can create path null path length throw new illegal argument exception can create path empty construct path string path strings ur is unescaped elements additional normalization path string path string check path arg path string we use new uri string directly since assumes things escaped require paths add slash front paths windows drive letters windows drive path string false path string path string parse uri components string scheme null string authority null start parse uri scheme colon path string index of slash path string index of colon slash colon slash scheme scheme path string substring colon start colon parse uri authority path string starts with start path string length start authority next slash path string index of start auth end next slash next slash path string length authority path string substring start auth end start auth end uri path rest query fragment supported string path path string substring start path string length initialize scheme authority path null construct path uri path uri uri uri uri construct path components path string scheme string authority string path check path arg path initialize scheme authority path null initialize string scheme string authority string path string fragment try uri new uri scheme authority normalize path path null fragment normalize catch uri syntax exception e throw new illegal argument exception e string normalize path string path remove slashes backslashes path path replace path path replace trim trailing slash non root path ignoring windows drive min length windows drive path true path length min length path ends with path path substring path length return path boolean windows drive string path boolean slashed windows return false start slashed return path length start slashed path char at true path char at start path char at start a path char at start z path char at start path char at start z convert uri uri uri return uri return file
68	common\src\java\org\apache\hadoop\fs\PathFilter.java	unrelated	package org apache hadoop fs path filter tests whether specified pathname included pathname list included boolean accept path path
69	common\src\java\org\apache\hadoop\fs\PositionedReadable.java	unrelated	package org apache hadoop fs stream permits positional reading positioned readable read upto specified number bytes given position within file return number bytes read this change current offset file thread safe read position byte buffer offset length throws io exception read specified number bytes given position within file this change current offset file thread safe read fully position byte buffer offset length throws io exception read number bytes equalt length buffer given position within file this change current offset file thread safe read fully position byte buffer throws io exception
70	common\src\java\org\apache\hadoop\fs\RawLocalFileSystem.java	unrelated	package org apache hadoop fs implement file system api raw local filesystem raw local file system extends file system uri name uri create file path working dir raw local file system working dir get initial working directory path make absolute path f f absolute return f else return new path working dir f convert path file file path to file path path check path path path absolute path new path get working directory path return new file path uri get path uri get uri return name initialize uri uri configuration conf throws io exception super initialize uri conf set conf conf tracking file input stream extends file input stream tracking file input stream file f throws io exception super f read throws io exception result super read result statistics increment bytes read return result read byte data throws io exception result super read data result statistics increment bytes read result return result read byte data offset length throws io exception result super read data offset length result statistics increment bytes read result return result for open fs input stream local fs file input stream extends fs input stream file input stream fis position local fs file input stream path f throws io exception fis new tracking file input stream path to file f seek pos throws io exception fis get channel position pos position pos get pos throws io exception return position boolean seek to new source target pos throws io exception return false just forward fis available throws io exception return fis available close throws io exception fis close boolean mark supported return false read throws io exception try value fis read value position return value catch io exception e unexpected exception throw new fs error e assume native fs error read byte b len throws io exception try value fis read b len value position value return value catch io exception e unexpected exception throw new fs error e assume native fs error read position byte b len throws io exception byte buffer bb byte buffer wrap b len try return fis get channel read bb position catch io exception e throw new fs error e skip n throws io exception value fis skip n value position value return value fs data input stream open path f buffer size throws io exception exists f throw new file not found exception f string return new fs data input stream new buffered fs input stream new local fs file input stream f buffer size for create fs output stream local fs file output stream extends output stream file output stream fos local fs file output stream path f boolean append throws io exception fos new file output stream path to file f append just forward fos close throws io exception fos close flush throws io exception fos flush write byte b len throws io exception try fos write b len catch io exception e unexpected exception throw new fs error e assume native fs error write b throws io exception try fos write b catch io exception e unexpected exception throw new
71	common\src\java\org\apache\hadoop\fs\RemoteIterator.java	unrelated	package org apache hadoop fs an iterator collection whose elements need fetched remotely remote iterator e returns tt true tt iteration elements boolean next throws io exception returns next element iteration e next throws io exception
72	common\src\java\org\apache\hadoop\fs\Seekable.java	unrelated	package org apache hadoop fs stream permits seeking seekable seek given offset start file the next read location can seek past end file seek pos throws io exception return current offset start file get pos throws io exception seeks different copy data returns true found new source false otherwise boolean seek to new source target pos throws io exception
73	common\src\java\org\apache\hadoop\fs\Syncable.java	unrelated	package org apache hadoop fs this flush sync operation syncable flush data client user buffer after return call new readers see data hflush throws io exception similar posix fsync flush data client user buffer way disk device disk may cache hsync throws io exception
74	common\src\java\org\apache\hadoop\fs\Trash.java	unrelated	package org apache hadoop fs provides trash facility supports pluggable trash policies see implementation configured trash policy details trash extends configured trash policy trash policy configured trash policy instance construct trash accessor trash configuration conf throws io exception file system get conf conf construct trash accessor file system provided trash file system fs configuration conf throws io exception super conf trash policy trash policy get instance conf fs fs get home directory in case symlinks mount points one move appropriate trashbin actual volume path p deleted hence get file system fully qualified resolved path move path p trashbin volume boolean move to appropriate trash file system fs path p configuration conf throws io exception path fully resolved path fs resolve path p trash trash new trash file system get fully resolved path uri conf conf boolean success trash move to trash fully resolved path success system println moved p trash trash get current trash dir return success returns whether trash enabled filesystem boolean enabled return trash policy enabled move file directory current trash directory boolean move to trash path path throws io exception return trash policy move to trash path create trash checkpoint checkpoint throws io exception trash policy create checkpoint delete old checkpoint expunge throws io exception trash policy delete checkpoint get current working directory path get current trash dir return trash policy get current trash dir get configured trash policy trash policy get trash policy return trash policy return link runnable periodically empties trash users intended run superuser runnable get emptier throws io exception return trash policy get emptier run emptier main string args throws exception new trash new configuration get emptier run
75	common\src\java\org\apache\hadoop\fs\TrashPolicy.java	unrelated	package org apache hadoop fs this used implementing different trash policies provides factory method create instances configured trash policy trash policy extends configured protected file system fs file system protected path trash path trash directory protected deletion interval deletion interval emptier used setup trash policy must implemented trash policy implementations initialize configuration conf file system fs path home returns whether trash policy enabled filesystem boolean enabled move file directory current trash directory boolean move to trash path path throws io exception create trash checkpoint create checkpoint throws io exception delete old trash checkpoint delete checkpoint throws io exception get current working directory trash policy path get current trash dir return link runnable periodically empties trash users intended run superuser runnable get emptier throws io exception get instance configured trash policy based value configuration paramater fs trash classname trash policy get instance configuration conf file system fs path home throws io exception class extends trash policy trash class conf get class fs trash classname trash policy default trash policy trash policy trash trash policy reflection utils new instance trash class conf trash initialize conf fs home initialize trash policy return trash
76	common\src\java\org\apache\hadoop\fs\TrashPolicyDefault.java	unrelated	package org apache hadoop fs provides trash feature files moved user trash directory subdirectory home directory named trash files initially moved current sub directory trash directory within sub directory original path preserved periodically one may checkpoint current trash remove older checkpoints this design permits trash management without enumeration full trash content without date support filesystem without clock synchronization trash policy default extends trash policy log log log factory get log trash policy default path current new path current path trash new path trash fs permission permission new fs permission fs action all fs action none fs action none date format checkpoint new simple date format yy m mdd h hmmss msecs per minute path current path homes parent trash policy default trash policy default path home configuration conf throws io exception initialize conf home get file system conf home initialize configuration conf file system fs path home fs fs trash new path home trash homes parent home get parent current new path trash current deletion interval conf get float fs trash interval key fs trash interval default msecs per minute path make trash relative path path base path path rm file path return new path base path rm file path uri get path boolean enabled return deletion interval boolean move to trash path path throws io exception enabled return false path absolute make path absolute path new path fs get working directory path fs exists path check path exists throw new file not found exception path string string qpath fs make qualified path string qpath starts with trash string return false already trash trash get parent string starts with qpath throw new io exception cannot move path trash contains trash path trash path make trash relative path current path path base trash path make trash relative path current path get parent io exception cause null try twice case checkpoint mkdirs rename try fs mkdirs base trash path permission create current log warn can create mkdir trash directory base trash path return false catch io exception e log warn can create trash directory base trash path cause e break try target path trash already exists append current time millisecs string orig trash path string fs exists trash path trash path new path orig system current time millis fs rename path trash path move current trash return true catch io exception e cause e throw io exception new io exception failed move trash path init cause cause create checkpoint throws io exception fs exists current trash checkpoint return path checkpoint synchronized checkpoint checkpoint new path trash checkpoint format new date fs rename current checkpoint log info created trash checkpoint checkpoint uri get path else throw new io exception failed checkpoint trash checkpoint delete checkpoint throws io exception file status dirs null try dirs fs list status trash scan trash sub directories catch file not found exception fnfe return system current time millis dirs length path path dirs get path string dir path uri get path string name path get name name equals current get name skip current continue time try synchronized checkpoint time checkpoint
77	common\src\java\org\apache\hadoop\fs\UnresolvedLinkException.java	unrelated	package org apache hadoop fs thrown symbolic link encountered path unresolved link exception extends io exception serial version uid l unresolved link exception super unresolved link exception string msg super msg
78	common\src\java\org\apache\hadoop\fs\UnsupportedFileSystemException.java	unrelated	package org apache hadoop fs file system given file system name scheme supported unsupported file system exception extends io exception serial version uid l constructs exception specified detail message unsupported file system exception string message super message
79	common\src\java\org\apache\hadoop\fs\ftp\FtpConfigKeys.java	unrelated	package org apache hadoop fs ftp this contains constants configuration keys used ftp file system ftp config keys extends common configuration keys string block size key ftp blocksize block size default string replication key ftp replication short replication default string stream buffer size key ftp stream buffer size stream buffer size default string bytes per checksum key ftp bytes per checksum bytes per checksum default string client write packet size key ftp client write packet size client write packet size default protected fs server defaults get server defaults throws io exception return new fs server defaults block size default bytes per checksum default client write packet size default replication default stream buffer size default
80	common\src\java\org\apache\hadoop\fs\ftp\FTPException.java	unrelated	package org apache hadoop fs ftp a wrap link throwable runtime exception ftp exception extends runtime exception serial version uid l ftp exception string message super message ftp exception throwable super ftp exception string message throwable super message
81	common\src\java\org\apache\hadoop\fs\ftp\FTPFileSystem.java	unrelated	package org apache hadoop fs ftp p a link file system backed ftp client provided href http commons apache org net apache commons net p ftp file system extends file system log log log factory get log ftp file system default buffer size default block size uri uri initialize uri uri configuration conf throws io exception get super initialize uri conf get host information uri overrides info conf string host uri get host host host null conf get fs ftp host null host host null throw new io exception invalid host specified conf set fs ftp host host get port information uri overrides info conf port uri get port port port ftp default port port conf set int fs ftp host port port get user password information uri overrides info conf string user and password uri get user info user and password null user and password conf get fs ftp user host null conf get fs ftp password host null user and password null throw new io exception invalid user passsword specified string user passwd info user and password split conf set fs ftp user host user passwd info user passwd info length conf set fs ftp password host user passwd info else conf set fs ftp password host null set conf conf uri uri connect ftp server using configuration parameters ftp client connect throws io exception ftp client client null configuration conf get conf string host conf get fs ftp host port conf get int fs ftp host port ftp default port string user conf get fs ftp user host string password conf get fs ftp password host client new ftp client client connect host port reply client get reply code ftp reply positive completion reply throw new io exception server host refused connection port port else client login user password client set file transfer mode ftp block transfer mode client set file type ftp binary file type client set buffer size default buffer size else throw new io exception login failed server host port port return client logout disconnect given ftp client disconnect ftp client client throws io exception client null client connected throw new ftp exception client connected boolean logout success client logout client disconnect logout success log warn logout failed disconnecting error code client get reply code resolve given working directory path make absolute path work dir path path path absolute return path return new path work dir path fs data input stream open path file buffer size throws io exception ftp client client connect path work dir new path client print working directory path absolute make absolute work dir file file status file stat get file status client absolute file stat directory disconnect client throw new io exception path file directory client allocate buffer size path parent absolute get parent change parent directory server only read file server opening input stream as side effect working directory server changed parent directory file the ftp client connection closed close called fs data input stream client change working directory parent uri get path input stream client retrieve file stream file
82	common\src\java\org\apache\hadoop\fs\ftp\FtpFs.java	unrelated	package org apache hadoop fs ftp the ftp fs implementation abstract file system this impl delegates old file system ftp fs extends delegate to file system this constructor signature needed link abstract file system create file system uri configuration ftp fs uri uri configuration conf throws io exception uri syntax exception super uri new ftp file system conf fs constants ftp scheme true get uri default port return ftp default port fs server defaults get server defaults throws io exception return ftp config keys get server defaults
83	common\src\java\org\apache\hadoop\fs\ftp\FTPInputStream.java	unrelated	package org apache hadoop fs ftp ftp input stream extends fs input stream input stream wrapped stream ftp client client file system statistics stats boolean closed pos ftp input stream input stream stream ftp client client file system statistics stats stream null throw new illegal argument exception null input stream client null client connected throw new illegal argument exception ftp client null connected wrapped stream stream client client stats stats pos closed false get pos throws io exception return pos we support seek seek pos throws io exception throw new io exception seek supported boolean seek to new source target pos throws io exception throw new io exception seek supported synchronized read throws io exception closed throw new io exception stream closed byte read wrapped stream read byte read pos stats null byte read stats increment bytes read return byte read synchronized read byte buf len throws io exception closed throw new io exception stream closed result wrapped stream read buf len result pos result stats null result stats increment bytes read result return result synchronized close throws io exception closed throw new io exception stream closed super close closed true client connected throw new ftp exception client connected boolean cmd completed client complete pending command client logout client disconnect cmd completed throw new ftp exception could complete transfer reply code client get reply code not supported boolean mark supported return false mark read limit do nothing reset throws io exception throw new io exception mark supported
84	common\src\java\org\apache\hadoop\fs\kfs\IFSImpl.java	unrelated	package org apache hadoop fs kfs ifs impl boolean exists string path throws io exception boolean directory string path throws io exception boolean file string path throws io exception string readdir string path throws io exception file status readdirplus path path throws io exception mkdirs string path throws io exception rename string source string dest throws io exception rmdir string path throws io exception remove string path throws io exception filesize string path throws io exception short get replication string path throws io exception short set replication string path short replication throws io exception string get data location string path start len throws io exception get modification time string path throws io exception fs data output stream create string path short replication buffer size progressable progress throws io exception fs data input stream open string path buffer size throws io exception fs data output stream append string path buffer size progressable progress throws io exception
85	common\src\java\org\apache\hadoop\fs\kfs\KFSConfigKeys.java	unrelated	package org apache hadoop fs kfs this contains constants configuration keys used kfs file system kfs config keys extends common configuration keys string kfs block size key kfs blocksize kfs block size default string kfs replication key kfs replication short kfs replication default string kfs stream buffer size key kfs stream buffer size kfs stream buffer size default string kfs bytes per checksum key kfs bytes per checksum kfs bytes per checksum default string kfs client write packet size key kfs client write packet size kfs client write packet size default
86	common\src\java\org\apache\hadoop\fs\kfs\KFSImpl.java	unrelated	package org apache hadoop fs kfs kfs impl implements ifs impl kfs access kfs access null file system statistics statistics kfs impl string meta server host meta server port throws io exception meta server host meta server port null kfs impl string meta server host meta server port file system statistics stats throws io exception kfs access new kfs access meta server host meta server port statistics stats boolean exists string path throws io exception return kfs access kfs exists path boolean directory string path throws io exception return kfs access kfs directory path boolean file string path throws io exception return kfs access kfs file path string readdir string path throws io exception return kfs access kfs readdir path file status readdirplus path path throws io exception string srep path uri get path kfs file attr fattr kfs access kfs readdirplus srep fattr null return null num entries fattr length fattr filename compare to fattr filename compare to continue num entries file status fstatus new file status num entries j fattr length fattr filename compare to fattr filename compare to continue path fn new path path fattr filename fattr directory fstatus j new file status true fattr modification time fn else fstatus j new file status fattr filesize fattr directory fattr replication fattr modification time fn j return fstatus mkdirs string path throws io exception return kfs access kfs mkdirs path rename string source string dest throws io exception return kfs access kfs rename source dest rmdir string path throws io exception return kfs access kfs rmdir path remove string path throws io exception return kfs access kfs remove path filesize string path throws io exception return kfs access kfs filesize path short get replication string path throws io exception return kfs access kfs get replication path short set replication string path short replication throws io exception return kfs access kfs set replication path replication string get data location string path start len throws io exception return kfs access kfs get data location path start len get modification time string path throws io exception return kfs access kfs get modification time path fs data input stream open string path buffer size throws io exception return new fs data input stream new kfs input stream kfs access path statistics fs data output stream create string path short replication buffer size progressable progress throws io exception return new fs data output stream new kfs output stream kfs access path replication false progress statistics fs data output stream append string path buffer size progressable progress throws io exception opening append replicas ignored return new fs data output stream new kfs output stream kfs access path short true progress statistics
87	common\src\java\org\apache\hadoop\fs\kfs\KFSInputStream.java	unrelated	package org apache hadoop fs kfs kfs input stream extends fs input stream kfs input channel kfs channel file system statistics statistics fsize kfs input stream kfs access kfs access string path kfs access path null kfs input stream kfs access kfs access string path file system statistics stats statistics stats kfs channel kfs access kfs open path kfs channel null fsize kfs access kfs filesize path else fsize get pos throws io exception kfs channel null throw new io exception file closed return kfs channel tell synchronized available throws io exception kfs channel null throw new io exception file closed return fsize get pos synchronized seek target pos throws io exception kfs channel null throw new io exception file closed kfs channel seek target pos synchronized boolean seek to new source target pos throws io exception return false synchronized read throws io exception kfs channel null throw new io exception file closed byte b new byte res read b res statistics null statistics increment bytes read return b xff return synchronized read byte b len throws io exception kfs channel null throw new io exception file closed res res kfs channel read byte buffer wrap b len use signify eof res return statistics null statistics increment bytes read res return res synchronized close throws io exception kfs channel null return kfs channel close kfs channel null boolean mark supported return false mark read limit do nothing reset throws io exception throw new io exception mark supported
88	common\src\java\org\apache\hadoop\fs\kfs\KFSOutputStream.java	unrelated	package org apache hadoop fs kfs kfs output stream extends output stream string path kfs output channel kfs channel progressable progress reporter kfs output stream kfs access kfs access string path short replication boolean append progressable prog path path append kfs access kfs file path kfs channel kfs access kfs append path else kfs channel kfs access kfs create path replication progress reporter prog get pos throws io exception kfs channel null throw new io exception file closed return kfs channel tell write v throws io exception kfs channel null throw new io exception file closed byte b new byte b byte v write b write byte b len throws io exception kfs channel null throw new io exception file closed touch progress going kfs since call block progress reporter progress kfs channel write byte buffer wrap b len flush throws io exception kfs channel null throw new io exception file closed touch progress going kfs since call block progress reporter progress kfs channel sync synchronized close throws io exception kfs channel null return flush kfs channel close kfs channel null
89	common\src\java\org\apache\hadoop\fs\kfs\KosmosFileSystem.java	unrelated	package org apache hadoop fs kfs a file system backed kfs kosmos file system extends file system file system local fs ifs impl kfs impl null uri uri path working dir new path kosmos file system kosmos file system ifs impl fsimpl kfs impl fsimpl uri get uri return uri initialize uri uri configuration conf throws io exception super initialize uri conf try kfs impl null uri get host null kfs impl new kfs impl conf get fs kfs meta server host conf get int fs kfs meta server port statistics else kfs impl new kfs impl uri get host uri get port statistics local fs file system get local conf uri uri create uri get scheme uri get authority working dir new path user system get property user name make qualified set conf conf catch exception e e print stack trace system println unable initialize kfs system exit path get working directory return working dir set working directory path dir working dir make absolute dir path make absolute path path path absolute return path return new path working dir path boolean mkdirs path path fs permission permission throws io exception path absolute make absolute path string srep absolute uri get path res system println calling mkdirs srep res kfs impl mkdirs srep return res boolean directory path path throws io exception path absolute make absolute path string srep absolute uri get path system println calling isdir srep return kfs impl directory srep boolean file path path throws io exception path absolute make absolute path string srep absolute uri get path return kfs impl file srep file status list status path path throws io exception path absolute make absolute path string srep absolute uri get path kfs impl exists srep throw new file not found exception file path exist kfs impl file srep return new file status get file status path return kfs impl readdirplus absolute file status get file status path path throws io exception path absolute make absolute path string srep absolute uri get path kfs impl exists srep throw new file not found exception file path exist kfs impl directory srep system println status path path dir return new file status true kfs impl get modification time srep path make qualified else system println status path path file return new file status kfs impl filesize srep false kfs impl get replication srep get default block size kfs impl get modification time srep path make qualified fs data output stream append path f buffer size progressable progress throws io exception path parent f get parent parent null mkdirs parent throw new io exception mkdirs failed create parent path absolute make absolute f string srep absolute uri get path return kfs impl append srep buffer size progress fs data output stream create path file fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception exists file overwrite delete file true else throw new io exception file already exists file path parent file get parent parent null mkdirs parent throw new io exception mkdirs failed create
90	common\src\java\org\apache\hadoop\fs\local\LocalConfigKeys.java	unrelated	package org apache hadoop fs local this contains constants configuration keys used local file system raw local fs checksum fs local config keys extends common configuration keys string block size key file blocksize block size default string replication key file replication short replication default string stream buffer size key file stream buffer size stream buffer size default string bytes per checksum key file bytes per checksum bytes per checksum default string client write packet size key file client write packet size client write packet size default fs server defaults get server defaults throws io exception return new fs server defaults block size default bytes per checksum default client write packet size default replication default stream buffer size default
91	common\src\java\org\apache\hadoop\fs\local\LocalFs.java	unrelated	package org apache hadoop fs local the local fs implementation checksum fs local fs extends checksum fs local fs configuration conf throws io exception uri syntax exception super new raw local fs conf this constructor signature needed link abstract file system create file system uri configuration local fs uri uri configuration conf throws io exception uri syntax exception conf
92	common\src\java\org\apache\hadoop\fs\local\package-info.java	unrelated	package org apache hadoop fs local
93	common\src\java\org\apache\hadoop\fs\local\RawLocalFs.java	unrelated	package org apache hadoop fs local the raw local fs implementation abstract file system this impl delegates old file system raw local fs extends delegate to file system raw local fs configuration conf throws io exception uri syntax exception fs constants local fs uri conf this constructor signature needed link abstract file system create file system uri configuration raw local fs uri uri configuration conf throws io exception uri syntax exception super uri new raw local file system conf fs constants local fs uri get scheme false get uri default port return no default port file fs server defaults get server defaults throws io exception return local config keys get server defaults boolean supports symlinks return true create symlink path target path link boolean create parent throws io exception string target scheme target uri get scheme target scheme null file equals target scheme throw new io exception unable create symlink non local file system target string create parent mkdir link get parent fs permission get default true nb use create symbolic link java nio file path available try shell exec command shell link command new uri target string get path new uri link string get path catch uri syntax exception x throw new io exception invalid symlink path x get message catch io exception x throw new io exception unable create symlink x get message returns target given symlink returns empty given path refer symlink error acessing symlink string read link path p nb use read symbolic link java nio file path available could use get canonical path file get target symlink indicate given path refers symlink try string path p uri get path return shell exec command shell read link command path trim catch io exception x return return file status representing given path if path refers symlink return file status representing link rather object link refers file status get file link status path f throws io exception string target read link f try file status fs get file status f if f refers regular file directory equals target return fs otherwise f refers symlink return new file status fs get len false fs get replication fs get block size fs get modification time fs get access time fs get permission fs get owner fs get group new path target f catch file not found exception e the exists method file returns false dangling links get file not found exception links exist it also possible raced delete link use read basic file attributes method java nio file attributes available equals target return new file status false fs permission get default new path target f f refers file directory exist throw e path get link target path f throws io exception we never get valid local links resolved transparently underlying local file system accessing dangling link result io exception unresolved link exception file context never call function throw new assertion error
94	common\src\java\org\apache\hadoop\fs\permission\AccessControlException.java	unrelated	package org apache hadoop fs permission an exception access control related issues instead access control exception extends io exception required link java io serializable serial version uid l default constructor needed unwrapping link org apache hadoop ipc remote exception access control exception super permission denied constructs link access control exception specified detail message access control exception string super constructs new exception specified cause detail message tt cause null null cause string tt typically contains detail message tt cause tt link get cause method a tt null tt value permitted indicates cause nonexistent unknown access control exception throwable cause super cause
95	common\src\java\org\apache\hadoop\fs\permission\ChmodParser.java	unrelated	package org apache hadoop fs permission parse permission mode passed chmod command apply mode existing file chmod parser extends permission parser pattern chmod octal pattern pattern compile pattern chmod normal pattern pattern compile g ugoa rwx xt chmod parser string mode str throws illegal argument exception super mode str chmod normal pattern chmod octal pattern apply permission specified file determine new mode would short apply new permission file status file fs permission perms file get permission existing perms short boolean exe ok file directory existing return short combine modes existing exe ok
96	common\src\java\org\apache\hadoop\fs\permission\FsAction.java	unrelated	package org apache hadoop fs permission file system actions e g read write etc enum fs action posix style none execute x write w write execute wx read r read execute r x read write rw all rwx retain reference value array fs action vals values symbolic representation string symbol fs action string symbol return true action implies action boolean implies fs action null return ordinal ordinal ordinal return false and operation fs action fs action return vals ordinal ordinal or operation fs action fs action return vals ordinal ordinal not operation fs action return vals ordinal
97	common\src\java\org\apache\hadoop\fs\permission\FsPermission.java	unrelated	package org apache hadoop fs permission a file directory permissions fs permission implements writable log log log factory get log fs permission writable factory factory new writable factory writable new instance return new fs permission register ctor writable factories set factory fs permission factory create immutable link fs permission object fs permission create immutable short permission return new fs permission permission fs permission apply u mask fs permission umask throw new unsupported operation exception read fields data input throws io exception throw new unsupported operation exception posix permission style fs action useraction null fs action groupaction null fs action otheraction null boolean sticky bit false fs permission construct given link fs action fs permission fs action u fs action g fs action u g false fs permission fs action u fs action g fs action boolean sb set u g sb construct given mode fs permission short mode short mode copy constructor fs permission fs permission useraction useraction groupaction groupaction otheraction otheraction sticky bit sticky bit construct given mode either octal symbolic format fs permission string mode new umask parser mode get u mask return user link fs action fs action get user action return useraction return group link fs action fs action get group action return groupaction return link fs action fs action get other action return otheraction set fs action u fs action g fs action boolean sb useraction u groupaction g otheraction sticky bit sb short short n fs action v fs action values set v n v n v n n inherit doc write data output throws io exception write short short inherit doc read fields data input throws io exception short read short create initialize link fs permission link data input fs permission read data input throws io exception fs permission p new fs permission p read fields return p encode object short short short sticky bit useraction ordinal groupaction ordinal otheraction ordinal return short inherit doc boolean equals object obj obj instanceof fs permission fs permission fs permission obj return useraction useraction groupaction groupaction otheraction otheraction sticky bit sticky bit return false inherit doc hash code return short inherit doc string string string str useraction symbol groupaction symbol otheraction symbol sticky bit string builder str new string builder str str replace str length str length otheraction implies fs action execute t str str string return str apply umask permission return new one fs permission apply u mask fs permission umask return new fs permission useraction umask useraction groupaction umask groupaction otheraction umask otheraction umask property label deprecated key code get u mask method accommodate may removed version string deprecated umask label dfs umask string umask label common configuration keys fs permissions umask key default umask common configuration keys fs permissions umask default get user file creation mask umask code umask label config param umask value either symbolic octal symbolic umask applied relative file mode creation mask permission op characters clears corresponding bit mask sets bits mask octal umask specified bits set file mode creation mask code deprecated umask label config param umask value set decimal fs
98	common\src\java\org\apache\hadoop\fs\permission\PermissionParser.java	unrelated	package org apache hadoop fs permission base parsing either chmod permissions umask permissions includes common code needed either operation implemented umask parser chmod parser permission parser protected boolean symbolic false protected short user mode protected short group mode protected short others mode protected short sticky mode protected char user type protected char group type protected char others type protected char sticky bit type begin parsing permission stored mode str permission parser string mode str pattern symbolic pattern octal throws illegal argument exception matcher matcher null matcher symbolic matcher mode str find apply normal pattern mode str matcher else matcher octal matcher mode str matches apply octal pattern mode str matcher else throw new illegal argument exception mode str apply normal pattern string mode str matcher matcher are multiple permissions stored one chmod boolean comma seperated false matcher end mode str length comma seperated matcher find throw new illegal argument exception mode str groups ugoa rwx xt string str matcher group char type str char at str length boolean user group others sticky bit user group others sticky bit false char c matcher group char array switch c case u user true break case g group true break case others true break case break default throw new runtime exception unexpected user group others specifying user group others true short mode char c matcher group char array switch c case r mode break case w mode break case x mode break case x mode break case sticky bit true break default throw new runtime exception unexpected user user mode mode user type type group group mode mode group type type others others mode mode others type type sticky mode short sticky bit sticky bit type type comma seperated matcher group contains symbolic true apply octal pattern string mode str matcher matcher user type group type others type check sticky bit specified string sb matcher group sb empty sticky mode short value of sb substring sticky bit type string str matcher group user mode short value of str substring group mode short value of str substring others mode short value of str substring protected combine modes existing boolean exe ok return combine mode segments sticky bit type sticky mode existing false combine mode segments user type user mode existing exe ok combine mode segments group type group mode existing exe ok combine mode segments others type others mode existing exe ok protected combine mode segments char type mode existing boolean exe ok boolean cap x false mode convert x x cap x true mode mode switch type case mode mode existing break case mode mode existing break case break default throw new runtime exception unexpected x specified add x exe ok x already set cap x exe ok mode existing mode remove x return mode
99	common\src\java\org\apache\hadoop\fs\permission\PermissionStatus.java	unrelated	package org apache hadoop fs permission store permission related information permission status implements writable writable factory factory new writable factory writable new instance return new permission status register ctor writable factories set factory permission status factory create immutable link permission status object permission status create immutable string user string group fs permission permission return new permission status user group permission permission status apply u mask fs permission umask throw new unsupported operation exception read fields data input throws io exception throw new unsupported operation exception string username string groupname fs permission permission permission status constructor permission status string user string group fs permission permission username user groupname group permission permission return user name string get user name return username return group name string get group name return groupname return permission fs permission get permission return permission apply umask permission status apply u mask fs permission umask permission permission apply u mask umask return inherit doc read fields data input throws io exception username text read string groupname text read string permission fs permission read inherit doc write data output throws io exception write username groupname permission create initialize link permission status link data input permission status read data input throws io exception permission status p new permission status p read fields return p serialize link permission status base components write data output string username string groupname fs permission permission throws io exception text write string username text write string groupname permission write inherit doc string string return username groupname permission
100	common\src\java\org\apache\hadoop\fs\permission\UmaskParser.java	unrelated	package org apache hadoop fs permission parse umask value provided either octal symbolic format return short value umask values slightly different standard modes cannot specify sticky bit x umask parser extends permission parser pattern chmod octal pattern pattern compile leading sticky bit pattern umask symbolic pattern allow x pattern compile g ugoa rwx short umask mode umask parser string mode str throws illegal argument exception super mode str umask symbolic pattern chmod octal pattern umask mode short combine modes false to used file directory creation symbolic umask applied relative file mode creation mask permission op characters results clearing corresponding bit mask results bits indicated permission set mask for octal umask specified bits set file mode creation mask short get u mask symbolic return complement octal equivalent umask computed return short umask mode return umask mode
101	common\src\java\org\apache\hadoop\fs\s3\Block.java	unrelated	package org apache hadoop fs holds metadata block data stored link file system store block id length block id length id id length length get id return id get length return length string string return block id length
102	common\src\java\org\apache\hadoop\fs\s3\FileSystemStore.java	unrelated	package org apache hadoop fs a facility storing retrieving link i node link block file system store initialize uri uri configuration conf throws io exception string get version throws io exception store i node path path i node inode throws io exception store block block block file file throws io exception boolean inode exists path path throws io exception boolean block exists block id throws io exception i node retrieve i node path path throws io exception file retrieve block block block byte range start throws io exception delete i node path path throws io exception delete block block block throws io exception set path list sub paths path path throws io exception set path list deep sub paths path path throws io exception delete everything used testing purge throws io exception diagnostic method dump i nodes console dump throws io exception
103	common\src\java\org\apache\hadoop\fs\s3\INode.java	unrelated	package org apache hadoop fs holds file metadata including type regular file directory list blocks pointers data i node enum file type directory file file type file types file type directory file type file i node directory inode new i node file type directory null file type file type block blocks i node file type file type block blocks file type file type directory blocks null throw new illegal argument exception a directory cannot contain blocks blocks blocks block get blocks return blocks file type get file type return file type boolean directory return file type file type directory boolean file return file type file type file get serialized length return l blocks null blocks length input stream serialize throws io exception byte array output stream bytes new byte array output stream data output stream new data output stream bytes try write byte file type ordinal file write int blocks length blocks length write long blocks get id write long blocks get length close null finally io utils close stream return new byte array input stream bytes byte array i node deserialize input stream throws io exception null return null data input stream data in new data input stream file type file type i node file types data in read byte switch file type case directory close return i node directory inode case file num blocks data in read int block blocks new block num blocks num blocks id data in read long length data in read long blocks new block id length close return new i node file type blocks default throw new illegal argument exception cannot deserialize inode
104	common\src\java\org\apache\hadoop\fs\s3\Jets3tFileSystemStore.java	unrelated	package org apache hadoop fs jets file system store implements file system store string file system name fs string file system value hadoop string file system type name fs type string file system type value block string file system version name fs version string file system version value map string string metadata new hash map string string metadata put file system name file system value metadata put file system type name file system type value metadata put file system version name file system version value string path delimiter path separator string block prefix block configuration conf s service service s bucket bucket buffer size log log log factory get log jets file system store get name initialize uri uri configuration conf throws io exception conf conf s credentials credentials new s credentials credentials initialize uri conf try aws credentials aws credentials new aws credentials credentials get access key credentials get secret access key service new rest s service aws credentials catch s service exception e e get cause instanceof io exception throw io exception e get cause throw new s exception e bucket new s bucket uri get host buffer size conf get int s file system config keys s stream buffer size key s file system config keys s stream buffer size default string get version throws io exception return file system version value delete string key throws io exception try service delete object bucket key catch s service exception e e get cause instanceof io exception throw io exception e get cause throw new s exception e delete i node path path throws io exception delete path to key path delete block block block throws io exception delete block to key block boolean inode exists path path throws io exception input stream get path to key path true null return false close return true boolean block exists block id throws io exception input stream get block to key block id false null return false close return true input stream get string key boolean check metadata throws io exception try s object object service get object bucket key check metadata check metadata object return object get data input stream catch s service exception e no such key equals e get s error code return null e get cause instanceof io exception throw io exception e get cause throw new s exception e input stream get string key byte range start throws io exception try s object object service get object bucket key null null null null byte range start null return object get data input stream catch s service exception e no such key equals e get s error code return null e get cause instanceof io exception throw io exception e get cause throw new s exception e check metadata s object object throws s file system exception s service exception string name string object get metadata file system name file system value equals name throw new s file system exception not hadoop s file string type string object get metadata file system type name file system type value equals
105	common\src\java\org\apache\hadoop\fs\s3\MigrationTool.java	unrelated	package org apache hadoop fs p this tool migrating data older newer version s filesystem p p all files filesystem migrated writing block metadata datafiles touched p migration tool extends configured implements tool s service service s bucket bucket main string args throws exception res tool runner run new migration tool args system exit res run string args throws exception args length system err println usage migration tool s file system uri system err println s file system uri tfilesystem migrate tool runner print generic command usage system err return uri uri uri create args initialize uri file system store new store new jets file system store new store initialize uri get conf get f null system err println current version number unversioned system err println target version number new store get version store old store new unversioned store migrate old store new store return else s object root get root null string version string root get metadata fs version version null system err println can detect version exiting else string new version new store get version system err println current version number version system err println target version number new version version equals new store get version system err println no migration required return use version number create store store old store migrate old store new store system err println not currently implemented return system err println can detect version exiting return initialize uri uri throws io exception try string access key null string secret access key null string user info uri get user info user info null index user info index of index access key user info substring index secret access key user info substring index else access key user info access key null access key get conf get fs aws access key id secret access key null secret access key get conf get fs aws secret access key access key null secret access key null throw new illegal argument exception aws access key id secret access key must specified username password respectively url setting fs aws access key id fs aws secret access key properties respectively else access key null throw new illegal argument exception aws access key id must specified username url setting fs aws access key id property else secret access key null throw new illegal argument exception aws secret access key must specified password url setting fs aws secret access key property aws credentials aws credentials new aws credentials access key secret access key service new rest s service aws credentials catch s service exception e e get cause instanceof io exception throw io exception e get cause throw new s exception e bucket new s bucket uri get host migrate store old store file system store new store throws io exception path path old store list all paths i node inode old store retrieve i node path old store delete i node path new store store i node path inode s object get string key try return service get object bucket key catch s service exception e no such key equals e get s error
106	common\src\java\org\apache\hadoop\fs\s3\S3Credentials.java	unrelated	package org apache hadoop fs p extracts aws credentials filesystem uri configuration p s credentials string access key string secret access key determined initialize uri uri configuration conf uri get host null throw new illegal argument exception invalid hostname uri uri string user info uri get user info user info null index user info index of index access key user info substring index secret access key user info substring index else access key user info string scheme uri get scheme string access key property string format fs aws access key id scheme string secret access key property string format fs aws secret access key scheme access key null access key conf get access key property secret access key null secret access key conf get secret access key property access key null secret access key null throw new illegal argument exception aws access key id secret access key must specified username password respectively scheme url setting access key property secret access key property properties respectively else access key null throw new illegal argument exception aws access key id must specified username scheme url setting access key property property else secret access key null throw new illegal argument exception aws secret access key must specified password scheme url setting secret access key property property string get access key return access key string get secret access key return secret access key
107	common\src\java\org\apache\hadoop\fs\s3\S3Exception.java	unrelated	package org apache hadoop fs thrown problem communicating amazon s s exception extends io exception serial version uid l s exception throwable super
108	common\src\java\org\apache\hadoop\fs\s3\S3FileSystem.java	unrelated	package org apache hadoop fs p a block based link file system backed href http aws amazon com amazon s p s file system extends file system uri uri file system store store path working dir s file system set store initialize s file system file system store store store store uri get uri return uri initialize uri uri configuration conf throws io exception super initialize uri conf store null store create default store conf store initialize uri conf set conf conf uri uri create uri get scheme uri get authority working dir new path user system get property user name make qualified file system store create default store configuration conf file system store store new jets file system store retry policy base policy retry policies retry up to maximum count with fixed sleep conf get int fs max retries conf get long fs sleep time seconds time unit seconds map class extends exception retry policy exception to policy map new hash map class extends exception retry policy exception to policy map put io exception base policy exception to policy map put s exception base policy retry policy method policy retry policies retry by exception retry policies try once then fail exception to policy map map string retry policy method name to policy map new hash map string retry policy method name to policy map put store block method policy method name to policy map put retrieve block method policy return file system store retry proxy create file system store store method name to policy map path get working directory return working dir set working directory path dir working dir make absolute dir path make absolute path path path absolute return path return new path working dir path boolean mkdirs path path fs permission permission throws io exception path absolute path make absolute path list path paths new array list path paths add absolute path absolute path absolute path get parent absolute path null boolean result true path p paths result mkdir p return result boolean mkdir path path throws io exception path absolute path make absolute path i node inode store retrieve i node absolute path inode null store store i node absolute path i node directory inode else inode file throw new io exception string format can make directory path since file absolute path return true boolean file path path throws io exception i node inode store retrieve i node make absolute path inode null return false return inode file i node check file path path throws io exception i node inode store retrieve i node make absolute path inode null throw new io exception no file inode directory throw new io exception path path directory return inode file status list status path f throws io exception path absolute path make absolute f i node inode store retrieve i node absolute path inode null throw new file not found exception file f exist inode file return new file status new s file status f make qualified inode array list file status ret new array list file status path p store
109	common\src\java\org\apache\hadoop\fs\s3\S3FileSystemConfigKeys.java	unrelated	package org apache hadoop fs this contains constants configuration keys used file system s file system config keys extends common configuration keys string s block size key blocksize s block size default string s replication key replication short s replication default string s stream buffer size key stream buffer size s stream buffer size default string s bytes per checksum key bytes per checksum s bytes per checksum default string s client write packet size key client write packet size s client write packet size default
110	common\src\java\org\apache\hadoop\fs\s3\S3FileSystemException.java	unrelated	package org apache hadoop fs thrown fatal exception using link s file system s file system exception extends io exception serial version uid l s file system exception string message super message
111	common\src\java\org\apache\hadoop\fs\s3\S3InputStream.java	unrelated	package org apache hadoop fs s input stream extends fs input stream file system store store block blocks boolean closed file length pos file block file data input stream block stream block end file system statistics stats log log log factory get log s input stream get name s input stream configuration conf file system store store i node inode conf store inode null s input stream configuration conf file system store store i node inode file system statistics stats store store stats stats blocks inode get blocks block block blocks file length block get length synchronized get pos throws io exception return pos synchronized available throws io exception return file length pos synchronized seek target pos throws io exception target pos file length throw new io exception cannot seek eof pos target pos block end synchronized boolean seek to new source target pos throws io exception return false synchronized read throws io exception closed throw new io exception stream closed result pos file length pos block end block seek to pos result block stream read result pos stats null result stats increment bytes read return result synchronized read byte buf len throws io exception closed throw new io exception stream closed pos file length pos block end block seek to pos real len math min len block end pos l result block stream read buf real len result pos result stats null result stats increment bytes read result return result return synchronized block seek to target throws io exception compute desired block target block target block start target block end blocks length block length blocks get length target block end target block start block length target target block start target target block end target block break else target block start target block end target block throw new io exception impossible situation could find target position target offset into block target target block start read block blocks target block position offset into block block file store retrieve block blocks target block offset into block pos target block end target block end block stream new data input stream new file input stream block file close throws io exception closed return block stream null block stream close block stream null block file null boolean b block file delete b log warn ignoring failed delete super close closed true we support marks boolean mark supported return false mark read limit do nothing reset throws io exception throw new io exception mark supported
112	common\src\java\org\apache\hadoop\fs\s3\S3OutputStream.java	unrelated	package org apache hadoop fs s output stream extends output stream configuration conf buffer size file system store store path path block size file backup file output stream backup stream random r new random boolean closed pos file pos bytes written to block byte buf list block blocks new array list block block next block log log log factory get log s output stream get name s output stream configuration conf file system store store path path block size progressable progress buffersize throws io exception conf conf store store path path block size block size backup file new backup file backup stream new file output stream backup file buffer size buffersize buf new byte buffer size file new backup file throws io exception file dir new file conf get fs buffer dir dir exists dir mkdirs throw new io exception cannot create s buffer directory dir file result file create temp file output tmp dir result delete on exit return result get pos throws io exception return file pos synchronized write b throws io exception closed throw new io exception stream closed bytes written to block pos block size pos buffer size flush buf pos byte b file pos synchronized write byte b len throws io exception closed throw new io exception stream closed len remaining buffer size pos write math min remaining len system arraycopy b buf pos write pos write write len write file pos write bytes written to block pos block size pos buffer size flush synchronized flush throws io exception closed throw new io exception stream closed bytes written to block pos block size flush data block size bytes written to block bytes written to block block size end block flush data pos synchronized flush data max pos throws io exception working pos math min pos max pos working pos to local block backup write bytes backup stream write buf working pos track position bytes written to block working pos system arraycopy buf working pos buf pos working pos pos working pos synchronized end block throws io exception done local copy backup stream close send s todo use passed progressable report progress next block output stream store store block next block backup file internal close delete local backup start new one boolean b backup file delete b log warn ignoring failed delete backup file new backup file backup stream new file output stream backup file bytes written to block synchronized next block output stream throws io exception block id r next long store block exists block id block id r next long next block new block block id bytes written to block blocks add next block bytes written to block synchronized internal close throws io exception i node inode new i node file type file blocks array new block blocks size store store i node path inode synchronized close throws io exception closed return flush file pos bytes written to block end block backup stream close boolean b backup file delete b log warn ignoring failed delete super close closed true
113	common\src\java\org\apache\hadoop\fs\s3\VersionMismatchException.java	unrelated	package org apache hadoop fs thrown hadoop cannot read version data stored link s file system version mismatch exception extends s file system exception serial version uid l version mismatch exception string client version string data version super version mismatch client expects version client version data version data version null unversioned data version
114	common\src\java\org\apache\hadoop\fs\s3native\FileMetadata.java	unrelated	package org apache hadoop fs native p holds basic metadata file stored link native file system store p file metadata string key length last modified file metadata string key length last modified key key length length last modified last modified string get key return key get length return length get last modified return last modified string string return file metadata key length last modified
115	common\src\java\org\apache\hadoop\fs\s3native\Jets3tNativeFileSystemStore.java	unrelated	package org apache hadoop fs native jets native file system store implements native file system store s service service s bucket bucket initialize uri uri configuration conf throws io exception s credentials credentials new s credentials credentials initialize uri conf try aws credentials aws credentials new aws credentials credentials get access key credentials get secret access key service new rest s service aws credentials catch s service exception e handle service exception e bucket new s bucket uri get host store file string key file file byte md hash throws io exception buffered input stream null try new buffered input stream new file input stream file s object object new s object key object set data input stream object set content type binary octet stream object set content length file length md hash null object set md hash md hash service put object bucket object catch s service exception e handle service exception e finally null try close catch io exception e ignore store empty file string key throws io exception try s object object new s object key object set data input stream new byte array input stream new byte object set content type binary octet stream object set content length service put object bucket object catch s service exception e handle service exception e file metadata retrieve metadata string key throws io exception try s object object service get object details bucket key return new file metadata key object get content length object get last modified date get time catch s service exception e following brittle is better way e get message contains response code return null handle service exception e return null never returned keep compiler happy input stream retrieve string key throws io exception try s object object service get object bucket key return object get data input stream catch s service exception e handle service exception key e return null never returned keep compiler happy input stream retrieve string key byte range start throws io exception try s object object service get object bucket key null null null null byte range start null return object get data input stream catch s service exception e handle service exception key e return null never returned keep compiler happy partial listing list string prefix max listing length throws io exception return list prefix max listing length null false partial listing list string prefix max listing length string prior last key boolean recurse throws io exception return list prefix recurse null path delimiter max listing length prior last key partial listing list string prefix string delimiter max listing length string prior last key throws io exception try prefix length prefix ends with path delimiter prefix path delimiter s objects chunk chunk service list objects chunked bucket get name prefix delimiter max listing length prior last key file metadata file metadata new file metadata chunk get objects length file metadata length s object object chunk get objects file metadata new file metadata object get key object get content length object get last modified date get time return new partial listing chunk get
116	common\src\java\org\apache\hadoop\fs\s3native\NativeFileSystemStore.java	unrelated	package org apache hadoop fs native p an abstraction key based link file store p native file system store initialize uri uri configuration conf throws io exception store file string key file file byte md hash throws io exception store empty file string key throws io exception file metadata retrieve metadata string key throws io exception input stream retrieve string key throws io exception input stream retrieve string key byte range start throws io exception partial listing list string prefix max listing length throws io exception partial listing list string prefix max listing length string prior last key boolean recursive throws io exception delete string key throws io exception copy string src key string dst key throws io exception delete keys given prefix used testing purge string prefix throws io exception diagnostic method dump state console dump throws io exception
117	common\src\java\org\apache\hadoop\fs\s3native\NativeS3FileSystem.java	unrelated	package org apache hadoop fs native p a link file system reading writing files stored href http aws amazon com amazon s unlike link org apache hadoop fs s file system implementation stores files s native form read s tools a note directories s course native support the idiom choose directory created use empty object dirpath folder marker further interoperate s tools also accept following object dirpath denoting directory marker exists objects prefix dirpath directory said exist file name directory marker directory exists file masks directory directory never returned p native s file system extends file system log log log factory get log native s file system string folder suffix folder string path delimiter path separator s max listing length native s fs input stream extends fs input stream native file system store store statistics statistics input stream string key pos native s fs input stream native file system store store statistics statistics input stream string key store store statistics statistics key key synchronized read throws io exception result try result read catch io exception e log info received io exception reading key attempting reopen seek pos result read result pos statistics null result statistics increment bytes read return result synchronized read byte b len throws io exception result try result read b len catch io exception e log info received io exception reading key attempting reopen seek pos result read b len result pos result statistics null result statistics increment bytes read result return result close throws io exception close synchronized seek pos throws io exception close log info opening key key reading position pos store retrieve key pos pos pos synchronized get pos throws io exception return pos boolean seek to new source target pos throws io exception return false native s fs output stream extends output stream configuration conf string key file backup file output stream backup stream message digest digest boolean closed native s fs output stream configuration conf native file system store store string key progressable progress buffer size throws io exception conf conf key key backup file new backup file log info output stream key key writing tempfile backup file try digest message digest get instance md backup stream new buffered output stream new digest output stream new file output stream backup file digest catch no such algorithm exception e log warn cannot load md digest algorithm skipping message integrity check e backup stream new buffered output stream new file output stream backup file file new backup file throws io exception file dir new file conf get fs buffer dir dir mkdirs dir exists throw new io exception cannot create s buffer directory dir file result file create temp file output tmp dir result delete on exit return result flush throws io exception backup stream flush synchronized close throws io exception closed return backup stream close log info output stream key key closed now beginning upload try byte md hash digest null null digest digest store store file key backup file md hash finally backup file delete log warn could delete temporary n file backup file
118	common\src\java\org\apache\hadoop\fs\s3native\PartialListing.java	unrelated	package org apache hadoop fs native p holds information directory listing link native file system store this includes link file metadata files directories names contained directory p p this listing may returned chunks code prior last key code provided next chunk may requested p partial listing string prior last key file metadata files string common prefixes partial listing string prior last key file metadata files string common prefixes prior last key prior last key files files common prefixes common prefixes file metadata get files return files string get common prefixes return common prefixes string get prior last key return prior last key
119	common\src\java\org\apache\hadoop\fs\s3native\S3NativeFileSystemConfigKeys.java	unrelated	package org apache hadoop fs native this contains constants configuration keys used file system s native file system config keys extends common configuration keys string s native block size key native blocksize s native block size default string s native replication key native replication short s native replication default string s native stream buffer size key native stream buffer size s native stream buffer size default string s native bytes per checksum key native bytes per checksum s native bytes per checksum default string s native client write packet size key native client write packet size s native client write packet size default
120	common\src\java\org\apache\hadoop\fs\shell\Command.java	unrelated	package org apache hadoop fs shell an execution file system command command extends configured default name command string name command usage switches arguments format string usage command description string description protected string args protected string name protected exit code protected num errors protected boolean recursive false protected array list exception exceptions new array list exception log log log factory get log command allows stdout captured necessary print stream system allows stderr captured necessary print stream err system err constructor protected command system err system err constructor protected command configuration conf super conf string get command name protected set recursive boolean flag recursive flag protected boolean recursive return recursive execute command input path protected run path path throws io exception for source path execute command run all exit code string src args try path data srcs path data expand as glob src get conf path data srcs run path catch io exception e exit code display error e return exit code invokes command handler the default behavior process options expand arguments process argument pre run link process options linked list link process raw arguments linked list link expand arguments linked list link expand argument string link process arguments linked list link process argument path data link process path argument path data link process paths path data path data link process path path data link process nonexistent path path data pre most commands chose implement link process options linked list link process path path data run string argv linked list string args new linked list string arrays list argv try deprecated display warning deprecated please use get replacement command instead process options args process raw arguments args catch io exception e display error e return num errors exit code exit code for error the exit code returned errors occur execution this method needed account inconsistency exit codes returned various commands protected exit code for error return must implemented commands process command line flags check bounds remaining arguments if illegal argument exception thrown fs shell object print short usage command protected process options linked list string args throws io exception allows commands use paths handle raw arguments default behavior expand arguments via link expand arguments linked list pass resulting list link process arguments linked list protected process raw arguments linked list string args throws io exception process arguments expand arguments args expands list arguments link path data objects the default behavior call link expand argument string element default globs argument the loop catches io exceptions increments error count displays exception protected linked list path data expand arguments linked list string args throws io exception linked list path data expanded args new linked list path data string arg args try expanded args add all expand argument arg catch io exception e exceptions probably nasty display error e return expanded args expand given argument list link path data objects the default behavior expand globs commands may perform expansions argument protected list path data expand argument string arg throws io exception path data items path data expand as glob arg get conf items length glob failed match throw
121	common\src\java\org\apache\hadoop\fs\shell\CommandFactory.java	unrelated	package org apache hadoop fs shell search register commands command factory extends configured implements configurable map string class extends command map new hash map string class extends command map string command object map new hash map string command factory constructor commands command factory null factory constructor commands command factory configuration conf super conf invokes register commands command factory given this method abstracts contract factory command do assume directly invoking register commands given effect register commands class registrar class try registrar class get method register commands command factory invoke null catch exception e throw new runtime exception string utils stringify exception e register given handling given list command names add class class extends command cmd class string names string name names map put name cmd class register given object handling given list command names avoid calling method use link add class class string whenever possible avoid startup overhead excessive command object instantiations this method intended handling nested non usable namely help usage add object command cmd object string names string name names object map put name cmd object map put name null shows list commands returns instance implementing given command the must registered via link add class class string command get instance string cmd return get instance cmd get conf get instance requested command command get instance string cmd name configuration conf conf null throw new null pointer exception configuration null command instance object map get cmd name instance null class extends command cmd class map get cmd name cmd class null instance reflection utils new instance cmd class conf instance set name cmd name return instance gets registered commands string get names string names map key set array new string arrays sort names return names
122	common\src\java\org\apache\hadoop\fs\shell\CommandFormat.java	unrelated	package org apache hadoop fs shell parse args command check format args command format min par max par map string boolean options new hash map string boolean boolean ignore unknown opts false command format string n min max string possible opt min max possible opt simple parsing command line arguments command format min max string possible opt min par min max par max string opt possible opt opt null ignore unknown opts true else options put opt boolean false parse parameters starting given position consider using variant directly takes list list string parse string args pos list string parameters new array list string arrays list args parameters sub list pos clear parse parameters return parameters parse parameters given list args the list destructively modified remove options parse list string args pos pos args size string arg args get pos stop opt stdin arg found arg starts with arg equals break else arg equals force end option processing args remove pos break string opt arg substring options contains key opt args remove pos options put opt boolean true else ignore unknown opts pos else throw new unknown option exception arg psize args size psize min par throw new not enough arguments exception min par psize psize max par throw new too many arguments exception max par psize return option set boolean get opt string option return options contains key option options get option false returns options set set string get opts set string opt set new hash set string map entry string boolean entry options entry set entry get value opt set add entry get key return opt set used arguments exceed bounds illegal number of arguments exception extends illegal argument exception serial version uid l protected expected protected actual protected illegal number of arguments exception want got expected want actual got string get message return expected expected got actual used many arguments supplied command too many arguments exception extends illegal number of arguments exception serial version uid l too many arguments exception expected actual super expected actual string get message return too many arguments super get message used arguments supplied command not enough arguments exception extends illegal number of arguments exception serial version uid l not enough arguments exception expected actual super expected actual string get message return not enough arguments super get message used unsupported option supplied command unknown option exception extends illegal argument exception serial version uid l protected string option null unknown option exception string unknown option super illegal option unknown option option unknown option string get option return option
123	common\src\java\org\apache\hadoop\fs\shell\CommandUtils.java	unrelated	package org apache hadoop fs shell command utils string format description string usage string desciptions string builder b new string builder usage desciptions desciptions length b append n desciptions return b string
124	common\src\java\org\apache\hadoop\fs\shell\CommandWithDestination.java	unrelated	package org apache hadoop fs shell provides argument processing ensure destination valid number source arguments a process paths accepts source resolved target sources resolved children destination directory command with destination extends fs command protected path data dst protected boolean overwrite false this method used enable force f option copying files protected set overwrite boolean flag overwrite flag the last arg expected local path one argument given destination current directory protected get local destination linked list string args throws io exception string path string args size path cur dir args remove last dst new path data new file path string get conf the last arg expected remote path one argument given destination remote user directory protected get remote destination linked list string args throws io exception args size dst new path data path cur dir get conf else string path string args remove last path glob must match one one path path data items path data expand as glob path string get conf switch items length case throw new path not found exception path string case dst items break default throw new path io exception path string too many matches protected process arguments linked list path data args throws io exception one arg destination must directory one arg dst must exist must directory args size dst exists throw new path not found exception dst string dst stat directory throw new path is not directory exception dst string else dst exists dst stat directory overwrite throw new path exists exception dst string super process arguments args protected process paths path data parent path data items throws io exception path data saved dst dst try modify dst descend append basename current directory processed parent null dst dst get path data for child parent super process paths parent items finally dst saved dst protected process path path data src throws io exception path data target destination directory make target child path else use destination dst exists dst stat directory target dst get path data for child src else target dst target exists overwrite throw new path exists exception target string try invoke process path source resolved target process path src target catch path io exception e add target unless already one e get target path null e set target path target string throw e called source target destination pair protected process path path data src path data target throws io exception
125	common\src\java\org\apache\hadoop\fs\shell\CopyCommands.java	unrelated	package org apache hadoop fs shell various commands copy files copy commands register commands command factory factory factory add class merge getmerge factory add class cp cp factory add class copy from local copy from local factory add class copy to local copy to local factory add class get get factory add class put put merge multiple files together merge extends fs command string name getmerge string usage src localdst addnl string description get files directories n match source file pattern merge sort n one file local fs src kept protected path data dst null protected string delimiter null protected process options linked list string args throws io exception command format cf new command format cf parse args todo really nl option args size boolean parse boolean args remove last delimiter n else delimiter null dst new path data new file args remove last get conf protected process path path data src throws io exception file util copy merge src fs src path dst fs dst path false get conf delimiter cp extends command with destination string name cp string usage src dst string description copy files match file pattern src n destination when copying multiple files destination n must directory protected process options linked list string args throws io exception command format cf new command format integer max value f cf parse args set overwrite cf get opt f get remote destination args protected process path path data src path data target throws io exception file util copy src fs src path target fs target path false overwrite get conf idea error file utils masks cases even report error throw new path io exception src string copy local files remote filesystem get extends command with destination string name get string usage ignore crc crc src localdst string description copy files match file pattern src n local name src kept when copying multiple n files destination must directory the prefix tmp file used copy to local it must least three characters required link java io file create temp file string string file string copytolocal prefix copy to local boolean copy crc boolean verify checksum local file system local fs protected process options linked list string args throws io exception local fs file system get local get conf command format cf new command format integer max value crc ignore crc cf parse args copy crc cf get opt crc verify checksum cf get opt ignore crc set recursive true get local destination args protected process path path data src path data target throws io exception src fs set verify checksum verify checksum copy crc src fs instanceof checksum file system display warning src fs does support checksums copy crc false file target file local fs path to file target path src stat file copy file maybe crc copy file to local src target path copy crc copy crc to local src target path else src stat directory create remote directory structure locally target file mkdirs throw new path io exception target string else throw new path operation exception src string copy file to
126	common\src\java\org\apache\hadoop\fs\shell\Count.java	unrelated	package org apache hadoop fs shell count number directories files bytes quota remaining quota count extends fs command register names count command register commands command factory factory factory add class count count string name count string usage q path string description count number directories files bytes paths n match specified file pattern the output columns n dir count file count content size file name n quota remaining quata space quota remaining space quota n dir count file count content size file name boolean show quotas constructor count constructor count string cmd pos configuration conf super conf args arrays copy of range cmd pos cmd length protected process options linked list string args command format cf new command format integer max value q cf parse args args empty default path current working directory args add show quotas cf get opt q protected process path path data src throws io exception content summary summary src fs get content summary src path println summary string show quotas src path
127	common\src\java\org\apache\hadoop\fs\shell\Delete.java	unrelated	package org apache hadoop fs shell classes delete paths delete register commands command factory factory factory add class rm rm factory add class rmdir rmdir factory add class rmr rmr factory add class expunge expunge remove non directory paths rm extends fs command string name rm string usage f r r skip trash src string description delete files match specified file pattern n equivalent unix command rm src n skip trash option bypasses trash enabled immediately n deletes src n f if file exist display diagnostic n message modify exit status reflect error n r r recursively deletes directories boolean skip trash false boolean delete dirs false boolean ignore fnf false protected process options linked list string args throws io exception command format cf new command format integer max value f r r skip trash cf parse args ignore fnf cf get opt f delete dirs cf get opt r cf get opt r skip trash cf get opt skip trash protected process nonexistent path path data item throws io exception ignore fnf super process nonexistent path item protected process path path data item throws io exception item stat directory delete dirs throw new path is directory exception item string todo user wants trash used problem ie creating trash dir moving item deleted etc path deleted move to trash returns false falls thru fs delete seem right move to trash item return item fs delete item path delete dirs throw new path io exception item string println deleted item boolean move to trash path data item throws io exception boolean success false skip trash try success trash move to appropriate trash item fs item path get conf catch file not found exception fnfe throw fnfe catch io exception ioe throw new io exception ioe get message consider using skip trash option ioe return success remove path rmr extends rm string name rmr protected process options linked list string args throws io exception args add first r super process options args string get replacement command return rm r remove empty directories rmdir extends fs command string name rmdir string usage ignore fail non empty dir string description removes directory entry specified directory argument n provided empty n boolean ignore non empty false protected process options linked list string args throws io exception command format cf new command format integer max value ignore fail non empty cf parse args ignore non empty cf get opt ignore fail non empty protected process path path data item throws io exception item stat directory throw new path is not directory exception item string item fs list status item path length item fs delete item path false throw new path io exception item string else ignore non empty throw new path is not empty directory exception item string empty trash expunge extends fs command string name expunge string usage string description empty trash todo probably allow path arguments filesystems protected process options linked list string args throws io exception command format cf new command format cf parse args protected process arguments linked list path data args throws io
128	common\src\java\org\apache\hadoop\fs\shell\Display.java	unrelated	package org apache hadoop fs shell display contents files display extends fs command register commands command factory factory factory add class cat cat factory add class text text displays file content stdout cat extends display string name cat string usage ignore crc src string description fetch files match file pattern src n display content stdout n boolean verify checksum true protected process options linked list string args throws io exception command format cf new command format integer max value ignore crc cf parse args verify checksum cf get opt ignore crc protected process path path data item throws io exception item stat directory throw new path is directory exception item string item fs set verify checksum verify checksum print to stdout get input stream item print to stdout input stream throws io exception try io utils copy bytes get conf false finally close protected input stream get input stream path data item throws io exception return item fs open item path same behavior cat handles zip text record input stream encodings text extends cat string name text string usage cat usage string description takes source file outputs file text format n the allowed formats zip text record input stream protected input stream get input stream path data item throws io exception fs data input stream fs data input stream super get input stream item check codecs compression codec factory cf new compression codec factory get conf compression codec codec cf get codec item path codec null return codec create input stream switch read short case x f b rfc seek return new gzip input stream case x s e read byte q close return new text record input stream item stat break seek return protected text record input stream extends input stream sequence file reader r writable comparable key writable val data input buffer inbuf data output buffer outbuf text record input stream file status f throws io exception path fpath f get path configuration lconf get conf r new sequence file reader lconf sequence file reader file fpath key reflection utils new instance r get key class subclass writable comparable lconf val reflection utils new instance r get value class subclass writable lconf inbuf new data input buffer outbuf new data output buffer read throws io exception ret null inbuf ret inbuf read r next key val return byte tmp key string get bytes outbuf write tmp tmp length outbuf write tmp val string get bytes outbuf write tmp tmp length outbuf write n inbuf reset outbuf get data outbuf get length outbuf reset ret inbuf read return ret close throws io exception r close super close
129	common\src\java\org\apache\hadoop\fs\shell\FsCommand.java	unrelated	package org apache hadoop fs shell base hadoop fs commands may look useful placeholder future functionality act registry fs commands currently used implement unnecessary methods base fs command extends command register command used fs subcommand register commands command factory factory factory register commands copy commands factory register commands count factory register commands delete factory register commands display factory register commands fs shell permissions factory register commands fs usage factory register commands ls factory register commands mkdir factory register commands move commands factory register commands set replication factory register commands stat factory register commands tail factory register commands test factory register commands touch protected fs command protected fs command configuration conf super conf historical method command string get command name return get name method normally invoked runall overridden protected run path path throws io exception throw new runtime exception supposed get run all return run args
130	common\src\java\org\apache\hadoop\fs\shell\FsUsage.java	unrelated	package org apache hadoop fs shell base commands related viewing filesystem usage du df fs usage extends fs command register commands command factory factory factory add class df df factory add class du du factory add class dus dus protected boolean human readable false protected table builder usages table protected string format size size return human readable string utils human readable int size string value of size show size partition filesystem df extends fs usage string name df string usage path string description shows capacity free used space filesystem n if filesystem multiple partitions path n particular partition specified status root n partitions shown n formats sizes files human readable fashion n rather number bytes n n protected process options linked list string args throws io exception command format cf new command format integer max value cf parse args human readable cf get opt args empty args add path separator protected process arguments linked list path data args throws io exception usages table new table builder filesystem size used available use usages table set right align super process arguments args usages table empty usages table print to stream protected process path path data item throws io exception fs status fs stats item fs get status item path size fs stats get capacity used fs stats get used free fs stats get remaining usages table add row item fs get uri format size size format size used format size free string utils format percent used size show disk usage du extends fs usage string name du string usage path string description show amount space bytes used files n match specified file pattern the following flags optional n rather showing size individual file n matches pattern shows total summary size n formats sizes files human readable fashion n rather number bytes n n note even without option shows size summaries n one level deep directory n the output form n tsize tname full path n protected boolean summary false protected process options linked list string args throws io exception command format cf new command format integer max value cf parse args human readable cf get opt summary cf get opt args empty args add path cur dir protected process path argument path data item throws io exception usages table new table builder go one level deep dirs cmdline unless summary mode summary item stat directory recurse path item else super process path argument item usages table print to stream protected process path path data item throws io exception length item stat directory length item fs get content summary item path get length else length item stat get len usages table add row format size length item show disk usage summary dus extends du string name dus protected process options linked list string args throws io exception args add first super process options args string get replacement command return du creates table aligned values based maximum width column table builder protected boolean header false protected list string rows protected widths protected boolean right align create table w headers table builder columns rows new array list
131	common\src\java\org\apache\hadoop\fs\shell\Ls.java	unrelated	package org apache hadoop fs shell get listing files match file patterns ls extends fs command register commands command factory factory factory add class ls ls factory add class lsr lsr string name ls string usage r path string description list contents match specified file pattern if n path specified contents user current user n listed directory entries form n tdir name full path dir n file entries form n tfile name full path r n size n n number replicas specified file n size size file bytes n directories listed plain files n formats sizes files human readable fashion n rather number bytes n r recursively list contents directories protected simple date format date format new simple date format yyyy mm dd hh mm protected max repl max len max owner max group protected string line format protected boolean dir recurse protected boolean human readable false protected string format size size return human readable string utils human readable int size string value of size protected process options linked list string args throws io exception command format cf new command format integer max value r cf parse args dir recurse cf get opt set recursive cf get opt r dir recurse human readable cf get opt args empty args add path cur dir protected process path argument path data item throws io exception implicitly recurse cmdline directories dir recurse item stat directory recurse path item else super process path argument item protected process paths path data parent path data items throws io exception recursive items length println found items length items adjust column widths items super process paths parent items protected process path path data item throws io exception file status stat item stat string line string format line format stat directory stat get permission stat file stat get replication stat get owner stat get group format size stat get len date format format new date stat get modification time item path uri get path println line compute column widths rebuild format adjust column widths path data items path data item items file status stat item stat max repl max length max repl stat get replication max len max length max len stat get len max owner max length max owner stat get owner max group max length max group stat get group string builder fmt new string builder fmt append permission fmt append max repl fmt append max owner fmt append max group fmt append max len fmt append mod time path line format fmt string max length n object value return math max n value null string value of value length get recursive listing files match file patterns same ls r lsr extends ls string name lsr protected process options linked list string args throws io exception args add first r super process options args string get replacement command return ls r
132	common\src\java\org\apache\hadoop\fs\shell\Mkdir.java	unrelated	package org apache hadoop fs shell create given dir mkdir extends fs command register commands command factory factory factory add class mkdir mkdir string name mkdir string usage path string description create directory specified location protected process options linked list string args command format cf new command format integer max value cf parse args protected process path path data item throws io exception item stat directory throw new path exists exception item string else throw new path is not directory exception item string protected process nonexistent path path data item throws io exception item fs mkdirs item path throw new path io exception item string
133	common\src\java\org\apache\hadoop\fs\shell\MoveCommands.java	unrelated	package org apache hadoop fs shell various commands moving files move commands register commands command factory factory factory add class move from local move from local factory add class move to local move to local factory add class rename mv move local files remote filesystem move from local extends copy from local string name move from local string usage localsrc dst string description same put except source n deleted copied protected process path path data src path data target throws io exception target fs move from local file src path target path move remote files local filesystem move to local extends fs command string name move to local string usage src localdst string description not implemented yet protected process options linked list string args throws io exception throw new io exception option move to local implemented yet move rename paths fileystem rename extends command with destination string name mv string usage src dst string description move files match specified file pattern src n destination dst when moving multiple files n destination must directory protected process options linked list string args throws io exception command format cf new command format integer max value cf parse args get remote destination args protected process path path data src path data target throws io exception src fs get uri equals target fs get uri throw new path io exception src string does match target filesystem target fs rename src path target path way know actual error throw new path io exception src string
134	common\src\java\org\apache\hadoop\fs\shell\package-info.java	unrelated	package org apache hadoop fs shell
135	common\src\java\org\apache\hadoop\fs\shell\PathData.java	unrelated	package org apache hadoop fs shell encapsulates path path file status stat file system fs the stat field null path exist path data protected string null path path file status stat file system fs boolean exists creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize path data string path string configuration conf throws io exception path string path new path path string fs path get file system conf set stat get stat fs path creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize path data file local path configuration conf throws io exception local path string path new path fs file system get local conf set stat get stat fs path creates object wrap given parameters fields path data file system fs path path file status stat path string path path fs fs set stat stat convenience ctor looks file status path if path exist status null path data file system fs path path throws io exception fs path get stat fs path creates object wrap given parameters fields the used create path recorded since path object return exactly used initialize if file status null path used initialized path else path used path data file system fs string path string file status stat path string path stat null stat get path new path path string fs fs set stat stat need method ctor file status get stat file system fs path path throws io exception file status status null try status fs get file status path catch file not found exception e ignore fnf return status set stat file status stat stat stat exists stat null convenience ctor extracts path given file status path data file system fs file status stat fs stat get path stat updates paths file status file status refresh status throws io exception set stat fs get file status path return stat returns list path data objects items contained given directory path data get directory contents throws io exception stat directory throw new path is not directory exception file status stats fs list status path path data items new path data stats length stats length preserve relative paths string basename stats get path get name string parent parent ends with path separator parent path separator items new path data fs parent basename stats return items creates new object child entry directory path data get path data for child path data child throws io exception stat directory throw new path is not directory exception return new path data fs new path path child path get name expand given path glob pattern non existent paths throw exception creation commands like touch mkdir need create the stat field null path exist exist list contain single path data null stat path data expand as glob string pattern configuration conf throws io exception path glob path new path pattern file system fs glob path get file system conf file status stats fs glob status glob path path data items null stats null glob file found
136	common\src\java\org\apache\hadoop\fs\shell\PathExceptions.java	unrelated	package org apache hadoop fs shell standardized posix linux style exceptions path related errors returns io exception format path standard error path exceptions eio path io exception extends io exception serial version uid l string eio input output error note really path path buggy return exact used construct path mangles uris authority string operation string path string target path constructor generic i o error exception path io exception string path path eio null appends text throwable default error message path io exception string path throwable cause path eio cause avoid using method use subclass path io exception possible path io exception string path string error path error null protected path io exception string path string error throwable cause super error cause path path format cmd operation path target error string get message string builder message new string builder operation null message append operation message append format path path target path null message append format path target path message append super get message get cause null message append get cause get message return message string path get path return new path path path get target path return target path null new path target path null optional operation preface path set operation string operation operation operation optional path exception involved two paths ex copy operation set target path string target path target path target path string format path string path return path enoent path not found exception extends path io exception serial version uid l path not found exception string path super path no file directory eexists path exists exception extends path io exception serial version uid l path exists exception string path super path file exists protected path exists exception string path string error super path error eisdir path is directory exception extends path exists exception serial version uid l path is directory exception string path super path is directory enotdir path is not directory exception extends path exists exception serial version uid l path is not directory exception string path super path is directory generated rm commands path is not empty directory exception extends path exists exception path is not empty directory exception string path super path directory empty eacces path access denied exception extends path io exception serial version uid l path access denied exception string path super path permission denied eperm path permission exception extends path io exception serial version uid l path permission exception string path super path operation permitted enotsup path operation exception extends path exists exception serial version uid l path operation exception string path super path operation supported
137	common\src\java\org\apache\hadoop\fs\shell\SetReplication.java	unrelated	package org apache hadoop fs shell modifies replication factor set replication extends fs command register commands command factory factory factory add class set replication setrep string name setrep string usage r w rep path file string description set replication level file n the r flag requests recursive change replication level n entire tree protected short new rep protected list path data wait list new linked list path data protected boolean wait opt false protected process options linked list string args throws io exception command format cf new command format integer max value r w cf parse args wait opt cf get opt w set recursive cf get opt r try new rep short parse short args remove first catch number format exception nfe display warning illegal replication positive integer expected throw nfe new rep throw new illegal argument exception replication must protected process arguments linked list path data args throws io exception super process arguments args wait opt wait for replication protected process path path data item throws io exception item stat symlink throw new path io exception item string symlinks unsupported item stat file item fs set replication item path new rep throw new io exception could set replication item println replication new rep set item wait opt wait list add item wait files wait list replication number equal rep wait for replication throws io exception path data item wait list print waiting item flush boolean printed warning false boolean done false done item refresh status block location locations item fs get file block locations item stat item stat get len locations length current rep locations get hosts length current rep new rep printed warning current rep new rep println n warning waiting time may decreasing number replications printed warning true break done locations length done break print flush try thread sleep catch interrupted exception e println done
138	common\src\java\org\apache\hadoop\fs\shell\Stat.java	unrelated	package org apache hadoop fs shell print statistics path specified format format sequences b size file blocks n filename block size r replication utc date quot yyyy mm dd hh mm ss quot y milliseconds since january utc stat extends fs command register commands command factory factory factory add class stat stat string name stat string usage format path string description print statistics file directory path n specified format format accepts filesize blocks b filename n n block size replication r modification date y n protected simple date format time fmt time fmt new simple date format yyyy mm dd hh mm ss time fmt set time zone time zone get time zone utc default format protected string format protected process options linked list string args throws io exception command format cf new command format integer max value r cf parse args set recursive cf get opt r args get first contains format args remove first cf parse args make sure still least one arg protected process path path data item throws io exception file status stat item stat string builder buf new string builder char fmt format char array fmt length fmt buf append fmt else silently drops trailing fmt length break switch fmt case b buf append stat get len break case f buf append stat directory directory stat file regular file symlink break case n buf append item path get name break case buf append stat get block size break case r buf append stat get replication break case buf append time fmt format new date stat get modification time break case y buf append stat get modification time break default leaves unknown alone causes potential future format options break strings use escape percents buf append fmt break println buf string
139	common\src\java\org\apache\hadoop\fs\shell\Tail.java	unrelated	package org apache hadoop fs shell get listing files match file patterns tail extends fs command register commands command factory factory factory add class tail tail string name tail string usage f file string description show last kb file n the f option shows appended data file grows n starting offset boolean follow false follow delay milliseconds protected process options linked list string args throws io exception command format cf new command format f cf parse args follow cf get opt f todo hadoop add glob support backwards compat protected list path data expand argument string arg throws io exception list path data items new linked list path data items add new path data arg get conf return items protected process path path data item throws io exception item stat directory throw new path is directory exception item string offset dump from offset item starting offset follow try thread sleep follow delay catch interrupted exception e break offset dump from offset item offset dump from offset path data item offset throws io exception file size item refresh status get len offset file size return file size treat negative offset relative end file floor offset offset math max file size offset fs data input stream item fs open item path try seek offset use conf system configured io block size used io utils copy bytes system get conf false offset get pos finally close return offset
140	common\src\java\org\apache\hadoop\fs\shell\Touchz.java	unrelated	package org apache hadoop fs shell unix touch like commands touch extends fs command register commands command factory factory factory add class touchz touchz re create zero length file specified path this replaced unix like touch files may modified touchz extends touch string name touchz string usage path string description creates file zero length n path current time timestamp path n an error returned file exists non zero length n protected process options linked list string args command format cf new command format integer max value cf parse args protected process path path data item throws io exception item stat directory todo handle throw new path is directory exception item string item stat get len throw new path io exception item string not zero length file touchz item protected process nonexistent path path data item throws io exception touchz item touchz path data item throws io exception item fs create item path close
141	common\src\java\org\apache\hadoop\fs\viewfs\ChRootedFileSystem.java	unrelated	package org apache hadoop fs viewfs code ch rooted file system code file system root path root base file system example for base file system hdfs nn ch root usr foo members setup shown ul li fs base file system points hdfs nn li li uri hdfs nn user foo li li ch root path part user foo li li working dir directory related ch root li ul the paths resolved follows ch rooted file system ul li absolute path b c resolved user foo b c fs li li relative path x resolved user foo working dir x li ul ch rooted file system extends file system file system fs base file system whose root changed uri uri base uri ch root path ch root path part root root base string ch root path part string path working dir protected file system get my fs return fs protected path full path path path super check path path return path absolute new path ch root path part string path uri get path new path ch root path part string working dir uri get path path constructor ch rooted file system file system fs path root throws uri syntax exception fs fs fs make qualified root check root valid path fs would like call fs check path root ch root path part new path root uri get path ch root path part string ch root path part uri get path try initialize fs get uri fs get conf catch io exception e this exception thrown throw new runtime exception this occur we making uri chrooted path e g file chrooted path this questionable since path make qualified uri path ignores path part uri since internal ignore issue make external needs resolved handle two cases scheme scheme authority uri new uri fs get uri string fs get uri get authority null path separator ch root path part string substring working dir get home directory we use wd fs called new file system instance constructed file system initialize uri name configuration conf throws io exception fs initialize name conf super initialize name conf set conf conf uri get uri return uri path make qualified path path return fs make qualified path not fs make qualified full path path strip root path string strip out root path p throws io exception try check path p catch illegal argument exception e throw new io exception internal error path p uri uri string path part p uri get path return path part length ch root path part string length path part substring ch root path part string length protected path get initial working directory choices null user uname strip root fs inital wd only reasonable choice initial wd chrooted fds null default rule wd applied return null path get resolved qualified path path f throws file not found exception return fs make qualified new path ch root path part string f uri string path get home directory return new path user system get property user name make qualified get uri null path get working directory return working dir
142	common\src\java\org\apache\hadoop\fs\viewfs\ChRootedFs.java	unrelated	package org apache hadoop fs viewfs code chrooted fs code file system root path root base file system example for base file system hdfs nn ch root usr foo members setup shown ul li fs base file system points hdfs nn li li uri hdfs nn user foo li li ch root path part user foo li li working dir directory related ch root li ul the paths resolved follows ch rooted file system ul li absolute path b c resolved user foo b c fs li li relative path x resolved user foo working dir x li ul ch rooted fs extends abstract file system abstract file system fs base file system whose root changed uri uri base uri chroot path ch root path part root root base string ch root path part string protected abstract file system get my fs return fs protected path full path path path super check path path return new path ch root path part string path uri get path ch rooted fs abstract file system fs path root throws uri syntax exception super fs get uri fs get uri get scheme fs get uri get authority null fs get uri default port fs fs fs check path root ch root path part new path fs get uri path root ch root path part string ch root path part uri get path we making uri chrooted path e g file chrooted path this questionable since path make qualified uri path ignores path part uri since internal ignore issue make external needs resolved handle two cases scheme scheme authority uri new uri fs get uri string fs get uri get authority null path separator ch root path part string substring super check path root uri get uri return uri strip root path string strip out root path p try check path p catch illegal argument exception e throw new runtime exception internal error path p uri uri string path part p uri get path return path part length ch root path part string length path part substring ch root path part string length path get home directory return fs get home directory path get initial working directory choices return null strip root fs inital wd only reasonable choice initial wd chrooted fds null return null path get resolved qualified path path f throws file not found exception return fs make qualified new path ch root path part string f uri string fs data output stream create internal path f enum set create flag flag fs permission absolute permission buffer size short replication block size progressable progress bytes per checksum boolean create parent throws io exception unresolved link exception return fs create internal full path f flag absolute permission buffer size replication block size progress bytes per checksum create parent boolean delete path f boolean recursive throws io exception unresolved link exception return fs delete full path f recursive block location get file block locations path f start len throws io exception unresolved link exception return fs get file block locations full path f start len file checksum
143	common\src\java\org\apache\hadoop\fs\viewfs\ConfigUtil.java	unrelated	package org apache hadoop fs viewfs utilities config variables view fs see link view fs config util get config variable prefix specified mount table string get config view fs prefix string mount table name return constants config viewfs prefix mount table name get config variable prefix default mount table string get config view fs prefix return get config view fs prefix constants config viewfs prefix default mount table add link config specified mount table add link configuration conf string mount table name string src uri target conf set get config view fs prefix mount table name constants config viewfs link src target string add link config default mount table add link configuration conf string src uri target add link conf constants config viewfs default mount table src target add config variable homedir default mount table set home dir conf configuration conf string homedir set home dir conf conf constants config viewfs default mount table homedir add config variable homedir specified mount table set home dir conf configuration conf string mount table name string homedir homedir starts with throw new illegal argument exception home dir start homedir conf set get config view fs prefix mount table name constants config viewfs homedir homedir get value home dir conf value default mount table string get home dir value configuration conf return get home dir value conf constants config viewfs default mount table get value home dir conf value specfied mount table string get home dir value configuration conf string mount table name return conf get get config view fs prefix mount table name constants config viewfs homedir
144	common\src\java\org\apache\hadoop\fs\viewfs\Constants.java	unrelated	package org apache hadoop fs viewfs config variable prefixes view fs see link org apache hadoop fs viewfs view fs examples the mount table specified config using prefixes see link org apache hadoop fs viewfs config util convenience lib constants prefix config variable prefix view fs mount table string config viewfs prefix fs viewfs mounttable prefix home dir mount table specified hadoop default value user used string config viewfs homedir homedir config variable name default mount table string config viewfs default mount table default config variable full prefix default mount table string config viewfs prefix default mount table config viewfs prefix config viewfs default mount table config variable specifying simple link string config viewfs link link config variable specifying merge link string config viewfs link merge link merge config variable specifying merge root mount table root another file system string config viewfs link merge slash link merge slash fs permission permission rrr new fs permission short
145	common\src\java\org\apache\hadoop\fs\viewfs\InodeTree.java	unrelated	package org apache hadoop fs viewfs inode tree implements mount table tree inodes it used implement view fs view file system in order use caller must subclass implement methods link get target file system i node dir etc the mountable initialized config variables specified link view fs the three main methods link inode treel configuration constructor link inode tree configuration string constructor link resolve string boolean inode tree t enum result kind internal dir external dir path slash path new path i node dir t root root mount table string homedir prefix homedir config value mount table list mount point t mount points new array list mount point t mount point t string src i node link t target mount point string src path i node link t mount link src src path target mount link breaks file path component names string break into path components string path return path null null path split path separator internal inode tree i node t string full path full path root i node string path to node user group information ugi full path path to node internal represent internal dir mount table i node dir t extends i node t map string i node t children new hash map string i node t t inode dir fs null file system internal directory mount t boolean root false i node dir string path to node user group information ugi super path to node ugi i node t resolve string path component throws file not found exception i node t result resolve internal path component result null throw new file not found exception return result i node t resolve internal string path component throws file not found exception return children get path component i node dir t add dir string path component user group information ugi throws file already exists exception children contains key path component throw new file already exists exception i node dir t new dir new i node dir t full path root path component ugi children put path component new dir return new dir add link string path component i node link t link throws file already exists exception children contains key path component throw new file already exists exception children put path component link in internal represent mount link a mount link single dir link merge dir link a merge dir link merge junction links dirs example merge dirs users hdfs nn users users hdfs nn users for merge target checked dir created target changed later ignored dir null entries i node link t extends i node t boolean merge link true merge link uri target dir link list t target file system file system object created link construct merge link i node link string path to node user group information ugi t target merge fs uri target dir link list super path to node ugi target file system target merge fs target dir link list target dir link list merge link true construct simple link e merge link i node link string path to node user group information ugi t target
146	common\src\java\org\apache\hadoop\fs\viewfs\ViewFileSystem.java	unrelated	package org apache hadoop fs viewfs view file system extends file system implements client side mount table its spec implementation identical link view fs view file system extends file system access control exception read only mount table string operation string p return new access control exception internal dir view file system readonly operation operation path p access control exception read only mount table string operation path p return read only mount table operation p string mount point path src src mount uri targets target mount multiple targets imply merge mount mount point path src path uri target ur is src src path targets target ur is path get src return src uri get targets return targets creation time mount table user group information ugi user group user created mtable uri uri path working dir configuration config inode tree file system fs state fs state ie mount table path home dir null prohibits names contain boolean valid name string src check string tokenizer tokens new string tokenizer src path separator tokens more tokens string element tokens next token element equals element equals element index of return false return true make path absolute get path part pathname checks uri matches file system path part valid name string get uri path path p check path p string make absolute p uri get path valid name throw new invalid path exception path part uri p valid filename return path make absolute path f return f absolute f new path working dir f this constructor signature needed link file system create file system uri configuration after constructor called initialize called view file system throws io exception ugi user group information get current user creation time system current time millis called new file system instance constructed file system initialize uri uri configuration conf throws io exception super initialize uri conf set conf conf config conf now build client side view e client side mount table config string authority uri get authority try uri new uri fs constants viewfs scheme authority null null fs state new inode tree file system conf authority protected file system get target file system uri uri throws uri syntax exception io exception return new ch rooted file system file system get uri config new path uri get path protected file system get target file system i node dir file system dir throws uri syntax exception return new internal dir of view fs dir creation time ugi uri protected file system get target file system uri merge fs uri list throws uri syntax exception unsupported file system exception throw new unsupported file system exception mergefs implemented return merge fs create merge fs merge fs uri list config working dir get home directory catch uri syntax exception e throw new io exception uri syntax exception uri convenience constructor apps call directly view file system uri uri configuration conf throws io exception initialize uri conf convenience constructor apps call directly view file system configuration conf throws io exception fs constants viewfs uri conf path get trash can location path f throws file not found exception inode tree
147	common\src\java\org\apache\hadoop\fs\viewfs\ViewFs.java	unrelated	package org apache hadoop fs viewfs view fs extends abstract file system implements client side mount table the view fs file system implemented completely memory client side the client side mount table allows client provide customized view file system namespace composed one individual file systems local fs hdfs s fs etc for example one could mount table provides links ul li user hdfs nn containing user dir user li project foo hdfs nn project projects foo li project bar hdfs nn project projects bar li tmp hdfs nn tmp tmp for user xxx ul view fs specified following uri b viewfs b p to use viewfs one would typically set default file system config e fs default name viewfs along mount table config variables described p b config variables specify mount table entries b p the file system initialized standard hadoop config config variables see link fs constants uri scheme constants see link constants config var constants see link config util convenient lib p all mount table config entries view fs prefixed b fs viewfs mounttable b for example example specified following config variables ul li fs viewfs mounttable default link user hdfs nn containing user dir user li fs viewfs mounttable default link project foo hdfs nn project projects foo li fs viewfs mounttable default link project bar hdfs nn project projects bar li fs viewfs mounttable default link tmp hdfs nn tmp tmp for user xxx ul the default mount table authority specified config variables prefixed b fs view fs mounttable default b the authority component uri used specify different mount table for example ul li viewfs sanjay mountable ul initialized fs view fs mounttable sanjay mountable config variables p b merge mounts b note merge mounts implemented yet p one also use merge mounts merge several directories sometimes called union mounts junction mounts literature for example home directories stored say two file systems fit one one could specify mount entry following merges two dirs ul li user hdfs nn user user hdfs nn user user ul such merge link specified following config var used separator links merged ul li fs viewfs mounttable default link merge user hdfs nn user user hdfs nn user user ul a special case merge mount mount table root merged root slash another file system ul li fs viewfs mounttable default link merge slash hdfs nn ul in cases root mount table merged root b hdfs nn b view fs extends abstract file system creation time mount table user group information ugi user group user created mtable configuration config inode tree abstract file system fs state fs state ie mount table path home dir null access control exception read only mount table string operation string p return new access control exception internal dir view file system readonly operation operation path p access control exception read only mount table string operation path p return read only mount table operation p string mount point path src src mount uri targets target mount multiple targets imply merge mount mount point path src path uri target ur is src
148	common\src\java\org\apache\hadoop\fs\viewfs\ViewFsFileStatus.java	unrelated	package org apache hadoop fs viewfs this needed address problem described link view file system get file status org apache hadoop fs path link view fs get file status org apache hadoop fs path view fs file status extends file status file status fs path modified path view fs file status file status fs path new path fs fs modified path new path boolean equals object return super equals hash code return super hash code get len return fs get len boolean file return fs file boolean directory return fs directory boolean dir return fs directory boolean symlink return fs symlink get block size return fs get block size short get replication return fs get replication get modification time return fs get modification time get access time return fs get access time fs permission get permission return fs get permission string get owner return fs get owner string get group return fs get group path get path return modified path set path path p modified path p path get symlink throws io exception return fs get symlink
149	common\src\java\org\apache\hadoop\http\AdminAuthorizedServlet.java	unrelated	package org apache hadoop http general servlet admin authorized admin authorized servlet extends default servlet serial version uid l protected get http servlet request request http servlet response response throws servlet exception io exception do authorization http server administrator access get servlet context request response authorization done just call super super get request response
150	common\src\java\org\apache\hadoop\http\FilterContainer.java	unrelated	package org apache hadoop http a container javax servlet filter filter container add filter container add filter string name string classname map string string parameters add global filter container add global filter string name string classname map string string parameters
151	common\src\java\org\apache\hadoop\http\FilterInitializer.java	unrelated	package org apache hadoop http initialize javax servlet filter filter initializer initialize filter filter container init filter filter container container configuration conf
152	common\src\java\org\apache\hadoop\http\HtmlQuoting.java	unrelated	package org apache hadoop http this responsible quoting html characters html quoting byte amp bytes amp get bytes byte apos bytes apos get bytes byte gt bytes gt get bytes byte lt bytes lt get bytes byte quot bytes quot get bytes does given need quoted boolean needs quoting byte data len len switch data case case case case case return true default break return false does given need quoted boolean needs quoting string str str null return false byte bytes str get bytes return needs quoting bytes bytes length quote active html characters given added buffer quote html chars output stream output byte buffer len throws io exception len switch buffer case output write amp bytes break case output write lt bytes break case output write gt bytes break case output write apos bytes break case output write quot bytes break default output write buffer quote given item make html safe string quote html chars string item item null return null byte bytes item get bytes needs quoting bytes bytes length byte array output stream buffer new byte array output stream try quote html chars buffer bytes bytes length catch io exception ioe won happen since bytearrayoutputstream return buffer string else return item return output stream quotes output output stream quote output stream output stream throws io exception return new output stream byte data new byte write byte data len throws io exception quote html chars data len write b throws io exception data byte b quote html chars data flush throws io exception flush close throws io exception close remove html quoting string unquote html chars string item item null return null next item index of nothing quoted next return item len item length posn string builder buffer new string builder next buffer append item substring posn next item starts with amp next buffer append next else item starts with apos next buffer append next else item starts with gt next buffer append next else item starts with lt next buffer append next else item starts with quot next buffer append next else end item index of next end end len throw new illegal argument exception bad html quoting item substring next end posn next next item index of posn buffer append item substring posn len return buffer string main string args throws exception string arg args system println original arg string quoted quote html chars arg system println quoted quoted string unquoted unquote html chars quoted system println unquoted unquoted system println
153	common\src\java\org\apache\hadoop\http\HttpServer.java	pooling	package org apache hadoop http create jetty embedded server answer http requests the primary goal serve status information server there three contexts logs points log directory points common files src webapps jsp server code src webapps name http server implements filter container log log log factory get log http server string filter initializer property hadoop http filter initializers string http max threads hadoop http max threads the servlet context attribute daemon configuration gets stored string conf context attribute hadoop conf string admins acl admins acl access control list admins acl protected server web server protected connector listener protected web app context web app context protected boolean find port protected map context boolean default contexts new hash map context boolean protected list string filter names new array list string max retries string state description alive alive string state description not live live boolean listener started externally same name bind address port find port null http server string name string bind address port boolean find port throws io exception name bind address port find port new configuration http server string name string bind address port boolean find port configuration conf connector connector throws io exception name bind address port find port conf null connector create status server given port the jsp scripts taken src webapps name increment finds free port http server string name string bind address port boolean find port configuration conf throws io exception name bind address port find port conf null null http server string name string bind address port boolean find port configuration conf access control list admins acl throws io exception name bind address port find port conf admins acl null create status server given port the jsp scripts taken src webapps name increment finds free port http server string name string bind address port boolean find port configuration conf access control list admins acl connector connector throws io exception web server new server find port find port admins acl admins acl connector null listener started externally false listener create base listener conf listener set host bind address listener set port port else listener started externally true listener connector web server add connector listener max threads conf get int http max threads if http max threads configured queue thread pool use default value currently queued thread pool thread pool max threads new queued thread pool new queued thread pool max threads web server set thread pool thread pool string app dir get web apps path name context handler collection contexts new context handler collection web server set handler contexts web app context new web app context web app context set display name wep apps context web app context set context path web app context set war app dir name web app context get servlet context set attribute conf context attribute conf web app context get servlet context set attribute admins acl admins acl web server add handler web app context add default apps contexts app dir conf define filter web app context krb filter krb and certs ssl socket connector krb ssl filter get name null null add global
154	common\src\java\org\apache\hadoop\http\package-info.java	unrelated	package org apache hadoop http
155	common\src\java\org\apache\hadoop\http\lib\StaticUserWebFilter.java	unrelated	package org apache hadoop http lib provides servlet filter pretends authenticate fake user dr who web ui usable secure cluster without authentication static user web filter extends filter initializer string deprecated ugi key dfs web ugi string username key hadoop http staticuser user string username default dr log log log factory get log static user web filter user implements principal string name user string name name name string get name return name hash code return name hash code boolean equals object return true else null get class get class return false return user name equals name string string return name static user filter implements filter user user string username destroy nothing filter servlet request request servlet response response filter chain chain throws io exception servlet exception http servlet request http request http servlet request request user already authenticated http request get remote user null chain filter request response else http servlet request wrapper wrapper new http servlet request wrapper http request principal get user principal return user string get remote user return username chain filter wrapper response init filter config conf throws servlet exception username conf get init parameter username key user new user username init filter filter container container configuration conf hash map string string options new hash map string string string username get username from conf conf options put username key username container add filter user filter static user filter get name options retrieve username configuration string get username from conf configuration conf string old style ugi conf get deprecated ugi key old style ugi null we use normal configuration deprecation mechanism since need split username configured ugi log warn deprecated ugi key used instead use username key string parts old style ugi split return parts else return conf get username key username default
156	common\src\java\org\apache\hadoop\io\AbstractMapWritable.java	unrelated	package org apache hadoop io abstract base map writable sorted map writable unlike org apache nutch crawl map writable allows creation map writable lt writable map writable gt class to id id to class maps travel instead class ids range distinct specific map instance abstract map writable implements writable configurable atomic reference configuration conf class id mappings map class byte to id map new concurrent hash map class byte id class mappings map byte class id to class map new concurrent hash map byte class the number new established constructor volatile byte new classes byte get new classes return new classes used add predefined writable copy new synchronized add to map class clazz byte id to id map contains key clazz byte b to id map get clazz b id throw new illegal argument exception class clazz get name already registered maps b id id to class map contains key id class c id to class map get id c equals clazz throw new illegal argument exception id id exists maps c get name clazz get name to id map put clazz id id to class map put id clazz add class maps already present protected synchronized add to map class clazz to id map contains key clazz return new classes byte max value throw new index out of bounds exception adding additional would exceed maximum number allowed byte id new classes add to map clazz id protected class get class byte id return id to class map get id protected byte get id class clazz return to id map contains key clazz to id map get clazz used child copy constructors protected synchronized copy writable null try data output buffer new data output buffer write data input buffer new data input buffer reset get data get length read fields catch io exception e throw new illegal argument exception map cannot copied e get message else throw new illegal argument exception source map cannot null constructor protected abstract map writable conf new atomic reference configuration add to map array writable byte value of integer value of byte value add to map boolean writable byte value of integer value of byte value add to map bytes writable byte value of integer value of byte value add to map float writable byte value of integer value of byte value add to map int writable byte value of integer value of byte value add to map long writable byte value of integer value of byte value add to map map writable byte value of integer value of byte value add to map md hash byte value of integer value of byte value add to map null writable byte value of integer value of byte value add to map object writable byte value of integer value of byte value add to map sorted map writable byte value of integer value of byte value add to map text byte value of integer value of byte value add to map two d array writable byte value of integer value of byte value utf deprecated support add to map
157	common\src\java\org\apache\hadoop\io\ArrayFile.java	unrelated	package org apache hadoop io a dense file based mapping integers values array file extends map file protected array file ctor write new array file writer extends map file writer long writable count new long writable create named file values named writer configuration conf file system fs string file class extends writable val class throws io exception super conf new path file key class long writable value class val class create named file values named writer configuration conf file system fs string file class extends writable val class compression type compress progressable progress throws io exception super conf new path file key class long writable value class val class compression compress progressable progress append value file synchronized append writable value throws io exception super append count value add map count set count get increment count provide access existing array file reader extends map file reader long writable key new long writable construct array reader named file reader file system fs string file configuration conf throws io exception super new path file conf positions reader code n code th value synchronized seek n throws io exception key set n seek key read return next value file synchronized writable next writable value throws io exception return next key value value null returns key associated recent call link seek link next writable link get writable synchronized key throws io exception return key get return code n code th value file synchronized writable get n writable value throws io exception key set n return get key value
158	common\src\java\org\apache\hadoop\io\ArrayPrimitiveWritable.java	unrelated	package org apache hadoop io this wrapper it wraps writable implementation around array primitives e g etc optimized wire format without creating new objects per element this wrapper make copy underlying array array primitive writable implements writable component type determined component type value array set operation it must primitive class component type null declared component type need declared using array primitive writable class constructor provide typechecking set operations class declared component type null length object value must array component type length map string class primitive names new hash map string class primitive names put boolean get name boolean primitive names put byte get name byte primitive names put char get name char primitive names put short get name short primitive names put get name primitive names put get name primitive names put get name primitive names put get name class get primitive class string name return primitive names get name check primitive class component type component type null throw new hadoop illegal argument exception null component type allowed primitive names contains key component type get name throw new hadoop illegal argument exception input array component type component type get name candidate primitive type check declared component type class component type declared component type null component type declared component type throw new hadoop illegal argument exception input array component type component type get name match declared type declared component type get name check array object value value null throw new hadoop illegal argument exception null value allowed value get class array throw new hadoop illegal argument exception non array value value get class allowed construct empty instance use writable read array primitive writable empty constructor construct instance known type value yet use type specific wrapper array primitive writable class component type check primitive component type declared component type component type wrap existing array primitives array primitive writable object value set value get original array client must cast back type component type may use type specific wrapper object get return value class get component type return component type class get declared component type return declared component type boolean declared component type class component type return component type declared component type set object value check array value class component type value get class get component type check primitive component type check declared component type component type component type component type value value length array get length value do use this internal purely object writable use label transparent conversions arrays primitives wire protocol reads writes internal extends array primitive writable internal use reads super internal object value use writes super value end internal subclass declaration write data output throws io exception write component type utf write string component type get name write length write int length inner loop walk decision tree component type boolean type boolean write boolean array else component type character type char write char array else component type byte type byte write byte array else component type short type short write short array else component type integer type write int array else component type long type write long array else component type float
159	common\src\java\org\apache\hadoop\io\ArrayWritable.java	unrelated	package org apache hadoop io a writable arrays containing instances the elements writable must instances if writable input reducer need create subclass sets value proper type for example code int array writable extends array writable int array writable super int writable code array writable implements writable class extends writable value class writable values array writable class extends writable value class value class null throw new illegal argument exception null value class value class value class array writable class extends writable value class writable values value class values values array writable string strings utf new writable strings length strings length values new utf strings class get value class return value class string strings string strings new string values length values length strings values string return strings object array object result array new instance value class values length values length array set result values return result set writable values values values writable get return values read fields data input throws io exception values new writable read int construct values values length writable value writable factories new instance value class value read fields read value values value store values write data output throws io exception write int values length write values values length values write
160	common\src\java\org\apache\hadoop\io\BinaryComparable.java	unrelated	package org apache hadoop io interface supported link org apache hadoop io writable comparable types supporting ordering permutation representative set bytes binary comparable implements comparable binary comparable return n st bytes n get bytes valid get length return representative byte array instance byte get bytes compare bytes get bytes compare to binary comparable return return writable comparator compare bytes get bytes get length get bytes get length compare bytes get bytes provided compare to byte len return writable comparator compare bytes get bytes get length len return true bytes get bytes match boolean equals object instanceof binary comparable return false binary comparable binary comparable get length get length return false return compare to return hash bytes returned get bytes hash code return writable comparator hash bytes get bytes get length
161	common\src\java\org\apache\hadoop\io\BloomMapFile.java	unrelated	package org apache hadoop io this extends link map file provides much functionality however uses dynamic bloom filters provide quick membership test keys offers fast version link reader get writable comparable writable operation especially case sparsely populated map file bloom map file log log log factory get log bloom map file string bloom file name bloom hash count delete file system fs string name throws io exception path dir new path name path data new path dir map file data file name path index new path dir map file index file name path bloom new path dir bloom file name fs delete data true fs delete index true fs delete bloom true fs delete dir true byte byte array for bloom key data output buffer buf clean length buf get length byte ba buf get data clean length ba length ba new byte clean length system arraycopy buf get data ba clean length return ba writer extends map file writer dynamic bloom filter bloom filter num keys vector size key bloom key new key data output buffer buf new data output buffer file system fs path dir writer configuration conf file system fs string dir name class extends writable comparable key class class extends writable val class compression type compress compression codec codec progressable progress throws io exception conf new path dir name key class key class value class val class compression compress codec progressable progress writer configuration conf file system fs string dir name class extends writable comparable key class class val class compression type compress progressable progress throws io exception conf new path dir name key class key class value class val class compression compress progressable progress writer configuration conf file system fs string dir name class extends writable comparable key class class val class compression type compress throws io exception conf new path dir name key class key class value class val class compression compress writer configuration conf file system fs string dir name writable comparator comparator class val class compression type compress compression codec codec progressable progress throws io exception conf new path dir name comparator comparator value class val class compression compress codec progressable progress writer configuration conf file system fs string dir name writable comparator comparator class val class compression type compress progressable progress throws io exception conf new path dir name comparator comparator value class val class compression compress progressable progress writer configuration conf file system fs string dir name writable comparator comparator class val class compression type compress throws io exception conf new path dir name comparator comparator value class val class compression compress writer configuration conf file system fs string dir name writable comparator comparator class val class throws io exception conf new path dir name comparator comparator value class val class writer configuration conf file system fs string dir name class extends writable comparable key class class val class throws io exception conf new path dir name key class key class value class val class writer configuration conf path dir sequence file writer option options throws io exception super conf dir options
162	common\src\java\org\apache\hadoop\io\BooleanWritable.java	unrelated	package org apache hadoop io a writable comparable booleans boolean writable implements writable comparable boolean value boolean writable boolean writable boolean value set value set value boolean writable set boolean value value value returns value boolean writable boolean get return value read fields data input throws io exception value read boolean write data output throws io exception write boolean value boolean equals object instanceof boolean writable return false boolean writable boolean writable return value value hash code return value compare to object boolean value boolean b boolean writable value return b false string string return boolean string get a comparator optimized boolean writable comparator extends writable comparator comparator super boolean writable compare byte b byte b return compare bytes b b writable comparator define boolean writable new comparator
163	common\src\java\org\apache\hadoop\io\BoundedByteArrayOutputStream.java	unrelated	package org apache hadoop io a byte array backed output stream limit the limit smaller buffer capacity the object reused code reset code api choose different limits round bounded byte array output stream extends output stream byte buffer limit count create bounded byte array output stream specified capacity bounded byte array output stream capacity capacity capacity create bounded byte array output stream specified capacity limit bounded byte array output stream capacity limit capacity limit capacity limit throw new illegal argument exception invalid capacity limit buffer new byte capacity limit limit count write b throws io exception count limit throw new eof exception reaching limit buffer buffer count byte b write byte b len throws io exception b length len len b length len throw new index out of bounds exception else len return count len limit throw new eof exception reach limit buffer system arraycopy b buffer count len count len reset limit reset newlim newlim buffer length throw new index out of bounds exception limit exceeds buffer size limit newlim count reset buffer reset limit buffer length count return current limit get limit return limit returns underlying buffer data valid link size byte get buffer return buffer returns length valid data currently buffer size return count
164	common\src\java\org\apache\hadoop\io\BytesWritable.java	unrelated	package org apache hadoop io a byte sequence usable key value it resizable distinguishes size seqeunce current capacity the hash function front md buffer the sort order memcmp bytes writable extends binary comparable implements writable comparable binary comparable length bytes byte empty bytes size byte bytes create zero size sequence bytes writable empty bytes create bytes writable using byte array initial value bytes writable byte bytes bytes bytes length create bytes writable using byte array initial value length length use constructor array larger value represents bytes writable byte bytes length bytes bytes size length get copy bytes exactly length data see link get bytes faster access underlying array byte copy bytes byte result new byte size system arraycopy bytes result size return result get data backing bytes writable please use link copy bytes need returned array precisely length data byte get bytes return bytes get data bytes writable byte get return get bytes get current size buffer get length return size get current size buffer get size return get length change size buffer the values old range preserved new values undefined the capacity changed necessary set size size size get capacity set capacity size size size get capacity maximum size could handled without resizing backing storage get capacity return bytes length change capacity backing storage the data preserved set capacity new cap new cap get capacity byte new data new byte new cap new cap size size new cap size system arraycopy bytes new data size bytes new data set bytes writable contents given new data set bytes writable new data set new data bytes new data size set value copy given byte range set byte new data offset length set size set size length system arraycopy new data offset bytes size inherit javadoc read fields data input throws io exception set size clear old data set size read int read fully bytes size inherit javadoc write data output throws io exception write int size write bytes size hash code return super hash code are two byte sequences equal boolean equals object right obj right obj instanceof bytes writable return super equals right obj return false generate stream bytes hex pairs separated string string string builder sb new string builder size idx idx size idx first put blank separator idx sb append string num integer hex string xff bytes idx one digit add leading num length sb append sb append num return sb string a comparator optimized bytes writable comparator extends writable comparator comparator super bytes writable compare buffers serialized form compare byte b byte b return compare bytes b length bytes length bytes b length bytes length bytes register comparator writable comparator define bytes writable new comparator
165	common\src\java\org\apache\hadoop\io\ByteWritable.java	unrelated	package org apache hadoop io a writable comparable single byte byte writable implements writable comparable byte value byte writable byte writable byte value set value set value byte writable set byte value value value return value byte writable byte get return value read fields data input throws io exception value read byte write data output throws io exception write byte value returns true iff code code byte writable value boolean equals object instanceof byte writable return false byte writable byte writable return value value hash code return value compares two byte writables compare to object value value value byte writable value return value value value value string string return byte string value a comparator optimized byte writable comparator extends writable comparator comparator super byte writable compare byte b byte b byte value b byte value b return value value value value register comparator writable comparator define byte writable new comparator
166	common\src\java\org\apache\hadoop\io\Closeable.java	unrelated	package org apache hadoop io closeable extends java io closeable
167	common\src\java\org\apache\hadoop\io\CompressedWritable.java	unrelated	package org apache hadoop io a base writables store compressed lazily inflate field access this useful large objects whose fields altered map reduce operation leaving field data compressed makes copying instance one file another much faster compressed writable implements writable non null compressed field data instance byte compressed compressed writable read fields data input throws io exception compressed new byte read int read fully compressed compressed length must called methods access fields ensure data uncompressed protected ensure inflated compressed null try byte array input stream deflated new byte array input stream compressed data input inflater new data input stream new inflater input stream deflated read fields compressed inflater compressed null catch io exception e throw new runtime exception e subclasses implement instead link read fields data input protected read fields compressed data input throws io exception write data output throws io exception compressed null byte array output stream deflated new byte array output stream deflater deflater new deflater deflater best speed data output stream dout new data output stream new deflater output stream deflated deflater write compressed dout dout close deflater end compressed deflated byte array write int compressed length write compressed subclasses implement instead link write data output protected write compressed data output throws io exception
168	common\src\java\org\apache\hadoop\io\DataInputBuffer.java	unrelated	package org apache hadoop io a reusable link data input implementation reads memory buffer p this saves memory creating new data input stream byte array input stream time data read p typical usage something like following pre data input buffer buffer new data input buffer loop condition byte data get data data length get data length buffer reset data data length read buffer using data input methods pre data input buffer extends data input stream buffer extends byte array input stream buffer super new byte reset byte input start length buf input count start length mark start pos start byte get data return buf get position return pos get length return count buffer buffer constructs new empty buffer data input buffer new buffer data input buffer buffer buffer super buffer buffer buffer resets data buffer reads reset byte input length buffer reset input length resets data buffer reads reset byte input start length buffer reset input start length byte get data return buffer get data returns current position input get position return buffer get position returns length input get length return buffer get length
169	common\src\java\org\apache\hadoop\io\DataInputByteBuffer.java	unrelated	package org apache hadoop io data input byte buffer extends data input stream buffer extends input stream byte scratch new byte byte buffer buffers new byte buffer bidx pos length read read scratch return return scratch x ff read byte b len bidx buffers length return cur rem math min len buffers bidx remaining buffers bidx get b rem cur rem rem len rem len bidx buffers length pos cur return cur reset byte buffer buffers bidx pos length buffers buffers byte buffer b buffers length b remaining get position return pos get length return length byte buffer get data return buffers buffer buffers data input byte buffer new buffer data input byte buffer buffer buffers super buffers buffers buffers reset byte buffer input buffers reset input byte buffer get data return buffers get data get position return buffers get position get length return buffers get length
170	common\src\java\org\apache\hadoop\io\DataOutputBuffer.java	unrelated	package org apache hadoop io a reusable link data output implementation writes memory buffer p this saves memory creating new data output stream byte array output stream time data written p typical usage something like following pre data output buffer buffer new data output buffer loop condition buffer reset write buffer using data output methods byte data buffer get data data length buffer get length write data ultimate destination pre data output buffer extends data output stream buffer extends byte array output stream byte get data return buf get length return count buffer super buffer size super size write data input len throws io exception newcount count len newcount buf length byte newbuf new byte math max buf length newcount system arraycopy buf newbuf count buf newbuf read fully buf count len count newcount buffer buffer constructs new empty buffer data output buffer new buffer data output buffer size new buffer size data output buffer buffer buffer super buffer buffer buffer returns current contents buffer data valid link get length byte get data return buffer get data returns length valid data currently buffer get length return buffer get length resets buffer empty data output buffer reset written buffer reset return writes bytes data input directly buffer write data input length throws io exception buffer write length write file stream write to output stream throws io exception buffer write to
171	common\src\java\org\apache\hadoop\io\DataOutputByteBuffer.java	unrelated	package org apache hadoop io data output byte buffer extends data output stream buffer extends output stream byte b new byte boolean direct list byte buffer active new array list byte buffer list byte buffer inactive new linked list byte buffer size length byte buffer current buffer size boolean direct direct direct size size current direct byte buffer allocate direct size byte buffer allocate size write b b byte b x ff write b write byte b write b b length write byte b len rem current remaining len rem current put b rem length rem current flip active add current rem len rem rem get buffer len current put b len length len get buffer newsize inactive empty size math max size newsize current direct byte buffer allocate direct size byte buffer allocate size else current inactive remove return current remaining byte buffer get data byte buffer ret active array new byte buffer active size byte buffer tmp current duplicate tmp flip ret ret length tmp slice return ret get length return length reset length current rewind inactive add current active size byte buffer b active remove b rewind inactive add b current inactive remove buffer buffers data output byte buffer data output byte buffer size size false data output byte buffer size boolean direct new buffer size direct data output byte buffer buffer buffers super buffers buffers buffers byte buffer get data return buffers get data get length return buffers get length reset written buffers reset
172	common\src\java\org\apache\hadoop\io\DataOutputOutputStream.java	unrelated	package org apache hadoop io output stream implementation wraps data output data output output stream extends output stream data output construct output stream given data output if already output stream simply returns otherwise wraps output stream output stream construct output stream data output instanceof output stream return output stream else return new data output output stream data output output stream data output write b throws io exception write byte b write byte b len throws io exception write b len write byte b throws io exception write b
173	common\src\java\org\apache\hadoop\io\DefaultStringifier.java	unrelated	package org apache hadoop io default stringifier default implementation link stringifier stringifies objects using base encoding serialized version objects the link serializer link deserializer obtained link serialization factory br default stringifier offers convenience methods store load objects configuration default stringifier t implements stringifier t string separator serializer t serializer deserializer t deserializer data input buffer buf data output buffer buf default stringifier configuration conf class t c serialization factory factory new serialization factory conf serializer factory get serializer c deserializer factory get deserializer c buf new data input buffer buf new data output buffer try serializer open buf deserializer open buf catch io exception ex throw new runtime exception ex t string string str throws io exception try byte bytes base decode base str get bytes utf buf reset bytes bytes length t restored deserializer deserialize null return restored catch unsupported charset exception ex throw new io exception ex string string string t obj throws io exception buf reset serializer serialize obj byte buf new byte buf get length system arraycopy buf get data buf buf length return new string base encode base buf close throws io exception buf close buf close deserializer close serializer close stores item configuration given key name link serialization k store configuration conf k item string key name throws io exception default stringifier k stringifier new default stringifier k conf generics util get class item conf set key name stringifier string item stringifier close restores object configuration link serialization k k load configuration conf string key name class k item class throws io exception default stringifier k stringifier new default stringifier k conf item class try string item str conf get key name return stringifier string item str finally stringifier close stores array items configuration given key name link serialization k store array configuration conf k items string key name throws io exception default stringifier k stringifier new default stringifier k conf generics util get class items try string builder builder new string builder k item items builder append stringifier string item append separator conf set key name builder string finally stringifier close restores array objects configuration link serialization k k load array configuration conf string key name class k item class throws io exception default stringifier k stringifier new default stringifier k conf item class try string item str conf get key name array list k list new array list k string parts item str split separator string part parts part equals list add stringifier string part return generics util array item class list finally stringifier close
174	common\src\java\org\apache\hadoop\io\DoubleWritable.java	unrelated	package org apache hadoop io writable double values double writable implements writable comparable value double writable double writable value set value read fields data input throws io exception value read double write data output throws io exception write double value set value value value get return value returns true iff code code double writable value boolean equals object instanceof double writable return false double writable double writable return value value hash code return double to long bits value compare to object double writable double writable return value value value value string string return double string value a comparator optimized double writable comparator extends writable comparator comparator super double writable compare byte b byte b value read double b value read double b return value value value value register comparator writable comparator define double writable new comparator
175	common\src\java\org\apache\hadoop\io\EnumSetWritable.java	unrelated	package org apache hadoop io a writable wrapper enum set enum set writable e extends enum e extends abstract collection e implements writable configurable enum set e value transient class e element type transient configuration conf enum set writable iterator e iterator return value iterator size return value size boolean add e e value null value enum set e set value null return value add e construct new enum set writable if tt value tt argument null size zero tt element type tt argument must null if argument tt value tt size bigger zero argument tt element type tt used enum set writable enum set e value class e element type set value element type construct new enum set writable argument tt value tt null empty enum set writable enum set e value value null reset enum set writable specified tt value value tt element type tt if tt value tt argument null size zero tt element type tt argument must null if argument tt value tt size bigger zero argument tt element type tt used set enum set e value class e element type value null value size element type null element type null throw new illegal argument exception the enum set argument null empty set element type provided value value value null value size iterator e iterator value iterator element type iterator next get declaring class else element type null element type element type return value enum set writable enum set e get return value inherit doc read fields data input throws io exception length read int length value null else length element type class e object writable load class conf writable utils read string value enum set none of element type else e first e object writable read object conf value enum set e enum set first length value add e object writable read object conf inherit doc write data output throws io exception value null write int writable utils write string element type get name else object array value array length array length write int length length element type null throw new unsupported operation exception unable serialize empty enum set element type provided writable utils write string element type get name length object writable write object array array get class conf returns true code code enum set writable value null boolean equals object null throw new illegal argument exception null argument passed equal instanceof enum set writable return false enum set writable enum set writable value value return true value null value must null reach return false return value equals value returns elements underlying enum set wriable it may return null class e get element type return element type inherit doc hash code value null return return value hash code inherit doc string string value null return null return value string inherit doc configuration get conf return conf inherit doc set conf configuration conf conf conf writable factories set factory enum set writable new writable factory writable new instance return new enum set writable
176	common\src\java\org\apache\hadoop\io\FloatWritable.java	unrelated	package org apache hadoop io a writable comparable floats float writable implements writable comparable value float writable float writable value set value set value float writable set value value value return value float writable get return value read fields data input throws io exception value read float write data output throws io exception write float value returns true iff code code float writable value boolean equals object instanceof float writable return false float writable float writable return value value hash code return float to int bits value compares two float writables compare to object value value value float writable value return value value value value string string return float string value a comparator optimized float writable comparator extends writable comparator comparator super float writable compare byte b byte b value read float b value read float b return value value value value register comparator writable comparator define float writable new comparator
177	common\src\java\org\apache\hadoop\io\GenericWritable.java	unrelated	package org apache hadoop io a wrapper writable instances p when two sequence files key type different value types mapped reduce multiple value types allowed in case help wrap instances different types p p compared code object writable code much effective code object writable code append declaration string output file every key value pair p p generic writable implements link configurable configured framework the configuration passed wrapped objects implementing link configurable deserialization p use br write generic object extends generic writable br implements method code get types code defines wrapped generic object application attention defined code get types code method must implement code writable code br br the code looks like blockquote pre generic object extends generic writable class classes class type class type class type protected class get types return classes pre blockquote generic writable implements writable configurable byte not set byte type not set writable instance configuration conf null set instance wrapped set writable obj instance obj class extends writable instance clazz instance get class class extends writable clazzes get types clazzes length class extends writable clazz clazzes clazz equals instance clazz type byte return throw new runtime exception the type instance instance get class not registered return wrapped instance writable get return instance string string return gw instance null instance get class get name value instance string null read fields data input throws io exception type read byte class extends writable clazz get types type xff try instance reflection utils new instance clazz conf catch exception e e print stack trace throw new io exception cannot initialize clazz instance read fields write data output throws io exception type not set instance null throw new io exception the generic writable not set correctly type type instance instance write byte type instance write return may wrapped subclasses implement return constant array protected class extends writable get types configuration get conf return conf set conf configuration conf conf conf
178	common\src\java\org\apache\hadoop\io\InputBuffer.java	unrelated	package org apache hadoop io a reusable link input stream implementation reads memory buffer p this saves memory creating new input stream byte array input stream time data read p typical usage something like following pre input buffer buffer new input buffer loop condition byte data get data data length get data length buffer reset data data length read buffer using input stream methods pre input buffer extends filter input stream buffer extends byte array input stream buffer super new byte reset byte input start length buf input count start length mark start pos start get position return pos get length return count buffer buffer constructs new empty buffer input buffer new buffer input buffer buffer buffer super buffer buffer buffer resets data buffer reads reset byte input length buffer reset input length resets data buffer reads reset byte input start length buffer reset input start length returns current position input get position return buffer get position returns length input get length return buffer get length
179	common\src\java\org\apache\hadoop\io\IntWritable.java	unrelated	package org apache hadoop io a writable comparable ints int writable implements writable comparable value int writable int writable value set value set value int writable set value value value return value int writable get return value read fields data input throws io exception value read int write data output throws io exception write int value returns true iff code code int writable value boolean equals object instanceof int writable return false int writable int writable return value value hash code return value compares two int writables compare to object value value value int writable value return value value value value string string return integer string value a comparator optimized int writable comparator extends writable comparator comparator super int writable compare byte b byte b value read int b value read int b return value value value value register comparator writable comparator define int writable new comparator
180	common\src\java\org\apache\hadoop\io\IOUtils.java	unrelated	package org apache hadoop io an utility i o related functionality io utils copies one stream another output stream end the streams closed finally clause copy bytes input stream output stream buff size boolean close throws io exception try copy bytes buff size close close null close null finally close close stream close stream copies one stream another copy bytes input stream output stream buff size throws io exception print stream ps instanceof print stream print stream null byte buf new byte buff size bytes read read buf bytes read write buf bytes read ps null ps check error throw new io exception unable write output stream bytes read read buf copies one stream another strong closes input output streams end strong copy bytes input stream output stream configuration conf throws io exception copy bytes conf get int io file buffer size true copies one stream another output stream end the streams closed finally clause copy bytes input stream output stream configuration conf boolean close throws io exception copy bytes conf get int io file buffer size close copies count bytes one stream another copy bytes input stream output stream count boolean close throws io exception byte buf new byte bytes remaining count bytes read try bytes remaining bytes to read bytes remaining buf length bytes remaining buf length bytes read read buf bytes to read bytes read break write buf bytes read bytes remaining bytes read close close null close null finally close close stream close stream reads len bytes loop reason including eof read fully input stream byte buf len throws io exception read len read ret read buf read ret throw new io exception premature eof input stream read ret ret similar read fully skips bytes loop reason including eof skip fully input stream len throws io exception len ret skip len ret throw new io exception premature eof input stream len ret close closeable objects b ignore b link io exception null pointers must used cleanup exception handlers cleanup log log java io closeable closeables java io closeable c closeables c null try c close catch io exception e log null log debug enabled log debug exception closing c e closes stream ignoring link io exception must called cleaning exception handlers close stream java io closeable stream cleanup null stream closes socket ignoring link io exception close socket socket sock sock null try sock close catch io exception ignored the dev null output streams null output stream extends output stream write byte b len throws io exception write b throws io exception
181	common\src\java\org\apache\hadoop\io\LongWritable.java	unrelated	package org apache hadoop io a writable comparable longs long writable implements writable comparable value long writable long writable value set value set value long writable set value value value return value long writable get return value read fields data input throws io exception value read long write data output throws io exception write long value returns true iff code code long writable value boolean equals object instanceof long writable return false long writable long writable return value value hash code return value compares two long writables compare to object value value value long writable value return value value value value string string return long string value a comparator optimized long writable comparator extends writable comparator comparator super long writable compare byte b byte b value read long b value read long b return value value value value a decreasing comparator optimized long writable decreasing comparator extends comparator compare writable comparable writable comparable b return super compare b compare byte b byte b return super compare b b register default comparator writable comparator define long writable new comparator
182	common\src\java\org\apache\hadoop\io\MapFile.java	unrelated	package org apache hadoop io a file based map keys values p a map directory containing two files code data code file containing keys values map smaller code index code file containing fraction keys the fraction determined link writer get index interval p the index file read entirely memory thus key implementations try keep small p map files created adding entries order to maintain large database perform updates copying previous version database merging sorted change list create new version database new file sorting large change lists done link sequence file sorter map file log log log factory get log map file the name index file string index file name index the name data file string data file name data protected map file ctor writes new map writer implements java io closeable sequence file writer data sequence file writer index string index interval io map index interval index interval size long writable position new long writable following fields used checking key order writable comparator comparator data input buffer buf new data input buffer data output buffer buf new data output buffer writable comparable last key what position bytes wrote got last index last index pos what size last wrote index set min value ensure index position zero mid key throw exception case last index key count long min value create named map keys named writer configuration conf file system fs string dir name class extends writable comparable key class class val class throws io exception conf new path dir name key class key class value class val class create named map keys named writer configuration conf file system fs string dir name class extends writable comparable key class class val class compression type compress progressable progress throws io exception conf new path dir name key class key class value class val class compression compress progressable progress create named map keys named writer configuration conf file system fs string dir name class extends writable comparable key class class val class compression type compress compression codec codec progressable progress throws io exception conf new path dir name key class key class value class val class compression compress codec progressable progress create named map keys named writer configuration conf file system fs string dir name class extends writable comparable key class class val class compression type compress throws io exception conf new path dir name key class key class value class val class compression compress create named map using named key comparator writer configuration conf file system fs string dir name writable comparator comparator class val class throws io exception conf new path dir name comparator comparator value class val class create named map using named key comparator writer configuration conf file system fs string dir name writable comparator comparator class val class sequence file compression type compress throws io exception conf new path dir name comparator comparator value class val class compression compress create named map using named key comparator writer configuration conf file system fs string dir name writable comparator comparator class val class sequence file compression type compress progressable progress throws io exception
183	common\src\java\org\apache\hadoop\io\MapWritable.java	unrelated	package org apache hadoop io a writable map map writable extends abstract map writable implements map writable writable map writable writable instance default constructor map writable super instance new hash map writable writable copy constructor map writable map writable copy inherit doc clear instance clear inherit doc boolean contains key object key return instance contains key key inherit doc boolean contains value object value return instance contains value value inherit doc set map entry writable writable entry set return instance entry set inherit doc boolean equals object obj obj return true obj instanceof map writable map map map obj size map size return false return entry set equals map entry set return false inherit doc writable get object key return instance get key inherit doc hash code return instance hash code inherit doc boolean empty return instance empty inherit doc set writable key set return instance key set inherit doc writable put writable key writable value add to map key get class add to map value get class return instance put key value inherit doc put all map extends writable extends writable map entry extends writable extends writable e entry set put e get key e get value inherit doc writable remove object key return instance remove key inherit doc size return instance size inherit doc collection writable values return instance values writable inherit doc write data output throws io exception super write write number entries map write int instance size then write key value pair map entry writable writable e instance entry set write byte get id e get key get class e get key write write byte get id e get value get class e get value write inherit doc read fields data input throws io exception super read fields first clear map otherwise accumulate entries every time method called instance clear read number entries map entries read int then read key value pair entries writable key writable reflection utils new instance get class read byte get conf key read fields writable value writable reflection utils new instance get class read byte get conf value read fields instance put key value
184	common\src\java\org\apache\hadoop\io\MD5Hash.java	unrelated	package org apache hadoop io a writable md hash values md hash implements writable comparable md hash md len thread local message digest digester factory new thread local message digest protected message digest initial value try return message digest get instance md catch no such algorithm exception e throw new runtime exception e byte digest constructs md hash md hash digest new byte md len constructs md hash hex md hash string hex set digest hex constructs md hash specified value md hash byte digest digest length md len throw new illegal argument exception wrong length digest length digest digest javadoc writable read fields data input throws io exception read fully digest constructs reads returns instance md hash read data input throws io exception md hash result new md hash result read fields return result javadoc writable write data output throws io exception write digest copy contents another instance instance set md hash system arraycopy digest digest md len returns digest bytes byte get digest return digest construct hash value byte array md hash digest byte data return digest data data length create thread local md digester message digest get digester message digest digester digester factory get digester reset return digester construct hash value content input stream md hash digest input stream throws io exception byte buffer new byte message digest digester get digester n n read buffer digester update buffer n return new md hash digester digest construct hash value byte array md hash digest byte data start len byte digest message digest digester get digester digester update data start len digest digester digest return new md hash digest construct hash value string md hash digest string return digest utf get bytes construct hash value string md hash digest utf utf return digest utf get bytes utf get length construct half sized version md fits half digest value value digest xff l return value return bit digest md quarter digest value value digest xff return value returns true iff code code md hash whose digest contains values boolean equals object instanceof md hash return false md hash md hash return arrays equals digest digest returns hash code value object only uses first bytes since md evenly distributed hash code return quarter digest compares object specified object order compare to md hash return writable comparator compare bytes digest md len digest md len a writable comparator optimized md hash keys comparator extends writable comparator comparator super md hash compare byte b byte b return compare bytes b md len b md len register comparator writable comparator define md hash new comparator char hex digits b c e f returns representation object string string string builder buf new string builder md len md len b digest buf append hex digits b xf buf append hex digits b xf return buf string sets digest value hex set digest string hex hex length md len throw new illegal argument exception wrong length hex length byte digest new byte md len md len j digest byte char to nibble hex char at j char to nibble
185	common\src\java\org\apache\hadoop\io\MultipleIOException.java	unrelated	package org apache hadoop io encapsulate list link io exception link io exception multiple io exception extends io exception require link java io serializable serial version uid l list io exception exceptions constructor use link create io exception list multiple io exception list io exception exceptions super exceptions size exceptions exceptions exceptions exceptions list io exception get exceptions return exceptions a convenient method create link io exception io exception create io exception list io exception exceptions exceptions null exceptions empty return null exceptions size return exceptions get return new multiple io exception exceptions
186	common\src\java\org\apache\hadoop\io\NullWritable.java	unrelated	package org apache hadoop io singleton writable data null writable implements writable comparable null writable this new null writable null writable ctor returns single instance null writable get return this string string return null hash code return compare to object instanceof null writable throw new class cast exception compare get class get name null writable return boolean equals object return instanceof null writable read fields data input throws io exception write data output throws io exception a comparator quot optimized quot null writable comparator extends writable comparator comparator super null writable compare buffers serialized form compare byte b byte b assert assert return register comparator writable comparator define null writable new comparator
187	common\src\java\org\apache\hadoop\io\ObjectWritable.java	unrelated	package org apache hadoop io a polymorphic writable writes instance name handles arrays strings primitive types without writable wrapper object writable implements writable configurable class declared class object instance configuration conf object writable object writable object instance set instance object writable class declared class object instance declared class declared class instance instance return instance null none object get return instance return meant class get declared class return declared class reset instance set object instance declared class instance get class instance instance string string return ow declared class value instance read fields data input throws io exception read object conf write data output throws io exception write object instance declared class conf map string class primitive names new hash map string class primitive names put boolean boolean type primitive names put byte byte type primitive names put char character type primitive names put short short type primitive names put integer type primitive names put long type primitive names put float type primitive names put double type primitive names put void type null instance extends configured implements writable class declared class null instance super null null instance class declared class configuration conf super conf declared class declared class read fields data input throws io exception string name utf read string declared class primitive names get name declared class null try declared class get conf get class by name name catch class not found exception e throw new runtime exception e string write data output throws io exception utf write string declared class get name write link writable link string primitive type array preceding write object data output object instance class declared class configuration conf throws io exception write object instance declared class conf false write link writable link string primitive type array preceding usages set false inter cluster file persisted output usages preserve ability interchange files clusters may running version software sometime consider removing parameter always using compact format write object data output object instance class declared class configuration conf boolean allow compact arrays throws io exception instance null null instance new null instance declared class conf declared class writable special case must come writing declared class if eligible array primitives wrap array primitive writable internal wrapper allow compact arrays declared class array instance get class get name equals declared class get name instance get class get component type primitive instance new array primitive writable internal instance declared class array primitive writable internal utf write string declared class get name always write declared declared class array non primitive non compact array length array get length instance write int length length write object array get instance declared class get component type conf allow compact arrays else declared class array primitive writable internal array primitive writable internal instance write else declared class string string utf write string string instance else declared class primitive primitive type declared class boolean type boolean write boolean boolean instance boolean value else declared class character type char write char character instance char value else declared class byte type byte write byte byte instance byte value else declared class short type short
188	common\src\java\org\apache\hadoop\io\OutputBuffer.java	unrelated	package org apache hadoop io a reusable link output stream implementation writes memory buffer p this saves memory creating new output stream byte array output stream time data written p typical usage something like following pre output buffer buffer new output buffer loop condition buffer reset write buffer using output stream methods byte data buffer get data data length buffer get length write data ultimate destination pre output buffer extends filter output stream buffer extends byte array output stream byte get data return buf get length return count reset count write input stream len throws io exception newcount count len newcount buf length byte newbuf new byte math max buf length newcount system arraycopy buf newbuf count buf newbuf io utils read fully buf count len count newcount buffer buffer constructs new empty buffer output buffer new buffer output buffer buffer buffer super buffer buffer buffer returns current contents buffer data valid link get length byte get data return buffer get data returns length valid data currently buffer get length return buffer get length resets buffer empty output buffer reset buffer reset return writes bytes input stream directly buffer write input stream length throws io exception buffer write length
189	common\src\java\org\apache\hadoop\io\RawComparator.java	unrelated	package org apache hadoop io p a link comparator operates directly byte representations objects p raw comparator t extends comparator t compare two objects binary b first object b second object compare byte b byte b
190	common\src\java\org\apache\hadoop\io\SecureIOUtils.java	unrelated	package org apache hadoop io this provides secure ap is opening creating files local disk the main issue tries handle symlink traversal br an example attack ol li malicious user removes task syslog file puts link job token file target user li li malicious user tries open syslog file via servlet tasktracker li li the tasktracker unaware symlink simply streams contents job token file the malicious user access potentially sensitive map outputs etc target user job li ol a similar attack possible involving task log truncation case due insecure write file br secure io utils ensure set run appropriate native support code if security disabled support code unavailable still tries best secure vulnerable race condition attacks if security enabled support code unavailable throws runtime exception since want run insecurely boolean be secure user group information security enabled boolean be secure native io available be secure be secure throw new runtime exception secure io possible without native code extensions pre cache instance raw file system since sometimes secure io shutdown hook call could fail try raw filesystem file system get local new configuration get raw catch io exception ie throw new runtime exception couldn obtain instance raw local file system secure io skips security checks case security disabled skip security be secure boolean skip security file system raw filesystem open given file read access verifying expected user group constraints security enabled note function provides additional checks hadoop security disabled since checks would expensive native libraries available user group match file input stream open for read file f string expected owner string expected group throws io exception user group information security enabled return new file input stream f return force secure open for read f expected owner expected group same open for read except run even security this used unit tests file input stream force secure open for read file f string expected owner string expected group throws io exception file input stream fis new file input stream f boolean success false try stat stat native io fstat fis get fd check stat f stat get owner stat get group expected owner expected group success true return fis finally success fis close file output stream insecure create for write file f permissions throws io exception if real security racy exists check followed open chmod f exists throw new already exists exception file f already exists file output stream fos new file output stream f boolean success false try raw filesystem set permission new path f get absolute path new fs permission short permissions success true return fos finally success fos close open specified file write access ensuring exist file output stream create for write file f permissions throws io exception skip security return insecure create for write f permissions else use native wrapper around open try file descriptor fd native io open f get absolute path native io o wronly native io o creat native io o excl permissions return new file output stream fd catch native io exception nioe nioe get errno errno eexist throw new already exists exception nioe throw nioe check
191	common\src\java\org\apache\hadoop\io\SequenceFile.java	pooling	package org apache hadoop io code sequence file code flat files consisting binary key value pairs p code sequence file code provides link writer link reader link sorter writing reading sorting respectively p there three code sequence file code code writer code based link compression type used compress key value pairs ol li code writer code uncompressed records li li code record compress writer code record compressed files compress values li li code block compress writer code block compressed files keys values collected blocks separately compressed the size block configurable ol p the actual compression algorithm used compress key values specified using appropriate link compression codec p p the recommended way use tt create writer tt methods provided code sequence file code chose preferred format p p the link reader acts bridge read code sequence file code formats p id formats sequence file formats p essentially different formats code sequence file code depending code compression type code specified all share href header common header described id header sequence file header ul li version bytes magic header b seq b followed byte actual version number e g seq seq li li key class name key li li value class name value li li compression a boolean specifies compression turned keys values file li li block compression a boolean specifies block compression turned keys values file li li compression codec code compression codec code used compression keys values compression enabled li li metadata link metadata file li li sync a sync marker denote end header li ul id uncompressed format uncompressed sequence file format ul li href header header li li record ul li record length li li key length li li key li li value li ul li li a sync marker every code code bytes li ul id record compressed format record compressed sequence file format ul li href header header li li record ul li record length li li key length li li key li li compressed value li ul li li a sync marker every code code bytes li ul id block compressed format block compressed sequence file format ul li href header header li li record block ul li uncompressed number records block li li compressed key lengths block size li li compressed key lengths block li li compressed keys block size li li compressed keys block li li compressed value lengths block size li li compressed value lengths block li li compressed values block size li li compressed values block li ul li li a sync marker every block li ul p the compressed blocks key lengths value lengths consist actual lengths individual keys values encoded zero compressed integer format p sequence file log log log factory get log sequence file sequence file ctor byte block compress version byte byte custom compress version byte byte version with metadata byte byte version new byte byte s byte e byte q version with metadata sync escape length sync entries sync hash size number bytes hash sync size sync hash size escape hash the number bytes sync points sync interval sync
192	common\src\java\org\apache\hadoop\io\SetFile.java	unrelated	package org apache hadoop io a file based set keys set file extends map file protected set file ctor write new set file writer extends map file writer create named set keys named writer file system fs string dir name class extends writable comparable key class throws io exception super new configuration fs dir name key class null writable create set naming element compression type writer configuration conf file system fs string dir name class extends writable comparable key class sequence file compression type compress throws io exception conf fs dir name writable comparator get key class compress create set naming element comparator compression type writer configuration conf file system fs string dir name writable comparator comparator sequence file compression type compress throws io exception super conf new path dir name comparator comparator value class null writable compression compress append key set the key must strictly greater previous key added set append writable comparable key throws io exception append key null writable get provide access existing set file reader extends map file reader construct set reader named set reader file system fs string dir name configuration conf throws io exception super fs dir name conf construct set reader named set using named comparator reader file system fs string dir name writable comparator comparator configuration conf throws io exception super new path dir name conf comparator comparator javadoc inherited boolean seek writable comparable key throws io exception return super seek key read next key set code key code returns true key exists false end set boolean next writable comparable key throws io exception return next key null writable get read matching key set code key code returns code key code null match exists writable comparable get writable comparable key throws io exception seek key next key return key else return null
193	common\src\java\org\apache\hadoop\io\SortedMapWritable.java	unrelated	package org apache hadoop io a writable sorted map sorted map writable extends abstract map writable implements sorted map writable comparable writable sorted map writable comparable writable instance default constructor sorted map writable super instance new tree map writable comparable writable copy constructor sorted map writable sorted map writable copy inherit doc comparator super writable comparable comparator returning null means use natural ordering keys return null inherit doc writable comparable first key return instance first key inherit doc sorted map writable comparable writable head map writable comparable key return instance head map key inherit doc writable comparable last key return instance last key inherit doc sorted map writable comparable writable sub map writable comparable key writable comparable key return instance sub map key key inherit doc sorted map writable comparable writable tail map writable comparable key return instance tail map key inherit doc clear instance clear inherit doc boolean contains key object key return instance contains key key inherit doc boolean contains value object value return instance contains value value inherit doc set java util map entry writable comparable writable entry set return instance entry set inherit doc writable get object key return instance get key inherit doc boolean empty return instance empty inherit doc set writable comparable key set return instance key set inherit doc writable put writable comparable key writable value add to map key get class add to map value get class return instance put key value inherit doc put all map extends writable comparable extends writable map entry extends writable comparable extends writable e entry set instance put e get key e get value inherit doc writable remove object key return instance remove key inherit doc size return instance size inherit doc collection writable values return instance values inherit doc read fields data input throws io exception super read fields read number entries map entries read int then read key value pair entries writable comparable key writable comparable reflection utils new instance get class read byte get conf key read fields writable value writable reflection utils new instance get class read byte get conf value read fields instance put key value inherit doc write data output throws io exception super write write number entries map write int instance size then write key value pair map entry writable comparable writable e instance entry set write byte get id e get key get class e get key write write byte get id e get value get class e get value write
194	common\src\java\org\apache\hadoop\io\Stringifier.java	unrelated	package org apache hadoop io stringifier offers two methods convert object representation restore object given representation stringifier t extends java io closeable converts object representation string string t obj throws io exception restores object representation t string string str throws io exception closes object close throws io exception
195	common\src\java\org\apache\hadoop\io\Text.java	unrelated	package org apache hadoop io this stores text using standard utf encoding it provides methods serialize deserialize compare texts byte level the type length integer serialized using zero compressed format p in addition provides methods traversal without converting byte array p also includes utilities serializing deserialing coding decoding checking byte array contains valid utf code calculating length encoded text extends binary comparable implements writable comparable binary comparable thread local charset encoder encoder factory new thread local charset encoder protected charset encoder initial value return charset name utf new encoder malformed input coding error action report unmappable character coding error action report thread local charset decoder decoder factory new thread local charset decoder protected charset decoder initial value return charset name utf new decoder malformed input coding error action report unmappable character coding error action report byte empty bytes new byte byte bytes length text bytes empty bytes construct text string set construct another text text text utf set utf construct byte array text byte utf set utf get copy bytes exactly length data see link get bytes faster access underlying array byte copy bytes byte result new byte length system arraycopy bytes result length return result returns raw bytes however data link get length valid please use link copy bytes need returned array precisely length data byte get bytes return bytes returns number bytes byte array get length return length returns unicode scalar value bit integer value character code position code note method avoids using converter string instatiation position invalid points trailing byte char at position position length return position return duh byte buffer bb byte buffer byte buffer wrap bytes position position return bytes to code point bb slice find string return find finds occurence code code backing buffer starting position code start code the starting position measured bytes return value terms byte position buffer the backing buffer converted operation utf buffer found find string start try byte buffer src byte buffer wrap bytes length byte buffer tgt encode byte b tgt get src position start src remaining b src get matching first byte src mark save position loop tgt mark save position target boolean found true pos src position tgt remaining src remaining src expired first tgt reset src reset found false break tgt get src get tgt reset src reset found false break match found return pos return found catch character coding exception e get e print stack trace return set contain contents set string try byte buffer bb encode true bytes bb array length bb limit catch character coding exception e throw new runtime exception should happened e set utf byte array set byte utf set utf utf length copy text set text set get bytes get length set text range bytes set byte utf start len set capacity len false system arraycopy utf start bytes len length len append range bytes end given text append byte utf start len set capacity length len true system arraycopy utf start bytes length len length len clear empty clear length sets capacity text object em least em code len code
196	common\src\java\org\apache\hadoop\io\TwoDArrayWritable.java	unrelated	package org apache hadoop io a writable d arrays containing matrix instances two d array writable implements writable class value class writable values two d array writable class value class value class value class two d array writable class value class writable values value class values values object array dimensions values length object result array new instance value class dimensions values length object result row array new instance value class values length array set result result row j j values length j array set result row j values j return result set writable values values values writable get return values read fields data input throws io exception construct matrix values new writable read int values length values new writable read int construct values values length j j values length j writable value construct value try value writable value class new instance catch instantiation exception e throw new runtime exception e string catch illegal access exception e throw new runtime exception e string value read fields read value values j value store values write data output throws io exception write int values length write values values length write int values length values length j j values length j values j write
197	common\src\java\org\apache\hadoop\io\UTF8.java	unrelated	package org apache hadoop io a writable comparable strings uses utf encoding p also includes utilities efficiently reading writing utf utf implements writable comparable log log log factory get log utf data input buffer ibuf new data input buffer thread local data output buffer obuf factory new thread local data output buffer protected data output buffer initial value return new data output buffer byte empty bytes new byte byte bytes empty bytes length utf set construct given utf string set construct given utf utf utf set utf the raw bytes byte get bytes return bytes the number bytes encoded get length return length set contain contents set string length xffff maybe log warn truncating length chars starting substring substring xffff length utf length compute length length xffff check length throw new runtime exception bytes null length bytes length grow buffer bytes new byte length try avoid sync allocations data output buffer obuf obuf factory get obuf reset write chars obuf length system arraycopy obuf get data bytes length catch io exception e throw new runtime exception e set contain contents set utf length length bytes null length bytes length grow buffer bytes new byte length system arraycopy bytes bytes length read fields data input throws io exception length read unsigned short bytes null bytes length length bytes new byte length read fully bytes length skips one utf input skip data input throws io exception length read unsigned short writable utils skip fully length write data output throws io exception write short length write bytes length compare two utf compare to object utf utf return writable comparator compare bytes bytes length bytes length convert string string string string builder buffer new string builder length try synchronized ibuf ibuf reset bytes length read chars ibuf buffer length catch io exception e throw new runtime exception e return buffer string returns true iff code code utf contents boolean equals object instanceof utf return false utf utf length length return false else return writable comparator compare bytes bytes length bytes length hash code return writable comparator hash bytes bytes length a writable comparator optimized utf keys comparator extends writable comparator comparator super utf compare byte b byte b n read unsigned short b n read unsigned short b return compare bytes b n b n register comparator writable comparator define utf new comparator static utilities from here down these probably used much anymore might removed convert utf encoded byte array byte get bytes string byte result new byte utf length try avoid sync allocations data output buffer obuf obuf factory get obuf reset write chars obuf length system arraycopy obuf get data result obuf get length catch io exception e throw new runtime exception e return result read utf encoded string read string data input throws io exception bytes read unsigned short string builder buffer new string builder bytes read chars buffer bytes return buffer string read chars data input string builder buffer n bytes throws io exception data output buffer obuf obuf factory get obuf reset obuf write n bytes byte bytes obuf get data
198	common\src\java\org\apache\hadoop\io\VersionedWritable.java	unrelated	package org apache hadoop io a base writables provides version checking p this useful may evolve instances written old version may still processed new version to handle situation link read fields data input implementations catch link version mismatch exception versioned writable implements writable return version number current implementation byte get version javadoc writable write data output throws io exception write byte get version store version javadoc writable read fields data input throws io exception byte version read byte read version version get version throw new version mismatch exception get version version
199	common\src\java\org\apache\hadoop\io\VersionMismatchException.java	unrelated	package org apache hadoop io thrown link versioned writable read fields data input version object read match current implementation version returned link versioned writable get version version mismatch exception extends io exception byte expected version byte found version version mismatch exception byte expected version in byte found version in expected version expected version in found version found version in returns representation object string string return a record version mismatch occured expecting v expected version found v found version
200	common\src\java\org\apache\hadoop\io\VIntWritable.java	unrelated	package org apache hadoop io a writable comparable integer values stored variable length format such values take one five bytes smaller values take fewer bytes v int writable implements writable comparable value v int writable v int writable value set value set value v int writable set value value value return value v int writable get return value read fields data input throws io exception value writable utils read v int write data output throws io exception writable utils write v int value returns true iff code code v int writable value boolean equals object instanceof v int writable return false v int writable v int writable return value value hash code return value compares two v int writables compare to object value value value v int writable value return value value value value string string return integer string value
201	common\src\java\org\apache\hadoop\io\VLongWritable.java	unrelated	package org apache hadoop io a writable comparable longs variable length format such values take one five bytes smaller values take fewer bytes v long writable implements writable comparable value v long writable v long writable value set value set value long writable set value value value return value long writable get return value read fields data input throws io exception value writable utils read v long write data output throws io exception writable utils write v long value returns true iff code code v long writable value boolean equals object instanceof v long writable return false v long writable v long writable return value value hash code return value compares two v long writables compare to object value value value v long writable value return value value value value string string return long string value
202	common\src\java\org\apache\hadoop\io\Writable.java	unrelated	package org apache hadoop io a serializable object implements simple efficient serialization protocol based link data input link data output p any code key code code value code type hadoop map reduce framework implements p p implementations typically implement code read data input code method constructs new instance calls link read fields data input returns instance p p example p p blockquote pre my writable implements writable some data counter timestamp write data output throws io exception write int counter write long timestamp read fields data input throws io exception counter read int timestamp read long my writable read data input throws io exception my writable w new my writable w read fields return w pre blockquote p writable serialize fields object code code write data output throws io exception deserialize fields object code code p for efficiency implementations attempt use storage existing object possible p read fields data input throws io exception
203	common\src\java\org\apache\hadoop\io\WritableComparable.java	unrelated	package org apache hadoop io a link writable also link comparable p code writable comparable code compared typically via code comparator code any type used code key code hadoop map reduce framework implement p p note code hash code code frequently used hadoop partition keys it important implementation hash code returns result across different instances jvm note also default code hash code code implementation code object code b b satisfy property p p example p p blockquote pre my writable comparable implements writable comparable some data counter timestamp write data output throws io exception write int counter write long timestamp read fields data input throws io exception counter read int timestamp read long compare to my writable comparable w value value value int writable value return value lt value value value hash code prime result result prime result counter result prime result timestamp timestamp gt gt gt return result pre blockquote p writable comparable t extends writable comparable t
204	common\src\java\org\apache\hadoop\io\WritableComparator.java	unrelated	package org apache hadoop io a comparator link writable comparable p this base implemenation uses natural ordering to define alternate orderings link compare writable comparable writable comparable p one may optimize compare intensive operations overriding link compare byte byte static utility methods provided assist optimized implementations method writable comparator implements raw comparator hash map class writable comparator comparators new hash map class writable comparator registry get comparator link writable comparable implementation synchronized writable comparator get class extends writable comparable c writable comparator comparator comparators get c comparator null force initializers run force init c look see defined comparator comparators get c use generic one comparator null comparator new writable comparator c true return comparator force initialization members as java referencing force initialize since requires initialized declare comparators force initialization happen force init class cls try class name cls get name true cls get class loader catch class not found exception e throw new illegal argument exception can initialize cls e register optimized comparator link writable comparable implementation comparators registered method must thread safe synchronized define class c writable comparator comparator comparators put c comparator class extends writable comparable key class writable comparable key writable comparable key data input buffer buffer construct link writable comparable implementation protected writable comparator class extends writable comparable key class key class false protected writable comparator class extends writable comparable key class boolean create instances key class key class create instances key new key key new key buffer new data input buffer else key key null buffer null returns writable comparable implementation class extends writable comparable get key class return key class construct new link writable comparable instance writable comparable new key return reflection utils new instance key class null optimization hook override make sequence file sorter scream p the default implementation reads data two link writable comparable using link writable read fields data input calls link compare writable comparable writable comparable compare byte b byte b try buffer reset b parse key key read fields buffer buffer reset b parse key key read fields buffer catch io exception e throw new runtime exception e return compare key key compare compare two writable comparables p the default implementation uses natural ordering calling link comparable compare to object compare writable comparable writable comparable b return compare to b compare object object b return compare writable comparable writable comparable b lexicographic order binary data compare bytes byte b byte b end end j end j end j b xff b b j xff b return b return compute hash binary data hash bytes byte bytes offset length hash offset offset length hash hash bytes return hash compute hash binary data hash bytes byte bytes length return hash bytes bytes length parse unsigned short byte array read unsigned short byte bytes start return bytes start xff bytes start xff parse integer byte array read int byte bytes start return bytes start xff bytes start xff bytes start xff bytes start xff parse byte array read float byte bytes start return float bits to float read int bytes start parse byte array
205	common\src\java\org\apache\hadoop\io\WritableFactories.java	unrelated	package org apache hadoop io factories non writables defining factory permits link object writable able construct instances non writable factories hash map class writable factory class to factory new hash map class writable factory writable factories singleton define factory synchronized set factory class c writable factory factory class to factory put c factory define factory synchronized writable factory get factory class c return class to factory get c create new instance defined factory writable new instance class extends writable c configuration conf writable factory factory writable factories get factory c factory null writable result factory new instance result instanceof configurable configurable result set conf conf return result else return reflection utils new instance c conf create new instance defined factory writable new instance class extends writable c return new instance c null
206	common\src\java\org\apache\hadoop\io\WritableFactory.java	unrelated	package org apache hadoop io a factory writable writable factory return new instance writable new instance
207	common\src\java\org\apache\hadoop\io\WritableName.java	unrelated	package org apache hadoop io utility permit renaming writable implementation without invalidiating files contain name writable name hash map string class name to class new hash map string class hash map class string class to name new hash map class string define important types writable name set name null writable null writable name set name long writable writable name set name utf utf writable name set name md hash md hash writable name ctor set name known something name synchronized set name class writable class string name class to name put writable class name name to class put name writable class add alternate name synchronized add name class writable class string name name to class put name writable class return name default link class get name synchronized string get name class writable class string name class to name get writable class name null return name return writable class get name return name default link class name string synchronized class get class string name configuration conf throws io exception class writable class name to class get name writable class null return writable class subclass writable try return conf get class by name name catch class not found exception e io exception new e new io exception writable name load name new e init cause e throw new e
208	common\src\java\org\apache\hadoop\io\WritableUtils.java	unrelated	package org apache hadoop io writable utils byte read compressed byte array data input throws io exception length read int length return null byte buffer new byte length read fully buffer could use read fully buffer length gzip input stream gzi new gzip input stream new byte array input stream buffer buffer length byte outbuf new byte length byte array output stream bos new byte array output stream len len gzi read outbuf outbuf length bos write outbuf len byte decompressed bos byte array bos close gzi close return decompressed skip compressed byte array data input throws io exception length read int length skip fully length write compressed byte array data output byte bytes throws io exception bytes null byte array output stream bos new byte array output stream gzip output stream gzout new gzip output stream bos try gzout write bytes bytes length gzout close gzout null finally io utils close stream gzout byte buffer bos byte array len buffer length write int len write buffer len debug once confidence lose return bytes length buffer length bytes length else write int return ugly utility maybe someone else better string read compressed string data input throws io exception byte bytes read compressed byte array bytes null return null return new string bytes utf write compressed string data output string throws io exception return write compressed byte array null get bytes utf null write string network int n followed n bytes alternative bit read write utf encoding standard write string data output string throws io exception null byte buffer get bytes utf len buffer length write int len write buffer len else write int read string network int n followed n bytes alternative bit read write utf encoding standard string read string data input throws io exception length read int length return null byte buffer new byte length read fully buffer could use read fully buffer length return new string buffer utf write string array nework int n followed int n byte array strings could generalised using introspection write string array data output string throws io exception write int length length write string write string array nework int n followed int n byte array compressed strings handles also null arrays null values could generalised using introspection write compressed string array data output string throws io exception null write int return write int length length write compressed string write string array nework int n followed int n byte array strings could generalised using introspection actually bit string read string array data input throws io exception len read int len return null string new string len len read string return write string array nework int n followed int n byte array strings could generalised using introspection handles null arrays null values string read compressed string array data input throws io exception len read int len return null string new string len len read compressed string return test utility method display byte array display byte array byte record record length system println system print integer hex string record x f system print integer hex string record
209	common\src\java\org\apache\hadoop\io\compress\BlockCompressorStream.java	unrelated	package org apache hadoop io compress a link org apache hadoop io compress compressor stream works block based based compression algorithms opposed stream based compression algorithms it noted wrapper guarantee blocks sized compressor if link org apache hadoop io compress compressor requires buffering effect meaningful compression responsible block compressor stream extends compressor stream the maximum size input data compressed account overhead compression algorithm max input size create link block compressor stream algorithm given buffer size block compressor stream output stream compressor compressor buffer size compression overhead super compressor buffer size max input size buffer size compression overhead create link block compressor stream given output stream compressor use default buffer size compression overhead buffer size bytes bytes zlib algorithm block compressor stream output stream compressor compressor compressor write data provided compression codec compressing buffer size less compression overhead specified construction block each block contains uncompressed length block followed one length prefixed blocks compressed data write byte b len throws io exception sanity checks compressor finished throw new io exception write beyond end stream b null throw new null pointer exception else b length len len b length throw new index out of bounds exception else len return limlen compressor get bytes read len limlen max input size limlen adding segment would exceed maximum size flush data finish compressor reset len max input size the data given exceeds maximum size any data flushed write chunk segments exceeding maximum size exhausted raw write int len buf len math min len max input size compressor set input b buf len compressor finish compressor finished compress compressor reset buf len len buf len len return give data compressor compressor set input b len compressor needs input compressor buffer size might smaller maximum size permit flush required raw write int compressor get bytes read compress compressor needs input finish throws io exception compressor finished raw write int compressor get bytes read compressor finish compressor finished compress protected compress throws io exception len compressor compress buffer buffer length len write compressed chunk raw write int len write buffer len raw write int v throws io exception write v x ff write v x ff write v x ff write v x ff
210	common\src\java\org\apache\hadoop\io\compress\BlockDecompressorStream.java	unrelated	package org apache hadoop io compress a link org apache hadoop io compress decompressor stream works block based based compression algorithms opposed stream based compression algorithms block decompressor stream extends decompressor stream original block size uncompressed bytes create link block decompressor stream block decompressor stream input stream decompressor decompressor buffer size throws io exception super decompressor buffer size create link block decompressor stream block decompressor stream input stream decompressor decompressor throws io exception super decompressor protected block decompressor stream input stream throws io exception super protected decompress byte b len throws io exception check beginning block uncompressed bytes original block size get original data size try original block size raw read int catch io exception ioe return uncompressed bytes eof original block size this occur decompressing previous compressed empty file original block size eof true return n n decompressor decompress b len decompressor finished decompressor needs dictionary uncompressed bytes original block size eof true return decompressor needs input get compressed data send read data decompressor decompressor set input buffer note decompressed bytes read current block uncompressed bytes n return n protected get compressed data throws io exception check stream get size compressed chunk always non negative len raw read int read len bytes underlying stream len buffer length buffer new byte len n n len count read buffer n len n count throw new eof exception unexpected end block input stream n count return len reset state throws io exception super reset state raw read int throws io exception b read b read b read b read b b b b throw new eof exception return b b b b
211	common\src\java\org\apache\hadoop\io\compress\BZip2Codec.java	unrelated	package org apache hadoop io compress this provides compression output stream compression input stream compression decompression currently dont implementation compressor decompressor interfaces methods compression codec compressor decompressor type argument throw unsupported operation exception b zip codec implements splittable compression codec string header bz header len header length string sub header sub header len sub header length creates new instance b zip codec b zip codec creates compression output stream b zip the output stream throws io exception compression output stream create output stream output stream throws io exception return new b zip compression output stream creates compressor using given output stream compression output stream create output stream output stream compressor compressor throws io exception return create output stream this functionality currently supported class extends org apache hadoop io compress compressor get compressor type return b zip dummy compressor this functionality currently supported compressor create compressor return new b zip dummy compressor creates compression input stream used read uncompressed data the input stream throws io exception compression input stream create input stream input stream throws io exception return new b zip compression input stream this functionality currently supported compression input stream create input stream input stream decompressor decompressor throws io exception return create input stream creates compression input stream used read uncompressed data one two reading modes e continuous blocked reading modes block boundaries split compression input stream create input stream input stream seekable in decompressor decompressor start end read mode read mode throws io exception seekable in instanceof seekable throw new io exception seekable in must instance seekable get name find position first b zip start marker seekable seekable in seek b zip start block markers bytes but first block also b zh making bytes this common case but time stream might start without leading bz first bzip block marker position cb zip input stream number of bytes till next marker seekable in adj start math max l start first bzip block marker position seekable seekable in seek adj start split compression input stream new b zip compression input stream seekable in adj start end read mode the following clause handles following case assume following scenario b zip compressed stream represent compressed data bit block bit block bit block bits bit bit block assume byte alignment current position stream we go back bytes stream find block marker we align wrong position while pos correct get pos start seekable seekable in seek start new b zip compression input stream seekable in start end read mode return this functionality currently supported class extends org apache hadoop io compress decompressor get decompressor type return b zip dummy decompressor this functionality currently supported decompressor create decompressor return new b zip dummy decompressor bz recognized default extension compressed b zip files string get default extension return bz b zip compression output stream extends compression output stream data starts cb zip output stream output boolean needs reset data ends b zip compression output stream output stream throws io exception super needs reset true write stream header throws io exception super null the compressed bzip stream start identifying characters
212	common\src\java\org\apache\hadoop\io\compress\CodecPool.java	pooling	package org apache hadoop io compress a global compressor decompressor pool used save reuse possibly native compression decompression codecs codec pool log log log factory get log codec pool a global compressor pool used save expensive construction destruction possibly native decompression codecs map class compressor list compressor compressor pool new hash map class compressor list compressor a global decompressor pool used save expensive construction destruction possibly native decompression codecs map class decompressor list decompressor decompressor pool new hash map class decompressor list decompressor t t borrow map class t list t pool class extends t codec class t codec null check appropriate codec available synchronized pool pool contains key codec class list t codec list pool get codec class codec list null synchronized codec list codec list empty codec codec list remove codec list size return codec t payback map class t list t pool t codec codec null class t codec class reflection utils get class codec synchronized pool pool contains key codec class pool put codec class new array list t list t codec list pool get codec class synchronized codec list codec list add codec get link compressor given link compression codec pool new one code compressor code code compression codec code pool new one compressor get compressor compression codec codec configuration conf compressor compressor borrow compressor pool codec get compressor type compressor null compressor codec create compressor log info got brand new compressor codec get default extension else compressor reinit conf log debug enabled log debug got recycled compressor return compressor compressor get compressor compression codec codec return get compressor codec null get link decompressor given link compression codec pool new one code decompressor code code compression codec code pool new one decompressor get decompressor compression codec codec decompressor decompressor borrow decompressor pool codec get decompressor type decompressor null decompressor codec create decompressor log info got brand new decompressor codec get default extension else log debug enabled log debug got recycled decompressor return decompressor return link compressor pool return compressor compressor compressor compressor null return compressor reused pool compressor get class annotation present do not pool return compressor reset payback compressor pool compressor return link decompressor pool pool return decompressor decompressor decompressor decompressor null return decompressor reused pool decompressor get class annotation present do not pool return decompressor reset payback decompressor pool decompressor
213	common\src\java\org\apache\hadoop\io\compress\CompressionCodec.java	unrelated	package org apache hadoop io compress this encapsulates streaming compression decompression pair compression codec create link compression output stream write given link output stream compression output stream create output stream output stream throws io exception create link compression output stream write given link output stream given link compressor compression output stream create output stream output stream compressor compressor throws io exception get type link compressor needed link compression codec class extends compressor get compressor type create new link compressor use link compression codec compressor create compressor create link compression input stream read given input stream compression input stream create input stream input stream throws io exception create link compression input stream read given link input stream given link decompressor compression input stream create input stream input stream decompressor decompressor throws io exception get type link decompressor needed link compression codec class extends decompressor get decompressor type create new link decompressor use link compression codec decompressor create decompressor get default filename extension kind compression string get default extension
214	common\src\java\org\apache\hadoop\io\compress\CompressionCodecFactory.java	unrelated	package org apache hadoop io compress a factory find correct codec given filename compression codec factory log log log factory get log compression codec factory get name a map reversed filename suffixes codecs this probably overkill maps small automatically supports finding longest matching suffix sorted map string compression codec codecs null a map reversed filename suffixes codecs this probably overkill maps small automatically supports finding longest matching suffix map string compression codec codecs by name null a map names codecs hash map string compression codec codecs by class name null add codec compression codec codec string suffix codec get default extension codecs put new string builder suffix reverse string codec codecs by class name put codec get class get canonical name codec string codec name codec get class get simple name codecs by name put codec name lower case codec codec name ends with codec codec name codec name substring codec name length codec length codecs by name put codec name lower case codec print extension map string string string builder buf new string builder iterator map entry string compression codec itr codecs entry set iterator buf append itr next map entry string compression codec entry itr next buf append entry get key buf append buf append entry get value get class get name itr next entry itr next buf append buf append entry get key buf append buf append entry get value get class get name buf append return buf string get list codecs listed configuration set list class extends compression codec get codec classes configuration conf string codecs string conf get io compression codecs codecs string null list class extends compression codec result new array list class extends compression codec string tokenizer codec split new string tokenizer codecs string codec split more elements string codec substring codec split next token codec substring length try class cls conf get class by name codec substring compression codec assignable from cls throw new illegal argument exception class codec substring compression codec result add cls subclass compression codec catch class not found exception ex throw new illegal argument exception compression codec codec substring found ex return result else return null sets list codec configuration set codec classes configuration conf list class string builder buf new string builder iterator class itr iterator itr next class cls itr next buf append cls get name itr next buf append buf append itr next get name conf set io compression codecs buf string find codecs specified config value io compression codecs register defaults gzip zip compression codec factory configuration conf codecs new tree map string compression codec codecs by class name new hash map string compression codec codecs by name new hash map string compression codec list class extends compression codec codec classes get codec classes conf codec classes null add codec new gzip codec add codec new default codec else iterator class extends compression codec itr codec classes iterator itr next compression codec codec reflection utils new instance itr next conf add codec codec find relevant compression codec given file based filename suffix compression codec get codec
215	common\src\java\org\apache\hadoop\io\compress\CompressionInputStream.java	unrelated	package org apache hadoop io compress a compression input stream p implementations assumed buffered this permits clients reposition underlying input stream call link reset state without also synchronize client buffers compression input stream extends input stream implements seekable the input stream compressed protected input stream protected max available data l create compression input stream reads decompressed bytes given stream protected compression input stream input stream throws io exception instanceof seekable instanceof positioned readable max available data available close throws io exception close read bytes stream made prevent leakage underlying stream read byte b len throws io exception reset decompressor initial state discard buffered data underlying stream may repositioned reset state throws io exception this method returns current position stream get pos throws io exception instanceof seekable instanceof positioned readable this way getting current position work file size fit hence returned available method return max available data available else return seekable get pos this method current supported seek pos throws unsupported operation exception throw new unsupported operation exception this method current supported boolean seek to new source target pos throws unsupported operation exception throw new unsupported operation exception
216	common\src\java\org\apache\hadoop\io\compress\CompressionOutputStream.java	unrelated	package org apache hadoop io compress a compression output stream compression output stream extends output stream the output stream compressed protected output stream create compression output stream writes compressed bytes given stream protected compression output stream output stream close throws io exception finish close flush throws io exception flush write compressed bytes stream made prevent leakage underlying stream write byte b len throws io exception finishes writing compressed data output stream without closing underlying stream finish throws io exception reset compression initial state does reset underlying stream reset state throws io exception
217	common\src\java\org\apache\hadoop\io\compress\Compressor.java	unrelated	package org apache hadoop io compress specification stream based compressor plugged link compression output stream compress data this modelled link java util zip deflater compressor sets input data compression this called whenever needs input returns code true code indicating input data required set input byte b len returns true input data buffer empty set input called provide input set input called order provide input boolean needs input sets preset dictionary compression a preset dictionary used history buffer predetermined set dictionary byte b len return number uncompressed bytes input far get bytes read return number compressed bytes output far get bytes written when called indicates compression end current contents input buffer finish returns true end compressed data output stream reached data output stream reached boolean finished fills specified buffer compressed data returns actual number bytes compressed data a return value indicates needs input called order determine input data required compress byte b len throws io exception resets compressor new set input data processed reset closes compressor discards unprocessed input end prepare compressor used new stream settings defined given configuration reinit configuration conf
218	common\src\java\org\apache\hadoop\io\compress\CompressorStream.java	unrelated	package org apache hadoop io compress compressor stream extends compression output stream protected compressor compressor protected byte buffer protected boolean closed false compressor stream output stream compressor compressor buffer size super null compressor null throw new null pointer exception else buffer size throw new illegal argument exception illegal buffer size compressor compressor buffer new byte buffer size compressor stream output stream compressor compressor compressor allow derived directly set underlying stream protected compressor stream output stream super write byte b len throws io exception sanity checks compressor finished throw new io exception write beyond end stream len len b length len throw new index out of bounds exception else len return compressor set input b len compressor needs input compress protected compress throws io exception len compressor compress buffer buffer length len write buffer len finish throws io exception compressor finished compressor finish compressor finished compress reset state throws io exception compressor reset close throws io exception closed finish close closed true byte one byte new byte write b throws io exception one byte byte b xff write one byte one byte length
219	common\src\java\org\apache\hadoop\io\compress\Decompressor.java	unrelated	package org apache hadoop io compress specification stream based de compressor plugged link compression input stream compress data this modelled link java util zip inflater decompressor sets input data decompression this called link needs input returns code true code indicating input data required both native non native versions various decompressors require data passed via code b code remain unmodified caller explicitly notified via link needs input buffer may safely modified with requirement extra buffer copy avoided set input byte b len returns true input data buffer empty link set input byte called provide input link set input byte called order provide input boolean needs input sets preset dictionary compression a preset dictionary used history buffer predetermined set dictionary byte b len returns code true code preset dictionary needed decompression boolean needs dictionary returns true end decompressed data output stream reached data output stream reached boolean finished fills specified buffer uncompressed data returns actual number bytes uncompressed data a return value indicates link needs input called order determine input data required decompress byte b len throws io exception returns number bytes remaining compressed data buffer typically called decompressor finished decompressing current gzip stream k member get remaining resets decompressor input output buffers new set input data processed reset closes decompressor discards unprocessed input end
220	common\src\java\org\apache\hadoop\io\compress\DecompressorStream.java	unrelated	package org apache hadoop io compress decompressor stream extends compression input stream protected decompressor decompressor null protected byte buffer protected boolean eof false protected boolean closed false last bytes sent decompressor stream input stream decompressor decompressor buffer size throws io exception super null decompressor null throw new null pointer exception else buffer size throw new illegal argument exception illegal buffer size decompressor decompressor buffer new byte buffer size decompressor stream input stream decompressor decompressor throws io exception decompressor allow derived directly set underlying stream protected decompressor stream input stream throws io exception super byte one byte new byte read throws io exception check stream return read one byte one byte length one byte xff read byte b len throws io exception check stream len len b length len throw new index out of bounds exception else len return return decompress b len protected decompress byte b len throws io exception n n decompressor decompress b len decompressor needs dictionary eof true return decompressor finished first see leftover buffered input previous stream attempt refill buffer if refill eof done else reset fix input buffer get ready next concatenated substream member n remaining decompressor get remaining n remaining get compressed data apparently previous end stream also end file return success never called get compressed data eof true return decompressor reset decompressor set input buffer last bytes sent else looks like concatenated stream reset low level zlib engine buffers resend remaining input data decompressor reset leftover offset last bytes sent n remaining assert leftover offset recopies user buf direct buffer using native libraries decompressor set input buffer leftover offset n remaining note one place not want save number bytes sent n remaining last bytes sent since resending already sent offset nonzero general way could zero already equals n remaining would screw offset calculation next time around iow get remaining terms original zero offset bufferload last bytes sent must well cheesy ascii art last bytes sent buffer n remaining n rem substream n remaining eof true if last bytes sent anything shown calculated incorrectly else decompressor needs input get compressed data throw new eof exception unexpected end input stream decompressor set input buffer last bytes sent return n protected get compressed data throws io exception check stream note caller required call set input throw return read buffer buffer length protected check stream throws io exception closed throw new io exception stream closed reset state throws io exception decompressor reset byte skip bytes new byte skip n throws io exception sanity checks n throw new illegal argument exception negative skip length check stream read n bytes skipped skipped n len math min n skipped skip bytes length len read skip bytes len len eof true break skipped len return skipped available throws io exception check stream return eof close throws io exception closed close closed true boolean mark supported return false synchronized mark readlimit synchronized reset throws io exception throw new io exception mark reset supported
221	common\src\java\org\apache\hadoop\io\compress\DefaultCodec.java	unrelated	package org apache hadoop io compress default codec implements configurable compression codec log log log factory get log default codec configuration conf set conf configuration conf conf conf configuration get conf return conf compression output stream create output stream output stream throws io exception this may leak memory called loop the create compressor call may cause allocation untracked direct backed buffer native libs used even close stream a compressor object reused successive calls log warn default codec create output stream may leak memory create compressor first return new compressor stream create compressor conf get int io file buffer size compression output stream create output stream output stream compressor compressor throws io exception return new compressor stream compressor conf get int io file buffer size class extends compressor get compressor type return zlib factory get zlib compressor type conf compressor create compressor return zlib factory get zlib compressor conf compression input stream create input stream input stream throws io exception return new decompressor stream create decompressor conf get int io file buffer size compression input stream create input stream input stream decompressor decompressor throws io exception return new decompressor stream decompressor conf get int io file buffer size class extends decompressor get decompressor type return zlib factory get zlib decompressor type conf decompressor create decompressor return zlib factory get zlib decompressor conf string get default extension return deflate
222	common\src\java\org\apache\hadoop\io\compress\DeflateCodec.java	unrelated	package org apache hadoop io compress alias default codec enable codec discovery deflate name deflate codec extends default codec
223	common\src\java\org\apache\hadoop\io\compress\DoNotPool.java	pooling	package org apache hadoop io compress this marker annotation marks compressor decompressor type pooled do not pool
224	common\src\java\org\apache\hadoop\io\compress\GzipCodec.java	unrelated	package org apache hadoop io compress this creates gzip compressors decompressors gzip codec extends default codec a bridge wraps around deflater output stream make compression output stream protected gzip output stream extends compressor stream resetable gzip output stream extends gzip output stream resetable gzip output stream output stream throws io exception super reset state throws io exception def reset gzip output stream output stream throws io exception super new resetable gzip output stream allow children types put different type protected gzip output stream compressor stream super close throws io exception close flush throws io exception flush write b throws io exception write b write byte data offset length throws io exception write data offset length finish throws io exception resetable gzip output stream finish reset state throws io exception resetable gzip output stream reset state compression output stream create output stream output stream throws io exception return zlib factory native zlib loaded conf new compressor stream create compressor conf get int io file buffer size new gzip output stream compression output stream create output stream output stream compressor compressor throws io exception return compressor null new compressor stream compressor conf get int io file buffer size create output stream compressor create compressor return zlib factory native zlib loaded conf new gzip zlib compressor conf null class extends compressor get compressor type return zlib factory native zlib loaded conf gzip zlib compressor null compression input stream create input stream input stream throws io exception return create input stream null compression input stream create input stream input stream decompressor decompressor throws io exception decompressor null decompressor create decompressor always succeeds throws return new decompressor stream decompressor conf get int io file buffer size decompressor create decompressor return zlib factory native zlib loaded conf new gzip zlib decompressor new built in gzip decompressor class extends decompressor get decompressor type return zlib factory native zlib loaded conf gzip zlib decompressor built in gzip decompressor string get default extension return gz gzip zlib compressor extends zlib compressor gzip zlib compressor super zlib compressor compression level default compression zlib compressor compression strategy default strategy zlib compressor compression header gzip format gzip zlib compressor configuration conf super zlib factory get compression level conf zlib factory get compression strategy conf zlib compressor compression header gzip format gzip zlib decompressor extends zlib decompressor gzip zlib decompressor super zlib decompressor compression header autodetect gzip zlib
225	common\src\java\org\apache\hadoop\io\compress\SnappyCodec.java	unrelated	package org apache hadoop io compress this creates snappy compressors decompressors snappy codec implements configurable compression codec load snappy loaded configuration conf set configuration used object set conf configuration conf conf conf return configuration used object configuration get conf return conf are native snappy libraries loaded initialized boolean native snappy loaded configuration conf return load snappy loaded conf get boolean common configuration keys io native lib available key common configuration keys io native lib available default create link compression output stream write given link output stream compression output stream create output stream output stream throws io exception return create output stream create compressor create link compression output stream write given link output stream given link compressor compression output stream create output stream output stream compressor compressor throws io exception native snappy loaded conf throw new runtime exception native snappy library available buffer size conf get int common configuration keys io compression codec snappy buffersize key common configuration keys io compression codec snappy buffersize default compression overhead buffer size return new block compressor stream compressor buffer size compression overhead get type link compressor needed link compression codec class extends compressor get compressor type native snappy loaded conf throw new runtime exception native snappy library available return snappy compressor create new link compressor use link compression codec compressor create compressor native snappy loaded conf throw new runtime exception native snappy library available buffer size conf get int common configuration keys io compression codec snappy buffersize key common configuration keys io compression codec snappy buffersize default return new snappy compressor buffer size create link compression input stream read given input stream compression input stream create input stream input stream throws io exception return create input stream create decompressor create link compression input stream read given link input stream given link decompressor compression input stream create input stream input stream decompressor decompressor throws io exception native snappy loaded conf throw new runtime exception native snappy library available return new block decompressor stream decompressor conf get int common configuration keys io compression codec snappy buffersize key common configuration keys io compression codec snappy buffersize default get type link decompressor needed link compression codec class extends decompressor get decompressor type native snappy loaded conf throw new runtime exception native snappy library available return snappy decompressor create new link decompressor use link compression codec decompressor create decompressor native snappy loaded conf throw new runtime exception native snappy library available buffer size conf get int common configuration keys io compression codec snappy buffersize key common configuration keys io compression codec snappy buffersize default return new snappy decompressor buffer size get default filename extension kind compression string get default extension return snappy
226	common\src\java\org\apache\hadoop\io\compress\SplitCompressionInputStream.java	unrelated	package org apache hadoop io compress an input stream covering range compressed data the start end offsets requested client may modified codec fit block boundaries algorithm dependent requirements split compression input stream extends compression input stream start end split compression input stream input stream start end throws io exception super start start end end protected set start start start start protected set end end end end after calling create input stream values start end might change so method used get new value start get adjusted start return start after calling create input stream values start end might change so method used get new value end get adjusted end return end
227	common\src\java\org\apache\hadoop\io\compress\SplittableCompressionCodec.java	unrelated	package org apache hadoop io compress this meant implemented compression codecs capable compress de compress stream starting arbitrary position especially process de compressing stream starting arbitrary position challenging most codecs able successfully de compress stream start beginning till end one reasons stored state beginning stream crucial de compression yet codecs save whole state beginning stream hence used de compress stream starting arbitrary points this meant used codecs such codecs highly valuable especially context hadoop input compressed file split hence worked multiple machines parallel splittable compression codec extends compression codec during decompression data read decompressor two modes namely continuous blocked few codecs e g b zip capable compressing data blocks decompressing blocks in blocked reading mode codecs inform end block events caller while continuous mode caller codecs unaware blocks uncompressed data spilled like continuous stream enum read mode continuous byblock create stream dictated read mode this method used codecs wants ability work underlying stream positions underlying codec underlying codec compressed stream block boundaries split compression input stream create input stream input stream seekable in decompressor decompressor start end read mode read mode throws io exception
228	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2Constants.java	unrelated	package org apache hadoop io compress bzip base compress decompress holds common arrays data p this historical purposes you need use p b zip constants base block size max alpha size max code len runa runb n groups g size n iters max selectors g size num overshoot bytes end b zip block end of block end b zip stream end of stream this array really again historical purposes p fixme this array package location since could modified malicious code p r nums
229	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2DummyCompressor.java	unrelated	package org apache hadoop io compress bzip this dummy compressor b zip b zip dummy compressor implements compressor compress byte b len throws io exception throw new unsupported operation exception end throw new unsupported operation exception finish throw new unsupported operation exception boolean finished throw new unsupported operation exception get bytes read throw new unsupported operation exception get bytes written throw new unsupported operation exception boolean needs input throw new unsupported operation exception reset nothing set dictionary byte b len throw new unsupported operation exception set input byte b len throw new unsupported operation exception reinit configuration conf nothing
230	common\src\java\org\apache\hadoop\io\compress\bzip2\BZip2DummyDecompressor.java	unrelated	package org apache hadoop io compress bzip this dummy decompressor b zip b zip dummy decompressor implements decompressor decompress byte b len throws io exception throw new unsupported operation exception end throw new unsupported operation exception boolean finished throw new unsupported operation exception boolean needs dictionary throw new unsupported operation exception boolean needs input throw new unsupported operation exception get remaining throw new unsupported operation exception reset nothing set dictionary byte b len throw new unsupported operation exception set input byte b len throw new unsupported operation exception
231	common\src\java\org\apache\hadoop\io\compress\bzip2\CBZip2InputStream.java	unrelated	package org apache hadoop io compress bzip an input stream decompresses b zip format without file header chars read stream p the decompression requires large amounts memory thus call link close close method soon possible force tt cb zip input stream tt release allocated memory see link cb zip output stream cb zip output stream information memory usage p p tt cb zip input stream tt reads bytes compressed source stream via single byte link java io input stream read read method exclusively thus consider use buffered source stream p p this ant code enhanced de compress blocks bzip data current position stream important statistic hadoop for example line record reader solely depend current position stream know progess the notion position becomes complicated compressed files the hadoop splitting done terms compressed file but compressed file deflates large amount data so handled problem following way on object creation time find next block start delimiter once marker found stream stops discard read compressed data process position updated e caller find stream location at point ready actual reading e decompression data the subsequent read calls give data the position updated caller read current block bytes in block reading position updated we update postion block boundaries p p instances threadsafe p cb zip input stream extends input stream implements b zip constants block delimiter x l start block eos delimiter x l end bzip stream delimiter bit length read mode read mode read mode continuous the variable records current advertised position stream reported bytes read from compressed stream l the following variable keep record compressed bytes read bytes read from compressed stream l boolean lazy initialization false byte array new byte index last char block block size last last index zptr original sorting orig ptr always range the current block size number block size k boolean block randomised false bs buff bs live crc crc new crc n in use buffered input stream current char a state machine keep track current state de coder enum state eof start block state rand part a state rand part b state rand part c state no rand part a state no rand part b state no rand part c state no process state state current state state start block state stored block crc stored combined crc computed block crc computed combined crc boolean skip result false used skip to next marker boolean skip decompression false variables used setup methods exclusively su count su ch su ch prev su su j su r n to go su r t pos su pos char su z all memory intensive stuff this field initialized init block cb zip input stream data data this method reports processed bytes far please note statistic updated block boundaries stream initiated byblock mode get processed byte count return reported bytes read from compressed stream this method keeps track raw processed compressed bytes added raw processed bytes protected update processed byte count count bytes read from compressed stream count this method called client case corrections stream position one common example client code removes starting bz characters compressed stream
232	common\src\java\org\apache\hadoop\io\compress\bzip2\CBZip2OutputStream.java	unrelated	package org apache hadoop io compress bzip an output stream compresses b zip format without file header chars another stream p the compression requires large amounts memory thus call link close close method soon possible force tt cb zip output stream tt release allocated memory p p you shrink amount allocated memory maybe raise compression speed choosing lower blocksize turn may cause lower compression ratio you avoid unnecessary memory allocation avoiding using blocksize bigger size input p p you compute memory usage compressing following formula p pre lt code gt k blocksize lt code gt pre p to get memory required decompression link cb zip input stream cb zip input stream use p pre lt code gt k blocksize lt code gt pre table width border colgroup col width col width col width colgroup tr th colspan memory usage blocksize th tr tr th align right blocksize th th align right compression br memory usage th th align right decompression br memory usage th tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr tr td align right k td td align right k td td align right k td tr table p for decompression tt cb zip input stream tt allocates less memory bzipped input smaller one block p p instances threadsafe p p todo update b zip p cb zip output stream extends output stream implements b zip constants the minimum supported blocksize tt tt min blocksize the maximum supported blocksize tt tt max blocksize this constant accessible subclasses historical purposes if know means need protected setmask this constant accessible subclasses historical purposes if know means need protected clearmask setmask this constant accessible subclasses historical purposes if know means need protected greater icost this constant accessible subclasses historical purposes if know means need protected lesser icost this constant accessible subclasses historical purposes if know means need protected small thresh this constant accessible subclasses historical purposes if know means need protected depth thresh this constant accessible subclasses historical purposes if know means need protected work factor this constant accessible subclasses historical purposes if know means need p if ever unlucky improbable enough get stack overflow whilst sorting increase following constant try in practice i never seen stack go elems following limit seems generous p protected qsort stack size knuth increments seem work better incerpi
233	common\src\java\org\apache\hadoop\io\compress\bzip2\CRC.java	unrelated	package org apache hadoop io compress bzip a simple hold calculate crc sanity checking data crc crc table x x c db x b e x x dc x c b b x db x e x edb x c f f x f ad x b bcb x c b x cd x c ea x fbdbd x c db x c c x e e x fda x f adac x bd b b x c x b x c x ed b f x b da x x x ddc da x f b x e cd x b e x ce ab x e x x b c c x fe dd b x fb x e e xbe b b xbaea ef xb xb xad f xa ee xa ad ea xa c b xd xd f xddb fe xd b xc b c xc f fb xceb xca xf xf fb f xfbb bb xff f xe ef f xe ffeb xe bccd xec dd x x dc x b x c ae x ab x b c x e dc x ac x e dcf x f x b ca x fcdbb x aeb x bf x x cc cdca x ab x c b b x x dde x b dddb x f c c x e b x fb x e f bf x e b x dd x dc x b x dd x b x ba xaca c xa db xa fdf xa e e e xbfa b b xbb adfc xb b xb e x aad b f x e c x f x ee df x df x x b x ea b xe b de xe xe xedf b e xf b b b xf c xfa xfef de xc bcf f xc dede xcf ecb xcbffd xd b xd b xdc abded xd fba x ce ee x dcdfd x edb x fc x x ec b x aad c x bb eb x f x bc e x x b f x c b x c x e x x b e x dc x c f f x e x x f x f b c x b b b x x cb x ed x e f ff x fa x bd x b x xf f e xf ee bb xf ad xfc c xe b xe ea xeba bbc xef b xd bbb xd e xdea xda f xc cd xc e dd xcda f xc ebb xbd e e xb ff c xb bcb xb daba xae afba xaafbe xa b c cc xa dd b x b c x ff x b ba x f x x cf bad x b x c x x b e x abf x c b x e ee x ffbf x cdd b x cdc c x b x f x f x c bf x bfd x c x x c x b x e x x cd x f x e
234	common\src\java\org\apache\hadoop\io\compress\bzip2\package-info.java	unrelated	package org apache hadoop io compress bzip
235	common\src\java\org\apache\hadoop\io\compress\snappy\LoadSnappy.java	unrelated	package org apache hadoop io compress snappy determines snappy native library available loads available load snappy log log log factory get log load snappy get name boolean available false boolean loaded false try system load library snappy log warn snappy native library available available true catch unsatisfied link error ex nop boolean hadoop native available native code loader native code loaded loaded available hadoop native available loaded log info snappy native library loaded else log warn snappy native library loaded returns snappy native library loaded code false code boolean available return available returns snappy native library loaded code false code boolean loaded return loaded
236	common\src\java\org\apache\hadoop\io\compress\snappy\SnappyCompressor.java	unrelated	package org apache hadoop io compress snappy a link compressor based snappy compression algorithm http code google com p snappy snappy compressor implements compressor log log log factory get log snappy compressor get name default direct buffer size hack use global lock jni layer class clazz snappy compressor direct buffer size buffer compressed direct buf null uncompressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finish finished bytes read l bytes written l load snappy loaded initialize native library try init i ds catch throwable ignore failure load initialize snappy log warn string else log error cannot load snappy compressor get name without snappy library creates new compressor snappy compressor direct buffer size direct buffer size direct buffer size uncompressed direct buf byte buffer allocate direct direct buffer size compressed direct buf byte buffer allocate direct direct buffer size compressed direct buf position direct buffer size creates new compressor default buffer size snappy compressor default direct buffer size sets input data compression this called whenever needs input returns code true code indicating input data required synchronized set input byte b len b null throw new null pointer exception len b length len throw new array index out of bounds exception finished false len uncompressed direct buf remaining save data needs input user buf b user buf off user buf len len else byte buffer uncompressed direct buf put b len uncompressed direct buf len uncompressed direct buf position bytes read len if write would exceed capacity direct buffers set aside loaded function compressed data consumed synchronized set input from saved data user buf len return finished false uncompressed direct buf len math min user buf len direct buffer size byte buffer uncompressed direct buf put user buf user buf off uncompressed direct buf len note much data fed snappy user buf off uncompressed direct buf len user buf len uncompressed direct buf len does nothing synchronized set dictionary byte b len nothing returns true input data buffer empty set input called provide input set input called order provide input synchronized boolean needs input return compressed direct buf remaining uncompressed direct buf remaining user buf len when called indicates compression end current contents input buffer synchronized finish finish true returns true end compressed data output stream reached data output stream reached synchronized boolean finished check uncompressed data consumed return finish finished compressed direct buf remaining fills specified buffer compressed data returns actual number bytes compressed data a return value indicates needs input called order determine input data required synchronized compress byte b len throws io exception b null throw new null pointer exception len b length len throw new array index out of bounds exception check compressed data n compressed direct buf remaining n n math min n len byte buffer compressed direct buf get b n bytes written n return n re initialize snappy output direct buffer compressed direct buf clear compressed direct buf limit uncompressed direct buf position no compressed data needs input finished set input from saved data uncompressed
237	common\src\java\org\apache\hadoop\io\compress\snappy\SnappyDecompressor.java	unrelated	package org apache hadoop io compress snappy a link decompressor based snappy compression algorithm http code google com p snappy snappy decompressor implements decompressor log log log factory get log snappy compressor get name default direct buffer size hack use global lock jni layer class clazz snappy decompressor direct buffer size buffer compressed direct buf null compressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finished load snappy loaded initialize native library try init i ds catch throwable ignore failure load initialize snappy log warn string else log error cannot load snappy decompressor get name without snappy library creates new compressor snappy decompressor direct buffer size direct buffer size direct buffer size compressed direct buf byte buffer allocate direct direct buffer size uncompressed direct buf byte buffer allocate direct direct buffer size uncompressed direct buf position direct buffer size creates new decompressor default buffer size snappy decompressor default direct buffer size sets input data decompression this called link needs input returns code true code indicating input data required both native non native versions various decompressors require data passed via code b code remain unmodified caller explicitly notified via link needs input buffer may safely modified with requirement extra buffer copy avoided synchronized set input byte b len b null throw new null pointer exception len b length len throw new array index out of bounds exception user buf b user buf off user buf len len set input from saved data reinitialize snappy output direct buffer uncompressed direct buf limit direct buffer size uncompressed direct buf position direct buffer size if write would exceed capacity direct buffers set aside loaded function compressed data consumed synchronized set input from saved data compressed direct buf len math min user buf len direct buffer size reinitialize snappy input direct buffer compressed direct buf rewind byte buffer compressed direct buf put user buf user buf off compressed direct buf len note much data fed snappy user buf off compressed direct buf len user buf len compressed direct buf len does nothing synchronized set dictionary byte b len nothing returns true input data buffer empty link set input byte called provide input link set input byte called order provide input synchronized boolean needs input consume remaining compressed data uncompressed direct buf remaining return false check snappy consumed input compressed direct buf len check consumed user input user buf len return true else set input from saved data return false returns code false code synchronized boolean needs dictionary return false returns true end decompressed data output stream reached data output stream reached synchronized boolean finished return finished uncompressed direct buf remaining fills specified buffer uncompressed data returns actual number bytes uncompressed data a return value indicates link needs input called order determine input data required synchronized decompress byte b len throws io exception b null throw new null pointer exception len b length len throw new array index out of bounds exception n check uncompressed data n uncompressed direct buf remaining n n math min n len
238	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInGzipDecompressor.java	pooling	package org apache hadoop io compress zlib a link decompressor based popular gzip compressed file format http www gzip org built in gzip decompressor implements decompressor gzip magic id x b f read le short gzip deflate method gzip flagbit header crc x gzip flagbit extra field x gzip flagbit filename x gzip flagbit comment x gzip flagbits reserved xe true nowrap inflater handle raw deflate stream inflater inflater new inflater true byte user buf null user buf off user buf len byte local buf new byte local buf off header bytes read trailer bytes read num extra field bytes remaining pure java crc crc new pure java crc boolean extra field false boolean filename false boolean comment false boolean header crc false gzip state label state the current state gzip decoder external inflater context technically variables local buf header crc also part state enum merely label enum gzip state label immediately prior strictly within byte basic gzip header header basic immediately prior within optional extra field header extra field immediately prior within optional filename field header filename immediately prior within optional comment field header comment immediately prior within optional byte header crc value header crc immediately prior within main compressed deflate data stream deflate stream immediately prior strictly within byte uncompressed crc trailer crc immediately prior strictly within byte uncompressed size trailer size immediately trailer potentially prior next gzip member substream header without reset called finished creates new pure java gzip decompressor built in gzip decompressor state gzip state label header basic crc reset fixme inflater docs say also necessary provide extra dummy byte input this required zlib native library order support certain optimizations however appear true case entirely clear byte go value perhaps suffices deflated bytes first buffer load but else would one inherit doc synchronized boolean needs input state gzip state label deflate stream common case return inflater needs input see user buf len comment top decompress currently need verify user buf len return state gzip state label finished inherit doc in case input data includes gzip header trailer bytes handle execute state deflate stream bytes hand inflater note this code assumes data passed via b remains unmodified signal safe modify via needs input the alternative would require additional buffer copy even bulk deflate stream performance hit want absorb decompressor documents requirement synchronized set input byte b len b null throw new null pointer exception len b length len throw new array index out of bounds exception user buf b user buf off user buf len len note might zero decompress data gzip header deflate stream gzip trailer provided buffer from caller perspective state machine lives the code written never return decompress data remaining user buf unless finished state data beyond current gzip member e g within concatenated gzip stream if ever changes link needs input also need modified e uncomment user buf len condition the actual deflate stream processing decompression handled java inflater unlike gzip header trailer code execute methods deflate stream never copied inflater operates directly user buffer synchronized decompress byte b len throws io exception num
239	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInZlibDeflater.java	unrelated	package org apache hadoop io compress zlib a wrapper around java util zip deflater make conform org apache hadoop io compress compressor built in zlib deflater extends deflater implements compressor log log log factory get log built in zlib deflater built in zlib deflater level boolean nowrap super level nowrap built in zlib deflater level super level built in zlib deflater super synchronized compress byte b len throws io exception return super deflate b len reinit compressor given configuration it reset compressor compression level compression strategy different tt zlib compressor tt tt built in zlib deflater tt support three kind compression strategy filtered huffman only default strategy it use default strategy default configured compression strategy supported reinit configuration conf reset conf null return set level zlib factory get compression level conf compression level zlib compressor compression strategy strategy zlib factory get compression strategy conf try set strategy strategy compression strategy catch illegal argument exception ill log warn strategy supported built in zlib deflater set strategy default strategy log debug enabled log debug reinit compressor new compression configuration
240	common\src\java\org\apache\hadoop\io\compress\zlib\BuiltInZlibInflater.java	unrelated	package org apache hadoop io compress zlib a wrapper around java util zip inflater make conform org apache hadoop io compress decompressor built in zlib inflater extends inflater implements decompressor built in zlib inflater boolean nowrap super nowrap built in zlib inflater super synchronized decompress byte b len throws io exception try return super inflate b len catch data format exception dfe throw new io exception dfe get message
241	common\src\java\org\apache\hadoop\io\compress\zlib\package-info.java	unrelated	package org apache hadoop io compress zlib
242	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibCompressor.java	unrelated	package org apache hadoop io compress zlib a link compressor based popular zlib compression algorithm http www zlib net zlib compressor implements compressor log log log factory get log zlib compressor default direct buffer size hack use global lock jni layer class clazz zlib compressor stream compression level level compression strategy strategy compression header window bits direct buffer size byte user buf null user buf off user buf len buffer uncompressed direct buf null uncompressed direct buf off uncompressed direct buf len boolean keep uncompressed buf false buffer compressed direct buf null boolean finish finished the compression level zlib library enum compression level compression level compression no compression compression level fastest compression best speed compression level best compression best compression default compression level default compression compression level compression level level compression level level compression level return compression level the compression level zlib library enum compression strategy compression strategy best used data consisting mostly small values somewhat random distribution forces huffman coding less matching filtered compression strategy huffman coding huffman only compression strategy limit match distances one run length encoding rle compression strategy prevent use dynamic huffman codes allowing simpler decoder special applications fixed default compression strategy default strategy compression strategy compression strategy strategy compression strategy strategy compression strategy return compression strategy the type header compressed data enum compression header no headers trailers checksums no header default headers trailers checksums default header simple gzip headers trailers gzip format window bits compression header window bits window bits window bits window bits return window bits boolean native zlib loaded false native code loader native code loaded try initialize native library init i ds native zlib loaded true catch throwable ignore failure load initialize native zlib boolean native zlib loaded return native zlib loaded protected construct compression level level compression strategy strategy compression header header direct buffer size creates new compressor default compression level compressed data generated zlib format zlib compressor compression level default compression compression strategy default strategy compression header default header default direct buffer size creates new compressor taking settings configuration zlib compressor configuration conf zlib factory get compression level conf zlib factory get compression strategy conf compression header default header default direct buffer size creates new compressor using specified compression level compressed data generated zlib format zlib compressor compression level level compression strategy strategy compression header header direct buffer size level level strategy strategy window bits header stream init level compression level strategy compression strategy window bits window bits direct buffer size direct buffer size uncompressed direct buf byte buffer allocate direct direct buffer size compressed direct buf byte buffer allocate direct direct buffer size compressed direct buf position direct buffer size prepare compressor used new stream settings defined given configuration it reset compressor compression level compression strategy synchronized reinit configuration conf reset conf null return end stream level zlib factory get compression level conf strategy zlib factory get compression strategy conf stream init level compression level strategy compression strategy window bits window bits log debug enabled log debug reinit compressor new compression configuration synchronized set input byte b len b null throw
243	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibDecompressor.java	unrelated	package org apache hadoop io compress zlib a link decompressor based popular zlib compression algorithm http www zlib net zlib decompressor implements decompressor default direct buffer size hack use global lock jni layer class clazz zlib decompressor stream compression header header direct buffer size buffer compressed direct buf null compressed direct buf off compressed direct buf len buffer uncompressed direct buf null byte user buf null user buf off user buf len boolean finished boolean need dict the headers detect compressed data enum compression header no headers trailers checksums no header default headers trailers checksums default header simple gzip headers trailers gzip format autodetect gzip zlib headers trailers autodetect gzip zlib window bits compression header window bits window bits window bits window bits return window bits boolean native zlib loaded false native code loader native code loaded try initialize native library init i ds native zlib loaded true catch throwable ignore failure load initialize native zlib boolean native zlib loaded return native zlib loaded creates new decompressor zlib decompressor compression header header direct buffer size header header direct buffer size direct buffer size compressed direct buf byte buffer allocate direct direct buffer size uncompressed direct buf byte buffer allocate direct direct buffer size uncompressed direct buf position direct buffer size stream init header window bits zlib decompressor compression header default header default direct buffer size synchronized set input byte b len b null throw new null pointer exception len b length len throw new array index out of bounds exception user buf b user buf off user buf len len set input from saved data reinitialize zlib output direct buffer uncompressed direct buf limit direct buffer size uncompressed direct buf position direct buffer size synchronized set input from saved data compressed direct buf off compressed direct buf len user buf len compressed direct buf len direct buffer size compressed direct buf len direct buffer size reinitialize zlib input direct buffer compressed direct buf rewind byte buffer compressed direct buf put user buf user buf off compressed direct buf len note much data fed zlib user buf off compressed direct buf len user buf len compressed direct buf len synchronized set dictionary byte b len stream b null throw new null pointer exception len b length len throw new array index out of bounds exception set dictionary stream b len need dict false synchronized boolean needs input consume remaining compressed data uncompressed direct buf remaining return false check zlib consumed input compressed direct buf len check consumed user input user buf len return true else set input from saved data return false synchronized boolean needs dictionary return need dict synchronized boolean finished check zlib says finished compressed data consumed return finished uncompressed direct buf remaining synchronized decompress byte b len throws io exception b null throw new null pointer exception len b length len throw new array index out of bounds exception n check uncompressed data n uncompressed direct buf remaining n n math min n len byte buffer uncompressed direct buf get b n return n re initialize zlib output direct buffer
244	common\src\java\org\apache\hadoop\io\compress\zlib\ZlibFactory.java	unrelated	package org apache hadoop io compress zlib a collection factories create right zlib gzip compressor decompressor instances zlib factory log log log factory get log zlib factory boolean native zlib loaded false native code loader native code loaded native zlib loaded zlib compressor native zlib loaded zlib decompressor native zlib loaded native zlib loaded log info successfully loaded initialized native zlib library else log warn failed load initialize native zlib library check native zlib code loaded initialized correctly loaded job loaded job else code false code boolean native zlib loaded configuration conf return native zlib loaded conf get boolean common configuration keys io native lib available key common configuration keys io native lib available default return appropriate type zlib compressor class extends compressor get zlib compressor type configuration conf return native zlib loaded conf zlib compressor built in zlib deflater return appropriate implementation zlib compressor compressor get zlib compressor configuration conf return native zlib loaded conf new zlib compressor conf new built in zlib deflater zlib factory get compression level conf compression level return appropriate type zlib decompressor class extends decompressor get zlib decompressor type configuration conf return native zlib loaded conf zlib decompressor built in zlib inflater return appropriate implementation zlib decompressor decompressor get zlib decompressor configuration conf return native zlib loaded conf new zlib decompressor new built in zlib inflater set compression strategy configuration conf compression strategy strategy conf set enum zlib compress strategy strategy compression strategy get compression strategy configuration conf return conf get enum zlib compress strategy compression strategy default strategy set compression level configuration conf compression level level conf set enum zlib compress level level compression level get compression level configuration conf return conf get enum zlib compress level compression level default compression
245	common\src\java\org\apache\hadoop\io\file\tfile\BCFile.java	unrelated	package org apache hadoop io file tfile block compressed file underlying physical storage layer t file bc file provides basic block level compression data block meta blocks it separated t file may used block compressed file implementation bc file current version bc file impl increment major minor made enough changes version api version new version short short log log log factory get log bc file prevent instantiation bc file objects bc file nothing bc file writer entry point creating new bc file writer implements closeable fs data output stream configuration conf single meta block containing index compressed data blocks data index data index index meta blocks meta index meta index boolean blk in progress false boolean meta blk seen false boolean closed false error count reusable buffers bytes writable fs output buffer call back register block block closed block register register block fully closed the size block terms uncompressed bytes the start offset block one byte end block compressed block size offset end offset start register raw offset start offset end intermediate maintain state writable compression block w block state algorithm compress algo compressor compressor null using native hadoop compression fs data output stream fs out pos start simple buffered output stream fs buffered output output stream the compression algorithm used compression w block state algorithm compression algo fs data output stream fs out bytes writable fs output buffer configuration conf throws io exception compress algo compression algo fs out fs out pos start fs out get pos fs output buffer set capacity t file get fs output buffer size conf fs buffered output new simple buffered output stream fs out fs output buffer get bytes compressor compress algo get compressor try compression algo create compression stream fs buffered output compressor catch io exception e compress algo return compressor compressor throw e get output stream block appender consumption output stream get output stream return get current position file get current pos throws io exception return fs out get pos fs buffered output size get start pos return pos start current size compressed data get compressed size throws io exception ret get current pos pos start return ret finishing current block finish throws io exception try null flush null finally compress algo return compressor compressor compressor null access point stuff data block todo change data output stream something else tracks size instead currently wrap around row block size greater gb block appender extends data output stream block register block register w block state w blk state boolean closed false constructor block register called block closed the writable compression block state block appender block register register w block state wbs super wbs get output stream block register register w blk state wbs get raw size block block appender far get raw size throws io exception expecting size block exceeding gb assuming size wrap negative integer exceeds gb return size x ffffffff l get compressed size block progress file the size may smaller actual need compress data written due internal buffering inside compressor get compressed size throws io exception return w blk state get compressed
246	common\src\java\org\apache\hadoop\io\file\tfile\BoundedRangeFileInputStream.java	unrelated	package org apache hadoop io file tfile bounded range f ile input stream abstracts contiguous region hadoop fs data input stream regular input stream one create multiple bounded range file input stream top fs data input stream would interfere bounded range file input stream extends input stream fs data input stream pos end mark byte one byte new byte constructor the fs data input stream connect begining offset region length region the actual length region may smaller begin length goes beyond end fs input stream bounded range file input stream fs data input stream offset length offset length throw new index out of bounds exception invalid offset length offset length pos offset end offset length mark available throws io exception avail available pos avail end avail end pos return avail read throws io exception ret read one byte ret return one byte xff return read byte b throws io exception return read b b length read byte b len throws io exception len len b length len throw new index out of bounds exception n math min integer max value math min len end pos n return ret synchronized seek pos ret read b n ret end pos return pos ret return ret we may skip beyond end file skip n throws io exception len math min n end pos pos len return len synchronized mark readlimit mark pos synchronized reset throws io exception mark throw new io exception resetting invalid mark pos mark boolean mark supported return true close invalidate state stream null pos end mark
247	common\src\java\org\apache\hadoop\io\file\tfile\ByteArray.java	unrelated	package org apache hadoop io file tfile adaptor wrap byte array backed objects including java byte array raw comparable objects byte array implements raw comparable byte buffer offset len constructing byte array link bytes writable byte array bytes writable get bytes get length wrap whole byte array raw comparable byte array buffer byte array byte buffer buffer buffer length wrap partial byte array raw comparable byte array buffer starting offset length consecutive bytes wrapped byte array byte buffer offset len offset len buffer length offset len throw new index out of bounds exception buffer buffer offset offset len len byte buffer return buffer offset return offset size return len
248	common\src\java\org\apache\hadoop\io\file\tfile\Chunk.java	unrelated	package org apache hadoop io file tfile several related support chunk encoded sub streams top regular stream chunk prevent instantiation chunk nothing decoding chain chunks encoded chunk encoder single chunk encoder chunk decoder extends input stream data input stream null boolean last chunk remain boolean closed chunk decoder last chunk true closed true reset data input stream stream need wind forward old input stream last chunk false remain closed false constructor the source input stream contains chunk encoded data stream chunk decoder data input stream last chunk false closed false have reached last chunk boolean last chunk throws io exception check eof return last chunk how many bytes remain current chunk get remain throws io exception check eof return remain reading length next chunk data available read length throws io exception remain utils read v int remain last chunk true else remain remain check whether reach end stream case available greater true otherwise i o errors boolean check eof throws io exception closed return true true remain return false last chunk return true read length this method never blocks caller returning mean reach end stream available return remain read throws io exception check eof return ret read ret throw new io exception corrupted chunk encoding stream remain return ret read byte b throws io exception return read b b length read byte b len throws io exception len len b length len throw new index out of bounds exception check eof n math min remain len ret read b n ret throw new io exception corrupted chunk encoding stream remain ret return ret return skip n throws io exception check eof ret skip math min remain n remain ret return ret return boolean mark supported return false boolean closed return closed close throws io exception closed false try check eof skip integer max value finally closed true chunk encoder encoding output data chain chunks following sequences len byte len len byte len len n byte len n where len len len n lengths data chunks non terminal chunks lengths negated non terminal chunks cannot length all lengths range integer max value encoded utils v int format chunk encoder extends output stream the data output stream connects data output stream the internal buffer used know advertised size byte buf the number valid bytes buffer this value always range tt tt tt buf length tt elements tt buf tt tt buf count tt contain valid byte data count constructor underlying output stream user supplied buffer the buffer would used exclusively chunk encoder life cycle chunk encoder data output stream byte buf buf buf count write chunk the chunk buffer offset chunk buffer beginning chunk is last call flush buffer write chunk byte chunk offset len boolean last throws io exception last always write length last chunk utils write v int len len write chunk offset len else len utils write v int len write chunk offset len write chunk concatenation internal buffer plus user supplied data this never last block user supplied data buffer offset user data buffer user data buffer size write buf
249	common\src\java\org\apache\hadoop\io\file\tfile\CompareUtils.java	unrelated	package org apache hadoop io file tfile compare utils prevent instantiation compare utils nothing a comparator compare anything implements link raw comparable using customized comparator bytes comparator implements comparator raw comparable raw comparator object cmp bytes comparator raw comparator object cmp cmp cmp compare raw comparable raw comparable return compare buffer offset size buffer offset size compare byte len byte b len return cmp compare len b len interface objects single integer magnitude scalar magnitude scalar long implements scalar magnitude scalar long magnitude magnitude return magnitude scalar comparator implements comparator scalar serializable compare scalar scalar diff magnitude magnitude diff return diff return return memcmp raw comparator implements raw comparator object serializable compare byte b byte b return writable comparator compare bytes b b compare object object throw new runtime exception object comparison supported
250	common\src\java\org\apache\hadoop\io\file\tfile\Compression.java	pooling	package org apache hadoop io file tfile compression related stuff compression log log log factory get log compression prevent instantiation compression nothing finish on flush compression stream extends filter output stream finish on flush compression stream compression output stream cout super cout write byte b len throws io exception write b len flush throws io exception compression output stream cout compression output stream cout finish cout flush cout reset state compression algorithms enum algorithm lzo t file compression lzo transient boolean checked false string default clazz org apache hadoop io compress lzo codec transient compression codec codec null synchronized boolean supported checked checked true string ext clazz conf get conf lzo class null system get property conf lzo class null string clazz ext clazz null ext clazz default clazz try log info trying load lzo codec clazz codec compression codec reflection utils new instance class name clazz conf catch class not found exception e okay return codec null compression codec get codec throws io exception supported throw new io exception lzo codec specified did forget set property conf lzo class return codec synchronized input stream create decompression stream input stream stream decompressor decompressor stream buffer size throws io exception supported throw new io exception lzo codec specified did forget set property conf lzo class input stream bis null stream buffer size bis new buffered input stream stream stream buffer size else bis stream conf set int io compression codec lzo buffersize compression input stream cis codec create input stream bis decompressor buffered input stream bis new buffered input stream cis data ibuf size return bis synchronized output stream create compression stream output stream stream compressor compressor stream buffer size throws io exception supported throw new io exception lzo codec specified did forget set property conf lzo class output stream bos null stream buffer size bos new buffered output stream stream stream buffer size else bos stream conf set int io compression codec lzo buffersize compression output stream cos codec create output stream bos compressor buffered output stream bos new buffered output stream new finish on flush compression stream cos data obuf size return bos gz t file compression gz transient default codec codec compression codec get codec codec null codec new default codec codec set conf conf return codec synchronized input stream create decompression stream input stream stream decompressor decompressor stream buffer size throws io exception set internal buffer size read stream stream buffer size codec get conf set int io file buffer size stream buffer size compression input stream cis codec create input stream stream decompressor buffered input stream bis new buffered input stream cis data ibuf size return bis synchronized output stream create compression stream output stream stream compressor compressor stream buffer size throws io exception output stream bos null stream buffer size bos new buffered output stream stream stream buffer size else bos stream codec get conf set int io file buffer size compression output stream cos codec create output stream bos compressor buffered output stream bos new buffered output stream new finish on flush compression stream cos data
251	common\src\java\org\apache\hadoop\io\file\tfile\MetaBlockAlreadyExists.java	unrelated	package org apache hadoop io file tfile exception meta block name already exists meta block already exists extends io exception constructor message meta block already exists string super
252	common\src\java\org\apache\hadoop\io\file\tfile\MetaBlockDoesNotExist.java	unrelated	package org apache hadoop io file tfile exception no meta block given name meta block does not exist extends io exception constructor message meta block does not exist string super
253	common\src\java\org\apache\hadoop\io\file\tfile\RawComparable.java	unrelated	package org apache hadoop io file tfile interface objects compared link raw comparator this useful places need single object reference specify range bytes byte array link comparable link collections binary search java util list object comparator the actual comparison among raw comparable requires external raw comparator applications responsibility ensure two raw comparable supposed semantically comparable raw comparator raw comparable get underlying byte array byte buffer get offset first byte byte array offset get size byte range byte array size
254	common\src\java\org\apache\hadoop\io\file\tfile\SimpleBufferedOutputStream.java	unrelated	package org apache hadoop io file tfile a simplified buffered output stream borrowed buffer allow users see much data buffered simple buffered output stream extends filter output stream protected byte buf borrowed buffer protected count bytes used buffer constructor simple buffered output stream output stream byte buf super buf buf flush buffer throws io exception count write buf count count write b throws io exception count buf length flush buffer buf count byte b write byte b len throws io exception len buf length flush buffer write b len return len buf length count flush buffer system arraycopy b buf count len count len synchronized flush throws io exception flush buffer flush get size internal buffer used size return count
255	common\src\java\org\apache\hadoop\io\file\tfile\TFile.java	unrelated	package org apache hadoop io file tfile a t file container key value pairs both keys values type less bytes keys restricted kb value length restricted practically limited available disk storage t file provides following features ul li block compression li named meta data blocks li sorted unsorted keys li seek key file offset ul the memory footprint t file includes following ul li some constant overhead reading writing compressed block ul li each compressed block requires one compression decompression codec i o li temporary space buffer key li temporary space buffer value t file writer values chunk encoded buffer one chunk user data by default chunk buffer mb reading chunked value require additional memory ul li t file index proportional total number data blocks the total amount memory needed hold index estimated avg key size num blocks li meta block index proportional total number meta blocks the total amount memory needed hold index meta blocks estimated avg meta block name num meta block ul p the behavior t file customized following variables configuration ul li b tfile io chunk size b value chunk size integer bytes default mb values length less chunk size guaranteed known value length read time see link t file reader scanner entry value length known li b tfile fs output buffer size b buffer size used fs data output stream integer bytes default kb li b tfile fs input buffer size b buffer size used fs data input stream integer bytes default kb ul p suggestions performance optimization ul li minimum block size we recommend setting minimum block size kb mb general usage larger block size preferred files primarily sequential access however would lead inefficient random access data decompress smaller blocks good random access require memory hold block index may slower create must flush compressor stream conclusion data block leads fs i o flush further due internal caching compression codec smallest possible block size would around kb kb li the current implementation offer true multi threading reading the implementation uses fs data input stream seek read shown much faster positioned read call single thread mode however also means multiple threads attempt access t file using multiple scanners simultaneously actual i o carried sequentially even access different dfs blocks li compression codec use none data compressable compressable i mean compression ratio least generally use lzo starting point experimenting gz overs slightly better compression ratio lzo requires x cpu compress x cpu decompress comparing lzo li file system buffering underlying fs data input stream fs data output stream already adequately buffered applications reads writes keys values large buffers reduce sizes input output buffering t file layer setting configuration parameters tfile fs input buffer size tfile fs output buffer size ul some design rationale behind t file found href https issues apache org jira browse hadoop hadoop t file log log log factory get log t file string chunk buf size attr tfile io chunk size string fs input buf size attr tfile fs input buffer size string fs output buf size attr tfile fs output buffer size get chunk
256	common\src\java\org\apache\hadoop\io\file\tfile\TFileDumper.java	unrelated	package org apache hadoop io file tfile dumping information t file t file dumper log log log factory get log t file dumper t file dumper namespace object constructable enum align left center right zero padded string format string width align align length width return room width length align align adjusted align room align adjusted left align adjusted left return string format room align adjusted right return string format room align adjusted center half room return string format half string format room half throw new illegal argument exception unsupported alignment string format width align align align zero padded return string format width return format long string width align calculate width string caption max return math max caption length long string max length dump information t file path t file print stream output information the configuration object dump info string file print stream configuration conf throws io exception max key sample len path path new path file file system fs path get file system conf length fs get file status path get len fs data input stream fsdis fs open path t file reader reader new t file reader fsdis length conf try linked hash map string string properties new linked hash map string string block cnt reader reader bcf get block count meta blk cnt reader reader bcf meta index index size properties put bc file version reader reader bcf version string properties put t file version reader tfile meta version string properties put file length long string length properties put data compression reader reader bcf get default compression name properties put record count long string reader get entry count properties put sorted boolean string reader sorted reader sorted properties put comparator reader get comparator name properties put data block count integer string block cnt data size data size uncompressed block cnt block cnt block region region reader reader bcf data index get block region list get data size region get compressed size data size uncompressed region get raw size properties put data block bytes long string data size reader reader bcf get default compression name none properties put data block uncompressed bytes long string data size uncompressed properties put data block compression ratio string format f data size uncompressed data size properties put meta block count integer string meta blk cnt meta size meta size uncompressed meta blk cnt collection meta index entry meta blks reader reader bcf meta index index values boolean calculate compression false iterator meta index entry meta blks iterator next meta index entry e next meta size e get region get compressed size meta size uncompressed e get region get raw size e get compression algorithm compression algorithm none calculate compression true properties put meta block bytes long string meta size calculate compression properties put meta block uncompressed bytes long string meta size uncompressed properties put meta block compression ratio string format f meta size uncompressed meta size properties put meta data size ratio string format f data size meta size left over bytes length data size meta size misc size bc file magic size long size
257	common\src\java\org\apache\hadoop\io\file\tfile\Utils.java	unrelated	package org apache hadoop io file tfile supporting utility used t file shared users t file utils prevent instantiation utils utils nothing encoding integer variable length encoding format synonymous code utils write v long n code output stream the integer encoded write v int data output n throws io exception write v long n encoding long integer variable length encoding format ul li n encode one byte actual value otherwise li n encode two bytes byte n byte n xff otherwise li n in encode three bytes byte n byte n xff byte n xff otherwise li n encode four bytes byte n byte n xff byte n xff byte n xff otherwise li n encode five bytes byte byte n xff byte n xff byte n xff byte n xff li n encode six bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff li n encode seven bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff li n encode eight bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff li n encode nine bytes byte byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff byte n xff ul output stream integer number write v long data output n throws io exception n n write byte n return un n n n many bytes need represent number sign bit len long size long number of leading zeros un first byte n len switch len case fall first byte len first byte case first byte first byte write byte first byte write byte n return fall first byte len first byte case first byte first byte write byte first byte write short n return fall first byte len first byte case first byte first byte write byte first byte write short n write byte n return write byte len write int n return case write byte len write int n write byte n return case write byte len write int n write short n return case write byte len write int n write short n write byte n return case write byte len write long n return default throw new runtime exception internel error decoding variable length integer synonymous code utils read v long code input stream read v int data input throws io exception ret read v long ret integer max value ret integer min value throw new runtime exception number large represented integer return ret decoding variable length integer suppose value first byte fb following bytes nb ul li fb return fb li fb return fb nb xff li fb return fb nb xff nb xff li fb return fb nb xff nb xff nb xff li fb return interpret nb fb signed big endian integer input stream read v long data input throws io exception first byte read byte first byte return first byte switch
258	common\src\java\org\apache\hadoop\io\nativeio\Errno.java	unrelated	package org apache hadoop io nativeio enum representing posix errno values enum errno eperm enoent esrch eintr eio enxio e big enoexec ebadf echild eagain enomem eacces efault enotblk ebusy eexist exdev enodev enotdir eisdir einval enfile emfile enotty etxtbsy efbig enospc espipe erofs emlink epipe edom erange unknown
259	common\src\java\org\apache\hadoop\io\nativeio\NativeIO.java	unrelated	package org apache hadoop io nativeio jni wrappers various native io related calls available java these functions generally used alongside fallback another portable mechanism native io flags open call bits fcntl o rdonly o wronly o rdwr o creat o excl o noctty o trunc o append o nonblock o sync o async o fsync o sync o ndelay o nonblock log log log factory get log native io boolean native loaded false boolean workaround non thread safe passwd calls false string workaround non threadsafe calls key hadoop workaround non threadsafe getpwuid boolean workaround non threadsafe calls default false native code loader native code loaded try configuration conf new configuration workaround non thread safe passwd calls conf get boolean workaround non threadsafe calls key workaround non threadsafe calls default init native native loaded true catch throwable this happen user older version libhadoop installed case continue without native io warning log error unable initialize native io libraries return true jni based native io extensions available boolean available return native code loader native code loaded native loaded wrapper around open native file descriptor open string path flags mode throws io exception wrapper around fstat native stat fstat file descriptor fd throws io exception wrapper around chmod native chmod string path mode throws io exception initialize jni method id id cache native init native result type fstat call stat string owner group mode mode constants s ifmt type file s ififo named pipe fifo s ifchr character special s ifdir directory s ifblk block special s ifreg regular s iflnk symbolic link s ifsock socket s ifwht whiteout s isuid set user id execution s isgid set group id execution s isvtx save swapped text even use s irusr read permission owner s iwusr write permission owner s ixusr execute search permission owner stat string owner string group mode owner owner group group mode mode string string return stat owner owner group group mode mode string get owner return owner string get group return group get mode return mode
260	common\src\java\org\apache\hadoop\io\nativeio\NativeIOException.java	unrelated	package org apache hadoop io nativeio an exception generated call native io code these exceptions simply wrap errno result codes native io exception extends io exception serial version uid l errno errno native io exception string msg errno errno super msg errno errno errno get errno return errno string string return errno string super get message
261	common\src\java\org\apache\hadoop\io\retry\DefaultFailoverProxyProvider.java	unrelated	package org apache hadoop io retry an implementation link failover proxy provider nothing event failover always returns proxy object default failover proxy provider implements failover proxy provider object proxy class iface default failover proxy provider class iface object proxy proxy proxy iface iface class get interface return iface object get proxy return proxy perform failover object current proxy nothing
262	common\src\java\org\apache\hadoop\io\retry\FailoverProxyProvider.java	unrelated	package org apache hadoop io retry an implementer capable providing proxy objects use ipc communication potentially modifying objects creating entirely new ones event certain types failures the determination whether fail handled link retry policy failover proxy provider get proxy object used next failover event occurs object get proxy called whenever associated link retry policy determines error warrants failing failover event perform failover object current proxy return reference provider proxy objects actually implement if methods annotated link idempotent fact passed link retry policy retry exception boolean method error use determining whether failover attempted link failover proxy provider get proxy class get interface
263	common\src\java\org\apache\hadoop\io\retry\Idempotent.java	unrelated	package org apache hadoop io retry used mark certain methods idempotent therefore warrant retried failover idempotent
264	common\src\java\org\apache\hadoop\io\retry\package-info.java	unrelated	package org apache hadoop io retry
265	common\src\java\org\apache\hadoop\io\retry\RetryInvocationHandler.java	unrelated	package org apache hadoop io retry retry invocation handler implements invocation handler log log log factory get log retry invocation handler failover proxy provider proxy provider retry policy default policy map string retry policy method name to policy map object current proxy retry invocation handler failover proxy provider proxy provider retry policy retry policy proxy provider proxy provider default policy retry policy method name to policy map collections empty map current proxy proxy provider get proxy retry invocation handler failover proxy provider proxy provider map string retry policy method name to policy map proxy provider proxy provider default policy retry policies try once then fail method name to policy map method name to policy map current proxy proxy provider get proxy object invoke object proxy method method object args throws throwable retry policy policy method name to policy map get method get name policy null policy default policy failovers retries true try return invoke method method args catch exception e boolean method idempotent proxy provider get interface get method method get name method get parameter types annotation present idempotent retry action action policy retry e retries failovers method idempotent action retry action fail log warn exception invoking method get name current proxy get class not retrying e method get return type equals void type throw e non methods fail without exception return null else action retry action failover and retry log warn exception invoking method get name current proxy get class trying fail e failovers proxy provider perform failover current proxy current proxy proxy provider get proxy log debug enabled log debug exception invoking method get name current proxy get class retrying e object invoke method method method object args throws throwable try method accessible method set accessible true return method invoke current proxy args catch invocation target exception e throw e get cause
266	common\src\java\org\apache\hadoop\io\retry\RetryPolicies.java	unrelated	package org apache hadoop io retry p a collection useful implementations link retry policy p retry policies log log log factory get log retry policies p try fail throwing exception this corresponds retry mechanism place p retry policy try once then fail new try once then fail p try fail silently code code methods throwing exception non code code methods p retry policy try once dont fail new try once dont fail p keep trying forever p retry policy retry forever new retry forever p keep trying limited number times waiting fixed time attempts fail throwing exception p retry policy retry up to maximum count with fixed sleep max retries sleep time time unit time unit return new retry up to maximum count with fixed sleep max retries sleep time time unit p keep trying maximum time waiting fixed time attempts fail throwing exception p retry policy retry up to maximum time with fixed sleep max time sleep time time unit time unit return new retry up to maximum time with fixed sleep max time sleep time time unit p keep trying limited number times waiting growing amount time attempts fail throwing exception the time attempts code sleep time code mutliplied number tries far p retry policy retry up to maximum count with proportional sleep max retries sleep time time unit time unit return new retry up to maximum count with proportional sleep max retries sleep time time unit p keep trying limited number times waiting growing amount time attempts fail throwing exception the time attempts code sleep time code mutliplied random number range number retries p retry policy exponential backoff retry max retries sleep time time unit time unit return new exponential backoff retry max retries sleep time time unit p set default policy explicit handlers specific exceptions p retry policy retry by exception retry policy default policy map class extends exception retry policy exception to policy map return new exception dependent retry default policy exception to policy map p a retry policy remote exception set default policy explicit handlers specific exceptions p retry policy retry by remote exception retry policy default policy map class extends exception retry policy exception to policy map return new remote exception dependent retry default policy exception to policy map retry policy failover on network exception max failovers return failover on network exception try once then fail max failovers retry policy failover on network exception retry policy fallback policy max failovers return new failover on network exception retry fallback policy max failovers try once then fail implements retry policy retry action retry exception e retries failovers boolean method idempotent throws exception throw e try once dont fail implements retry policy retry action retry exception e retries failovers boolean method idempotent throws exception return retry action fail retry forever implements retry policy retry action retry exception e retries failovers boolean method idempotent throws exception return retry action retry retry limited implements retry policy max retries sleep time time unit time unit retry limited max retries sleep time time unit time unit max retries max retries sleep time
267	common\src\java\org\apache\hadoop\io\retry\RetryPolicy.java	unrelated	package org apache hadoop io retry p specifies policy retrying method failures implementations immutable p retry policy returned link retry policy retry exception boolean enum retry action fail retry failover and retry p determines whether framework retry method given exception number retries made operation far p different backend implementation reasonably retried failover know previous attempt reached server code false code method retried fail exception methods method failed retried retry action retry exception e retries failovers boolean method idempotent throws exception
268	common\src\java\org\apache\hadoop\io\retry\RetryProxy.java	unrelated	package org apache hadoop io retry p a factory creating retry proxies p retry proxy p create proxy implementation using retry policy method p object create class iface object implementation retry policy retry policy return retry proxy create iface new default failover proxy provider iface implementation retry policy create proxy implementations using given link failover proxy provider retry policy method object create class iface failover proxy provider proxy provider retry policy retry policy return proxy new proxy instance proxy provider get interface get class loader new class iface new retry invocation handler proxy provider retry policy create proxy implementation using set retry policies specified method name if retry policy defined method default link retry policies try once then fail used object create class iface object implementation map string retry policy method name to policy map return retry proxy create iface new default failover proxy provider iface implementation method name to policy map create proxy implementations using given link failover proxy provider set retry policies specified method name if retry policy defined method default link retry policies try once then fail used object create class iface failover proxy provider proxy provider map string retry policy method name to policy map return proxy new proxy instance proxy provider get interface get class loader new class iface new retry invocation handler proxy provider method name to policy map
269	common\src\java\org\apache\hadoop\io\serializer\Deserializer.java	unrelated	package org apache hadoop io serializer p provides facility deserializing objects type t link input stream p p deserializers stateful must buffer input since producers may read input calls link deserialize object p deserializer t p prepare deserializer reading p open input stream throws io exception p deserialize next object underlying input stream if object code code non null deserializer may set internal state next object read input stream otherwise object code code null new deserialized object created p t deserialize t throws io exception p close underlying input stream clear resources p close throws io exception
270	common\src\java\org\apache\hadoop\io\serializer\DeserializerComparator.java	unrelated	package org apache hadoop io serializer p a link raw comparator uses link deserializer deserialize objects compared standard link comparator used compare p p one may optimize compare intensive operations using custom implementation link raw comparator operates directly byte representations p deserializer comparator t implements raw comparator t input buffer buffer new input buffer deserializer t deserializer t key t key protected deserializer comparator deserializer t deserializer throws io exception deserializer deserializer deserializer open buffer compare byte b byte b try buffer reset b key deserializer deserialize key buffer reset b key deserializer deserialize key catch io exception e throw new runtime exception e return compare key key
271	common\src\java\org\apache\hadoop\io\serializer\JavaSerialization.java	unrelated	package org apache hadoop io serializer p an experimental link serialization java link serializable p java serialization implements serialization serializable java serialization deserializer t extends serializable implements deserializer t object input stream ois open input stream throws io exception ois new object input stream header t deserialize t object throws io exception try ignore passed object return t ois read object catch class not found exception e throw new io exception e string close throws io exception ois close java serialization serializer implements serializer serializable object output stream oos open output stream throws io exception oos new object output stream header serialize serializable object throws io exception oos reset clear back references oos write object object close throws io exception oos close boolean accept class c return serializable assignable from c deserializer serializable get deserializer class serializable c return new java serialization deserializer serializable serializer serializable get serializer class serializable c return new java serialization serializer
272	common\src\java\org\apache\hadoop\io\serializer\JavaSerializationComparator.java	unrelated	package org apache hadoop io serializer p a link raw comparator uses link java serialization link deserializer deserialize objects compared via link comparable interfaces p java serialization comparator t extends serializable comparable t extends deserializer comparator t java serialization comparator throws io exception super new java serialization java serialization deserializer t compare t t return compare to
273	common\src\java\org\apache\hadoop\io\serializer\Serialization.java	unrelated	package org apache hadoop io serializer p encapsulates link serializer link deserializer pair p serialization t allows clients test whether link serialization supports given boolean accept class c serializer t get serializer class t c deserializer t get deserializer class t c
274	common\src\java\org\apache\hadoop\io\serializer\SerializationFactory.java	unrelated	package org apache hadoop io serializer p a factory link serialization p serialization factory extends configured log log log factory get log serialization factory get name list serialization serializations new array list serialization p serializations found reading code io serializations code property code conf code comma delimited list classnames p serialization factory configuration conf super conf string serializer name conf get strings io serializations new string writable serialization get name avro specific serialization get name avro reflect serialization get name add conf serializer name add configuration conf string serialization name try class extends serialization serializion class class extends serialization conf get class by name serialization name serializations add serialization reflection utils new instance serializion class get conf catch class not found exception e log warn serialization found e t serializer t get serializer class t c return get serialization c get serializer c t deserializer t get deserializer class t c return get serialization c get deserializer c t serialization t get serialization class t c serialization serialization serializations serialization accept c return serialization t serialization return null
275	common\src\java\org\apache\hadoop\io\serializer\Serializer.java	unrelated	package org apache hadoop io serializer p provides facility serializing objects type t link output stream p p serializers stateful must buffer output since producers may write output calls link serialize object p serializer t p prepare serializer writing p open output stream throws io exception p serialize code code underlying output stream p serialize t throws io exception p close underlying output stream clear resources p close throws io exception
276	common\src\java\org\apache\hadoop\io\serializer\WritableSerialization.java	unrelated	package org apache hadoop io serializer a link serialization link writable delegates link writable write java io data output link writable read fields java io data input writable serialization extends configured implements serialization writable writable deserializer extends configured implements deserializer writable class writable class data input stream data in writable deserializer configuration conf class c set conf conf writable class c open input stream instanceof data input stream data in data input stream else data in new data input stream writable deserialize writable w throws io exception writable writable w null writable writable reflection utils new instance writable class get conf else writable w writable read fields data in return writable close throws io exception data in close writable serializer extends configured implements serializer writable data output stream data out open output stream instanceof data output stream data out data output stream else data out new data output stream serialize writable w throws io exception w write data out close throws io exception data out close boolean accept class c return writable assignable from c serializer writable get serializer class writable c return new writable serializer deserializer writable get deserializer class writable c return new writable deserializer get conf c
277	common\src\java\org\apache\hadoop\io\serializer\avro\AvroReflectSerializable.java	unrelated	package org apache hadoop io serializer avro tag avro reflect serializable classes implementing serialized deserialized using link avro reflect serialization avro reflect serializable
278	common\src\java\org\apache\hadoop\io\serializer\avro\AvroReflectSerialization.java	unrelated	package org apache hadoop io serializer avro serialization avro reflect for accepted serialization must either package list configured via code avro reflect pkgs code implement link avro reflect serializable avro reflect serialization extends avro serialization object key configure packages contain serialized deserialized using multiple packages specified using comma separated list string avro reflect packages avro reflect pkgs set string packages synchronized boolean accept class c packages null get packages return avro reflect serializable assignable from c packages contains c get package get name get packages string pkg list get conf get strings avro reflect packages packages new hash set string pkg list null string pkg pkg list packages add pkg trim datum reader get reader class object clazz try return new reflect datum reader clazz catch exception e throw new runtime exception e schema get schema object return reflect data get get schema get class datum writer get writer class object clazz return new reflect datum writer
279	common\src\java\org\apache\hadoop\io\serializer\avro\AvroSerialization.java	unrelated	package org apache hadoop io serializer avro base providing serialization avro types avro serialization t extends configured implements serialization t string avro schema key avro schema deserializer t get deserializer class t c return new avro deserializer c serializer t get serializer class t c return new avro serializer c return avro schema instance given schema get schema t create return avro datum writer given datum writer t get writer class t clazz create return avro datum reader given datum reader t get reader class t clazz avro serializer implements serializer t datum writer t writer binary encoder encoder output stream stream avro serializer class t clazz writer get writer clazz close throws io exception encoder flush stream close open output stream throws io exception stream encoder new binary encoder serialize t throws io exception writer set schema get schema writer write encoder avro deserializer implements deserializer t datum reader t reader binary decoder decoder input stream stream avro deserializer class t clazz reader get reader clazz close throws io exception stream close t deserialize t throws io exception return reader read decoder open input stream throws io exception stream decoder decoder factory default factory create binary decoder null
280	common\src\java\org\apache\hadoop\io\serializer\avro\AvroSpecificSerialization.java	unrelated	package org apache hadoop io serializer avro serialization avro specific this serialization used generated avro specific compiler avro specific serialization extends avro serialization specific record boolean accept class c return specific record assignable from c datum reader get reader class specific record clazz try return new specific datum reader clazz new instance get schema catch exception e throw new runtime exception e schema get schema specific record return get schema datum writer get writer class specific record clazz return new specific datum writer
281	common\src\java\org\apache\hadoop\ipc\AvroRpcEngine.java	unrelated	package org apache hadoop ipc tunnel avro format rpc requests hadoop link rpc connection this give cross language wire compatibility since hadoop rpc wire format non standard permit use avro protocol versioning features inter java rp cs avro rpc engine implements rpc engine log log log factory get log rpc version implementation tunnel rpc engine engine new writable rpc engine tunnel avro rpc request response hadoop rpc tunnel protocol extends versioned protocol writable rpc engine expects version id every protocol version id l all avro methods responses go buffer list writable call buffer list writable request throws io exception a writable holds list byte buffer the avro rpc transceiver basic unit data transfer buffer list writable implements writable list byte buffer buffers buffer list writable required rpc writables buffer list writable list byte buffer buffers buffers buffers read fields data input throws io exception size read int buffers new array list byte buffer size size length read int byte buffer buffer byte buffer allocate length read fully buffer array length buffers add buffer write data output throws io exception write int buffers size byte buffer buffer buffers write int buffer remaining write buffer array buffer position buffer remaining an avro rpc transceiver tunnels client requests hadoop rpc client transceiver extends transceiver tunnel protocol tunnel inet socket address remote client transceiver inet socket address addr user group information ticket configuration conf socket factory factory rpc timeout throws io exception tunnel engine get proxy tunnel protocol version addr ticket conf factory rpc timeout get proxy remote addr string get remote name return remote string list byte buffer transceive list byte buffer request throws io exception return tunnel call new buffer list writable request buffers list byte buffer read buffers throws io exception throw new unsupported operation exception write buffers list byte buffer buffers throws io exception throw new unsupported operation exception close throws io exception engine stop proxy tunnel construct client side proxy object implements named protocol talking server named address t protocol proxy t get proxy class t protocol client version inet socket address addr user group information ticket configuration conf socket factory factory rpc timeout throws io exception return new protocol proxy t protocol t proxy new proxy instance protocol get class loader new class protocol new invoker protocol addr ticket conf factory rpc timeout false stop proxy stop proxy object proxy try invoker proxy get invocation handler proxy close catch io exception e log warn error stopping proxy e invoker implements invocation handler closeable client transceiver tx specific requestor requestor invoker class protocol inet socket address addr user group information ticket configuration conf socket factory factory rpc timeout throws io exception tx new client transceiver addr ticket conf factory rpc timeout requestor create requestor protocol tx throws throwable return requestor invoke proxy method args close throws io exception tx close protected specific requestor create requestor class protocol transceiver transeiver throws io exception return new reflect requestor protocol transeiver protected responder create responder class iface object impl return new reflect responder iface impl an avro rpc responder process requests passed via hadoop
282	common\src\java\org\apache\hadoop\ipc\AvroSpecificRpcEngine.java	unrelated	package org apache hadoop ipc avro rpc engine uses avro specific ap is the protocols generated via avro idl needs use engine avro specific rpc engine extends avro rpc engine protected specific requestor create requestor class protocol transceiver transeiver throws io exception return new specific requestor protocol transeiver protected responder create responder class iface object impl return new specific responder iface impl
283	common\src\java\org\apache\hadoop\ipc\Client.java	authenticate	package org apache hadoop ipc a client ipc service ipc calls take single link writable parameter return link writable value a service runs port defined parameter value client log log log factory get log client hashtable connection id connection connections new hashtable connection id connection class extends writable value class call values counter counter call ids atomic boolean running new atomic boolean true client runs configuration conf socket factory socket factory create sockets ref count string ping interval name ipc ping interval default ping interval min ping call id set ping interval value configuration set ping interval configuration conf ping interval conf set int ping interval name ping interval get ping interval configuration if set configuration return default value get ping interval configuration conf return conf get int ping interval name default ping interval the time rpc timeout if ping enabled via ipc client ping timeout value ping interval if ping enabled timeout value get timeout configuration conf conf get boolean ipc client ping true return get ping interval conf return increment client reference count synchronized inc count ref count decrement client reference count synchronized dec count ref count return client reference synchronized boolean zero reference return ref count a call waiting value call id call id writable param parameter writable value value null error io exception error exception null value boolean done true call done protected call writable param param param synchronized client id counter indicate call complete value error available notifies default protected synchronized call complete done true notify notify caller set exception error notify caller call done synchronized set exception io exception error error error call complete set return value error notify caller call done synchronized set value writable value value value call complete synchronized writable get value return value thread reads responses notifies callers each connection owns socket connected remote address calls multiplexed socket responses may delivered order connection extends thread inet socket address server server ip port string server principal server krb principal name connection header header connection header connection id remote id connection id auth method auth method authentication method boolean use sasl token extends token identifier token sasl rpc client sasl rpc client socket socket null connected socket data input stream data output stream rpc timeout max idle time connections culled idle max idle time msecs max retries max retries socket connections boolean tcp no delay t disable nagle algorithm boolean ping need send ping message ping interval often sends ping server msecs currently active calls hashtable integer call calls new hashtable integer call atomic long last activity new atomic long last i o activity time atomic boolean close connection new atomic boolean indicate connection closed io exception close exception close reason connection connection id remote id throws io exception remote id remote id server remote id get address server unresolved throw new unknown host exception unknown host remote id get address get host name rpc timeout remote id get rpc timeout max idle time remote id get max idle time max retries remote id get max retries tcp no delay remote id get tcp
284	common\src\java\org\apache\hadoop\ipc\ClientCache.java	pooling	package org apache hadoop ipc cache client using socket factory hash key client cache map socket factory client clients new hash map socket factory client construct cache ipc client user provided socket factory cached client exists synchronized client get client configuration conf socket factory factory class extends writable value class construct cache client the configuration used timeout clients connection pools so either lose connection pooling leak sockets b use timeout configurations since ipc usually intended globally per job choose client client clients get factory client null client new client value class conf factory clients put factory client else client inc count return client construct cache ipc client default socket factory default value class cached client exists synchronized client get client configuration conf return get client conf socket factory get default object writable construct cache ipc client user provided socket factory cached client exists default response type object writable synchronized client get client configuration conf socket factory factory return get client conf factory object writable stop rpc client connection a rpc client closed reference count becomes zero stop client client client synchronized client dec count client zero reference clients remove client get socket factory client zero reference client stop
285	common\src\java\org\apache\hadoop\ipc\ConnectionHeader.java	authenticate	package org apache hadoop ipc the ipc connection header sent client server connection establishment connection header implements writable log log log factory get log connection header string protocol user group information ugi null auth method auth method connection header create new link connection header given code protocol code link user group information server server connection header string protocol user group information ugi auth method auth method protocol protocol ugi ugi auth method auth method read fields data input throws io exception protocol text read string protocol empty protocol null boolean ugi username present read boolean ugi username present string username read utf boolean real user name present read boolean real user name present string real user name read utf user group information real user ugi user group information create remote user real user name ugi user group information create proxy user username real user ugi else ugi user group information create remote user username else ugi null write data output throws io exception text write string protocol null protocol ugi null auth method auth method kerberos send effective user kerberos auth write boolean true write utf ugi get user name write boolean false else auth method auth method digest don send user token auth write boolean false else send effective user real user simple auth write boolean true write utf ugi get user name ugi get real user null write boolean true write utf ugi get real user get user name else write boolean false else write boolean false string get protocol return protocol user group information get ugi return ugi string string return protocol ugi
286	common\src\java\org\apache\hadoop\ipc\package-info.java	unrelated	package org apache hadoop ipc
287	common\src\java\org\apache\hadoop\ipc\ProtocolProxy.java	unrelated	package org apache hadoop ipc wraps around server proxy containing list supported methods a list methods value null indicates client server protocol protocol proxy t class t protocol t proxy hash set integer server methods null boolean support server method check boolean server methods fetched false constructor methods method supported always return true if true server methods fetched first call method supported protocol proxy class t protocol t proxy boolean support server method check protocol protocol proxy proxy support server method check support server method check fetch server methods method method throws io exception client version try field version field method get declaring class get field version id version field set accessible true client version version field get long method get declaring class catch no such field exception ex throw new runtime exception ex catch illegal access exception ex throw new runtime exception ex client methods hash protocol signature get fingerprint method get declaring class get methods protocol signature server info versioned protocol proxy get protocol signature protocol get name client version client methods hash server version server info get version server version client version throw new rpc version mismatch protocol get name client version server version server methods codes server info get methods server methods codes null server methods new hash set integer server methods codes length server methods codes server methods add integer value of server methods fetched true get proxy t get proxy return proxy check method supported server synchronized boolean method supported string method name class parameter types throws io exception support server method check return true method method try method protocol get declared method method name parameter types catch security exception e throw new io exception e catch no such method exception e throw new io exception e server methods fetched fetch server methods method server methods null client server protocol return true return server methods contains integer value of protocol signature get fingerprint method
288	common\src\java\org\apache\hadoop\ipc\ProtocolSignature.java	unrelated	package org apache hadoop ipc protocol signature implements writable register ctor writable factories set factory protocol signature new writable factory writable new instance return new protocol signature version methods null array method hash codes default constructor protocol signature constructor protocol signature version method hashcodes version version methods method hashcodes get version return version get methods return methods read fields data input throws io exception version read long boolean methods read boolean methods num methods read int methods new num methods num methods methods read int write data output throws io exception write long version methods null write boolean false else write boolean true write int methods length method methods write int method calculate method hash code considering method name returning type parameter types get fingerprint method method hashcode method get name hash code hashcode hashcode method get return type get name hash code class type method get parameter types hashcode hashcode type get name hash code return hashcode convert array method array hash codes get fingerprints method methods methods null return null hash codes new methods length methods length hash codes get fingerprint methods return hash codes get hash code array methods methods sorted hashcode calculated so returned value irrelevant method order array get fingerprint method methods return get fingerprint get fingerprints methods get hash code array hashcodes hashcodes sorted hashcode calculated so returned value irrelevant hashcode order array get fingerprint hashcodes arrays sort hashcodes return arrays hash code hashcodes protocol sig fingerprint protocol signature signature fingerprint protocol sig fingerprint protocol signature sig fingerprint signature sig fingerprint fingerprint a cache maps protocol name signature finger print hash map string protocol sig fingerprint protocol fingerprint cache new hash map string protocol sig fingerprint return protocol signature finger print cache protocol sig fingerprint get sig fingerprint class extends versioned protocol protocol server version string protocol name protocol get name synchronized protocol fingerprint cache protocol sig fingerprint sig protocol fingerprint cache get protocol name sig null server method hashcodes get fingerprints protocol get methods sig new protocol sig fingerprint new protocol signature server version server method hashcodes get fingerprint server method hashcodes protocol fingerprint cache put protocol name sig return sig get server protocol signature protocol signature get protocol signature client methods hash code server version class extends versioned protocol protocol try get finger print signature cache protocol sig fingerprint sig get sig fingerprint protocol server version check client side protocol matches one server side client methods hash code sig fingerprint return new protocol signature server version null null indicates match return sig signature get server protocol signature protocol signature get protocol signature versioned protocol server string protocol client version client methods hash throws io exception class extends versioned protocol inter try inter class extends versioned protocol class name protocol catch exception e throw new io exception e server version server get protocol version protocol client version return protocol signature get protocol signature client methods hash server version inter
289	common\src\java\org\apache\hadoop\ipc\RemoteException.java	unrelated	package org apache hadoop ipc remote exception extends io exception for java io serializable serial version uid l string name remote exception string name string msg super msg name name string get class name return name if remote exception wraps one lookup types return exception p unwraps io exception io exception unwrap remote exception class lookup types lookup types null return class lookup class lookup types lookup class get name equals get class name continue try return instantiate exception lookup class subclass io exception catch exception e cannot instantiate lookup class return return wrapped exception lookup types return return instantiate return exception wrapped remote exception p this unwraps code throwable code constructor taking code string code parameter otherwise returns io exception unwrap remote exception try class real class class name get class name return instantiate exception real class subclass io exception catch exception e cannot instantiate original exception return return io exception instantiate exception class extends io exception cls throws exception constructor extends io exception cn cls get constructor string cn set accessible true io exception ex cn new instance get message ex init cause return ex create remote exception attributes remote exception value of attributes attrs return new remote exception attrs get value attrs get value message string string return name get message
290	common\src\java\org\apache\hadoop\ipc\RPC.java	unrelated	package org apache hadoop ipc a simple rpc mechanism a protocol java all parameters return types must one ul li primitive type code boolean code code byte code code char code code short code code code code code code code code code code code li li link string li li link writable li li array types li ul all methods protocol throw io exception no field data protocol instance transmitted rpc log log log factory get log rpc rpc ctor cache rpc engines protocol map class rpc engine protocol engines new hash map class rpc engine track rpc engine used proxy stop proxy map class rpc engine proxy engines new hash map class rpc engine string engine prop rpc engine set protocol use non default rpc engine set protocol engine configuration conf class protocol class engine conf set class engine prop protocol get name engine rpc engine return rpc engine configured handle protocol synchronized rpc engine get protocol engine class protocol configuration conf rpc engine engine protocol engines get protocol engine null class impl conf get class engine prop protocol get name writable rpc engine engine rpc engine reflection utils new instance impl conf protocol interface proxy engines put proxy get proxy class protocol get class loader protocol engine protocol engines put protocol engine return engine return rpc engine handles proxy object synchronized rpc engine get proxy engine object proxy return proxy engines get proxy get class a version mismatch rpc protocol version mismatch extends io exception serial version uid string name client version server version create version mismatch exception version mismatch string name client version server version super protocol name version mismatch client client version server server version name name client version client version server version server version get name eg org apache hadoop mapred inter tracker protocol string get interface name return name get client preferred version get client version return client version get server agreed version get server version return server version get proxy connection remote server t t wait for proxy class t protocol client version inet socket address addr configuration conf throws io exception return wait for protocol proxy protocol client version addr conf get proxy get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t wait for protocol proxy class t protocol client version inet socket address addr configuration conf throws io exception return wait for protocol proxy protocol client version addr conf long max value get proxy connection remote server t t wait for proxy class t protocol client version inet socket address addr configuration conf conn timeout throws io exception return wait for protocol proxy protocol client version addr conf conn timeout get proxy get protocol proxy contains proxy connection remote server set methods supported server t protocol proxy t wait for protocol proxy class t protocol client version inet socket address addr configuration conf conn timeout throws io exception return wait for protocol proxy protocol client version addr conf conn timeout get proxy connection remote server t t wait for proxy class t protocol client version inet
291	common\src\java\org\apache\hadoop\ipc\RpcClientException.java	unrelated	package org apache hadoop ipc indicates exception rpc client rpc client exception extends rpc exception serial version uid l constructs exception specified detail message rpc client exception string message super message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc client exception string message throwable cause super message cause
292	common\src\java\org\apache\hadoop\ipc\RpcEngine.java	unrelated	package org apache hadoop ipc an rpc implementation rpc engine construct client side proxy object t protocol proxy t get proxy class t protocol client version inet socket address addr user group information ticket configuration conf socket factory factory rpc timeout throws io exception stop proxy stop proxy object proxy expert make multiple parallel calls set servers object call method method object params inet socket address addrs user group information ticket configuration conf throws io exception interrupted exception construct server protocol implementation instance rpc server get server class protocol object instance string bind address port num handlers num readers queue size per handler boolean verbose configuration conf secret manager extends token identifier secret manager throws io exception
293	common\src\java\org\apache\hadoop\ipc\RpcException.java	unrelated	package org apache hadoop ipc indicates exception execution remote procedure call rpc exception extends io exception serial version uid l constructs exception specified detail message rpc exception string message super message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc exception string message throwable cause super message cause
294	common\src\java\org\apache\hadoop\ipc\RpcServerException.java	unrelated	package org apache hadoop ipc indicates exception rpc server rpc server exception extends rpc exception serial version uid l constructs exception specified detail message rpc server exception string message super message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown rpc server exception string message throwable cause super message cause
295	common\src\java\org\apache\hadoop\ipc\Server.java	authenticate	package org apache hadoop ipc an ipc service ipc calls take single link writable parameter return link writable value a service runs port defined parameter value server boolean authorize boolean security enabled the first four bytes hadoop rpc connections byte buffer header byte buffer wrap hrpc get bytes introduce ping server throw away rp cs introduce protocol rpc connection header introduced sasl security layer introduced use link array primitive writable internal object writable efficiently transmit arrays primitives byte current version initial max size response buffer initial resp buf size log log log factory get log server log auditlog log factory get log security logger server get name string auth failed for auth failed string auth successfull for auth successfull thread local server server new thread local server map string class protocol cache new concurrent hash map string class class get protocol class string protocol name configuration conf throws class not found exception class protocol protocol cache get protocol name protocol null protocol conf get class by name protocol name protocol cache put protocol name protocol return protocol returns server instance called null may called link call writable implementations link writable methods paramters return values permits applications access server context server get return server get this set call object handler invokes rpc reset call returns thread local call cur call new thread local call returns remote side ip address invoked inside rpc returns null incase error inet address get remote ip call call cur call get call null return call connection get host inet address return null returns remote address invoked inside rpc returns null case error string get remote address inet address addr get remote ip return addr null null addr get host address return true invocation rpc boolean rpc invocation return cur call get null string bind address port port listen handler count number handler threads read threads number read threads class extends writable param class call parameters max idle time maximum idle time client may disconnected threshold idle connections number idle connections start cleaning idle connections max connections to nuke max number connections nuke cleanup protected rpc metrics rpc metrics protected rpc detailed metrics rpc detailed metrics configuration conf secret manager token identifier secret manager service authorization manager service authorization manager new service authorization manager max queue size max resp size socket send buffer size boolean tcp no delay t disable nagle algorithm volatile boolean running true true server runs blocking queue call call queue queued calls list connection connection list collections synchronized list new linked list connection maintain list client connections listener listener null responder responder null num connections handler handlers null a convenience method bind given address report better exceptions address valid host bind server socket socket inet socket address address backlog throws io exception try socket bind address backlog catch bind exception e bind exception bind exception new bind exception problem binding address e get message bind exception init cause e throw bind exception catch socket exception e if try bind different host address give better error message unresolved address equals e get message throw new unknown
296	common\src\java\org\apache\hadoop\ipc\StandbyException.java	unrelated	package org apache hadoop ipc thrown remote server active server set servers subset may active standby exception extends exception serial version uid x ad l standby exception string msg super msg
297	common\src\java\org\apache\hadoop\ipc\Status.java	unrelated	package org apache hadoop ipc status hadoop ipc call enum status success error fatal state status state state state
298	common\src\java\org\apache\hadoop\ipc\UnexpectedServerException.java	unrelated	package org apache hadoop ipc indicates rpc server encountered undeclared exception service unexpected server exception extends rpc exception serial version uid l constructs exception specified detail message unexpected server exception string message super message constructs exception specified detail message cause a tt null tt value permitted indicates cause nonexistent unknown unexpected server exception string message throwable cause super message cause
299	common\src\java\org\apache\hadoop\ipc\VersionedProtocol.java	unrelated	package org apache hadoop ipc superclass protocols use hadoop rpc subclasses also supposed version id field versioned protocol return protocol version corresponding protocol get protocol version string protocol client version throws io exception return protocol version corresponding protocol list supported methods default implementation protocol signature get protocol signature string protocol client version client methods hash throws io exception
300	common\src\java\org\apache\hadoop\ipc\WritableRpcEngine.java	unrelated	package org apache hadoop ipc an rpc engine implementation writable data writable rpc engine implements rpc engine log log log factory get log rpc writable rpc version updated change format rpc messages writable rpc version l a method invocation including method name parameters invocation implements writable configurable string method name class parameter classes object parameters configuration conf client version client methods hash this could different writable rpc version received server client using different version rpc version invocation invocation method method object parameters method name method get name parameter classes method get parameter types parameters parameters rpc version writable rpc version method get declaring class equals versioned protocol versioned protocol exempted version check client version client methods hash else try field version field method get declaring class get field version id version field set accessible true client version version field get long method get declaring class catch no such field exception ex throw new runtime exception ex catch illegal access exception ex throw new runtime exception ex client methods hash protocol signature get fingerprint method get declaring class get methods the name method invoked string get method name return method name the parameter class get parameter classes return parameter classes the parameter instances object get parameters return parameters get protocol version return client version get client methods hash return client methods hash returns rpc version used client get rpc version return rpc version read fields data input throws io exception rpc version read long method name utf read string client version read long client methods hash read int parameters new object read int parameter classes new class parameters length object writable object writable new object writable parameters length parameters object writable read object object writable conf parameter classes object writable get declared class write data output throws io exception write long rpc version utf write string method name write long client version write int client methods hash write int parameter classes length parameter classes length object writable write object parameters parameter classes conf true string string string builder buffer new string builder buffer append method name buffer append parameters length buffer append buffer append parameters buffer append buffer append rpc version rpc version buffer append client version client version buffer append methods finger print client methods hash return buffer string set conf configuration conf conf conf configuration get conf return conf client cache clients new client cache invoker implements invocation handler client connection id remote id client client boolean closed false invoker class protocol inet socket address address user group information ticket configuration conf socket factory factory rpc timeout throws io exception remote id client connection id get connection id address protocol ticket rpc timeout conf client clients get client conf factory object invoke object proxy method method object args throws throwable start time log debug enabled start time system current time millis object writable value object writable client call new invocation method args remote id log debug enabled call time system current time millis start time log debug call method get name call time return value get close ipc client responsible invoker
301	common\src\java\org\apache\hadoop\ipc\metrics\package-info.java	unrelated	package org apache hadoop ipc metrics
302	common\src\java\org\apache\hadoop\ipc\metrics\RpcDetailedMetrics.java	unrelated	package org apache hadoop ipc metrics this maintaining rpc method related statistics publishing metrics interfaces rpc detailed metrics log log log factory get log rpc detailed metrics metrics registry registry string name rpc detailed metrics port name rpc detailed activity for port port registry new metrics registry rpcdetailed tag port rpc port string value of port log debug registry info string name return name rpc detailed metrics create port rpc detailed metrics new rpc detailed metrics port return default metrics system instance register name null initialize metrics jmx protocol methods init class protocol rates init protocol add rpc processing time sample add processing time string name processing time rates add name processing time shutdown instrumentation process shutdown
303	common\src\java\org\apache\hadoop\ipc\metrics\RpcMetrics.java	authenticate	package org apache hadoop ipc metrics this maintaining various rpc statistics publishing metrics interfaces rpc metrics log log log factory get log rpc metrics server server metrics registry registry string name rpc metrics server server string port string value of server get listener address get port name rpc activity for port port server server registry new metrics registry rpc tag port rpc port port log debug initialized registry string name return name rpc metrics create server server rpc metrics new rpc metrics server return default metrics system instance register name null mutable counter int rpc authentication failures mutable counter int rpc authentication successes mutable counter int rpc authorization failures mutable counter int rpc authorization successes return server get num open connections return server get call queue len public instrumentation methods could extracted decide custom instrumentation la job tracker instrumenation the methods override comment candidates methods instrumentation one authentication failure event incr authentication failures rpc authentication failures incr one authentication success event incr authentication successes rpc authentication successes incr one authorization success event incr authorization successes rpc authorization successes incr one authorization failure event incr authorization failures rpc authorization failures incr shutdown instrumentation process shutdown increment sent bytes count incr sent bytes count sent bytes incr count increment received bytes count incr received bytes count received bytes incr count add rpc queue time sample add rpc queue time q time rpc queue time add q time add rpc processing time sample add rpc processing time processing time rpc processing time add processing time
304	common\src\java\org\apache\hadoop\jmx\JMXJsonServlet.java	unrelated	package org apache hadoop jmx this servlet based jmx proxy servlet tomcat it rewritten read output json format really close original provides read web access jmx p this servlet generally placed jmx url http server it provides read access jmx metrics the optional code qry code parameter may used query subset jmx beans this query functionality provided link m bean server query names object name javax management query exp method p for example code http jmx qry hadoop code return hadoop metrics exposed jmx p the optional code get code parameter used query specific attribute jmx bean the format url code http jmx get mx bean name attribute name code p for example code http jmx get hadoop service name node name name node info cluster id code return cluster id namenode mxbean p if code qry code code get code parameter formatted correctly bad request http response code returned p if resouce mbean attribute found sc not found http response code returned p the return format json form p code pre beans name bean name pre code p the servlet attempts convert jmx beans json each bean attributes converted json object member if attribute boolean number array converted json equivalent if value link composite data converted json object keys name json member value converted following rules if value link tabular data converted array link composite data elements contains all objects converted output the bean name modeler type returned beans jmx json servlet extends http servlet log log log factory get log jmx json servlet serial version uid l instance variables m bean server protected transient m bean server bean server null public methods initialize servlet init throws servlet exception retrieve m bean server bean server management factory get platform m bean server process get request specified resource the servlet request processing the servlet response creating get http servlet request request http servlet response response try do authorization http server administrator access get servlet context request response return response set content type application json charset utf print writer writer response get writer json factory json factory new json factory json generator jg json factory create json generator writer jg use default pretty printer jg write start object bean server null jg write string field result error jg write string field message no m bean server could found jg close log error no m bean server could found response set status http servlet response sc not found return query per mbean attribute string getmethod request get parameter get getmethod null string split strings getmethod split split strings length jg write string field result error jg write string field message query format expected jg close response set status http servlet response sc bad request return list beans jg new object name split strings split strings response jg close return query per mbean string qry request get parameter qry qry null qry list beans jg new object name qry null response jg close catch io exception e log error caught exception processing jmx request e response set status http servlet response sc internal server error
305	common\src\java\org\apache\hadoop\jmx\package-info.java	unrelated	package org apache hadoop jmx
306	common\src\java\org\apache\hadoop\log\EventCounter.java	unrelated	package org apache hadoop log a log j appender simply counts logging events three levels fatal error warn the name used log j properties event counter extends org apache hadoop log metrics event counter the logging system started yet system err println warning event counter get name deprecated please use org apache hadoop log metrics event counter get name log j properties files
307	common\src\java\org\apache\hadoop\log\LogLevel.java	unrelated	package org apache hadoop log change log level runtime log level string usages n usages n java log level get name getlevel host port name n java log level get name setlevel host port name level n a command line implementation main string args args length getlevel equals args process http args log level log args return else args length setlevel equals args process http args log level log args level args return system err println usages system exit process string urlstring try url url new url urlstring system println connecting url url connection connection url open connection connection connect buffered reader new buffered reader new input stream reader connection get input stream string line line read line null line starts with marker system println tag matcher line replace all close catch io exception ioe system err println ioe string marker output pattern tag pattern compile a servlet implementation servlet extends http servlet serial version uid l get http servlet request request http servlet response response throws servlet exception io exception do authorization http server administrator access get servlet context request response return print writer servlet util init html response log level string log name servlet util get parameter request log string level servlet util get parameter request level log name null println br hr results println marker submitted log name b log name b br log log log factory get log log name println marker log class b log get class get name b br level null println marker submitted level b level b br log instanceof log j logger process log j logger log get logger level else log instanceof jdk logger process jdk logger log get logger level else println sorry log get class supported br println forms println servlet util html tail string forms n br hr get set n form log input type text size name log input type submit value get log level form n form log input type text size name log level input type text name level input type submit value set log level form process org apache log j logger log string level print writer throws io exception level null level equals org apache log j level level level string println marker bad level b level b br else log set level org apache log j level level level println marker setting level level br println marker effective level b log get effective level b br process java util logging logger log string level print writer throws io exception level null log set level java util logging level parse level println marker setting level level br java util logging level lev lev log get level null log log get parent println marker effective level b lev b br
308	common\src\java\org\apache\hadoop\log\metrics\EventCounter.java	unrelated	package org apache hadoop log metrics a log j appender simply counts logging events three levels fatal error warn the name used log j properties event counter extends appender skeleton fatal error warn info event counts counts synchronized incr counts synchronized get return counts event counts counts new event counts get fatal return counts get fatal get error return counts get error get warn return counts get warn get info return counts get info append logging event event level level event get level depends api might work see hadoop details level equals level info counts incr info else level equals level warn counts incr warn else level equals level error counts incr error else level equals level fatal counts incr fatal close boolean requires layout return false
309	common\src\java\org\apache\hadoop\metrics\ContextFactory.java	unrelated	package org apache hadoop metrics factory creating metrics context objects to obtain instance use code get factory code method context factory string properties file hadoop metrics properties string context class suffix string default context classname org apache hadoop metrics spi null context context factory factory null map string object attribute map new hash map string object map string metrics context context map new hash map string metrics context used contexts context factory cannot created map string metrics context null context map new hash map string metrics context creates new instance context factory protected context factory returns value named attribute null attribute name object get attribute string attribute name return attribute map get attribute name returns names factory attributes string get attribute names string result new string attribute map size string attribute name attribute map key set iterator attribute map key set iterator next result string next return result sets named factory attribute specified value creating already exist if value null calling remove attribute set attribute string attribute name object value attribute map put attribute name value removes named attribute exists remove attribute string attribute name attribute map remove attribute name returns named metrics context instance constructing necessary using factory current configuration attributes p when constructing instance factory property context name code exists value taken name instantiate otherwise default create instance code org apache hadoop metrics spi null context code dummy op context cause metric data discarded synchronized metrics context get context string ref name string context name throws io exception class not found exception instantiation exception illegal access exception metrics context metrics context context map get ref name metrics context null string name attribute ref name context class suffix string name string get attribute name attribute name null name default context classname class context class class name name metrics context metrics context context class new instance metrics context init context name context map put context name metrics context return metrics context synchronized metrics context get context string context name throws io exception class not found exception instantiation exception illegal access exception return get context context name context name returns metrics contexts built factory synchronized collection metrics context get all contexts make copy avoid race conditions creating new contexts return new array list metrics context context map values returns null context one nothing synchronized metrics context get null context string context name metrics context null context null context map get context name null context null null context new null context null context map put context name null context return null context returns singleton context factory instance constructing necessary p when instance constructed method checks file code hadoop metrics properties code exists path if exists must format defined java util properties properties file set attributes newly created context factory instance synchronized context factory get factory throws io exception factory null factory new context factory factory set attributes return factory set attributes throws io exception input stream get class get resource as stream properties file null try properties properties new properties properties load object property name obj properties key set iterator properties key set iterator
310	common\src\java\org\apache\hadoop\metrics\MetricsContext.java	unrelated	package org apache hadoop metrics the main metrics package metrics context default period seconds data sent metrics system default period initialize context init string context name context factory factory returns context name string get context name starts restarts monitoring emitting metrics records updated start monitoring throws io exception stops monitoring this free data implementation may buffered sending next timer event it ok call code start monitoring code calling stop monitoring returns true monitoring currently progress boolean monitoring stops monitoring also frees buffered data returning object initial state close creates new metrics record instance given code record name code throws exception metrics implementation configured fixed set record names code record name code set metrics record create record string record name registers callback called regular time intervals determined implementation specific configuration metrics records return register updater updater updater removes callback exists unregister updater updater updater returns timer period get period retrieves records managed metrics context useful monitoring systems polling based map string collection output record get all records
311	common\src\java\org\apache\hadoop\metrics\MetricsException.java	unrelated	package org apache hadoop metrics general purpose unchecked metrics exception metrics exception extends runtime exception serial version uid l creates new instance metrics exception metrics exception creates new instance metrics exception metrics exception string message super message
312	common\src\java\org\apache\hadoop\metrics\MetricsRecord.java	unrelated	package org apache hadoop metrics a named optionally tagged set records sent metrics system p a record name identifies kind data reported for example program reporting statistics relating disks computer might use record name disk stats p a record zero tags a tag name value to continue example disk stats record might use tag named disk name identify particular disk sometimes useful one tag might also disk type value ide scsi whatever p a record also zero metrics these named values reported metrics system in disk stats example possible metric names would disk percent full disk percent busy kb read per second etc p the general procedure using metrics record fill tag metric values call code update code pass record client library metric data immediately sent metrics system time code update code called an internal table maintained identified record name this table columns corresponding tag metric names rows corresponding unique set tag values an update either modifies existing row table adds new row set tag values different rows note tags one row table p once row added table data sent metrics system every timer period whether updated since previous timer period if inappropriate example metrics reported transient object application code remove code method used remove row thus stop data sent p note code update code method atomic this means safe different threads updating metric more precisely ok different threads call code update code metrics record instances set tag names tag values different threads b b use metrics record instance time metrics record returns record name string get record name sets named tag specified value the tag value may null treated empty string set tag string tag name string tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name tag value sets named tag specified value set tag string tag name short tag value sets named tag specified value set tag string tag name byte tag value removes tag specified name remove tag string tag name sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name metric value sets named metric specified value conflicts configuration set metric string metric name short metric value sets named metric specified value conflicts configuration set metric string metric name byte metric value sets named metric specified value conflicts configuration set metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name metric value increments named metric specified value conflicts configuration incr metric string metric name short metric value increments named metric specified value conflicts configuration incr metric string metric name byte metric value increments named metric specified value conflicts configuration incr metric string metric name metric value updates table buffered data sent periodically if tag values match existing row row updated otherwise new row added update removes buffered data table rows tags equal tags set record
313	common\src\java\org\apache\hadoop\metrics\MetricsServlet.java	unrelated	package org apache hadoop metrics a servlet print metrics data by default servlet returns textual representation promises made parseability users use format json parseable output metrics servlet extends http servlet a helper hold tag map metric map tags metrics pair implements json convertible tag map tag map metric map metric map tags metrics pair tag map tag map metric map metric map tag map tag map metric map metric map json map map throw new unsupported operation exception converts json providing array json output add new object tag map metric map collects metric data returns map context name record name tag tag value metric metric value the values either string number the value implemented list tags metrics pair map string map string list tags metrics pair make map collection metrics context contexts throws io exception map string map string list tags metrics pair map new tree map string map string list tags metrics pair metrics context context contexts map string list tags metrics pair records new tree map string list tags metrics pair map put context get context name records map entry string collection output record r context get all records entry set list tags metrics pair metrics and tags new array list tags metrics pair records put r get key metrics and tags output record output record r get value tag map tag map output record get tags copy metric map metric map output record get metrics copy metrics and tags add new tags metrics pair tag map metric map return map get http servlet request request http servlet response response throws servlet exception io exception do authorization http server administrator access get servlet context request response return string format request get parameter format collection metrics context contexts context factory get factory get all contexts json equals format response set content type application json charset utf print writer response get writer try uses jetty built json support convert map json print new json json make map contexts finally close else print writer response get writer try print map make map contexts finally close prints metrics data multi line text form print map print writer map string map string list tags metrics pair map map entry string map string list tags metrics pair context map entry set println context get key map entry string list tags metrics pair record context get value entry set indent println record get key tags metrics pair pair record get value indent prints tag values form key value key value print boolean first true map entry string object tag value pair tag map entry set first first false else print print tag value get key print print tag value get value string println now print metric values one per line map entry string number metric value pair metric map entry set indent print metric value get key print println metric value get value string indent print writer indent indent append
314	common\src\java\org\apache\hadoop\metrics\MetricsUtil.java	unrelated	package org apache hadoop metrics utility simplify creation reporting hadoop metrics for examples usage see name node metrics metrics util log log log factory get log metrics util don allow creation new instance metrics metrics util metrics context get context string context name return get context context name context name utility method return named context if desired context cannot created reason exception logged null context returned metrics context get context string ref name string context name metrics context metrics context try metrics context context factory get factory get context ref name context name metrics context monitoring metrics context start monitoring catch exception ex log error unable create metrics context context name ex metrics context context factory get null context context name return metrics context utility method create return new metrics record instance within given context this record tagged host name metrics record create record metrics context context string record name metrics record metrics record context create record record name metrics record set tag host name get host name return metrics record returns host name if host name unobtainable logs exception returns unknown string get host name string host name null try host name inet address get local host get host name catch unknown host exception ex log info unable obtain host name ex host name unknown return host name
315	common\src\java\org\apache\hadoop\metrics\Updater.java	unrelated	package org apache hadoop metrics call back see code metrics context register updater code updater timer based call back metric library updates metrics context context
316	common\src\java\org\apache\hadoop\metrics\file\FileContext.java	unrelated	package org apache hadoop metrics file metrics context writing metrics file p this configured setting context factory attributes turn usually configured properties file all attributes prefixed context name for example properties file might contain pre context name file name tmp metrics log context name period pre file context extends abstract metrics context configuration attribute names protected string file name property file name protected string period property period file file null file metrics written print writer writer null creates new instance file context file context init string context name context factory factory super init context name factory string file name get attribute file name property file name null file new file file name parse and set period period property returns configured file name null string get file name file null return null else return file get name starts restarts monitoring opening append mode file specified code file name code attribute specified otherwise data written standard output start monitoring throws io exception file null writer new print writer new buffered output stream system else writer new print writer new file writer file true super start monitoring stops monitoring closing file stop monitoring super stop monitoring writer null writer close writer null emits metrics record file emit record string context name string record name output record rec writer print context name writer print writer print record name string separator string tag name rec get tag names writer print separator separator writer print tag name writer print writer print rec get tag tag name string metric name rec get metric names writer print separator separator writer print metric name writer print writer print rec get metric metric name writer println flushes output writer forcing updates disk flush writer flush
317	common\src\java\org\apache\hadoop\metrics\ganglia\GangliaContext.java	unrelated	package org apache hadoop metrics ganglia context sending metrics ganglia ganglia context extends abstract metrics context string period property period string servers property servers string units property units string slope property slope string tmax property tmax string dmax property dmax string default units string default slope default tmax default dmax default port buffer size per libgmond c log log log factory get log get class map class string type table new hash map class string type table put string type table put byte type table put short type table put integer type table put long type table put float protected byte buffer new byte buffer size protected offset protected list extends socket address metrics servers map string string units table map string string slope table map string string tmax table map string string dmax table protected datagram socket datagram socket creates new instance ganglia context ganglia context init string context name context factory factory super init context name factory parse and set period period property metrics servers util parse get attribute servers property default port units table get attribute table units property slope table get attribute table slope property tmax table get attribute table tmax property dmax table get attribute table dmax property try datagram socket new datagram socket catch socket exception se se print stack trace method close datagram socket close super close datagram socket null datagram socket close emit record string context name string record name output record rec throws io exception setup records proper leader names unambiguous ganglia level prevents lot rework string builder sb new string builder sb append context name sb append sb append record name sb append sb base len sb length emit metric turn string metric name rec get metric names object metric rec get metric metric name string type type table get metric get class type null sb append metric name emit metric sb string type metric string sb set length sb base len else log warn unknown metrics type metric get class protected emit metric string name string type string value throws io exception string units get units name slope get slope name tmax get tmax name dmax get dmax name offset xdr metric user defined xdr type xdr name xdr value xdr units xdr slope xdr tmax xdr dmax socket address socket address metrics servers datagram packet packet new datagram packet buffer offset socket address datagram socket send packet protected string get units string metric name string result units table get metric name result null result default units return result protected get slope string metric name string slope string slope table get metric name slope string null slope string default slope return zero equals slope string see gmetric c protected get tmax string metric name tmax table null return default tmax string tmax string tmax table get metric name tmax string null return default tmax else return integer parse int tmax string protected get dmax string metric name string dmax string dmax table get metric name dmax string null return default dmax else return integer parse int dmax string puts buffer first
318	common\src\java\org\apache\hadoop\metrics\ganglia\GangliaContext31.java	unrelated	package org apache hadoop metrics ganglia context sending metrics ganglia version x slightly different wire portal compared x ganglia context extends ganglia context string host name unknown example com log log log factory get log org apache hadoop util ganglia context init string context name context factory factory super init context name factory log debug initializing ganglia context ganglia metrics take hostname dns configuration conf new configuration conf get slave host name null host name conf get slave host name else try host name dns get default host conf get dfs datanode dns default conf get dfs datanode dns nameserver default catch unknown host exception uhe log error uhe host name unknown example com protected emit metric string name string type string value throws io exception name null log warn metric emitted name return else value null log warn metric name name emitted null value return else type null log warn metric name name value value type return log debug emitting metric name type type value value hostname host name string units get units name units null log warn metric name name value value null units units slope get slope name tmax get tmax name dmax get dmax name offset string group name name substring name last index of the following xdr recipe done careful reading gm protocol x ganglia carefully examining output gmetric utility strace first send metadata message xdr metric id metadata msg xdr host name hostname xdr name metric name xdr spoof false xdr type metric type xdr name metric name xdr units units xdr slope slope xdr tmax tmax maximum time metrics xdr dmax dmax maximum data value xdr num entries extra value field ganglia x xdr group group attribute xdr group name group value socket address socket address metrics servers datagram packet packet new datagram packet buffer offset socket address datagram socket send packet now send message actual value technically need send metadata message metric i want record metrics send offset xdr sending value xdr host name host name xdr name metric name xdr spoof false xdr format field xdr value metric value socket address socket address metrics servers datagram packet packet new datagram packet buffer offset socket address datagram socket send packet
319	common\src\java\org\apache\hadoop\metrics\jvm\EventCounter.java	unrelated	package org apache hadoop metrics jvm a log j appender simply counts logging events three levels fatal error warn event counter extends org apache hadoop log metrics event counter the logging system started yet system err println warning event counter get name deprecated please use org apache hadoop log metrics event counter get name log j properties files
320	common\src\java\org\apache\hadoop\metrics\jvm\JvmMetrics.java	unrelated	package org apache hadoop metrics jvm singleton reports java virtual machine metrics metrics api any application create instance order emit java vm metrics jvm metrics implements updater m jvm metrics instance null log log log factory get log jvm metrics metrics record metrics garbage collection counters gc count gc time millis logging event counters fatal count error count warn count info count synchronized jvm metrics init string process name string session id return init process name session id metrics synchronized jvm metrics init string process name string session id string record name instance null log info cannot initialize jvm metrics process name process name session id session id already initialized else log info initializing jvm metrics process name process name session id session id instance new jvm metrics process name session id record name return instance creates new instance jvm metrics jvm metrics string process name string session id string record name metrics context context metrics util get context jvm metrics metrics util create record context record name metrics set tag process name process name metrics set tag session id session id context register updater this called periodically period configuration dependent updates metrics context context memory updates garbage collection updates thread updates event count updates metrics update memory updates memory mx bean memory mx bean management factory get memory mx bean memory usage mem non heap memory mx bean get non heap memory usage memory usage mem heap memory mx bean get heap memory usage runtime runtime runtime get runtime metrics set metric mem non heap used m mem non heap get used m metrics set metric mem non heap committed m mem non heap get committed m metrics set metric mem heap used m mem heap get used m metrics set metric mem heap committed m mem heap get committed m metrics set metric max memory m runtime max memory m garbage collection updates list garbage collector mx bean gc beans management factory get garbage collector mx beans count time millis garbage collector mx bean gc bean gc beans count gc bean get collection count time millis gc bean get collection time metrics incr metric gc count count gc count metrics incr metric gc time millis time millis gc time millis gc count count gc time millis time millis thread updates thread mx bean thread mx bean management factory get thread mx bean thread ids thread mx bean get all thread ids thread info thread infos thread mx bean get thread info thread ids threads new threads runnable threads blocked threads waiting threads timed waiting threads terminated thread info thread info thread infos thread info null thread alive exist thread info null continue thread state state thread info get thread state state new threads new else state runnable threads runnable else state blocked threads blocked else state waiting threads waiting else state timed waiting threads timed waiting else state terminated threads terminated metrics set metric threads new threads new metrics set metric threads runnable threads runnable metrics set metric threads blocked threads blocked metrics set metric threads waiting threads waiting metrics
321	common\src\java\org\apache\hadoop\metrics\jvm\package-info.java	unrelated	package org apache hadoop metrics jvm
322	common\src\java\org\apache\hadoop\metrics\spi\AbstractMetricsContext.java	unrelated	package org apache hadoop metrics spi the main service provider interface this extended order integrate metrics api specific metrics client library p this implements internal table metric data timer data sent metrics system subclasses must code emit record code method order transmit data p abstract metrics context implements metrics context period metrics context default period timer timer null set updater updaters new hash set updater volatile boolean monitoring false context factory factory null string context name null tag map extends tree map string object serial version uid l tag map super tag map tag map orig super orig returns true tagmap contains every tag boolean contains all tag map map entry string object entry entry set object value get entry get key value null value equals entry get value either key exist value different return false return true metric map extends tree map string number serial version uid l metric map super metric map metric map orig super orig record map extends hash map tag map metric map serial version uid l map string record map buffered data new hash map string record map creates new instance abstract metrics context protected abstract metrics context initializes context init string context name context factory factory context name context name factory factory convenience method subclasses access factory attributes protected string get attribute string attribute name string factory attribute context name attribute name return string factory get attribute factory attribute returns attribute value map derived factory attributes finding factory attributes begin context name table name the returned map consists attributes context name table name stripped protected map string string get attribute table string table name string prefix context name table name map string string result new hash map string string string attribute name factory get attribute names attribute name starts with prefix string name attribute name substring prefix length string value string factory get attribute attribute name result put name value return result returns context name string get context name return context name returns factory context created context factory get context factory return factory starts restarts monitoring emitting metrics records synchronized start monitoring throws io exception monitoring start timer monitoring true stops monitoring this free buffered data synchronized stop monitoring monitoring stop timer monitoring false returns true monitoring currently progress boolean monitoring return monitoring stops monitoring frees buffered data returning object initial state synchronized close stop monitoring clear updaters creates new abstract metrics record instance given code record name code throws exception metrics implementation configured fixed set record names code record name code set synchronized metrics record create record string record name buffered data get record name null buffered data put record name new record map return new record record name subclasses subclass metrics record impl protected metrics record new record string record name return new metrics record impl record name registers callback called time intervals determined configuration metrics records synchronized register updater updater updater updaters contains updater updaters add updater removes callback exists synchronized unregister updater updater updater updaters remove updater synchronized clear updaters updaters clear starts timer already started synchronized start timer timer null
323	common\src\java\org\apache\hadoop\metrics\spi\CompositeContext.java	unrelated	package org apache hadoop metrics spi composite context extends abstract metrics context log log log factory get log composite context string arity label arity string sub fmt sub array list metrics context subctxt new array list metrics context composite context init string context name context factory factory super init context name factory n kids try string kids get attribute arity label n kids integer value of kids catch exception e log error unable initialize composite metric context name could init arity e return n kids metrics context ctxt metrics util get context string format sub fmt context name context name null ctxt subctxt add ctxt metrics record new record string record name return metrics record proxy new proxy instance metrics record get class loader new class metrics record new metrics record delegator record name subctxt protected emit record string context name string record name output record rec throws io exception metrics context ctxt subctxt try abstract metrics context ctxt emit record context name record name rec context name null record name null rec null throw new io exception context name record name rec catch io exception e log warn emit record failed ctxt get context name e protected flush throws io exception metrics context ctxt subctxt try abstract metrics context ctxt flush catch io exception e log warn flush failed ctxt get context name e start monitoring throws io exception metrics context ctxt subctxt try ctxt start monitoring catch io exception e log warn start monitoring failed ctxt get context name e stop monitoring metrics context ctxt subctxt ctxt stop monitoring return true subcontexts monitoring boolean monitoring boolean ret true metrics context ctxt subctxt ret ctxt monitoring return ret close metrics context ctxt subctxt ctxt close register updater updater updater metrics context ctxt subctxt ctxt register updater updater unregister updater updater updater metrics context ctxt subctxt ctxt unregister updater updater metrics record delegator implements invocation handler method get record name init method method init method try return metrics record get method get record name new class catch exception e throw new runtime exception internal error e string record name array list metrics record subrecs metrics record delegator string record name array list metrics context ctxts record name record name subrecs new array list metrics record ctxts size metrics context ctxt ctxts subrecs add ctxt create record record name object invoke object p method object args throws throwable get record name equals return record name assert void type equals get return type metrics record rec subrecs invoke rec args return null
324	common\src\java\org\apache\hadoop\metrics\spi\MetricsRecordImpl.java	unrelated	package org apache hadoop metrics spi an implementation metrics record keeps back pointer context created delegates back code update code code remove code metrics record impl implements metrics record tag map tag table new tag map map string metric value metric table new linked hash map string metric value string record name abstract metrics context context creates new instance file record protected metrics record impl string record name abstract metrics context context record name record name context context returns record name string get record name return record name sets named tag specified value set tag string tag name string tag value tag value null tag value tag table put tag name tag value sets named tag specified value set tag string tag name tag value tag table put tag name integer value of tag value sets named tag specified value set tag string tag name tag value tag table put tag name long value of tag value sets named tag specified value set tag string tag name short tag value tag table put tag name short value of tag value sets named tag specified value set tag string tag name byte tag value tag table put tag name byte value of tag value removes tag specified name remove tag string tag name tag table remove tag name sets named metric specified value conflicts configuration set metric string metric name metric value set absolute metric name integer value of metric value sets named metric specified value conflicts configuration set metric string metric name metric value set absolute metric name long value of metric value sets named metric specified value conflicts configuration set metric string metric name short metric value set absolute metric name short value of metric value sets named metric specified value conflicts configuration set metric string metric name byte metric value set absolute metric name byte value of metric value sets named metric specified value conflicts configuration set metric string metric name metric value set absolute metric name new float metric value increments named metric specified value conflicts configuration incr metric string metric name metric value set increment metric name integer value of metric value increments named metric specified value conflicts configuration incr metric string metric name metric value set increment metric name long value of metric value increments named metric specified value conflicts configuration incr metric string metric name short metric value set increment metric name short value of metric value increments named metric specified value conflicts configuration incr metric string metric name byte metric value set increment metric name byte value of metric value increments named metric specified value conflicts configuration incr metric string metric name metric value set increment metric name new float metric value set absolute string metric name number metric value metric table put metric name new metric value metric value metric value absolute set increment string metric name number metric value metric table put metric name new metric value metric value metric value increment updates table buffered data sent periodically if tag values match existing row row updated otherwise new row added update context update
325	common\src\java\org\apache\hadoop\metrics\spi\MetricValue.java	unrelated	package org apache hadoop metrics spi a number either absolute incremental amount metric value boolean absolute false boolean increment true boolean increment number number creates new instance metric value metric value number number boolean increment number number increment increment boolean increment return increment boolean absolute return increment number get number return number
326	common\src\java\org\apache\hadoop\metrics\spi\NoEmitMetricsContext.java	unrelated	package org apache hadoop metrics spi a metrics context emit data unlike null context with update save retrieval get all records this useful want support link metrics servlet emit metrics way no emit metrics context extends abstract metrics context string period property period creates new instance null context with update thread no emit metrics context init string context name context factory factory super init context name factory parse and set period period property do nothing version emit record protected emit record string context name string record name output record rec
327	common\src\java\org\apache\hadoop\metrics\spi\NullContext.java	unrelated	package org apache hadoop metrics spi null metrics context metrics context nothing used default context performance data emitted configuration data found null context extends abstract metrics context creates new instance null context null context do nothing version start monitoring start monitoring do nothing version emit record protected emit record string context name string record name output record rec do nothing version update protected update metrics record impl record do nothing version remove protected remove metrics record impl record
328	common\src\java\org\apache\hadoop\metrics\spi\NullContextWithUpdateThread.java	unrelated	package org apache hadoop metrics spi a null context thread calling periodically monitoring started this keeps data sampled correctly in respects like null context no data emitted this suitable monitoring systems like jmx reads metrics someone reads data jmx the default impl start stop monitoring abstract metrics context good enough null context with update thread extends abstract metrics context string period property period creates new instance null context with update thread null context with update thread init string context name context factory factory super init context name factory parse and set period period property do nothing version emit record protected emit record string context name string record name output record rec do nothing version update protected update metrics record impl record do nothing version remove protected remove metrics record impl record
329	common\src\java\org\apache\hadoop\metrics\spi\OutputRecord.java	unrelated	package org apache hadoop metrics spi represents record metric data sent metrics system output record tag map tag map metric map metric map creates new instance output record output record tag map tag map metric map metric map tag map tag map metric map metric map returns set tag names set string get tag names return collections unmodifiable set tag map key set returns tag object string integer short byte object get tag string name return tag map get name returns set metric names set string get metric names return collections unmodifiable set metric map key set returns metric object float integer short byte number get metric string name return metric map get name returns copy record tags tag map get tags copy return new tag map tag map returns copy record metrics metric map get metrics copy return new metric map metric map
330	common\src\java\org\apache\hadoop\metrics\spi\Util.java	unrelated	package org apache hadoop metrics spi static utility methods util this intended instantiated util parses space comma separated sequence server specifications form hostname hostname port if specs null defaults localhost default port list inet socket address parse string specs default port list inet socket address result new array list inet socket address specs null result add new inet socket address localhost default port else string spec strings specs split string spec string spec strings colon spec string index of colon colon spec string length result add new inet socket address spec string default port else string hostname spec string substring colon port integer parse int spec string substring colon result add new inet socket address hostname port return result
331	common\src\java\org\apache\hadoop\metrics\util\MBeanUtil.java	unrelated	package org apache hadoop metrics util this util provides method register m bean using standard naming convention described doc link link register m bean string string object m bean util register m bean using standard m bean name format hadoop service service name name name name where service name name name supplied parameters object name register m bean string service name string name name object mbean m bean server mbs management factory get platform m bean server object name name get m bean name service name name name try mbs register m bean mbean name return name catch instance already exists exception ie ignore instance already exists catch exception e e print stack trace return null unregister m bean object name mbean name m bean server mbs management factory get platform m bean server mbean name null return try mbs unregister m bean mbean name catch instance not found exception e ignore catch exception e e print stack trace object name get m bean name string service name string name name object name name null try name new object name hadoop service service name name name name catch malformed object name exception e e print stack trace return name
332	common\src\java\org\apache\hadoop\metrics\util\MetricsBase.java	unrelated	package org apache hadoop metrics util this base metrics metrics base string no description no description string name string description protected metrics base string nam name nam description no description protected metrics base string nam string desc name nam description desc push metric metrics record mr string get name return name string get description return description
333	common\src\java\org\apache\hadoop\metrics\util\MetricsDynamicMBeanBase.java	unrelated	package org apache hadoop metrics util this base facilitates creating dynamic mbeans automatically metrics the metrics constructors registers metrics registry different categories metrics differnt registry name node metrics data node metrics then m bean created passing registry constructor the m bean registered using mbean name example metrics holder metrics new metrics holder metrics registry metrics test m bean m bean new metrics test m bean metrics mregistry object name mbean name m bean util register m bean service foo test statistics m bean metrics dynamic m bean base implements dynamic m bean string avg time avg time string min time min time string max time max time string num ops num ops string reset all min max op reset all min max metrics registry metrics registry m bean info mbean info map string metrics base metrics rate attribute mod num entries in registry string mbean description protected metrics dynamic m bean base metrics registry mr string m bean description metrics registry mr mbean description m bean description create m bean info update mbean info if metrics list changed num entries in registry metrics registry size create m bean info create m bean info metrics rate attribute mod new hash map string metrics base boolean needs min max reset operation false list m bean attribute info attributes info new array list m bean attribute info m bean operation info operations info null num entries in registry metrics registry size metrics base metrics registry get metrics list metrics time varying rate instance for metrics different attributes attributes info add new m bean attribute info get name num ops java lang integer get description true false false attributes info add new m bean attribute info get name avg time java lang long get description true false false attributes info add new m bean attribute info get name min time java lang long get description true false false attributes info add new m bean attribute info get name max time java lang long get description true false false needs min max reset operation true min max reset note special attributes avg time min time derived metrics rather check suffix store map metrics rate attribute mod put get name num ops metrics rate attribute mod put get name avg time metrics rate attribute mod put get name min time metrics rate attribute mod put get name max time else metrics int value instance metrics time varying int instance attributes info add new m bean attribute info get name java lang integer get description true false false else metrics long value instance metrics time varying long instance attributes info add new m bean attribute info get name java lang long get description true false false else metrics util log error unknown metrics type get class get name needs min max reset operation operations info new m bean operation info new m bean operation info reset all min max op reset zero all min max null m bean operation info action m bean attribute info attr array new m bean attribute info attributes info size mbean info new m
334	common\src\java\org\apache\hadoop\metrics\util\MetricsIntValue.java	unrelated	package org apache hadoop metrics util the metrics int value metric time varied changes set each time value set published next update call metrics int value extends metrics base log log log factory get log org apache hadoop metrics util value boolean changed constructor create new metric metrics int value string nam metrics registry registry string description super nam description value changed false registry add nam constructor create new metric a description link no description used metrics int value string nam metrics registry registry nam registry no description set value synchronized set new value value new value changed true get value synchronized get return value push metric mr the metric pushed updated since last push note not push jmx jmx gets info via link get synchronized push metric metrics record mr changed try mr set metric get name value catch exception e log info push metric failed get name n e changed false
335	common\src\java\org\apache\hadoop\metrics\util\MetricsLongValue.java	unrelated	package org apache hadoop metrics util the metrics long value metric time varied changes set each time value set published next update call metrics long value extends metrics base value boolean changed constructor create new metric metrics long value string nam metrics registry registry string description super nam description value changed false registry add nam constructor create new metric a description link no description used metrics long value string nam metrics registry registry nam registry no description set value synchronized set new value value new value changed true get value synchronized get return value push metric mr the metric pushed updated since last push note not push jmx jmx gets info via link get synchronized push metric metrics record mr changed mr set metric get name value changed false
336	common\src\java\org\apache\hadoop\metrics\util\MetricsRegistry.java	unrelated	package org apache hadoop metrics util this registry metrics related set metrics declared holding registered registry metrics also stored holding metrics registry map string metrics base metrics list new hash map string metrics base metrics registry size return metrics list size add new metrics registry synchronized add string metrics name metrics base metrics obj metrics list contains key metrics name throw new illegal argument exception duplicate metrics name metrics name metrics list put metrics name metrics obj returns null none registered synchronized metrics base get string metrics name return metrics list get metrics name synchronized collection string get key list return metrics list key set synchronized collection metrics base get metrics list return metrics list values
337	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingInt.java	unrelated	package org apache hadoop metrics util the metrics time varying int metric naturally varies time e g number files created the metrics accumulated interval set metrics config file metrics published end interval reset zero hence counter value current interval note one wants time associated metric use metrics time varying int extends metrics base log log log factory get log org apache hadoop metrics util current value previous interval value constructor create new metric metrics time varying int string nam metrics registry registry string description super nam description current value previous interval value registry add nam constructor create new metric a description link no description used metrics time varying int string nam metrics registry registry nam registry no description inc metrics incr vlaue synchronized inc incr current value incr inc metrics one synchronized inc current value synchronized interval heart beat previous interval value current value current value push delta metrics mr the delta since last push interval note not push jmx jmx gets info via link previous interval value synchronized push metric metrics record mr interval heart beat try mr incr metric get name get previous interval value catch exception e log info push metric failed get name n e the value previous interval synchronized get previous interval value return previous interval value the value current interval synchronized get current interval value return current value
338	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingLong.java	unrelated	package org apache hadoop metrics util licensed apache software foundation asf one contributor license agreements see notice file distributed work additional information regarding copyright ownership the asf licenses file apache license version license may use file except compliance license you may obtain copy license http www apache org licenses license unless required applicable law agreed writing software distributed license distributed as is basis without warranties or conditions of any kind either express implied see license specific language governing permissions limitations license the metrics time varying long metric naturally varies time e g number files created the metrics accumulated interval set metrics config file metrics published end interval reset zero hence counter value current interval note one wants time associated metric use metrics time varying long extends metrics base log log log factory get log org apache hadoop metrics util current value previous interval value constructor create new metric metrics time varying long string nam metrics registry registry string description super nam description current value previous interval value registry add nam constructor create new metric a description link no description used metrics time varying long string nam metrics registry registry nam registry no description inc metrics incr vlaue synchronized inc incr current value incr inc metrics one synchronized inc current value synchronized interval heart beat previous interval value current value current value push delta metrics mr the delta since last push interval note not push jmx jmx gets info via link previous interval value synchronized push metric metrics record mr interval heart beat try mr incr metric get name get previous interval value catch exception e log info push metric failed get name n e the value previous interval synchronized get previous interval value return previous interval value the value current interval synchronized get current interval value return current value
339	common\src\java\org\apache\hadoop\metrics\util\MetricsTimeVaryingRate.java	unrelated	package org apache hadoop metrics util the metrics time varying rate rate based metric naturally varies time e g time taken create file the rate averaged interval heart beat interval set metrics config file this also keeps track min max rates along method reset min max metrics time varying rate extends metrics base log log log factory get log org apache hadoop metrics util metrics num operations time total time average time set metrics reset to num operations reset to num operations time reset to time reset num operations time min max min time max time set min max new val min time new val min time max time new val max time reset min time max time update time update min max min time min time time math min min time time min time math min min time time max time math max max time time metrics current data metrics previous interval data min max min max constructor create new metric metrics time varying rate string nam metrics registry registry string description super nam description current data new metrics previous interval data new metrics min max new min max registry add nam constructor create new metric a description link no description used metrics time varying rate string nam metrics registry registry nam registry no description increment metrics num ops operations synchronized inc num ops time current data num operations num ops current data time time time per ops time num ops min max update time per ops increment metrics one operation synchronized inc time current data num operations current data time time min max update time synchronized interval heart beat previous interval data num operations current data num operations previous interval data time current data num operations current data time current data num operations current data reset push delta metrics mr the delta since last push interval note not push jmx jmx gets info via link get previous interval average time link get previous interval num ops synchronized push metric metrics record mr interval heart beat try mr incr metric get name num ops get previous interval num ops mr set metric get name avg time get previous interval average time catch exception e log info push metric failed get name n e the number operations previous interval synchronized get previous interval num ops return previous interval data num operations the average rate operation previous interval synchronized get previous interval average time return previous interval data time the min time single operation since last reset link reset min max synchronized get min time return min max min time the max time single operation since last reset link reset min max synchronized get max time return min max max time reset min max values synchronized reset min max min max reset
340	common\src\java\org\apache\hadoop\metrics\util\package-info.java	unrelated	package org apache hadoop metrics util
341	common\src\java\org\apache\hadoop\metrics2\AbstractMetric.java	unrelated	package org apache hadoop metrics the immutable metric abstract metric implements metrics info metrics info info construct metric protected abstract metric metrics info info info check not null info metric info return info name return info description protected metrics info info return info get value metric number value get type metric metric type type accept visitor visit metrics visitor visitor obj instanceof abstract metric abstract metric abstract metric obj return objects equal info info objects equal value value return false return objects hash code info value return objects string helper add info info add value value string
342	common\src\java\org\apache\hadoop\metrics2\MetricsCollector.java	unrelated	package org apache hadoop metrics the metrics collector metrics collector add metrics record metrics record builder add record string name add metrics record metrics record builder add record metrics info info
343	common\src\java\org\apache\hadoop\metrics2\MetricsException.java	unrelated	package org apache hadoop metrics a general metrics exception wrapper metrics exception extends runtime exception serial version uid l construct exception message metrics exception string message super message construct exception message cause metrics exception string message throwable cause super message cause construct exception cause metrics exception throwable cause super cause
344	common\src\java\org\apache\hadoop\metrics2\MetricsFilter.java	unrelated	package org apache hadoop metrics the metrics filter metrics filter implements metrics plugin whether accept name boolean accepts string name whether accept tag boolean accepts metrics tag tag whether accept tags boolean accepts iterable metrics tag tags whether accept record boolean accepts metrics record record return accepts record tags
345	common\src\java\org\apache\hadoop\metrics2\MetricsInfo.java	unrelated	package org apache hadoop metrics interface provide immutable meta info metrics metrics info string name string description
346	common\src\java\org\apache\hadoop\metrics2\MetricsPlugin.java	unrelated	package org apache hadoop metrics the plugin metrics framework metrics plugin initialize plugin init subset configuration conf
347	common\src\java\org\apache\hadoop\metrics2\MetricsRecord.java	unrelated	package org apache hadoop metrics an immutable snapshot metrics timestamp metrics record get timestamp metrics timestamp string name string description string context get tags record note returning collection instead iterable need use tags keys hence collection hash code etc maps collection metrics tag tags get metrics record iterable abstract metric metrics
348	common\src\java\org\apache\hadoop\metrics2\MetricsRecordBuilder.java	unrelated	package org apache hadoop metrics the metrics record builder metrics record builder add metrics tag metrics record builder tag metrics info info string value add immutable metrics tag object metrics record builder add metrics tag tag add pre made immutable metric object metrics record builder add abstract metric metric set context tag metrics record builder set context string value add integer metric metrics record builder add counter metrics info info value add metric metrics record builder add counter metrics info info value add integer gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value add gauge metric metrics record builder add gauge metrics info info value metrics collector parent syntactic sugar add multiple records collector one liner metrics collector end record return parent
349	common\src\java\org\apache\hadoop\metrics2\MetricsSink.java	unrelated	package org apache hadoop metrics the metrics sink metrics sink extends metrics plugin put metrics record sink put metrics metrics record record flush buffered metrics flush
350	common\src\java\org\apache\hadoop\metrics2\MetricsSource.java	unrelated	package org apache hadoop metrics the metrics source metrics source get metrics source get metrics metrics collector collector boolean
351	common\src\java\org\apache\hadoop\metrics2\MetricsSystem.java	unrelated	package org apache hadoop metrics the metrics system metrics system implements metrics system mx bean metrics system init string prefix register metrics source annotations source object t t register string name string desc t source register metrics source deriving name description object t t register t source return register null null source metrics source get source string name register metrics sink t extends metrics sink t register string name string desc t sink register callback jmx events register callback callback shutdown metrics system completely usually server shutdown the metrics system mx bean unregistered boolean shutdown the metrics system callback needed proxies callback called start pre start called start post start called stop pre stop called stop post stop convenient implementing callback abstract callback implements callback
352	common\src\java\org\apache\hadoop\metrics2\MetricsSystemMXBean.java	unrelated	package org apache hadoop metrics the jmx metrics system metrics system mx bean start metrics system start stop metrics system stop start metrics m beans start metrics m beans stop metrics m beans note stop metrics system control m bean e stop metrics m beans avoided get config turn config attribute support multiple line values jconsole string current config
353	common\src\java\org\apache\hadoop\metrics2\MetricsTag.java	unrelated	package org apache hadoop metrics immutable tag metrics grouping host queue username etc metrics tag implements metrics info metrics info info string value construct tag name description value metrics tag metrics info info string value info check not null info tag info value value return info name return info description metrics info info return info get value tag string value return value obj instanceof metrics tag metrics tag metrics tag obj return objects equal info info objects equal value value return false return objects hash code info value return objects string helper add info info add value value string
354	common\src\java\org\apache\hadoop\metrics2\MetricsVisitor.java	unrelated	package org apache hadoop metrics a visitor metrics metrics visitor callback integer value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback value gauges gauge metrics info info value callback integer value counters counter metrics info info value callback value counters counter metrics info info value
355	common\src\java\org\apache\hadoop\metrics2\MetricType.java	unrelated	package org apache hadoop metrics enum metric type a monotonically increasing metric used calculate throughput counter an arbitrary varying metric gauge
356	common\src\java\org\apache\hadoop\metrics2\package-info.java	unrelated	package org apache hadoop metrics
357	common\src\java\org\apache\hadoop\metrics2\annotation\Metric.java	unrelated	package org apache hadoop metrics annotation annotation single metric metric enum type default counter gauge tag shorthand optional name description string value default string default string sample name default ops string value name default time boolean always default false type type default type default
358	common\src\java\org\apache\hadoop\metrics2\annotation\Metrics.java	unrelated	package org apache hadoop metrics annotation annotation group metrics metrics string name default string default string context
359	common\src\java\org\apache\hadoop\metrics2\annotation\package-info.java	unrelated	package org apache hadoop metrics annotation
360	common\src\java\org\apache\hadoop\metrics2\filter\AbstractPatternFilter.java	unrelated	package org apache hadoop metrics filter base pattern based filters abstract pattern filter extends metrics filter protected string include key protected string exclude key exclude protected string include tags key tags protected string exclude tags key exclude tags pattern pattern pattern exclude pattern map string pattern tag patterns map string pattern exclude tag patterns pattern tag pattern pattern compile w abstract pattern filter tag patterns maps new hash map exclude tag patterns maps new hash map init subset configuration conf string pattern string conf get string include key pattern string null pattern string empty set include pattern compile pattern string pattern string conf get string exclude key pattern string null pattern string empty set exclude pattern compile pattern string string pattern strings conf get string array include tags key pattern strings null pattern strings length string pstr pattern strings matcher matcher tag pattern matcher pstr matcher matches throw new metrics exception illegal tag pattern pstr set include tag pattern matcher group compile matcher group pattern strings conf get string array exclude tags key pattern strings null pattern strings length string pstr pattern strings matcher matcher tag pattern matcher pstr matcher matches throw new metrics exception illegal tag pattern pstr set exclude tag pattern matcher group compile matcher group set include pattern pattern pattern pattern pattern set exclude pattern pattern exclude pattern exclude pattern exclude pattern set include tag pattern string name pattern pattern tag patterns put name pattern set exclude tag pattern string name pattern pattern exclude tag patterns put name pattern boolean accepts metrics tag tag accept whitelisted pattern ipat tag patterns get tag name ipat null ipat matcher tag value matches return true reject blacklisted pattern epat exclude tag patterns get tag name epat null epat matcher tag value matches return false reject match whitelist mode ipat null epat null return false return true boolean accepts iterable metrics tag tags accept tag pattern matches metrics tag tags pattern pat tag patterns get name pat null pat matcher value matches return true reject exclude tag pattern matches metrics tag tags pattern pat exclude tag patterns get name pat null pat matcher value matches return false reject match whitelist mode tag patterns empty exclude tag patterns empty return false return true boolean accepts string name accept whitelisted pattern null pattern matcher name matches return true reject blacklisted exclude pattern null exclude pattern matcher name matches return false reject match whitelist mode pattern null exclude pattern null return false return true compile pattern pattern object protected pattern compile string
361	common\src\java\org\apache\hadoop\metrics2\filter\GlobFilter.java	unrelated	package org apache hadoop metrics filter a glob pattern filter metrics the name used metrics config files glob filter extends abstract pattern filter protected pattern compile string return glob pattern compile
362	common\src\java\org\apache\hadoop\metrics2\filter\package-info.java	unrelated	package org apache hadoop metrics filter
363	common\src\java\org\apache\hadoop\metrics2\filter\RegexFilter.java	unrelated	package org apache hadoop metrics filter a regex pattern filter metrics regex filter extends abstract pattern filter protected pattern compile string return pattern compile
364	common\src\java\org\apache\hadoop\metrics2\impl\AbstractMetricsRecord.java	unrelated	package org apache hadoop metrics impl abstract metrics record implements metrics record obj instanceof metrics record metrics record metrics record obj return objects equal timestamp timestamp objects equal name name objects equal description description objects equal tags tags iterables elements equal metrics metrics return false should make sense time record used key return objects hash code name description tags return objects string helper add timestamp timestamp add name name add description description add tags tags add metrics iterables string metrics string
365	common\src\java\org\apache\hadoop\metrics2\impl\MBeanInfoBuilder.java	unrelated	package org apache hadoop metrics impl helper build m bean info metrics records m bean info builder implements metrics visitor string name description list m bean attribute info attrs iterable metrics record impl recs cur rec no m bean info builder string name string desc name name description desc attrs lists new array list m bean info builder reset iterable metrics record impl recs recs recs attrs clear return m bean attribute info new attr info string name string desc string type return new m bean attribute info get attr name name type desc true false false read non m bean attribute info new attr info metrics info info string type return new attr info info name info description type gauge metrics info info value attrs add new attr info info java lang integer gauge metrics info info value attrs add new attr info info java lang long gauge metrics info info value attrs add new attr info info java lang float gauge metrics info info value attrs add new attr info info java lang double counter metrics info info value attrs add new attr info info java lang integer counter metrics info info value attrs add new attr info info java lang long string get attr name string name return cur rec no name cur rec no name m bean info get cur rec no metrics record impl rec recs metrics tag rec tags attrs add new attr info tag name description java lang string abstract metric rec metrics visit cur rec no metrics system impl log debug attrs m bean attribute info attrs array new m bean attribute info attrs size return new m bean info name description attrs array attrs array null null null ops ctors notifications
366	common\src\java\org\apache\hadoop\metrics2\impl\MetricCounterInt.java	unrelated	package org apache hadoop metrics impl metric counter int extends abstract metric value metric counter int metrics info info value super info value value integer value return value metric type type return metric type counter visit metrics visitor visitor visitor counter value
367	common\src\java\org\apache\hadoop\metrics2\impl\MetricCounterLong.java	unrelated	package org apache hadoop metrics impl metric counter long extends abstract metric value metric counter long metrics info info value super info value value long value return value metric type type return metric type counter visit metrics visitor visitor visitor counter value
368	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeDouble.java	unrelated	package org apache hadoop metrics impl metric gauge double extends abstract metric value metric gauge double metrics info info value super info value value double value return value metric type type return metric type gauge visit metrics visitor visitor visitor gauge value
369	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeFloat.java	unrelated	package org apache hadoop metrics impl metric gauge float extends abstract metric value metric gauge float metrics info info value super info value value float value return value metric type type return metric type gauge visit metrics visitor visitor visitor gauge value
370	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeInt.java	unrelated	package org apache hadoop metrics impl metric gauge int extends abstract metric value metric gauge int metrics info info value super info value value integer value return value metric type type return metric type gauge visit metrics visitor visitor visitor gauge value
371	common\src\java\org\apache\hadoop\metrics2\impl\MetricGaugeLong.java	unrelated	package org apache hadoop metrics impl metric gauge long extends abstract metric value metric gauge long metrics info info value super info value value long value return value metric type type return metric type gauge visit metrics visitor visitor visitor gauge value
372	common\src\java\org\apache\hadoop\metrics2\impl\MetricsBuffer.java	unrelated	package org apache hadoop metrics impl an immutable element sink queues metrics buffer implements iterable metrics buffer entry iterable entry mutable metrics buffer iterable metrics buffer entry mutable mutable mutable iterator entry iterator return mutable iterator entry string source name iterable metrics record impl records entry string name iterable metrics record impl records source name name records records string name return source name iterable metrics record impl records return records
373	common\src\java\org\apache\hadoop\metrics2\impl\MetricsBufferBuilder.java	unrelated	package org apache hadoop metrics impl builder immutable metrics buffers metrics buffer builder extends array list metrics buffer entry serial version uid l boolean add string name iterable metrics record impl records return add new metrics buffer entry name records metrics buffer get return new metrics buffer
374	common\src\java\org\apache\hadoop\metrics2\impl\MetricsCollectorImpl.java	unrelated	package org apache hadoop metrics impl metrics collector impl implements metrics collector iterable metrics record builder impl list metrics record builder impl rbs lists new array list metrics filter record filter metric filter metrics record builder impl add record metrics info info boolean acceptable record filter null record filter accepts info name metrics record builder impl rb new metrics record builder impl info record filter metric filter acceptable acceptable rbs add rb return rb metrics record builder impl add record string name return add record info name name record list metrics record impl get records list metrics record impl recs lists new array list with capacity rbs size metrics record builder impl rb rbs metrics record impl mr rb get record mr null recs add mr return recs iterator metrics record builder impl iterator return rbs iterator clear rbs clear metrics collector impl set record filter metrics filter rf record filter rf return metrics collector impl set metric filter metrics filter mf metric filter mf return
375	common\src\java\org\apache\hadoop\metrics2\impl\MetricsConfig.java	unrelated	package org apache hadoop metrics impl metrics configuration metrics system impl metrics config extends subset configuration log log log factory get log metrics config string default file name hadoop metrics properties string prefix default string period key period period default seconds string queue capacity key queue capacity queue capacity default string retry delay key retry delay retry delay default seconds string retry backoff key retry backoff retry backoff default back factor string retry count key retry count retry count default string jmx cache ttl key jmx cache ttl string start mbeans key source start mbeans string plugin urls key plugin urls string context key context string name key name string desc key description string source key source string sink key sink string metric filter key metric filter string record filter key record filter string source filter key source filter pattern instance regex pattern compile splitter splitter splitter trim results class loader plugin loader metrics config configuration c string prefix super c prefix lower case locale us metrics config create string prefix return load first prefix hadoop metrics prefix lower case locale us properties default file name metrics config create string prefix string file names return load first prefix file names load configuration list files first successful load metrics config load first string prefix string file names string fname file names try configuration cf new properties configuration fname interpolated configuration log info loaded properties fname log debug string cf metrics config mc new metrics config cf prefix log debug mc return mc catch configuration exception e e get message starts with cannot locate configuration continue throw new metrics config exception e log warn cannot locate configuration tried joiner join file names default empty configuration return new metrics config new properties configuration prefix metrics config subset string prefix return new metrics config prefix return sub configs instance specified config assuming format specified follows pre type instance option value pre note special default instance excluded result map string metrics config get instance configs string type map string metrics config map maps new hash map metrics config sub subset type string key sub keys matcher matcher instance regex matcher key matcher matches string instance matcher group map contains key instance map put instance sub subset instance return map iterable string keys return new iterable string iterator string iterator return iterator string get keys will poke parents defaults object get property string key object value super get property key value null log debug enabled log debug poking parent get parent get class get simple name key key return get parent get property key starts with prefix default key prefix default key log debug enabled log debug returning value key key return value t extends metrics plugin t get plugin string name string cls name get class name name cls name null return null try class cls class name cls name true get plugin loader t plugin t cls new instance plugin init name empty subset name return plugin catch exception e throw new metrics config exception error creating plugin cls name e string get class name string
376	common\src\java\org\apache\hadoop\metrics2\impl\MetricsConfigException.java	unrelated	package org apache hadoop metrics impl the metrics configuration runtime exception metrics config exception extends metrics exception serial version uid l metrics config exception string message super message metrics config exception string message throwable cause super message cause metrics config exception throwable cause super cause
377	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordBuilderImpl.java	unrelated	package org apache hadoop metrics impl metrics record builder impl extends metrics record builder metrics collector parent timestamp metrics info rec info list abstract metric metrics list metrics tag tags metrics filter record filter metric filter boolean acceptable metrics record builder impl metrics collector parent metrics info info metrics filter rf metrics filter mf boolean acceptable parent parent timestamp system current time millis rec info info metrics lists new array list tags lists new array list record filter rf metric filter mf acceptable acceptable metrics collector parent return parent metrics record builder impl tag metrics info info string value acceptable tags add interns tag info value return metrics record builder impl add metrics tag tag tags add tag return metrics record builder impl add abstract metric metric metrics add metric return metrics record builder impl add counter metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric counter int info value return metrics record builder impl add counter metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric counter long info value return metrics record builder impl add gauge metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric gauge int info value return metrics record builder impl add gauge metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric gauge long info value return metrics record builder impl add gauge metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric gauge float info value return metrics record builder impl add gauge metrics info info value acceptable metric filter null metric filter accepts info name metrics add new metric gauge double info value return metrics record builder impl set context string value return tag ms info context value metrics record impl get record acceptable record filter null record filter accepts tags return new metrics record impl rec info timestamp tags metrics return null list metrics tag tags return collections unmodifiable list tags list abstract metric metrics return collections unmodifiable list metrics
378	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordFiltered.java	unrelated	package org apache hadoop metrics impl metrics record filtered extends abstract metrics record metrics record delegate metrics filter filter metrics record filtered metrics record delegate metrics filter filter delegate delegate filter filter return delegate timestamp return delegate name return delegate description return delegate context return delegate tags return new iterable abstract metric iterator abstract metric delegate metrics iterator return new abstract iterator abstract metric next abstract metric next next filter accepts next name return next return end of data
379	common\src\java\org\apache\hadoop\metrics2\impl\MetricsRecordImpl.java	unrelated	package org apache hadoop metrics impl metrics record impl extends abstract metrics record protected string default context default timestamp metrics info info list metrics tag tags iterable abstract metric metrics construct metrics record metrics record impl metrics info info timestamp list metrics tag tags iterable abstract metric metrics timestamp check arg timestamp timestamp timestamp info check not null info info tags check not null tags tags metrics check not null metrics metrics return timestamp return info name metrics info info return info return info description usually first tag metrics tag tags info ms info context return value return default context list metrics tag tags return tags already unmodifiable metrics record builder impl tags return metrics
380	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSinkAdapter.java	unrelated	package org apache hadoop metrics impl an adapter metrics sink associated filters metrics sink adapter implements sink queue consumer metrics buffer log log log factory get log metrics sink adapter string name description context metrics sink sink metrics filter source filter record filter metric filter sink queue metrics buffer queue thread sink thread volatile boolean stopping false volatile boolean error false period first retry delay retry count retry backoff metrics registry registry new metrics registry sinkadapter mutable stat latency mutable counter int dropped mutable gauge int qsize metrics sink adapter string name string description metrics sink sink string context metrics filter source filter metrics filter record filter metrics filter metric filter period queue capacity retry delay retry backoff retry count name check not null name name description description sink check not null sink sink object context context source filter source filter record filter record filter metric filter metric filter period check arg period period period first retry delay check arg retry delay retry delay retry delay retry backoff check arg retry backoff retry backoff retry backoff retry count retry count queue new sink queue metrics buffer check arg queue capacity queue capacity queue capacity latency registry new rate sink name sink end end latency false dropped registry new counter sink name dropped dropped updates per sink qsize registry new gauge sink name qsize queue size sink thread new thread publish metrics from queue sink thread set name name sink thread set daemon true boolean put metrics metrics buffer buffer logical time logical time period log debug enqueue logical time logical time queue enqueue buffer return true dropped incr return false return true ok publish metrics from queue retry delay first retry delay n retry count min delay math min retry delay millis random rng new random system nano time stopping try queue consume all retry delay first retry delay n retry count error false catch interrupted exception e log info name thread interrupted catch exception e n retry window math max retry delay min delay awhile rng next int retry window min delay error log error got sink exception retry awhile ms e retry delay retry backoff try thread sleep awhile catch interrupted exception e log info name thread interrupted waiting retry e n else error log error got sink exception retry limit suppressing error messages e queue clear error true don keep complaining ad infinitum consume metrics buffer buffer ts metrics buffer entry entry buffer source filter null source filter accepts entry name metrics record impl record entry records context null context equals record context record filter null record filter accepts record log debug enabled log debug pushing record entry name record context record name name sink put metrics metric filter null record new metrics record filtered record metric filter ts ts record timestamp ts sink flush latency add system current time millis ts log debug done start sink thread start log info sink name started stop stopping true sink thread interrupt try sink thread join catch interrupted exception e log warn stop interrupted e string name return name string description
381	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSourceAdapter.java	unrelated	package org apache hadoop metrics impl an adapter metrics source associated filter jmx impl metrics source adapter implements dynamic m bean log log log factory get log metrics source adapter string prefix name metrics source source metrics filter record filter metric filter hash map string attribute attr cache m bean info builder info builder iterable metrics tag injected tags iterable metrics record impl last recs jmx cache ts jmx cache ttl m bean info info cache object name mbean name boolean start m beans metrics source adapter string prefix string name string description metrics source source iterable metrics tag injected tags metrics filter record filter metrics filter metric filter jmx cache ttl boolean start m beans prefix check not null prefix prefix name check not null name name source check not null source source attr cache maps new hash map info builder new m bean info builder name description injected tags injected tags record filter record filter metric filter metric filter jmx cache ttl check arg jmx cache ttl jmx cache ttl jmx cache ttl start m beans start m beans metrics source adapter string prefix string name string description metrics source source iterable metrics tag injected tags period metrics config conf prefix name description source injected tags conf get filter record filter key conf get filter metric filter key period hack avoid innocuous races conf get boolean start mbeans key true start start m beans start m beans synchronized object get attribute string attribute throws attribute not found exception m bean exception reflection exception update jmx cache attribute attr cache get attribute null throw new attribute not found exception attribute found log debug enabled log debug attribute return get value set attribute attribute attribute throws attribute not found exception invalid attribute value exception m bean exception reflection exception throw new unsupported operation exception metrics read synchronized attribute list get attributes string attributes update jmx cache attribute list ret new attribute list string key attributes attribute attr attr cache get key log debug enabled log debug key attr ret add attr return ret attribute list set attributes attribute list attributes throw new unsupported operation exception metrics read object invoke string action name object params string signature throws m bean exception reflection exception throw new unsupported operation exception not supported yet synchronized m bean info get m bean info update jmx cache return info cache synchronized update jmx cache system current time millis jmx cache ts jmx cache ttl last recs null metrics collector impl builder new metrics collector impl get metrics builder true old cache size attr cache size new cache size update attr cache old cache size new cache size update info cache jmx cache ts system current time millis last recs null case regular interval update running iterable metrics record impl get metrics metrics collector impl builder boolean builder set record filter record filter set metric filter metric filter synchronized last recs null jmx cache ts true get metrics populate sink caches try source get metrics builder catch exception e log error error getting metrics source name e metrics record
382	common\src\java\org\apache\hadoop\metrics2\impl\MetricsSystemImpl.java	unrelated	package org apache hadoop metrics impl a base metrics system singletons metrics system impl extends metrics system implements metrics source log log log factory get log metrics system impl string ms name metrics system string ms stats name ms name sub stats string ms stats desc metrics system metrics string ms control name ms name sub control string ms init mode key hadoop metrics init mode enum init mode normal standby map string metrics source adapter sources map string metrics source sources map string metrics sink adapter sinks map string metrics sink sinks list callback callbacks metrics collector impl collector metrics registry registry new metrics registry ms name list metrics tag injected tags things changed init start stop string prefix metrics filter source filter metrics config config map string metrics config source configs sink configs boolean monitoring false timer timer period seconds logical time number timer invocations period object name mbean name boolean publish self metrics true metrics source adapter sys source ref count mini cluster mode construct metrics system metrics system impl string prefix prefix prefix sources maps new hash map sources maps new linked hash map sinks maps new hash map sinks maps new linked hash map source configs maps new hash map sink configs maps new hash map callbacks lists new array list injected tags lists new array list collector new metrics collector impl prefix null prefix could null default ctor requires init later init system m bean construct system initializing read config etc metrics system impl null initialized metrics system prefix synchronized metrics system init string prefix monitoring default metrics system mini cluster mode log warn prefix metrics system already initialized return prefix check not null prefix prefix ref count monitoring mini cluster mode log info prefix metrics system started return switch init mode case normal try start catch metrics config exception e configuration errors e g typos fatal we always start metrics system later via jmx log warn metrics system started e get message log debug stacktrace e break case standby log info prefix metrics system started standby mode init system m bean return synchronized start check not null prefix prefix monitoring log warn prefix metrics system already started new metrics exception illegal start return callback cb callbacks cb pre start configure prefix start timer monitoring true log info prefix metrics system started callback cb callbacks cb post start synchronized stop monitoring default metrics system mini cluster mode log warn prefix metrics system yet started new metrics exception illegal stop return monitoring mini cluster mode log info prefix metrics system stopped return callback cb callbacks cb pre stop log info stopping prefix metrics system stop timer stop sources stop sinks clear configs monitoring false log info prefix metrics system stopped callback cb callbacks cb post stop t register string name string desc t source metrics source builder sb metrics annotations new source builder source metrics source sb build metrics info si sb info string name name null si name name string desc desc null si description desc string name friendly non metrics tests default metrics system source
383	common\src\java\org\apache\hadoop\metrics2\impl\MsInfo.java	unrelated	package org apache hadoop metrics impl metrics system related metrics info instances enum ms info implements metrics info num active sources number active metrics sources num all sources number registered metrics sources num active sinks number active metrics sinks num all sinks number registered metrics sinks context metrics context hostname local hostname session id session id process name process name string desc ms info string desc desc desc return desc return objects string helper add name name add description desc string
384	common\src\java\org\apache\hadoop\metrics2\impl\package-info.java	unrelated	package org apache hadoop metrics impl
385	common\src\java\org\apache\hadoop\metrics2\impl\SinkQueue.java	unrelated	package org apache hadoop metrics impl a half blocking nonblocking producers blocking consumers queue metrics sinks new elements dropped queue full preserve interesting elements onset queue filling events sink queue t consumer t consume t object throws interrupted exception a fixed size circular buffer minimize garbage t data head head position tail tail position size number elements thread current consumer null sink queue capacity data t new object math max capacity head tail size synchronized boolean enqueue t e data length size return false size tail tail data length data tail e notify return true consume one element block queue empty only one consumer time allowed consume consumer t consumer throws interrupted exception t e wait for data try consumer consume e take forever dequeue finally clear consumer lock consume elements block queue empty consume all consumer t consumer throws interrupted exception wait for data try size consumer consume front take forever dequeue finally clear consumer lock dequeue one element head queue block queue empty synchronized t dequeue throws interrupted exception check consumer size wait return dequeue synchronized t wait for data throws interrupted exception check consumer size wait set consumer lock return front synchronized check consumer current consumer null throw new concurrent modification exception the current consumer get name thread consuming queue synchronized set consumer lock current consumer thread current thread synchronized clear consumer lock current consumer null synchronized t dequeue size throw new illegal state exception size must size head head data length t ret data head data head null hint gc return ret synchronized t front return data head data length synchronized t back return data tail synchronized clear check consumer data length data null size synchronized size return size capacity return data length
386	common\src\java\org\apache\hadoop\metrics2\lib\DefaultMetricsFactory.java	unrelated	package org apache hadoop metrics lib experimental extend metrics dynamically enum default metrics factory instance singleton mutable metrics factory mmf impl mutable metrics factory get annotated metrics factory return instance get instance mutable metrics factory synchronized t t get instance class t cls cls mutable metrics factory mmf impl null mmf impl new mutable metrics factory return t mmf impl throw new metrics exception unknown metrics factory type cls get name synchronized set instance mutable metrics factory factory mmf impl factory
387	common\src\java\org\apache\hadoop\metrics2\lib\DefaultMetricsSystem.java	unrelated	package org apache hadoop metrics lib the default metrics system singleton enum default metrics system instance singleton metrics system impl new metrics system impl volatile boolean mini cluster mode false unique names bean names new unique names unique names source names new unique names convenience method initialize metrics system metrics system initialize string prefix return instance init prefix synchronized metrics system init string prefix return impl init prefix metrics system instance return instance get impl shutdown metrics system shutdown instance shutdown instance synchronized shutdown instance impl shutdown bean names map clear source names map clear metrics system set instance metrics system ms return instance set impl ms synchronized metrics system set impl metrics system ms metrics system old impl impl ms return old synchronized metrics system get impl return impl set mini cluster mode boolean choice instance mini cluster mode choice boolean mini cluster mode return instance mini cluster mode object name new m bean name string name return instance new object name name string source name string name boolean dup ok return instance new source name name dup ok synchronized object name new object name string name try bean names map contains key name mini cluster mode throw new metrics exception name already exists return new object name bean names unique name name catch exception e throw new metrics exception e synchronized string new source name string name boolean dup ok source names map contains key name dup ok return name else mini cluster mode throw new metrics exception metrics source name already exists return source names unique name name
388	common\src\java\org\apache\hadoop\metrics2\lib\Interns.java	unrelated	package org apache hadoop metrics lib helpers create interned metrics info interns log log log factory get log interns a simple intern cache two keys avoid creating new combined key objects lookup cache with keys k k v map k map k v k map new linked hash map k map k v serial version uid l boolean got overflow false protected boolean remove eldest entry map entry k map k v e boolean overflow expire key at size overflow got overflow log warn metrics intern cache overflow size e got overflow true return overflow protected boolean expire key at size protected boolean expire key at size protected v new value k k k k synchronized v add k k k k map k v k map k map get k k map null k map new linked hash map k v serial version uid l boolean got overflow false boolean overflow expire key at size overflow got overflow log warn metrics intern cache overflow size e got overflow true return overflow k map put k k map v v k map get k v null v new value k k k map put k v return v sanity limits case misuse abuse max info names max info descs distinct per name enum info instance cache with keys string string metrics info cache new cache with keys string string metrics info return size max info names return size max info descs return new metrics info impl name desc get metric info object metrics info info string name string description return info instance cache add name description sanity limits max tag names max tag values distinct per name enum tags instance cache with keys metrics info string metrics tag cache new cache with keys metrics info string metrics tag return size max tag names return size max tag values return new metrics tag info value get metrics tag metrics tag tag metrics info info string value return tags instance cache add info value get metrics tag metrics tag tag string name string description string value return tags instance cache add info name description value
389	common\src\java\org\apache\hadoop\metrics2\lib\MethodMetric.java	unrelated	package org apache hadoop metrics lib metric generated method mostly used annotation method metric extends mutable metric log log log factory get log method metric object obj method method metrics info info mutable metric impl method metric object obj method method metrics info info metric type type obj check not null obj object method check arg method method get parameter types length metric method arguments method set accessible true info check not null info info impl new impl check not null type metric type mutable metric new impl metric type metric type class res type method get return type switch metric type case counter return new counter res type case gauge return new gauge res type case default return res type string new tag res type new gauge res type case tag return new tag res type default check arg metric type false unsupported metric type return null mutable metric new counter class type int type long type return new mutable metric try object ret method invoke obj object null int type rb add counter info integer ret value else rb add counter info long ret value catch exception ex log error error invoking method method get name ex throw new metrics exception unsupported counter type type get name boolean int class type boolean ret type integer type type integer return ret boolean long class type return type long type type long boolean float class type return type float type type float boolean double class type return type double type type double mutable metric new gauge class int long float double return new mutable metric try object ret method invoke obj object null int rb add gauge info integer ret value else long rb add gauge info long ret value else float rb add gauge info float ret value else rb add gauge info double ret value catch exception ex log error error invoking method method get name ex throw new metrics exception unsupported gauge type get name mutable metric new tag class res type res type string return new mutable metric try object ret method invoke obj object null rb tag info string ret catch exception ex log error error invoking method method get name ex throw new metrics exception unsupported tag type res type get name impl snapshot builder metrics info metric info method method return interns info name from method metric method get name string name from method method string method name method get name method name starts with get return string utils capitalize method name substring return string utils capitalize method name
390	common\src\java\org\apache\hadoop\metrics2\lib\MetricsAnnotations.java	unrelated	package org apache hadoop metrics lib metrics annotation helpers metrics annotations make metrics source annotated object metrics source make source object source return new metrics source builder source default metrics factory get annotated metrics factory build metrics source builder new source builder object source return new metrics source builder source default metrics factory get annotated metrics factory
391	common\src\java\org\apache\hadoop\metrics2\lib\MetricsInfoImpl.java	unrelated	package org apache hadoop metrics lib making implementing metric info little easier metrics info impl implements metrics info string name description metrics info impl string name string description name check not null name name description check not null description description return name return description obj instanceof metrics info metrics info metrics info obj return objects equal name name objects equal description description return false return objects hash code name description return objects string helper add name name add description description string
392	common\src\java\org\apache\hadoop\metrics2\lib\MetricsRegistry.java	unrelated	package org apache hadoop metrics lib an optional metrics registry creating maintaining collection metrics mutables making writing metrics source easier metrics registry map string mutable metric metrics map maps new linked hash map map string metrics tag tags map maps new linked hash map metrics info metrics info construct registry record name metrics registry string name metrics info interns info name name construct registry metadata object metrics registry metrics info info metrics info info metrics info info return metrics info get metric name synchronized mutable metric get string name return metrics map get name get tag name synchronized metrics tag get tag string name return tags map get name create mutable integer counter mutable counter int new counter string name string desc val return new counter interns info name desc val create mutable integer counter synchronized mutable counter int new counter metrics info info val check metric name info name mutable counter int ret new mutable counter int info val metrics map put info name ret return ret create mutable integer counter mutable counter long new counter string name string desc val return new counter interns info name desc val create mutable integer counter synchronized mutable counter long new counter metrics info info val check metric name info name mutable counter long ret new mutable counter long info val metrics map put info name ret return ret create mutable integer gauge mutable gauge int new gauge string name string desc val return new gauge interns info name desc val create mutable integer gauge synchronized mutable gauge int new gauge metrics info info val check metric name info name mutable gauge int ret new mutable gauge int info val metrics map put info name ret return ret create mutable integer gauge mutable gauge long new gauge string name string desc val return new gauge interns info name desc val create mutable integer gauge synchronized mutable gauge long new gauge metrics info info val check metric name info name mutable gauge long ret new mutable gauge long info val metrics map put info name ret return ret create mutable metric stats synchronized mutable stat new stat string name string desc string sample name string value name boolean extended check metric name name mutable stat ret new mutable stat name desc sample name value name extended metrics map put name ret return ret create mutable metric stats mutable stat new stat string name string desc string sample name string value name return new stat name desc sample name value name false create mutable rate metric mutable rate new rate string name return new rate name name false create mutable rate metric mutable rate new rate string name string description return new rate name description false create mutable rate metric throughput measurement mutable rate new rate string name string desc boolean extended return new rate name desc extended true synchronized mutable rate new rate string name string desc boolean extended boolean return existing return existing mutable metric rate metrics map get name rate null rate instanceof mutable rate return mutable rate rate throw new metrics exception
393	common\src\java\org\apache\hadoop\metrics2\lib\MetricsSourceBuilder.java	unrelated	package org apache hadoop metrics lib helper build metrics source object annotations metrics source builder log log log factory get log metrics source builder object source mutable metrics factory factory metrics registry registry metrics info info boolean at metric false boolean registry false metrics source builder object source mutable metrics factory factory source check not null source source factory check not null factory mutable metrics factory class cls source get class registry init registry source field field cls get declared fields add source field method method cls get declared methods add source method metrics source build source instanceof metrics source at metric registry throw new metrics exception hybrid metrics registry required return metrics source source else at metric throw new metrics exception no valid metric annotation found return new metrics source get metrics metrics collector builder boolean registry snapshot builder add record registry info metrics info info return info metrics registry init registry object source class cls source get class metrics registry r null get registry already exists field field cls get declared fields field get type metrics registry continue try field set accessible true r metrics registry field get source registry r null break catch exception e log warn error accessing field field e continue create new registry according annotation annotation annotation cls get annotations annotation instanceof metrics metrics metrics annotation info factory get info cls r null r new metrics registry info r set context context r null return new metrics registry cls get simple name return r add object source field field annotation annotation field get annotations annotation instanceof metric continue try skip fields already set field set accessible true field get source null continue catch exception e log warn error accessing field field annotated annotation e continue mutable metric mutable factory new for field field metric annotation registry mutable null try field set source mutable at metric true catch exception e throw new metrics exception error setting field field annotated annotation e add object source method method annotation annotation method get annotations annotation instanceof metric continue factory new for method source method metric annotation registry at metric true
394	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounter.java	unrelated	package org apache hadoop metrics lib the mutable counter monotonically increasing metric mutable counter extends mutable metric metrics info info protected mutable counter metrics info info info check not null info counter info protected metrics info info return info increment metric value incr
395	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounterInt.java	unrelated	package org apache hadoop metrics lib a mutable counter implementing metrics sources mutable counter int extends mutable counter volatile value mutable counter int metrics info info init value super info value init value synchronized incr value set changed increment value delta synchronized incr delta value delta set changed value return value snapshot metrics record builder builder boolean changed builder add counter info value clear changed
396	common\src\java\org\apache\hadoop\metrics2\lib\MutableCounterLong.java	unrelated	package org apache hadoop metrics lib a mutable counter mutable counter long extends mutable counter volatile value mutable counter long metrics info info init value super info value init value synchronized incr value set changed increment value delta synchronized incr delta value delta set changed value return value snapshot metrics record builder builder boolean changed builder add counter info value clear changed
397	common\src\java\org\apache\hadoop\metrics2\lib\MutableGauge.java	unrelated	package org apache hadoop metrics lib the mutable gauge metric mutable gauge extends mutable metric metrics info info protected mutable gauge metrics info info info check not null info metric info protected metrics info info return info increment value metric incr decrement value metric decr
398	common\src\java\org\apache\hadoop\metrics2\lib\MutableGaugeInt.java	unrelated	package org apache hadoop metrics lib a mutable gauge mutable gauge int extends mutable gauge volatile value mutable gauge int metrics info info init value super info value init value value return value synchronized incr value set changed increment delta synchronized incr delta value delta set changed synchronized decr value set changed decrement delta synchronized decr delta value delta set changed set value metric set value value value set changed snapshot metrics record builder builder boolean changed builder add gauge info value clear changed
399	common\src\java\org\apache\hadoop\metrics2\lib\MutableGaugeLong.java	unrelated	package org apache hadoop metrics lib a mutable gauge mutable gauge long extends mutable gauge volatile value mutable gauge long metrics info info init value super info value init value value return value synchronized incr value set changed increment delta synchronized incr delta value delta set changed synchronized decr value set changed decrement delta synchronized decr delta value delta set changed set value metric set value value value set changed snapshot metrics record builder builder boolean changed builder add gauge info value clear changed
400	common\src\java\org\apache\hadoop\metrics2\lib\MutableMetric.java	unrelated	package org apache hadoop metrics lib the mutable metric mutable metric volatile boolean changed true get snapshot metric snapshot metrics record builder builder boolean get snapshot metric changed snapshot metrics record builder builder snapshot builder false set changed flag mutable operations protected set changed changed true clear changed flag snapshot operations protected clear changed changed false boolean changed return changed
401	common\src\java\org\apache\hadoop\metrics2\lib\MutableMetricsFactory.java	unrelated	package org apache hadoop metrics lib mutable metrics factory log log log factory get log mutable metrics factory mutable metric new for field field field metric annotation metrics registry registry log debug enabled log debug field field annotation annotation metrics info info get info annotation field mutable metric metric new for field field annotation metric null registry add info name metric return metric class cls field get type cls mutable counter int return registry new counter info cls mutable counter long return registry new counter info l cls mutable gauge int return registry new gauge info cls mutable gauge long return registry new gauge info l cls mutable rate return registry new rate info name info description annotation always cls mutable rates return new mutable rates registry cls mutable stat return registry new stat info name info description annotation sample name annotation value name annotation always throw new metrics exception unsupported metric field field get name type field get type get name mutable metric new for method object source method method metric annotation metrics registry registry log debug enabled log debug method method annotation annotation metrics info info get info annotation method mutable metric metric new for method source method annotation metric metric null metric new method metric source method info annotation type registry add info name metric return metric override handle custom mutable metrics fields protected mutable metric new for field field field metric annotation return null override handle custom mutable metrics methods protected mutable metric new for method object source method method metric annotation return null protected metrics info get info metric annotation field field return get info annotation get name field protected string get name field field return string utils capitalize field get name protected metrics info get info metric annotation method method return get info annotation get name method protected metrics info get info class cls metrics annotation string name annotation name string annotation string name name empty cls get simple name name return interns info name empty name protected string get name method method string method name method get name method name starts with get return string utils capitalize method name substring return string utils capitalize method name protected metrics info get info metric annotation string default name string value annotation value value length return interns info value value value length return interns info default name value return interns info default name default name
402	common\src\java\org\apache\hadoop\metrics2\lib\MutableRate.java	unrelated	package org apache hadoop metrics lib a convenient mutable metric throughput measurement mutable rate extends mutable stat mutable rate string name string description boolean extended super name description ops time extended
403	common\src\java\org\apache\hadoop\metrics2\lib\MutableRates.java	unrelated	package org apache hadoop metrics lib helper manage group mutable rate metrics mutable rates extends mutable metric log log log factory get log mutable rates metrics registry registry set class protocol cache sets new hash set mutable rates metrics registry registry registry check not null registry metrics registry initialize registry methods protocol show first snapshot convenient jmx implementations init class protocol protocol cache contains protocol return protocol cache add protocol method method protocol get declared methods string name method get name log debug name try registry new rate name name false true catch exception e log error error creating rate metrics method get name e add rate sample rate metric add string name elapsed registry add name elapsed snapshot metrics record builder rb boolean registry snapshot rb
404	common\src\java\org\apache\hadoop\metrics2\lib\MutableStat.java	unrelated	package org apache hadoop metrics lib a mutable metric stats useful keeping throughput latency stats mutable stat extends mutable metric metrics info num info metrics info avg info metrics info stdev info metrics info min info metrics info max info metrics info min info metrics info max info sample stat interval stat new sample stat sample stat prev stat new sample stat sample stat min max min max new sample stat min max num samples boolean extended false construct sample statistics metric mutable stat string name string description string sample name string value name boolean extended string uc name string utils capitalize name string us name string utils capitalize sample name string uv name string utils capitalize value name string desc string utils uncapitalize description string ls name string utils uncapitalize sample name string lv name string utils uncapitalize value name num info info uc name num us name number ls name desc avg info info uc name avg uv name average lv name desc stdev info info uc name stdev uv name standard deviation lv name desc min info info uc name i min uv name interval min lv name desc max info info uc name i max uv name interval max lv name desc min info info uc name min uv name min lv name desc max info info uc name max uv name max lv name desc extended extended construct snapshot stat metric extended stat default mutable stat string name string description string sample name string value name name description sample name value name false add number samples sum running stat synchronized add num samples sum interval stat add num samples sum set changed add snapshot metric synchronized add value interval stat add value min max add value set changed synchronized snapshot metrics record builder builder boolean changed num samples interval stat num samples builder add counter num info num samples add gauge avg info last stat mean extended builder add gauge stdev info last stat stddev add gauge min info last stat min add gauge max info last stat max add gauge min info min max min add gauge max info min max max changed num samples interval stat copy to prev stat interval stat reset clear changed sample stat last stat return changed interval stat prev stat reset time min max metric reset min max min max reset
405	common\src\java\org\apache\hadoop\metrics2\lib\package-info.java	unrelated	package org apache hadoop metrics lib
406	common\src\java\org\apache\hadoop\metrics2\lib\UniqueNames.java	unrelated	package org apache hadoop metrics lib generates predictable user friendly unique names unique names count string base name value count string name value base name name value value joiner joiner joiner map string count map maps new hash map synchronized string unique name string name count c map get name c null c new count name map put name c return name c base name equals name c new count name string new name joiner join name c value count c map get new name c null map put new name c return new name handle collisons assume rare cases eg people explicitly passed name names true
407	common\src\java\org\apache\hadoop\metrics2\sink\FileSink.java	unrelated	package org apache hadoop metrics sink a metrics sink writes file file sink implements metrics sink string filename key filename print writer writer init subset configuration conf string filename conf get string filename key try writer filename null new print writer system new print writer new file writer new file filename true catch exception e throw new metrics exception error creating filename e put metrics metrics record record writer print record timestamp writer print writer print record context writer print writer print record name string separator metrics tag tag record tags writer print separator separator writer print tag name writer print writer print tag value abstract metric metric record metrics writer print separator separator writer print metric name writer print writer print metric value writer println flush writer flush
408	common\src\java\org\apache\hadoop\metrics2\sink\package-info.java	unrelated	package org apache hadoop metrics sink
409	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\AbstractGangliaSink.java	unrelated	package org apache hadoop metrics sink ganglia this base ganglia sink using metrics lot code derived org apache hadoop metrics ganglia ganglia context as per documentation sink implementations worry thread safety hence code written thread safety modified case assumption changes future abstract ganglia sink implements metrics sink log log log factory get log get class output gmetric help showing allowable values type string either uint uint uint u units string unit measure value e g kilobytes celcius default slope string either zero positive negative default x tmax int the maximum time seconds gmetric calls default string default units default tmax default dmax ganglia slope default slope ganglia slope default port string servers property servers buffer size per libgmond c string support sparse metrics property supportsparse boolean support sparse metrics default false string equal string host name unknown example com datagram socket datagram socket list extends socket address metrics servers byte buffer new byte buffer size offset boolean support sparse metrics support sparse metrics default used visiting metrics protected ganglia metric visitor ganglia metric visitor new ganglia metric visitor subset configuration conf map string ganglia conf ganglia conf map ganglia conf default ganglia conf new ganglia conf ganglia slope values equal ordinal enum ganglia slope zero positive negative define enum various type conf enum ganglia conf type slope units dmax tmax non javadoc org apache hadoop metrics metrics plugin init org apache commons configuration subset configuration init subset configuration conf log debug initializing ganglia sink ganglia metrics conf conf take hostname dns conf get string slave host name null host name conf get string slave host name else try host name dns get default host conf get string dfs datanode dns default conf get string dfs datanode dns nameserver default catch unknown host exception uhe log error uhe host name unknown example com load gannglia servers properties metrics servers servers parse conf get string servers property default port extract ganglia conf per metrics ganglia conf map new hash map string ganglia conf load ganglia conf ganglia conf type units load ganglia conf ganglia conf type tmax load ganglia conf ganglia conf type dmax load ganglia conf ganglia conf type slope try datagram socket new datagram socket catch socket exception se log error se see sparse metrics supported default false support sparse metrics conf get boolean support sparse metrics property support sparse metrics default non javadoc flush nothing buffering data load configurations conf type load ganglia conf ganglia conf type gtype string propertyarr conf get string array gtype name propertyarr null propertyarr length string metric n value propertyarr string metric n value arr metric n value split equal metric n value arr length metric n value arr length log error invalid propertylist gtype name string metric name metric n value arr trim string metric value metric n value arr trim ganglia conf gconf ganglia conf map get metric name gconf null gconf new ganglia conf ganglia conf map put metric name gconf switch gtype case units gconf set units metric value break case dmax gconf set dmax integer parse int metric value break case
410	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaConf.java	unrelated	package org apache hadoop metrics sink ganglia used store ganglia properties ganglia conf string units abstract ganglia sink default units ganglia slope slope dmax abstract ganglia sink default dmax tmax abstract ganglia sink default tmax string string string builder buf new string builder buf append unit append units append slope append slope append dmax append dmax append tmax append tmax return buf string string get units return units set units string units units units ganglia slope get slope return slope set slope ganglia slope slope slope slope get dmax return dmax set dmax dmax dmax dmax get tmax return tmax set tmax tmax tmax tmax
411	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaMetricVisitor.java	unrelated	package org apache hadoop metrics sink ganglia since implementations metric hence use visitor figure type slope metric counters positive slope ganglia metric visitor implements metrics visitor string int string float string double string type ganglia slope slope string get type return type null others ganglia slope get slope return slope gauge metrics info info value metric gauge int type int slope null set null cannot figure metric gauge metrics info info value metric gauge long type float slope null set null cannot figure metric gauge metrics info info value metric gauge float type float slope null set null cannot figure metric gauge metrics info info value metric gauge double type double slope null set null cannot figure metric counter metrics info info value metric counter int type int counters positive slope slope ganglia slope positive counter metrics info info value metric counter long type float counters positive slope slope ganglia slope positive
412	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaSink30.java	unrelated	package org apache hadoop metrics sink ganglia this code supports ganglia ganglia sink extends abstract ganglia sink log log log factory get log get class metrics cache metrics cache new metrics cache put metrics metrics record record the method handles cases whether ganglia support dense publish metrics sparse change publish metrics try string record name record name string context name record context string builder sb new string builder sb append context name sb append sb append record name string group name sb string sb append sb base len sb length string type null ganglia slope slope from metric null ganglia slope calculated slope null record cached metrics null reset buffer reset buffer beginning support sparse metrics sending dense metrics update metrics cache get updated data cached metrics metrics cache update record cached metrics null cached metrics metrics entry set null map entry string abstract metric entry cached metrics metrics entry set abstract metric metric entry get value sb append metric name string name sb string visit metric identify ganglia type slope metric visit ganglia metric visitor type ganglia metric visitor get type slope from metric ganglia metric visitor get slope ganglia conf g conf get ganglia conf for metric name calculated slope calculate slope g conf slope from metric send metric ganglia emit metric group name name type metric value string g conf calculated slope reset length buffer next iteration sb set length sb base len else support sparse updates collection abstract metric metrics collection abstract metric record metrics metrics size got metrics send latest abstract metric metric record metrics sb append metric name string name sb string visit metric identify ganglia type slope metric visit ganglia metric visitor type ganglia metric visitor get type slope from metric ganglia metric visitor get slope ganglia conf g conf get ganglia conf for metric name calculated slope calculate slope g conf slope from metric send metric ganglia emit metric group name name type metric value string g conf calculated slope reset length buffer next iteration sb set length sb base len catch io exception io throw new metrics exception failed put metrics io calculate slope properties metric ganglia slope calculate slope ganglia conf g conf ganglia slope slope from metric g conf get slope null slope specified properties use return g conf get slope else slope from metric null slope specified properties use derived metric return slope from metric else return default slope the method sends metrics ganglia servers the method taken org apache hadoop metrics ganglia ganglia context minimal changes order keep sync protected emit metric string group name string name string type string value ganglia conf g conf ganglia slope g slope throws io exception name null log warn metric emitted name return else value null log warn metric name name emitted null value return else type null log warn metric name name value value type return log debug enabled log debug emitting metric name type type value value slope g slope name hostname get host name xdr metric user defined xdr type xdr name xdr value xdr g conf get units
413	common\src\java\org\apache\hadoop\metrics2\sink\ganglia\GangliaSink31.java	unrelated	package org apache hadoop metrics sink ganglia this code supports ganglia ganglia sink extends ganglia sink log log log factory get log get class the method sends metrics ganglia servers the method taken org apache hadoop metrics ganglia ganglia context minimal changes order keep sync protected emit metric string group name string name string type string value ganglia conf g conf ganglia slope g slope throws io exception name null log warn metric emitted name return else value null log warn metric name name emitted null value return else type null log warn metric name name value value type return log debug enabled log debug emitting metric name type type value value slope g slope name hostname get host name the following xdr recipe done careful reading gm protocol x ganglia carefully examining output gmetric utility strace first send metadata message xdr metric id metadata msg xdr get host name hostname xdr name metric name xdr spoof false xdr type metric type xdr name metric name xdr g conf get units units xdr g slope ordinal slope xdr g conf get tmax tmax maximum time metrics xdr g conf get dmax dmax maximum data value xdr num entries extra value field ganglia x xdr group group attribute xdr group name group value send metric ganglia hosts emit to ganglia hosts now send message actual value technically need send metadata message metric i want record metrics send xdr sending value xdr get host name host name xdr name metric name xdr spoof false xdr format field xdr value metric value send metric ganglia hosts emit to ganglia hosts
414	common\src\java\org\apache\hadoop\metrics2\source\JvmMetrics.java	unrelated	package org apache hadoop metrics source jvm logging related metrics mostly used various servers part metrics export jvm metrics implements metrics source enum singleton instance jvm metrics impl synchronized jvm metrics init string process name string session id impl null impl create process name session id default metrics system instance return impl m memory mx bean memory mx bean management factory get memory mx bean list garbage collector mx bean gc beans management factory get garbage collector mx beans thread mx bean thread mx bean management factory get thread mx bean string process name session id map string metrics info gc info cache maps new hash map jvm metrics string process name string session id process name process name session id session id jvm metrics create string process name string session id metrics system ms return ms register jvm metrics name jvm metrics description new jvm metrics process name session id jvm metrics init singleton string process name string session id return singleton instance init process name session id get metrics metrics collector collector boolean metrics record builder rb collector add record jvm metrics set context jvm tag process name process name tag session id session id get memory usage rb get gc usage rb get thread usage rb get event counters rb get memory usage metrics record builder rb memory usage mem non heap memory mx bean get non heap memory usage memory usage mem heap memory mx bean get heap memory usage rb add gauge mem non heap used m mem non heap get used m add gauge mem non heap committed m mem non heap get committed m add gauge mem heap used m mem heap get used m add gauge mem heap committed m mem heap get committed m get gc usage metrics record builder rb count time millis garbage collector mx bean gc bean gc beans c gc bean get collection count gc bean get collection time metrics info gc info get gc info gc bean get name rb add counter gc info c add counter gc info count c time millis rb add counter gc count count add counter gc time millis time millis synchronized metrics info get gc info string gc name metrics info gc info gc info cache get gc name gc info null gc info new metrics info gc info interns info gc count gc name gc count gc name gc info interns info gc time millis gc name gc time gc name gc info cache put gc name gc info return gc info get thread usage metrics record builder rb threads new threads runnable threads blocked threads waiting threads timed waiting threads terminated thread ids thread mx bean get all thread ids thread info thread info thread mx bean get thread info thread ids thread info null continue race protection switch thread info get thread state case new threads new break case runnable threads runnable break case blocked threads blocked break case waiting threads waiting break case timed waiting threads timed waiting break case terminated threads terminated break rb add gauge threads
415	common\src\java\org\apache\hadoop\metrics2\source\JvmMetricsInfo.java	unrelated	package org apache hadoop metrics source jvm logging related metrics info instances enum jvm metrics info implements metrics info jvm metrics jvm related metrics etc record info metrics mem non heap used m non heap memory used mb mem non heap committed m non heap memory committed mb mem heap used m heap memory used mb mem heap committed m heap memory committed mb gc count total gc count gc time millis total gc time milliseconds threads new number new threads threads runnable number runnable threads threads blocked number blocked threads threads waiting number waiting threads threads timed waiting number timed waiting threads threads terminated number terminated threads log fatal total number fatal log events log error total number error log events log warn total number warning log events log info total number info log events string desc jvm metrics info string desc desc desc return objects string helper add name name add description desc string
416	common\src\java\org\apache\hadoop\metrics2\util\Contracts.java	unrelated	package org apache hadoop metrics util additional helpers besides guava preconditions programming contract contracts contracts check argument false conditions t t check arg t arg boolean expression object msg expression throw new illegal argument exception string value of msg arg return arg check argument false conditions check arg arg boolean expression object msg expression throw new illegal argument exception string value of msg arg return arg check argument false conditions check arg arg boolean expression object msg expression throw new illegal argument exception string value of msg arg return arg check argument false conditions check arg arg boolean expression object msg expression throw new illegal argument exception string value of msg arg return arg check argument false conditions check arg arg boolean expression object msg expression throw new illegal argument exception string value of msg arg return arg
417	common\src\java\org\apache\hadoop\metrics2\util\MBeans.java	unrelated	package org apache hadoop metrics util this util provides method register m bean using standard naming convention described doc link link register string string object m beans log log log factory get log m beans register m bean using standard m bean name format hadoop service service name name name name where service name name name supplied parameters object name register string service name string name name object mbean m bean server mbs management factory get platform m bean server object name name get m bean name service name name name try mbs register m bean mbean name log debug registered name return name catch exception e log warn error registering name e return null unregister object name mbean name log debug unregistering mbean name m bean server mbs management factory get platform m bean server mbean name null log debug stacktrace new throwable return try mbs unregister m bean mbean name catch exception e log warn error unregistering mbean name e object name get m bean name string service name string name name object name name null string name str hadoop service service name name name name try name default metrics system new m bean name name str catch exception e log warn error creating m bean object name name str e return name
418	common\src\java\org\apache\hadoop\metrics2\util\MetricsCache.java	unrelated	package org apache hadoop metrics util a metrics cache sinks support sparse updates metrics cache log log log factory get log metrics cache max recs per name default map string record cache map maps new hash map max recs per name record cache extends linked hash map collection metrics tag record serial version uid l boolean got overflow false protected boolean remove eldest entry map entry collection metrics tag record eldest boolean overflow size max recs per name overflow got overflow log warn metrics cache overflow size eldest got overflow true return overflow cached record record map string string tags maps new hash map map string abstract metric metrics maps new hash map lookup tag value string get tag string key return tags get key lookup metric value number get metric string key abstract metric metric metrics get key return metric null metric value null lookup metric instance abstract metric get metric instance string key return metrics get key set map entry string string tags return tags entry set set map entry string number metrics map string number map new linked hash map string number metrics size map entry string abstract metric map entry metrics entry set map put map entry get key map entry get value value return map entry set set map entry string abstract metric metrics entry set return metrics entry set return objects string helper add tags tags add metrics metrics string metrics cache max recs per name default construct metrics cache metrics cache max recs per name max recs per name max recs per name update cache return current cached record record update metrics record mr boolean including tags string name mr name record cache record cache map get name record cache null record cache new record cache map put name record cache collection metrics tag tags mr tags record record record cache get tags record null record new record record cache put tags record abstract metric mr metrics record metrics put name including tags mostly sinks tags part dense schema metrics tag mr tags record tags put name value return record update cache return current cache record record update metrics record mr return update mr false get cached record record get string name collection metrics tag tags record cache rc map get name rc null return null return rc get tags
419	common\src\java\org\apache\hadoop\metrics2\util\package-info.java	unrelated	package org apache hadoop metrics util
420	common\src\java\org\apache\hadoop\metrics2\util\SampleStat.java	unrelated	package org apache hadoop metrics util helper compute running sample stats sample stat min max minmax new min max num samples construct new running sample stat sample stat reset num samples minmax reset we want reuse object sometimes reset num samples min max minmax num samples num samples minmax reset minmax copy values saves object creation gc copy to sample stat reset num samples minmax add sample running stat sample stat add x minmax add x return add x add sample partial sum running stat note min max evaluated using method sample stat add n samples x num samples n samples num samples x else the welford method numerical stability x num samples x x return num samples return num samples mean return num samples variance return num samples num samples stddev return math sqrt variance min return minmax min max return minmax max helper keep running min max min max min double max value max double min value add value value max max value value min min value min return min max return max reset min double max value max double min value reset min max min min max max
421	common\src\java\org\apache\hadoop\metrics2\util\Servers.java	unrelated	package org apache hadoop metrics util helpers handle server addresses servers this intended instantiated servers parses space comma separated sequence server specifications form hostname hostname port if specs null defaults localhost default port list inet socket address parse string specs default port list inet socket address result lists new array list specs null result add new inet socket address localhost default port else string spec strings specs split string spec string spec strings colon spec string index of colon colon spec string length result add new inet socket address spec string default port else string hostname spec string substring colon port integer parse int spec string substring colon result add new inet socket address hostname port return result
422	common\src\java\org\apache\hadoop\net\CachedDNSToSwitchMapping.java	unrelated	package org apache hadoop net a cached implementation dns to switch mapping takes raw dns to switch mapping stores resolved network location cache the following calls resolved network location get location cache cached dns to switch mapping implements dns to switch mapping map string string cache new concurrent hash map string string protected dns to switch mapping raw mapping cached dns to switch mapping dns to switch mapping raw mapping raw mapping raw mapping returns hosts names cached previously list string get uncached hosts list string names find names without cached resolved location list string un cached hosts new array list string names size string name names cache get name null un cached hosts add name return un cached hosts caches resolved hosts cache resolved hosts list string uncached hosts list string resolved hosts cache result resolved hosts null uncached hosts size cache put uncached hosts get resolved hosts get returns cached resolution list hostnames addresses returns null names currently cache list string get cached hosts list string names list string result new array list string names size construct result string name names string network location cache get name network location null result add network location else return null return result resolves host names adds cache unlike resolve method hide unknown host exceptions list string resolve valid hosts list string names throws unknown host exception names empty return new array list string list string addresses new array list string names size string name names addresses add inet address get by name name get host address list string uncached hosts get uncached hosts names resolve uncached hosts list string resolved hosts raw mapping resolve valid hosts uncached hosts cache resolved hosts uncached hosts resolved hosts return get cached hosts addresses list string resolve list string names normalize input names form ip addresses names net utils normalize host names names list string result new array list string names size names empty return result list string uncached hosts get uncached hosts names resolve uncached hosts list string resolved hosts raw mapping resolve uncached hosts cache resolved hosts uncached hosts resolved hosts return get cached hosts names
423	common\src\java\org\apache\hadoop\net\DNS.java	unrelated	package org apache hadoop net a provides direct reverse lookup functionalities allowing querying specific network interfaces nameservers dns log log log factory get log dns the cached hostname initially null string cached hostname resolve local hostname string cached host address resolve local host ip address string localhost localhost returns hostname associated specified ip address provided nameserver loopback addresses string reverse dns inet address host ip string ns throws naming exception builds reverse ip lookup form this formed reversing ip numbers appending addr arpa string parts host ip get host address split string reverse ip parts parts parts parts addr arpa dir context ictx new initial dir context attributes attribute try attribute ictx get attributes dns use dns default ns null ns nameserver used reverse ip new string ptr finally ictx close return attribute get ptr get string returns i ps associated provided textual form the name network query e g eth if unknown host exception encountered querying default string get i ps string str interface throws unknown host exception try network interface net if network interface get by name str interface net if null return new string cached host address else vector string ips new vector string enumeration e net if get inet addresses e more elements ips add inet address e next element get host address return ips array new string catch socket exception e return new string cached host address returns first available ip address associated provided network the name network query e g eth if one encountered querying default string get default ip string str interface throws unknown host exception string ips get i ps str interface return ips returns host names associated provided nameserver address bound specified network the name network query e g eth the dns host name specified string get hosts string str interface string nameserver throws unknown host exception string ips get i ps str interface vector string hosts new vector string ctr ctr ips length ctr try hosts add reverse dns inet address get by name ips ctr nameserver catch unknown host exception ignored catch naming exception ignored hosts empty return new string cached hostname else return hosts array new string hosts size determine local hostname retrieving cache known if cannot determine host name return localhost string resolve local hostname string localhost try localhost inet address get local host get canonical host name catch unknown host exception e log info unable determine local hostname falling back localhost e localhost localhost return localhost get ip address local host this loop back value local host address cannot determined if loopback address localhost resolve system network state nothing going work a message logged error level null pointer returned pointer trigger failures later application string resolve local host ip address string address try address inet address get local host get host address catch unknown host exception e log info unable determine address host falling back localhost address e try address inet address get by name localhost get host address catch unknown host exception local host address exception point deep trouble log error unable determine local loopback address localhost
424	common\src\java\org\apache\hadoop\net\DNSToSwitchMapping.java	unrelated	package org apache hadoop net an implemented allow pluggable dns name ip address rack id resolvers dns to switch mapping resolves list dns names ip addresses returns back list switch information network paths one one correspondence must maintained elements lists consider element argument list x com the switch information returned must network path form foo rack root foo switch rack connected note hostname ip address part returned path the network topology cluster would determine number components network path list string resolve list string names resolves list dns names ip addresses returns back list switch information network paths one one correspondence must maintained elements lists consider element argument list x com the switch information returned must network path form foo rack root foo switch rack connected note hostname ip address part returned path the network topology cluster would determine number components network path unlike resolve names must resolvable list string resolve valid hosts list string names throws unknown host exception
425	common\src\java\org\apache\hadoop\net\NetUtils.java	unrelated	package org apache hadoop net net utils log log log factory get log net utils map string string host to resolved new hash map string string get socket factory given according configuration parameter tt hadoop rpc socket factory lt class name gt tt when parameter exists fall back default socket factory configured tt hadoop rpc socket factory default tt if default socket factory configured fall back jvm default socket factory socket factory get socket factory configuration conf class clazz socket factory factory null string prop value conf get hadoop rpc socket factory clazz get simple name prop value null prop value length factory get socket factory from property conf prop value factory null factory get default socket factory conf return factory get default socket factory specified configuration parameter tt hadoop rpc socket factory default tt jvm default socket factory configuration contain default socket factory property socket factory get default socket factory configuration conf string prop value conf get hadoop rpc socket factory default prop value null prop value length return socket factory get default return get socket factory from property conf prop value get socket factory corresponding given proxy uri if given proxy uri corresponds absence configuration parameter returns null if uri malformed raises exception socket factory instantiate assumed non null non empty socket factory get socket factory from property configuration conf string prop value try class class conf get class by name prop value return socket factory reflection utils new instance class conf catch class not found exception cnfe throw new runtime exception socket factory found cnfe util method build socket addr either host post fs host port path inet socket address create socket addr string target return create socket addr target util method build socket addr either host host post fs host port path inet socket address create socket addr string target default port target null throw new illegal argument exception target address cannot null colon index target index of colon index default port throw new runtime exception not host port pair target string hostname port target contains colon index hostname target else must old style host port hostname target substring colon index port integer parse int target substring colon index else new uri uri addr new path target uri hostname addr get host port addr get port port port default port get static resolution hostname null hostname get static resolution hostname return new inet socket address hostname port adds resolution host this used setting hostnames names fake point well known host for e g testcases require daemons different hostnames running machine in order create connections daemons one set mappings hostnames localhost link net utils get static resolution string used query actual hostname add static resolution string host string resolved name synchronized host to resolved host to resolved put host resolved name retrieves resolved name passed host the resolved name must set earlier using link net utils add static resolution string string string get static resolution string host synchronized host to resolved return host to resolved get host this used get resolutions added using link net utils add static resolution
426	common\src\java\org\apache\hadoop\net\NetworkTopology.java	unrelated	package org apache hadoop net the represents cluster computer tree hierarchical network topology for example cluster may consists many data centers filled racks computers in network topology leaves represent data nodes computers inner nodes represent switches routers manage traffic data centers racks network topology string default rack default rack default host level log log log factory get log network topology inner node represent switch router data center rack different leave node non null children inner node extends node base array list node children new array list node num of leaves construct inner node path like inner node string path super path construct inner node name network location inner node string name string location super name location construct inner node name network location parent level inner node string name string location inner node parent level super name location parent level get children collection node get children return children return number children node get num of children return children size judge node represents rack return true child children inner nodes boolean rack children empty return true node first child children get first child instanceof inner node return false return true judge node ancestor node n boolean ancestor node n return get path equals node base path separator str n get network location node base path separator str starts with get path node base path separator str judge node parent node n boolean parent node n return n get network location equals get path return child name node ancestor node n string get next ancestor name node n ancestor n throw new illegal argument exception ancestor n string name n get network location substring get path length name char at path separator name name substring index name index of path separator index name name substring index return name add node n subtree node boolean add node n ancestor n throw new illegal argument exception n get name located n get network location decendent get path parent n node parent n add n directly n set parent n set level level children size children get get name equals n get name children set n return false children add n num of leaves return true else find next ancestor node string parent name get next ancestor name n inner node parent node null children size children get get name equals parent name parent node inner node children get break parent node null create new inner node parent node new inner node parent name get path get level children add parent node add n subtree next ancestor node parent node add n num of leaves return true else return false remove node n subtree node boolean remove node n string parent n get network location string current path get path ancestor n throw new illegal argument exception n get name located parent descendent current path parent n node parent n remove n directly children size children get get name equals n get name children remove num of leaves n set parent null return true return false else find next ancestor node parent node string parent name get next
427	common\src\java\org\apache\hadoop\net\Node.java	unrelated	package org apache hadoop net the defines node network topology a node may leave representing data node inner node representing datacenter rack each data name location network decided syntax similar file name for example data node name hostname port located rack orange datacenter dog representation network location dog orange node return representation node network location string get network location set node network location set network location string location return node name string get name return node parent node get parent set node parent set parent node parent return node level tree e g root tree returns children return get level set node level tree set level
428	common\src\java\org\apache\hadoop\net\NodeBase.java	unrelated	package org apache hadoop net a base implements node node base implements node char path separator string path separator str string root representation root protected string name host port protected string location representation node location protected level level tree node resides protected node parent parent default constructor node base construct node path concatenation node location path seperator name node base string path path normalize path index path last index of path separator index set root path else set path substring index path substring index construct node name location node base string name string location set name normalize location construct node name location node base string name string location node parent level set name normalize location parent parent level level set node name location set string name string location name null name contains path separator str throw new illegal argument exception network location name contains name name name null name location location return node name string get name return name return node network location string get network location return location set node network location set network location string location location location return node path string get path node node return node get network location path separator str node get name return node representation string string return get path normalize path string normalize string path path null path length return root path char at path separator throw new illegal argument exception network location path start path separator str path len path length path char at len path separator return path substring len return path return node parent node get parent return parent set node parent set parent node parent parent parent return node level tree e g root tree returns children return get level return level set node level tree set level level level level
429	common\src\java\org\apache\hadoop\net\ScriptBasedMapping.java	unrelated	package org apache hadoop net this implements link dns to switch mapping using script configured via net topology script file name script based mapping extends cached dns to switch mapping implements configurable script based mapping super new raw script based mapping script must accept least many args min allowable args default arg count common configuration keys net topology script number args default string script filename key common configuration keys net topology script file name key string script arg count key common configuration keys net topology script number args key script based mapping configuration conf set conf conf configuration get conf return raw script based mapping raw mapping get conf set conf configuration conf raw script based mapping raw mapping set conf conf raw script based mapping implements dns to switch mapping string script name configuration conf max args max hostnames per call script log log log factory get log script based mapping set conf configuration conf script name conf get script filename key max args conf get int script arg count key default arg count conf conf configuration get conf return conf raw script based mapping list string resolve list string names list string new array list string names size names empty return script name null names size add network topology default rack return string output run resolve command names output null string tokenizer switch info new string tokenizer output switch info more tokens string switch info switch info next token add switch info size names size invalid number entries returned script log warn script script name returned integer string size values integer string names size expected return null else error occurred return null signify exn already logged run resolve command return null return list string resolve valid hosts list string names throws unknown host exception list string result resolve names result null return result else throw new unknown host exception unknown host returned script based mapping string run resolve command list string args loop count args size return null string builder output new string builder num processed max args min allowable args log warn invalid value integer string max args script arg count key must integer string min allowable args return null num processed args size start max args loop count list string cmd list new array list string cmd list add script name num processed start num processed start max args num processed args size num processed cmd list add args get num processed file dir null string user dir user dir system get property user dir null dir new file user dir shell command executor new shell command executor cmd list array new string dir try execute output append get output catch exception e log warn exception e return null loop count return output string
430	common\src\java\org\apache\hadoop\net\SocketInputStream.java	unrelated	package org apache hadoop net this implements input stream timeout reading this sets non blocking flag socket channel so create object read link socket get input stream write link socket get output stream associated socket throw illegal blocking mode exception please use link socket output stream writing socket input stream extends input stream implements readable byte channel reader reader reader extends socket io with timeout readable byte channel channel reader readable byte channel channel timeout throws io exception super selectable channel channel timeout channel channel perform io byte buffer buf throws io exception return channel read buf create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking channel reading also link selectable channel the channel configured non blocking socket input stream readable byte channel channel timeout throws io exception socket io with timeout check channel validity channel reader new reader channel timeout same socket input stream socket get channel timeout br br create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket input stream socket socket timeout throws io exception socket get channel timeout same socket input stream socket get channel socket get so timeout br br create new input stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket input stream socket socket throws io exception socket get channel socket get so timeout read throws io exception allocation removed required probably need optimize encourage single byte read byte buf new byte ret read buf ret return buf xff ret unexpected throw new io exception could read stream return ret read byte b len throws io exception return read byte buffer wrap b len synchronized close throws io exception close channel since socket get input stream close closes socket reader channel close reader close returns underlying channel used inputstream this useful certain cases like channel link file channel transfer from readable byte channel readable byte channel get channel return reader channel readable byte channel boolean open return reader open read byte buffer dst throws io exception return reader io dst selection key op read waits underlying channel ready reading the timeout specified stream applies wait select channel times i o error occurs wait for readable throws io exception reader wait for io selection key op read
431	common\src\java\org\apache\hadoop\net\SocketIOWithTimeout.java	pooling	package org apache hadoop net this supports input output streams socket channels these streams timeout socket io with timeout this intentionally package log log log factory get log socket io with timeout selectable channel channel timeout boolean closed false selector pool selector new selector pool a timeout value implies wait ever we value timeout implies zero wait e read write returns immediately this set channel non blocking socket io with timeout selectable channel channel timeout throws io exception check channel validity channel channel channel timeout timeout set non blocking channel configure blocking false close closed true boolean open return closed channel open selectable channel get channel return channel utility function check channel ok mainly throw io exception instead runtime exception case mismatch this mismatch occur many runtime reasons check channel validity object channel throws io exception channel null most common reason original socket channel so making io exception rather runtime exception throw new io exception channel null check channel socket created channel instanceof selectable channel throw new io exception channel selectable channel performs actual io operations this expected block channel drained completely we wait io required perform io byte buffer buf throws io exception performs one io returns number bytes read written it waits specified timeout if channel read timeout socket timeout exception thrown selection key op read reading selection key op write writing io byte buffer buf ops throws io exception for one thread allowed if user want read write multiple threads multiple streams could created in case multiple threads work well underlying channel supports buf remaining throw new illegal argument exception buffer data left return buf remaining closed return try n perform io buf n successful io error return n catch io exception e channel open closed true throw e wait socket ready count try count selector select channel ops timeout catch io exception e unexpected io exception closed true throw e count throw new socket timeout exception timeout exception string channel timeout ops otherwise socket ready io return reach the contract similar link socket channel connect socket address timeout connect socket channel channel socket address endpoint timeout throws io exception boolean blocking on channel blocking blocking on channel configure blocking false try channel connect endpoint return timeout left timeout end time timeout system current time millis timeout true might call finish connect channels user level protocols ret selector select selectable channel channel selection key op connect timeout left ret channel finish connect return ret timeout timeout left end time system current time millis throw new socket timeout exception timeout exception string channel timeout selection key op connect catch io exception e javadoc socket channel connect says channel closed try channel close catch io exception ignored throw e finally blocking on channel open channel configure blocking true this similar link io byte buffer except perform i o it waits channel ready i o specified ops select channel times i o error occurs wait for io ops throws io exception selector select channel ops timeout throw new socket timeout exception timeout exception string channel timeout ops string timeout exception string selectable
432	common\src\java\org\apache\hadoop\net\SocketOutputStream.java	unrelated	package org apache hadoop net this implements output stream timeout writing this sets non blocking flag socket channel so creating object read link socket get input stream write link socket get output stream associated socket throw llegal blocking mode exception please use link socket input stream reading socket output stream extends output stream implements writable byte channel writer writer writer extends socket io with timeout writable byte channel channel writer writable byte channel channel timeout throws io exception super selectable channel channel timeout channel channel perform io byte buffer buf throws io exception return channel write buf create new ouput stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking channel writing also link selectable channel the channel configured non blocking socket output stream writable byte channel channel timeout throws io exception socket io with timeout check channel validity channel writer new writer channel timeout same socket output stream socket get channel timeout br br create new ouput stream given timeout if timeout zero treated infinite timeout the socket channel configured non blocking socket output stream socket socket timeout throws io exception socket get channel timeout write b throws io exception if need optimize allocation probably need optimize encourage single byte writes byte buf new byte buf byte b write buf write byte b len throws io exception byte buffer buf byte buffer wrap b len buf remaining try write buf throw new io exception the stream closed catch io exception e unlike read write inform user partial writes so close partial write buf capacity buf remaining writer close throw e synchronized close throws io exception close channel since socket get ouput stream close closes socket writer channel close writer close returns underlying channel used stream this useful certain cases like channel link file channel transfer to writable byte channel writable byte channel get channel return writer channel writable byte channle boolean open return writer open write byte buffer src throws io exception return writer io src selection key op write waits underlying channel ready writing the timeout specified stream applies wait select channel times i o error occurs wait for writable throws io exception writer wait for io selection key op write transfers data file channel using link file channel transfer to writable byte channel similar read fully waits till requested amount data transfered if end input file reached requested number bytes transfered if channel blocks transfer longer timeout stream link file channel transfer to writable byte channel transfer to fully file channel file ch position count throws io exception count ideally wait transfer to returns but bug jre linux http bugs sun com view bug bug id throws exception instead returning wait channel writable writing if ever see io exception message resource temporarily unavailable thrown please let us know once move java se wait moved correct place wait for writable n transfered file ch transfer to position count get channel n transfered check end file reached position file ch size throw new eof exception eof reached file size file ch size count bytes left transfered
433	common\src\java\org\apache\hadoop\net\SocksSocketFactory.java	unrelated	package org apache hadoop net specialized socket factory create sockets socks proxy socks socket factory extends socket factory implements configurable configuration conf proxy proxy default empty constructor use reflection api socks socket factory proxy proxy no proxy constructor supplied proxy socks socket factory proxy proxy proxy proxy socket create socket throws io exception return new socket proxy socket create socket inet address addr port throws io exception socket socket create socket socket connect new inet socket address addr port return socket socket create socket inet address addr port inet address local host addr local port throws io exception socket socket create socket socket bind new inet socket address local host addr local port socket connect new inet socket address addr port return socket socket create socket string host port throws io exception unknown host exception socket socket create socket socket connect new inet socket address host port return socket socket create socket string host port inet address local host addr local port throws io exception unknown host exception socket socket create socket socket bind new inet socket address local host addr local port socket connect new inet socket address host port return socket hash code return proxy hash code boolean equals object obj obj return true obj null return false obj instanceof socks socket factory return false socks socket factory socks socket factory obj proxy null proxy null return false else proxy equals proxy return false return true configuration get conf return conf set conf configuration conf conf conf string proxy str conf get hadoop socks server proxy str null proxy str length set proxy proxy str set proxy socket factory described parameter set proxy string proxy str string strs proxy str split strs length throw new runtime exception bad socks proxy parameter proxy str string host strs port integer parse int strs proxy new proxy proxy type socks inet socket address create unresolved host port
434	common\src\java\org\apache\hadoop\net\StandardSocketFactory.java	unrelated	package org apache hadoop net specialized socket factory create sockets socks proxy standard socket factory extends socket factory default empty constructor use reflection api standard socket factory socket create socket throws io exception note this returns nio socket associated socket channel as unfortunately makes streams returned socket get input stream socket get output stream unusable blocking read input stream blocks write output stream vice versa so users socket factories use net utils get input stream socket net utils get output stream socket instead a solution hiding user write filter socket lines filter input stream extend overriding get input stream get output stream return socket channel open socket socket create socket inet address addr port throws io exception socket socket create socket socket connect new inet socket address addr port return socket socket create socket inet address addr port inet address local host addr local port throws io exception socket socket create socket socket bind new inet socket address local host addr local port socket connect new inet socket address addr port return socket socket create socket string host port throws io exception unknown host exception socket socket create socket socket connect new inet socket address host port return socket socket create socket string host port inet address local host addr local port throws io exception unknown host exception socket socket create socket socket bind new inet socket address local host addr local port socket connect new inet socket address host port return socket boolean equals object obj obj return true obj null return false return obj get class equals get class hash code return get class hash code
435	common\src\java\org\apache\hadoop\record\BinaryRecordInput.java	unrelated	package org apache hadoop record binary record input implements record input data input binary index implements index nelems binary index nelems nelems nelems boolean done return nelems incr nelems binary record input set data input data input inp inp thread local b in new thread local protected synchronized object initial value return new binary record input get thread local record input supplied data input binary record input get data input inp binary record input bin binary record input b in get bin set data input inp return bin creates new instance binary record input binary record input input stream strm new data input stream strm creates new instance binary record input binary record input data input din din byte read byte string tag throws io exception return read byte boolean read bool string tag throws io exception return read boolean read int string tag throws io exception return utils read v int read long string tag throws io exception return utils read v long read float string tag throws io exception return read float read double string tag throws io exception return read double string read string string tag throws io exception return utils binary string buffer read buffer string tag throws io exception len utils read v int byte barr new byte len read fully barr return new buffer barr start record string tag throws io exception op end record string tag throws io exception op index start vector string tag throws io exception return new binary index read int tag end vector string tag throws io exception op index start map string tag throws io exception return new binary index read int tag end map string tag throws io exception op
436	common\src\java\org\apache\hadoop\record\BinaryRecordOutput.java	unrelated	package org apache hadoop record binary record output implements record output data output binary record output set data output data output thread local b out new thread local protected synchronized object initial value return new binary record output get thread local record output supplied data output binary record output get data output binary record output bout binary record output b out get bout set data output return bout creates new instance binary record output binary record output output stream new data output stream creates new instance binary record output binary record output data output write byte byte b string tag throws io exception write byte b write bool boolean b string tag throws io exception write boolean b write int string tag throws io exception utils write v int write long string tag throws io exception utils write v long write float f string tag throws io exception write float f write double string tag throws io exception write double write string string string tag throws io exception utils binary string write buffer buffer buf string tag throws io exception byte barr buf get len buf get count utils write v int len write barr len start record record r string tag throws io exception end record record r string tag throws io exception start vector array list v string tag throws io exception write int v size tag end vector array list v string tag throws io exception start map tree map v string tag throws io exception write int v size tag end map tree map v string tag throws io exception
437	common\src\java\org\apache\hadoop\record\Buffer.java	unrelated	package org apache hadoop record a byte sequence used java native type buffer it resizable distinguishes count seqeunce current capacity buffer implements comparable cloneable number valid bytes bytes count backing store buffer byte bytes null create zero count sequence buffer count create buffer using byte array initial value buffer byte bytes bytes bytes count bytes null bytes length create buffer using byte range initial value buffer byte bytes offset length copy bytes offset length use specified bytes array underlying sequence set byte bytes count bytes null bytes length bytes bytes copy specified byte array buffer replaces current buffer copy byte bytes offset length bytes null bytes length length bytes new byte length system arraycopy bytes offset bytes length count length get data buffer byte get bytes null bytes new byte return bytes get current count buffer get count return count get capacity maximum count could handled without resizing backing storage get capacity return get length change capacity backing storage the data preserved new capacity get count set capacity new capacity new capacity throw new illegal argument exception invalid capacity argument new capacity new capacity bytes null count return new capacity get capacity byte data new byte new capacity new capacity count count new capacity count system arraycopy get data count bytes data reset buffer size reset set capacity change capacity backing store current count buffer truncate set capacity count append specified bytes buffer append byte bytes offset length set capacity count length system arraycopy bytes offset get count length count count length append specified bytes buffer append byte bytes append bytes bytes length inherit javadoc hash code hash byte b get count hash hash b return hash define sort order buffer negative smaller compare to object buffer right buffer byte lb get byte rb right get count right count lb xff b rb xff b return b return count right count inherit javadoc boolean equals object instanceof buffer return compare to return inheric javadoc string string string builder sb new string builder count idx idx count idx sb append character digit bytes idx x f sb append character digit bytes idx x f return sb string convert byte buffer specific character encoding string string string charset name throws unsupported encoding exception return new string get get count charset name inherit javadoc object clone throws clone not supported exception buffer result buffer super clone result copy get get count return result
438	common\src\java\org\apache\hadoop\record\CsvRecordInput.java	unrelated	package org apache hadoop record csv record input implements record input pushback reader stream csv index implements index boolean done char c try c char stream read stream unread c catch io exception ex return c true false incr throw exception on error string tag throws io exception throw new io exception error deserializing tag string read field string tag throws io exception try string builder buf new string builder true char c char stream read switch c case return buf string case case n case r stream unread c return buf string default buf append c catch io exception ex throw new io exception error reading tag creates new instance csv record input csv record input input stream try stream new pushback reader new input stream reader utf catch unsupported encoding exception ex throw new runtime exception ex byte read byte string tag throws io exception return byte read long tag boolean read bool string tag throws io exception string sval read field tag return t equals sval true false read int string tag throws io exception return read long tag read long string tag throws io exception string sval read field tag try lval long parse long sval return lval catch number format exception ex throw new io exception error deserializing tag read float string tag throws io exception return read double tag read double string tag throws io exception string sval read field tag try dval double parse double sval return dval catch number format exception ex throw new io exception error deserializing tag string read string string tag throws io exception string sval read field tag return utils csv string sval buffer read buffer string tag throws io exception string sval read field tag return utils csv buffer sval start record string tag throws io exception tag null equals tag char c char stream read char c char stream read c c throw new io exception error deserializing tag end record string tag throws io exception char c char stream read tag null equals tag c n c r throw new io exception error deserializing record else return c throw new io exception error deserializing tag c char stream read c stream unread c return index start vector string tag throws io exception char c char stream read char c char stream read c v c throw new io exception error deserializing tag return new csv index end vector string tag throws io exception char c char stream read c throw new io exception error deserializing tag c char stream read c stream unread c return index start map string tag throws io exception char c char stream read char c char stream read c c throw new io exception error deserializing tag return new csv index end map string tag throws io exception char c char stream read c throw new io exception error deserializing tag c char stream read c stream unread c return
439	common\src\java\org\apache\hadoop\record\CsvRecordOutput.java	unrelated	package org apache hadoop record csv record output implements record output print stream stream boolean first true throw exception on error string tag throws io exception stream check error throw new io exception error serializing tag print comma unless first first stream print first false creates new instance csv record output csv record output output stream try stream new print stream true utf catch unsupported encoding exception ex throw new runtime exception ex write byte byte b string tag throws io exception write long b tag write bool boolean b string tag throws io exception print comma unless first string val b t f stream print val throw exception on error tag write int string tag throws io exception write long tag write long string tag throws io exception print comma unless first stream print throw exception on error tag write float f string tag throws io exception write double f tag write double string tag throws io exception print comma unless first stream print throw exception on error tag write string string string tag throws io exception print comma unless first stream print utils csv string throw exception on error tag write buffer buffer buf string tag throws io exception print comma unless first stream print utils csv buffer buf throw exception on error tag start record record r string tag throws io exception tag null equals tag print comma unless first stream print first true end record record r string tag throws io exception tag null equals tag stream print n first true else stream print first false start vector array list v string tag throws io exception print comma unless first stream print v first true end vector array list v string tag throws io exception stream print first false start map tree map v string tag throws io exception print comma unless first stream print first true end map tree map v string tag throws io exception stream print first false
440	common\src\java\org\apache\hadoop\record\Index.java	unrelated	package org apache hadoop record interface acts iterator deserializing maps the deserializer returns instance record uses read vectors maps an example usage follows code index idx start vector idx done read element vector idx incr code index boolean done incr
441	common\src\java\org\apache\hadoop\record\Record.java	unrelated	package org apache hadoop record abstract extended generated record implements writable comparable cloneable serialize record tag ususally field name serialize record output rout string tag throws io exception deserialize record tag usually field name deserialize record input rin string tag throws io exception inheric javadoc compare to object peer throws class cast exception serialize record without tag serialize record output rout throws io exception serialize rout deserialize record without tag deserialize record input rin throws io exception deserialize rin inherit javadoc write data output throws java io io exception binary record output bout binary record output get serialize bout inherit javadoc read fields data input din throws java io io exception binary record input rin binary record input get din deserialize rin inherit javadoc string string try byte array output stream new byte array output stream csv record output new csv record output serialize return new string byte array utf catch throwable ex throw new runtime exception ex
442	common\src\java\org\apache\hadoop\record\RecordComparator.java	unrelated	package org apache hadoop record a raw record comparator base record comparator extends writable comparator construct raw link record comparison implementation protected record comparator class extends writable comparable record class super record class inheric java doc compare byte b byte b register optimized comparator link record implementation synchronized define class c record comparator comparator writable comparator define c comparator
443	common\src\java\org\apache\hadoop\record\RecordInput.java	unrelated	package org apache hadoop record interface deserializers implement record input read byte serialized record byte read byte string tag throws io exception read boolean serialized record boolean read bool string tag throws io exception read integer serialized record read int string tag throws io exception read integer serialized record read long string tag throws io exception read single precision serialized record read float string tag throws io exception read precision number serialized record read double string tag throws io exception read utf encoded serialized record string read string string tag throws io exception read byte array serialized record buffer read buffer string tag throws io exception check mark start serialized record start record string tag throws io exception check mark end serialized record end record string tag throws io exception check mark start serialized vector index start vector string tag throws io exception check mark end serialized vector end vector string tag throws io exception check mark start serialized map index start map string tag throws io exception check mark end serialized map end map string tag throws io exception
444	common\src\java\org\apache\hadoop\record\RecordOutput.java	unrelated	package org apache hadoop record interface alll serializers implement record output write byte serialized record write byte byte b string tag throws io exception write boolean serialized record write bool boolean b string tag throws io exception write integer serialized record write int string tag throws io exception write integer serialized record write long string tag throws io exception write single precision serialized record write float f string tag throws io exception write precision floating point number serialized record write double string tag throws io exception write unicode serialized record write string string string tag throws io exception write buffer serialized record write buffer buffer buf string tag throws io exception mark start record serialized start record record r string tag throws io exception mark end serialized record end record record r string tag throws io exception mark start vector serialized start vector array list v string tag throws io exception mark end serialized vector end vector array list v string tag throws io exception mark start map serialized start map tree map string tag throws io exception mark end serialized map end map tree map string tag throws io exception
445	common\src\java\org\apache\hadoop\record\Utils.java	unrelated	package org apache hadoop record various utility functions hadooop record i o runtime utils cannot create new instance utils utils char hexchars a b c d e f string xml string string string builder sb new string builder idx idx length idx char ch char at idx ch sb append lt else ch sb append amp else ch sb append else ch x ch x d ff ch x e ch x fffd sb append sb append hexchars ch x f sb append hexchars ch x f sb append hexchars ch x f sb append hexchars ch x f else sb append ch return sb string c char ch ch ch return ch else ch a ch f return ch a else ch ch f return ch return string xml string string string builder sb new string builder idx idx length char ch char at idx ch ch c char at idx ch c char at idx ch c char at idx ch c char at idx char res char ch ch ch ch sb append res else sb append ch return sb string string csv string string string builder sb new string builder length sb append len length len char c char at switch c case sb append break case n sb append a break case r sb append d break case sb append c break case sb append d break case sb append break default sb append c return sb string string csv string string throws io exception char at throw new io exception error deserializing len length string builder sb new string builder len len char c char at c char ch char at char ch char at ch ch sb append else ch ch a sb append n else ch ch d sb append r else ch ch c sb append else ch ch d sb append else ch ch sb append else throw new io exception error deserializing else sb append c return sb string string xml buffer buffer return string buffer xml buffer string throws io exception length return new buffer blen length byte barr new byte blen idx idx blen idx char c char at idx char c char at idx barr idx byte integer parse int c c return new buffer barr string csv buffer buffer buf string builder sb new string builder sb append buf string return sb string converts csv serialized representation buffer new buffer buffer csv buffer string throws io exception char at throw new io exception error deserializing buffer length return new buffer blen length byte barr new byte blen idx idx blen idx char c char at idx char c char at idx barr idx byte integer parse int c c return new buffer barr utf len for code point cpt throws io exception cpt cpt x f return cpt x cpt x ff return cpt x cpt x d cpt x dfff cpt x fffd return cpt x cpt x ffff return throw new io exception illegal unicode codepoint integer hex string cpt b integer parse int b
446	common\src\java\org\apache\hadoop\record\XmlRecordInput.java	unrelated	package org apache hadoop record xml deserializer xml record input implements record input value string type string buffer sb value string type sb new string buffer add chars char buf offset len sb append buf offset len string get value return sb string string get type return type xml parser extends default handler boolean chars valid false array list value val list xml parser array list value vlist val list vlist start document throws sax exception end document throws sax exception start element string ns string sname string qname attributes attrs throws sax exception chars valid false boolean equals qname equals qname equals qname equals qname equals qname ex equals qname ex equals qname ex equals qname chars valid true val list add new value qname else equals qname array equals qname val list add new value qname end element string ns string sname string qname throws sax exception chars valid false equals qname array equals qname val list add new value qname characters char buf offset len throws sax exception chars valid value v val list get val list size v add chars buf offset len xml index implements index boolean done value v val list get v idx array equals v get type val list set v idx null v idx return true else return false incr array list value val list v len v idx value next throws io exception v idx v len value v val list get v idx val list set v idx null v idx return v else throw new io exception error deserialization creates new instance xml record input xml record input input stream try val list new array list value default handler handler new xml parser val list sax parser factory factory sax parser factory new instance sax parser parser factory new sax parser parser parse handler v len val list size v idx catch exception ex throw new runtime exception ex byte read byte string tag throws io exception value v next ex equals v get type throw new io exception error deserializing tag return byte parse byte v get value boolean read bool string tag throws io exception value v next boolean equals v get type throw new io exception error deserializing tag return equals v get value read int string tag throws io exception value v next equals v get type equals v get type throw new io exception error deserializing tag return integer parse int v get value read long string tag throws io exception value v next ex equals v get type throw new io exception error deserializing tag return long parse long v get value read float string tag throws io exception value v next ex equals v get type throw new io exception error deserializing tag return float parse float v get value read double string tag throws io exception value v next equals v get type throw new io exception error deserializing tag return double parse double v get value string read string string tag throws io exception value v next equals v get type
447	common\src\java\org\apache\hadoop\record\XmlRecordOutput.java	unrelated	package org apache hadoop record xml serializer xml record output implements record output print stream stream indent stack string compound stack put indent string builder sb new string builder idx idx indent idx sb append stream print sb string add indent indent close indent indent print begin envelope string tag compound stack empty string compound stack peek equals put indent stream print member n add indent put indent stream print name tag name n put indent stream print value else vector equals stream print value else map equals stream print value else stream print value print end envelope string tag compound stack empty string compound stack peek equals stream print value n close indent put indent stream print member n else vector equals stream print value n else map equals stream print value n else stream print value n inside vector string tag print begin envelope tag compound stack push vector outside vector string tag throws io exception string compound stack pop vector equals throw new io exception error serializing vector print end envelope tag inside map string tag print begin envelope tag compound stack push map outside map string tag throws io exception string compound stack pop map equals throw new io exception error serializing map print end envelope tag inside record string tag print begin envelope tag compound stack push outside record string tag throws io exception string compound stack pop equals throw new io exception error serializing record print end envelope tag creates new instance xml record output xml record output output stream try stream new print stream true utf compound stack new stack string catch unsupported encoding exception ex throw new runtime exception ex write byte byte b string tag throws io exception print begin envelope tag stream print ex stream print byte string b stream print ex print end envelope tag write bool boolean b string tag throws io exception print begin envelope tag stream print boolean stream print b stream print boolean print end envelope tag write int string tag throws io exception print begin envelope tag stream print stream print integer string stream print print end envelope tag write long string tag throws io exception print begin envelope tag stream print ex stream print long string stream print ex print end envelope tag write float f string tag throws io exception print begin envelope tag stream print ex stream print float string f stream print ex print end envelope tag write double string tag throws io exception print begin envelope tag stream print stream print double string stream print print end envelope tag write string string string tag throws io exception print begin envelope tag stream print stream print utils xml string stream print print end envelope tag write buffer buffer buf string tag throws io exception print begin envelope tag stream print stream print utils xml buffer buf stream print print end envelope tag start record record r string tag throws io exception inside record tag stream print n add indent end record record r string tag throws io exception close indent put indent
448	common\src\java\org\apache\hadoop\record\compiler\CGenerator.java	unrelated	package org apache hadoop record compiler c code generator front end hadoop record i o c generator extends code generator c generator generate c code this method creates requested file spits file level elements statements etc record level code generated j record gen code string name array list j file ilist array list j record rlist string dest dir array list string options throws io exception name new file dest dir new file name get name get absolute path file writer cc new file writer name c try file writer hh new file writer name try hh write ifndef name upper case replace n hh write define name upper case replace n hh write recordio n iterator j file iter ilist iterator iter next hh write iter next get name n cc write name n iterator j record iter rlist iterator iter next iter next gen cpp code hh cc hh write endif name upper case replace n finally hh close finally cc close
449	common\src\java\org\apache\hadoop\record\compiler\CodeBuffer.java	unrelated	package org apache hadoop record compiler a wrapper around string buffer automatically indentation code buffer array list character start markers new array list character array list character end markers new array list character add markers add markers add markers char ch char ch start markers add ch end markers add ch level num spaces boolean first char true string buffer sb creates new instance code buffer code buffer code buffer string code buffer num spaces string sb new string buffer num spaces num spaces append append string length length idx idx length idx char ch char at idx append ch append char ch end markers contains ch level first char idx idx level idx num num num spaces num raw append raw append ch first char false start markers contains ch level ch n first char true raw append char ch sb append ch string string return sb string
450	common\src\java\org\apache\hadoop\record\compiler\CodeGenerator.java	unrelated	package org apache hadoop record compiler code generator factory base hadoop record i o translators different translators register creation methods factory code generator hash map string code generator generators new hash map string code generator register c new c generator register c new cpp generator register java new java generator register string lang code generator gen generators put lang gen code generator get string lang return generators get lang gen code string file array list j file incl files array list j record records string dest dir array list string options throws io exception
451	common\src\java\org\apache\hadoop\record\compiler\Consts.java	unrelated	package org apache hadoop record compiler definitions record i o compiler consts cannot create new instance consts prefix use variables generated string rio prefix rio vars used generated string rti var rio prefix rec type info string rti filter rio prefix rti filter string rti filter fields rio prefix rti filter fields string record output rio prefix string record input rio prefix string tag rio prefix tag
452	common\src\java\org\apache\hadoop\record\compiler\CppGenerator.java	unrelated	package org apache hadoop record compiler c code generator front end hadoop record i o cpp generator extends code generator cpp generator generate c code this method creates requested file spits file level elements statements etc record level code generated j record gen code string name array list j file ilist array list j record rlist string dest dir array list string options throws io exception name new file dest dir new file name get name get absolute path file writer cc new file writer name cc try file writer hh new file writer name hh try string file name new file name get name hh write ifndef file name upper case replace n hh write define file name upper case replace n hh write recordio hh n hh write record type info hh n iterator j file iter ilist iterator iter next hh write iter next get name hh n cc write file name hh n cc write utils hh n iterator j record iter rlist iterator iter next iter next gen cpp code hh cc options hh write endif file name upper case replace n finally hh close finally cc close
453	common\src\java\org\apache\hadoop\record\compiler\JavaGenerator.java	unrelated	package org apache hadoop record compiler java code generator front end hadoop record i o java generator extends code generator java generator generate java code records this method front end j record since one file generated record gen code string name array list j file ilist array list j record rlist string dest dir array list string options throws io exception iterator j record iter rlist iterator iter next j record rec iter next rec gen java code dest dir options
454	common\src\java\org\apache\hadoop\record\compiler\JBoolean.java	unrelated	package org apache hadoop record compiler j boolean extends j type java boolean extends j type java type java boolean super boolean bool boolean type id rio type bool gen compare to code buffer cb string fname string cb append consts rio prefix ret fname fname n string get type id object string return org apache hadoop record meta type id bool type id gen hash code code buffer cb string fname cb append consts rio prefix ret fname n in binary format boolean written byte true false gen slurp bytes code buffer cb string b string string cb append n cb append n cb append throw new java io io exception boolean exactly byte provided buffer smaller n cb append n cb append n cb append n in binary format boolean written byte true false gen compare bytes code buffer cb cb append n cb append n cb append throw new java io io exception boolean exactly byte provided buffer smaller n cb append n cb append b b n cb append return b b n cb append n cb append n cb append n cpp boolean extends cpp type cpp boolean super bool string get type id object string return new hadoop type id hadoop riotype bool creates new instance j boolean j boolean set java type new java boolean set cpp type new cpp boolean set c type new c type string get signature return z
455	common\src\java\org\apache\hadoop\record\compiler\JBuffer.java	unrelated	package org apache hadoop record compiler code generator buffer type j buffer extends j comp type java buffer extends java comp type java buffer super org apache hadoop record buffer buffer org apache hadoop record buffer type id rio type buffer string get type id object string return org apache hadoop record meta type id buffer type id gen compare to code buffer cb string fname string cb append consts rio prefix ret fname compare to n gen equals code buffer cb string fname string peer cb append consts rio prefix ret fname equals peer n gen hash code code buffer cb string fname cb append consts rio prefix ret fname hash code n gen slurp bytes code buffer cb string b string string cb append n cb append org apache hadoop record utils read v int b n cb append z org apache hadoop record utils get v int size n cb append z z n cb append n gen compare bytes code buffer cb cb append n cb append org apache hadoop record utils read v int b n cb append org apache hadoop record utils read v int b n cb append z org apache hadoop record utils get v int size n cb append z org apache hadoop record utils get v int size n cb append z z z z n cb append r org apache hadoop record utils compare bytes b b n cb append r return r n cb append n cb append n cpp buffer extends cpp comp type cpp buffer super std gen get set code buffer cb string fname cb append virtual get type get camel case fname n cb append return fname n cb append n cb append virtual get type get camel case fname n cb append return fname n cb append n string get type id object string return new hadoop type id hadoop riotype buffer creates new instance j buffer j buffer set java type new java buffer set cpp type new cpp buffer set c type new c comp type string get signature return b
456	common\src\java\org\apache\hadoop\record\compiler\JByte.java	unrelated	package org apache hadoop record compiler code generator byte type j byte extends j type java byte extends java type java byte super byte byte byte type id rio type byte string get type id object string return org apache hadoop record meta type id byte type id gen slurp bytes code buffer cb string b string string cb append n cb append n cb append throw new java io io exception byte exactly byte provided buffer smaller n cb append n cb append n cb append n gen compare bytes code buffer cb cb append n cb append n cb append throw new java io io exception byte exactly byte provided buffer smaller n cb append n cb append b b n cb append return b b n cb append n cb append n cb append n cpp byte extends cpp type cpp byte super string get type id object string return new hadoop type id hadoop riotype byte j byte set java type new java byte set cpp type new cpp byte set c type new c type string get signature return b
457	common\src\java\org\apache\hadoop\record\compiler\JCompType.java	unrelated	package org apache hadoop record compiler abstract base compound types ustring buffer vector map record j comp type extends j type java comp type extends java type java comp type string type string suffix string wrapper string type id byte string super type suffix wrapper type id byte string gen compare to code buffer cb string fname string cb append consts rio prefix ret fname compare to n gen equals code buffer cb string fname string peer cb append consts rio prefix ret fname equals peer n gen hash code code buffer cb string fname cb append consts rio prefix ret fname hash code n gen clone code buffer cb string fname cb append consts rio prefix fname get type fname clone n cpp comp type extends cpp type cpp comp type string type super type gen get set code buffer cb string fname cb append virtual get type get camel case fname n cb append return fname n cb append n cb append virtual get type get camel case fname n cb append return fname n cb append n c comp type extends c type
458	common\src\java\org\apache\hadoop\record\compiler\JDouble.java	unrelated	package org apache hadoop record compiler j double extends j type java double extends java type java double super double double type id rio type double string get type id object string return org apache hadoop record meta type id double type id gen hash code code buffer cb string fname string tmp double to long bits fname cb append consts rio prefix ret tmp tmp n gen slurp bytes code buffer cb string b string string cb append n cb append n cb append throw new java io io exception double exactly bytes provided buffer smaller n cb append n cb append n cb append n gen compare bytes code buffer cb cb append n cb append n cb append throw new java io io exception double exactly bytes provided buffer smaller n cb append n cb append org apache hadoop record utils read double b n cb append org apache hadoop record utils read double b n cb append n cb append return n cb append n cb append n cb append n cpp double extends cpp type cpp double super string get type id object string return new hadoop type id hadoop riotype double creates new instance j double j double set java type new java double set cpp type new cpp double set c type new c type string get signature return
459	common\src\java\org\apache\hadoop\record\compiler\JField.java	unrelated	package org apache hadoop record compiler a thin wrappper around record field j field t string name t type creates new instance j field j field string name t type type type name name string get name return name t get type return type
460	common\src\java\org\apache\hadoop\record\compiler\JFile.java	unrelated	package org apache hadoop record compiler container hadoop record ddl the main components file filename list included files records defined file j file possibly full name file string name ordered list included files array list j file incl files ordered list records declared file array list j record records creates new instance j file j file string name array list j file incl files array list j record rec list name name incl files incl files records rec list strip pathname components return basename string get name idx name last index of return idx name substring idx name generate record code given language language lowercase gen code string language string dest dir array list string options throws io exception code generator gen code generator get language gen null gen gen code name incl files records dest dir options else system err println cannnot recognize language language return return
461	common\src\java\org\apache\hadoop\record\compiler\JFloat.java	unrelated	package org apache hadoop record compiler j float extends j type java float extends java type java float super float float type id rio type float string get type id object string return org apache hadoop record meta type id float type id gen hash code code buffer cb string fname cb append consts rio prefix ret float to int bits fname n gen slurp bytes code buffer cb string b string string cb append n cb append n cb append throw new java io io exception float exactly bytes provided buffer smaller n cb append n cb append n cb append n gen compare bytes code buffer cb cb append n cb append n cb append throw new java io io exception float exactly bytes provided buffer smaller n cb append n cb append f org apache hadoop record utils read float b n cb append f org apache hadoop record utils read float b n cb append f f n cb append return f f n cb append n cb append n cb append n cpp float extends cpp type cpp float super string get type id object string return new hadoop type id hadoop riotype float creates new instance j float j float set java type new java float set cpp type new cpp float set c type new c type string get signature return f
462	common\src\java\org\apache\hadoop\record\compiler\JInt.java	unrelated	package org apache hadoop record compiler code generator type j int extends j type java int extends java type java int super int integer type id rio type int string get type id object string return org apache hadoop record meta type id int type id gen slurp bytes code buffer cb string b string string cb append n cb append org apache hadoop record utils read v int b n cb append z org apache hadoop record utils get v int size n cb append z z n cb append n gen compare bytes code buffer cb cb append n cb append org apache hadoop record utils read v int b n cb append org apache hadoop record utils read v int b n cb append n cb append return n cb append n cb append z org apache hadoop record utils get v int size n cb append z org apache hadoop record utils get v int size n cb append z z z z n cb append n cpp int extends cpp type cpp int super string get type id object string return new hadoop type id hadoop riotype int creates new instance j int j int set java type new java int set cpp type new cpp int set c type new c type string get signature return
463	common\src\java\org\apache\hadoop\record\compiler\JLong.java	unrelated	package org apache hadoop record compiler code generator type j long extends j type java long extends java type java long super long long type id rio type long string get type id object string return org apache hadoop record meta type id long type id gen hash code code buffer cb string fname cb append consts rio prefix ret fname fname n gen slurp bytes code buffer cb string b string string cb append n cb append org apache hadoop record utils read v long b n cb append z org apache hadoop record utils get v int size n cb append z z n cb append n gen compare bytes code buffer cb cb append n cb append org apache hadoop record utils read v long b n cb append org apache hadoop record utils read v long b n cb append n cb append return n cb append n cb append z org apache hadoop record utils get v int size n cb append z org apache hadoop record utils get v int size n cb append z z z z n cb append n cpp long extends cpp type cpp long super string get type id object string return new hadoop type id hadoop riotype long creates new instance j long j long set java type new java long set cpp type new cpp long set c type new c type string get signature return
464	common\src\java\org\apache\hadoop\record\compiler\JMap.java	unrelated	package org apache hadoop record compiler j map extends j comp type level string get level return integer string level incr level level decr level level string get id string id return id get level j type key type j type value type java map extends java comp type j type java type key j type java type value java map j type java type key j type java type value super java util tree map key get wrapper type value get wrapper type map java util tree map key get wrapper type value get wrapper type type id rio type map key key value value string get type id object string return new org apache hadoop record meta map type id key get type id object string value get type id object string gen set rti filter code buffer cb map string integer nested struct map key gen set rti filter cb nested struct map value gen set rti filter cb nested struct map gen compare to code buffer cb string fname string string set type java util set key get wrapper type string iter type java util iterator key get wrapper type cb append n cb append set type get id consts rio prefix set fname key set n cb append set type get id consts rio prefix set key set n cb append iter type get id consts rio prefix miter get id consts rio prefix set iterator n cb append iter type get id consts rio prefix miter get id consts rio prefix set iterator n cb append get id consts rio prefix miter next get id consts rio prefix miter next n cb append key get type get id consts rio prefix k get id consts rio prefix miter next n cb append key get type get id consts rio prefix k get id consts rio prefix miter next n key gen compare to cb get id consts rio prefix k get id consts rio prefix k cb append consts rio prefix ret return consts rio prefix ret n cb append n cb append consts rio prefix ret get id consts rio prefix set size get id consts rio prefix set size n cb append n gen read method code buffer cb string fname string tag boolean decl decl cb append get type fname n cb append n incr level cb append org apache hadoop record index get id consts rio prefix midx consts record input start map tag n cb append fname new get type n cb append get id consts rio prefix midx done get id consts rio prefix midx incr n key gen read method cb get id consts rio prefix k get id consts rio prefix k true value gen read method cb get id consts rio prefix v get id consts rio prefix v true cb append fname put get id consts rio prefix k get id consts rio prefix v n cb append n cb append consts record input end map tag n decr level cb append n gen write method
465	common\src\java\org\apache\hadoop\record\compiler\JRecord.java	unrelated	package org apache hadoop record compiler j record extends j comp type java record extends java comp type string full name string name string module array list j field java type fields new array list j field java type java record string name array list j field j type flist super name record name type id rio type struct full name name idx name last index of name name substring idx module name substring idx iterator j field j type iter flist iterator iter next j field j type f iter next fields add new j field java type f get name f get type get java type string get type id object string return new org apache hadoop record meta struct type id full name get type info gen set rti filter code buffer cb map string integer nested struct map ignore ev already set type filter record nested struct map contains key full name set rti filter cb append full name set type filter rti get nested struct type info name n nested struct map put full name null type info filter see similar one record since store type infos array lists thsi search o n squared we faster also store map type info index since setup rti fields called deserializing sticking former code easier gen setup rti fields code buffer cb cb append setup rti fields n n cb append null consts rti filter return n cb append may already done n cb append null consts rti filter fields return n cb append consts rio prefix consts rio prefix j n cb append consts rti filter fields new consts rio prefix rti filter get field type infos size n cb append consts rio prefix consts rio prefix consts rti filter fields length consts rio prefix n cb append consts rti filter fields consts rio prefix n cb append n cb append java util iterator org apache hadoop record meta field type info consts rio prefix filter consts rio prefix rti filter get field type infos iterator n cb append consts rio prefix n cb append consts rio prefix filter next n cb append org apache hadoop record meta field type info consts rio prefix info filter consts rio prefix filter next n cb append java util iterator org apache hadoop record meta field type info consts rio prefix consts rti var get field type infos iterator n cb append consts rio prefix j n cb append consts rio prefix next n cb append org apache hadoop record meta field type info consts rio prefix info consts rio prefix next n cb append consts rio prefix info equals consts rio prefix info filter n cb append consts rti filter fields consts rio prefix consts rio prefix j n cb append break n cb append n cb append consts rio prefix j n cb append n ct iterator j field java type fields iterator next ct j field java type jf next java type type jf get type string name jf get name ct cb append else type gen rti field
466	common\src\java\org\apache\hadoop\record\compiler\JString.java	unrelated	package org apache hadoop record compiler j string extends j comp type java string extends java comp type java string super string string string type id rio type string string get type id object string return org apache hadoop record meta type id string type id gen slurp bytes code buffer cb string b string string cb append n cb append org apache hadoop record utils read v int b n cb append z org apache hadoop record utils get v int size n cb append z z n cb append n gen compare bytes code buffer cb cb append n cb append org apache hadoop record utils read v int b n cb append org apache hadoop record utils read v int b n cb append z org apache hadoop record utils get v int size n cb append z org apache hadoop record utils get v int size n cb append z z z z n cb append r org apache hadoop record utils compare bytes b b n cb append r return r n cb append n cb append n gen clone code buffer cb string fname cb append consts rio prefix fname fname n cpp string extends cpp comp type cpp string super std string get type id object string return new hadoop type id hadoop riotype string creates new instance j string j string set java type new java string set cpp type new cpp string set c type new c comp type string get signature return
467	common\src\java\org\apache\hadoop\record\compiler\JType.java	unrelated	package org apache hadoop record compiler abstract base types supported hadoop record i o j type string camel case string name char first char name char at character lower case first char return character upper case first char name substring return name java type java type cpp type cpp type c type c type java type string name string method suffix string wrapper string type id byte string points type id rio type java type string javaname string suffix string wrapper string type id byte string name javaname method suffix suffix wrapper wrapper type id byte string type id byte string gen decl code buffer cb string fname cb append name fname n gen static type info code buffer cb string fname cb append consts rti var add field fname get type id object string n string get type id object string gen set rti filter code buffer cb map string integer nested struct map nothing default return gen rti field condition code buffer cb string fname ct cb append info field id equals fname type val org apache hadoop record meta get type id byte string n cb append rti filter fields ct n cb append n gen rti nested field condition code buffer cb string var name ct cb append var name get element type id get type val org apache hadoop record meta get type id byte string n cb append rti filter fields ct n cb append n gen constructor param code buffer cb string fname cb append name fname gen get set code buffer cb string fname cb append name get camel case fname n cb append return fname n cb append n cb append set camel case fname name fname n cb append fname fname n cb append n string get type return name string get wrapper type return wrapper string get method suffix return method suffix string get type id byte string return type id byte string gen write method code buffer cb string fname string tag cb append consts record output write method suffix fname tag n gen read method code buffer cb string fname string tag boolean decl decl cb append name fname n cb append fname consts record input read method suffix tag n gen compare to code buffer cb string fname string cb append consts rio prefix ret fname fname n gen compare bytes code buffer cb gen slurp bytes code buffer cb string b string string gen equals code buffer cb string fname string peer cb append consts rio prefix ret fname peer n gen hash code code buffer cb string fname cb append consts rio prefix ret fname n gen constructor set code buffer cb string fname cb append fname fname n gen clone code buffer cb string fname cb append consts rio prefix fname fname n cpp type string name cpp type string cppname name cppname gen decl code buffer cb string fname cb append name fname n gen static type info code buffer cb string fname cb append p add field new std fname get type id
468	common\src\java\org\apache\hadoop\record\compiler\JVector.java	unrelated	package org apache hadoop record compiler j vector extends j comp type level string get id string id return id get level string get level return integer string level incr level level decr level level j type type java vector extends java comp type j type java type element java vector j type java type super java util array list get wrapper type vector java util array list get wrapper type type id rio type vector element string get type id object string return new org apache hadoop record meta vector type id element get type id object string gen set rti filter code buffer cb map string integer nested struct map element gen set rti filter cb nested struct map gen compare to code buffer cb string fname string cb append n incr level cb append get id consts rio prefix len fname size n cb append get id consts rio prefix len size n cb append get id consts rio prefix vidx get id consts rio prefix vidx get id consts rio prefix len get id consts rio prefix vidx get id consts rio prefix len get id consts rio prefix vidx n cb append element get type get id consts rio prefix e fname get get id consts rio prefix vidx n cb append element get type get id consts rio prefix e get get id consts rio prefix vidx n element gen compare to cb get id consts rio prefix e get id consts rio prefix e cb append consts rio prefix ret return consts rio prefix ret n cb append n cb append consts rio prefix ret get id consts rio prefix len get id consts rio prefix len n decr level cb append n gen read method code buffer cb string fname string tag boolean decl decl cb append get type fname n cb append n incr level cb append org apache hadoop record index get id consts rio prefix vidx consts record input start vector tag n cb append fname new get type n cb append get id consts rio prefix vidx done get id consts rio prefix vidx incr n element gen read method cb get id consts rio prefix e get id consts rio prefix e true cb append fname add get id consts rio prefix e n cb append n cb append consts record input end vector tag n decr level cb append n gen write method code buffer cb string fname string tag cb append n incr level cb append consts record output start vector fname tag n cb append get id consts rio prefix len fname size n cb append get id consts rio prefix vidx get id consts rio prefix vidx get id consts rio prefix len get id consts rio prefix vidx n cb append element get type get id consts rio prefix e fname get get id consts rio prefix vidx n element gen write method cb get id consts rio prefix e get id consts rio prefix e cb append n cb append consts record output
469	common\src\java\org\apache\hadoop\record\compiler\ant\RccTask.java	unrelated	package org apache hadoop record compiler ant hadoop record compiler ant task p this task takes given record definition files compiles java c files it user compile generated files p the task requires code file code nested fileset element specified optional attributes code language code set output language default java code destdir code name destination directory generated java c code default code failonerror code specifies error handling behavior default true p usage pre lt recordcc destdir basedir gensrc language java gt lt fileset jr gt lt recordcc gt pre rcc task extends task string language java file src file dest new file array list file set filesets new array list file set boolean fail on error true creates new instance rcc task rcc task sets output language option set language string language language language sets record definition file attribute set file file file src file given multiple files via fileset set error handling behavior set failonerror boolean flag fail on error flag sets directory output files generated set destdir file dir dest dir adds fileset consist one files add fileset file set set filesets add set invoke hadoop record compiler record definition file execute throws build exception src null filesets size throw new build exception there must file attribute fileset child element src null compile src project project get project filesets size file set fs filesets get directory scanner ds fs get directory scanner project file dir fs get dir project string srcs ds get included files j j srcs length j compile new file dir srcs j compile file file throws build exception string args new string args language args language args destdir args dest get path args file get path ret val rcc driver args ret val fail on error throw new build exception hadoop record compiler returned error code ret val
470	common\src\java\org\apache\hadoop\record\compiler\generated\ParseException.java	unrelated	package org apache hadoop record compiler generated this exception thrown parse errors encountered you explicitly create objects exception type calling method generate parse exception generated parser you modify customize error reporting mechanisms retain fields parse exception extends exception this constructor used method generate parse exception generated parser calling constructor generates new object type fields current token expected token sequences token image set the boolean flag special constructor also set true indicate constructor used create object this constructor calls super empty force string method parent throwable print error message form parse exception result get message parse exception token current token val expected token sequences val string token image val super special constructor true current token current token val expected token sequences expected token sequences val token image token image val the following constructors use whatever purpose think constructing exception manner makes exception behave normal way e documented throwable the fields error token expected token sequences token image contain relevant information the java cc generated code use constructors parse exception super special constructor false parse exception string message super message special constructor false this variable determines constructor used create object thereby affects semantics get message method see protected boolean special constructor this last token consumed successfully if object created due parse error token followng token therefore first error token token current token each entry array array integers each array integers represents sequence tokens ordinal values expected point parse expected token sequences this reference token image array generated parser within parse error occurred this array defined generated constants string token image this method standard behavior object created using standard constructors otherwise uses current token expected token sequences generate parse error message returns if object created due parse error catch gets thrown parser method called printing stack trace hence correct error message gets displayed string get message special constructor return super get message string buffer expected new string buffer max size expected token sequences length max size expected token sequences length max size expected token sequences length j j expected token sequences length j expected append token image expected token sequences j append expected token sequences expected token sequences length expected append expected append eol append string retval encountered token tok current token next max size retval tok kind retval token image break retval add escapes tok image tok tok next retval line current token next begin line column current token next begin column retval eol expected token sequences length retval was expecting eol else retval was expecting one eol retval expected string return retval the end line machine protected string eol system get property line separator n used convert raw characters escaped version raw version cannot used part ascii literal protected string add escapes string str string buffer retval new string buffer char ch str length switch str char at case continue case b retval append b continue case retval append continue case n retval append n continue case f retval append f continue case r retval append r continue case retval append continue case retval append continue case retval append continue default ch str
471	common\src\java\org\apache\hadoop\record\compiler\generated\Rcc.java	unrelated	package org apache hadoop record compiler generated rcc implements rcc constants string language java string dest dir array list string rec files new array list string array list string cmdargs new array list string j file cur file hashtable string j record rec tab string cur dir string cur file name string cur module name main string args system exit driver args usage system err println usage rcc language java c ddl files driver string args args length equals ignore case args language equals ignore case args language args lower case else equals ignore case args destdir equals ignore case args dest dir args else args starts with string arg args substring arg starts with arg arg substring cmdargs add arg lower case else rec files add args rec files size usage return rec files size cur file name rec files get file file new file cur file name try file reader reader new file reader file rcc parser new rcc reader try rec tab new hashtable string j record cur file parser input catch parse exception e system err println e string return try reader close catch io exception e catch file not found exception e system err println file rec files get not found return try ret code cur file gen code language dest dir cmdargs ret code return ret code catch io exception e system err println e string return return j file input throws parse exception array list j file ilist new array list j file array list j record rlist new array list j record j file array list j record label true switch jj ntk jj ntk jj ntk case include tkn include ilist add break case module tkn module rlist add all break default jj la jj gen jj consume token throw new parse exception switch jj ntk jj ntk jj ntk case module tkn case include tkn break default jj la jj gen break label jj consume token true return new j file cur file name ilist rlist throw new error missing return statement function j file include throws parse exception string fname token jj consume token include tkn jj consume token cstring tkn j file ret null fname image replace all replace all file file new file cur dir fname string tmp dir cur dir string tmp file cur file name cur dir file get parent cur file name file get name try file reader reader new file reader file rcc parser new rcc reader try ret parser input system println fname parsed successfully catch parse exception e system println e string system exit try reader close catch io exception e catch file not found exception e system println file fname not found system exit cur dir tmp dir cur file name tmp file true return ret throw new error missing return statement function array list j record module throws parse exception string name array list j record rlist jj consume token module tkn name module name cur module name name jj consume token lbrace tkn rlist record list jj consume token rbrace
472	common\src\java\org\apache\hadoop\record\compiler\generated\RccConstants.java	unrelated	package org apache hadoop record compiler generated rcc constants eof module tkn record tkn include tkn byte tkn boolean tkn int tkn long tkn float tkn double tkn ustring tkn buffer tkn vector tkn map tkn lbrace tkn rbrace tkn lt tkn gt tkn semicolon tkn comma tkn dot tkn cstring tkn ident tkn default within one line comment within multi line comment string token image eof n r token kind token kind token kind module byte boolean ustring buffer vector map cstring tkn ident tkn
473	common\src\java\org\apache\hadoop\record\compiler\generated\RccTokenManager.java	unrelated	package org apache hadoop record compiler generated rcc token manager implements rcc constants java io print stream debug stream system set debug stream java io print stream ds debug stream ds jj move string literal dfa return jj move nfa jj check n add state jjrounds state jjround jjstate set jjnew state cnt state jjrounds state jjround jj add states start end jjstate set jjnew state cnt jjnext states start start end jj check n add two states state state jj check n add state jj check n add state jj check n add states start end jj check n add jjnext states start start end jj check n add states start jj check n add jjnext states start jj check n add jjnext states start jj move nfa start state cur pos next states starts at jjnew state cnt jjstate set start state j kind x fffffff jjround x fffffff re init rounds cur char l cur char match loop switch jjstate set case x l l kind kind cur char jjstate set jjnew state cnt break case cur char kind kind break case cur char jjstate set jjnew state cnt break default break starts at else cur char l cur char match loop switch jjstate set default break starts at else cur char xff l cur char match loop switch jjstate set default break starts at kind x fffffff jjmatched kind kind jjmatched pos cur pos kind x fffffff cur pos jjnew state cnt starts at jjnew state cnt starts at return cur pos try cur char input stream read char catch java io io exception e return cur pos jj stop string literal dfa pos active switch pos case active xfff l l jjmatched kind return return case active xfff l l jjmatched kind jjmatched pos return return case active x ef l l jjmatched kind jjmatched pos return active x l l return return case active x l l return active x cb l l jjmatched kind jjmatched pos return return case active x l l return active x l l jjmatched kind jjmatched pos return return case active x l l return active x l l jjmatched kind jjmatched pos return return default return jj start nfa pos active return jj move nfa jj stop string literal dfa pos active pos jj stop at pos pos kind jjmatched kind kind jjmatched pos pos return pos jj start nfa with states pos kind state jjmatched kind kind jjmatched pos pos try cur char input stream read char catch java io io exception e return pos return jj move nfa state pos jj move string literal dfa switch cur char case return jj stop at pos case return jj stop at pos case return jj move string literal dfa x l case return jj stop at pos case return jj stop at pos case return jj stop at pos case return jj move string literal dfa x c l case return jj move string literal dfa x l case return jj move string literal dfa x l case return
474	common\src\java\org\apache\hadoop\record\compiler\generated\SimpleCharStream.java	unrelated	package org apache hadoop record compiler generated an implementation char stream stream assumed contain ascii characters without unicode processing simple char stream boolean flag false bufsize available token begin bufpos protected bufline protected bufcolumn protected column protected line protected boolean prev char is cr false protected boolean prev char is lf false protected java io reader input stream protected char buffer protected max next char ind protected buf protected tab size protected set tab size tab size protected get tab size return tab size protected expand buff boolean wrap around char newbuffer new char bufsize newbufline new bufsize newbufcolumn new bufsize try wrap around system arraycopy buffer token begin newbuffer bufsize token begin system arraycopy buffer newbuffer bufsize token begin bufpos buffer newbuffer system arraycopy bufline token begin newbufline bufsize token begin system arraycopy bufline newbufline bufsize token begin bufpos bufline newbufline system arraycopy bufcolumn token begin newbufcolumn bufsize token begin system arraycopy bufcolumn newbufcolumn bufsize token begin bufpos bufcolumn newbufcolumn max next char ind bufpos bufsize token begin else system arraycopy buffer token begin newbuffer bufsize token begin buffer newbuffer system arraycopy bufline token begin newbufline bufsize token begin bufline newbufline system arraycopy bufcolumn token begin newbufcolumn bufsize token begin bufcolumn newbufcolumn max next char ind bufpos token begin catch throwable throw new error get message bufsize available bufsize token begin protected fill buff throws java io io exception max next char ind available available bufsize token begin bufpos max next char ind available token begin else token begin bufpos max next char ind else expand buff false else available token begin available bufsize else token begin available expand buff true else available token begin try input stream read buffer max next char ind available max next char ind input stream close throw new java io io exception else max next char ind return catch java io io exception e bufpos backup token begin token begin bufpos throw e char begin token throws java io io exception token begin char c read char token begin bufpos return c protected update line column char c column prev char is lf prev char is lf false line column else prev char is cr prev char is cr false c n prev char is lf true else line column switch c case r prev char is cr true break case n prev char is lf true break case column column tab size column tab size break default break bufline bufpos line bufcolumn bufpos column char read char throws java io io exception buf buf bufpos bufsize bufpos return buffer bufpos bufpos max next char ind fill buff char c buffer bufpos update line column c return c get end column return bufcolumn bufpos get end line return bufline bufpos get begin column return bufcolumn token begin get begin line return bufline token begin backup amount buf amount bufpos amount bufpos bufsize simple char stream java io reader dstream startline startcolumn buffersize input stream dstream line startline column startcolumn available bufsize buffersize buffer new char buffersize bufline new buffersize bufcolumn new buffersize simple char stream
475	common\src\java\org\apache\hadoop\record\compiler\generated\Token.java	unrelated	package org apache hadoop record compiler generated describes input token stream token an integer describes kind token this numbering system determined java cc parser table numbers stored file constants java kind begin line begin column describe position first character token end line end column describe position last character token begin line begin column end line end column the image token string image a reference next regular non special token input stream if last token input stream token manager read tokens beyond one field set null this true token also regular token otherwise see description contents field token next this field used access special tokens occur prior token immediately preceding regular non special token if special tokens field set null when one special token field refers last special tokens turn refers next previous special token special token field first special token whose special token field null the next fields special tokens refer special tokens immediately follow without intervening regular token if token field null token special token returns image string string return image returns new token object default however want create return subclass objects based value kind simply add cases switch special cases for example subclass token called id token want create kind id simlpy add something like case my parser constants id return new id token following switch statement then cast matched token variable appropriate type use lexical actions token new token kind switch kind default return new token
476	common\src\java\org\apache\hadoop\record\compiler\generated\TokenMgrError.java	unrelated	package org apache hadoop record compiler generated token mgr error extends error ordinals various reasons error type thrown lexical error occured lexical error an attempt wass made create second instance token manager static lexer error tried change invalid lexical state invalid lexical state detected bailed infinite loop token manager loop detected indicates reason exception thrown it one values error code replaces unprintable characters espaced unicode escaped equivalents given protected string add escapes string str string buffer retval new string buffer char ch str length switch str char at case continue case b retval append b continue case retval append continue case n retval append n continue case f retval append f continue case r retval append r continue case retval append continue case retval append continue case retval append continue default ch str char at x ch x e string integer string ch retval append u substring length length else retval append ch continue return retval string returns detailed message error thrown token manager indicate lexical error parameters eof seen indicates eof caused lexicl error cur lex state lexical state error occured error line line number error occured error column column number error occured error after prefix seen error occured curchar offending character note you customize lexical error message modifying method protected string lexical error boolean eof seen lex state error line error column string error after char cur char return lexical error line error line column error column encountered eof seen eof add escapes string value of cur char cur char add escapes error after you also modify body method customize error messages for example cases like loop detected invalid lexical state end users concern return something like internal error please file bug report method cases release version parser string get message return super get message constructors various flavors follow token mgr error token mgr error string message reason super message error code reason token mgr error boolean eof seen lex state error line error column string error after char cur char reason lexical error eof seen lex state error line error column error after cur char reason
477	common\src\java\org\apache\hadoop\record\meta\FieldTypeInfo.java	unrelated	package org apache hadoop record meta represents type information field made id name type type id object field type info string field id type id type id construct filed type info given field name type field type info string field id type id type id field id field id type id type id get field type id object type id get type id return type id get field id name string get field id return field id write record output rout string tag throws io exception rout write string field id tag type id write rout tag two field type infos equal ach fields matches boolean equals object return true instanceof field type info return false field type info fti field type info first check field id matches field id equals fti field id return false see type id matches return type id equals fti type id we use basic hashcode implementation since likely used hashmap key hash code return type id hash code field id hash code boolean equals field type info ti first check field id matches field id equals ti field id return false see type id matches return type id equals ti type id
478	common\src\java\org\apache\hadoop\record\meta\MapTypeID.java	unrelated	package org apache hadoop record meta represents type id map map type id extends type id type id type id key type id type id value map type id type id type id key type id type id value super rio type map type id key type id key type id value type id value get type id map key element type id get key type id return type id key get type id map value element type id get value type id return type id value write record output rout string tag throws io exception rout write byte type val tag type id key write rout tag type id value write rout tag two map type i ds equal constituent elements type boolean equals object super equals return false map type id mti map type id return type id key equals mti type id key type id value equals mti type id value we use basic hashcode implementation since likely used hashmap key hash code return type id key hash code type id value hash code
479	common\src\java\org\apache\hadoop\record\meta\RecordTypeInfo.java	unrelated	package org apache hadoop record meta a record type information object read write type information record comprises metadata record well collection type information field record record type info extends org apache hadoop record record string name a record type info really wrapper around struct type id struct type id tid a record type info object collection type info objects fields array list field type info type infos new array list field type info keep hashmap record names type information need set filters reading nested structs this map used deserialization map string record type info rt is new hash map string record type info create empty record type info object record type info tid new struct type id create record type info object representing record given name record type info string name name name tid new struct type id constructor record type info string name struct type id stid tid stid name name return name record string get name return name set name record set name string name name name add field add field string field name type id tid tid get field type infos add new field type info field name tid add all collection field type info tis tid get field type infos add all tis return collection field type infos collection field type info get field type infos return tid get field type infos return type info nested record we consider nesting one level record type info get nested struct type info string name struct type id stid tid find struct name null stid return null return new record type info name stid serialize type information record serialize record output rout string tag throws io exception write header version info rout start record tag rout write string name tag tid write rest rout tag rout end record tag deserialize type information record deserialize record input rin string tag throws io exception read header version info rin start record tag name name rin read string tag tid read rin tag rin end record tag this implement comparable meant used anything besides de serializing so always throw exception not implemented always returns another record type info passed compare to object peer throws class cast exception peer instanceof record type info throw new class cast exception comparing different types records throw new unsupported operation exception compare to supported
480	common\src\java\org\apache\hadoop\record\meta\StructTypeID.java	unrelated	package org apache hadoop record meta represents type id struct type id extends type id array list field type info type infos new array list field type info struct type id super rio type struct create struct type id based record type info record struct type id record type info rti super rio type struct type infos add all rti get field type infos add field type info ti type infos add ti collection field type info get field type infos return type infos return struct typei d given field struct type id find struct string name walk list searching not efficient way intended used rarely keep simple as optimization keep hashmap record name rti later field type info ti type infos ti get field id compare to name ti get type id get type val rio type struct return struct type id ti get type id return null write record output rout string tag throws io exception rout write byte type val tag write rest rout tag writes rest excluding type value as optimization method directly called rti top level record write byte indicating since top level records always structs write rest record output rout string tag throws io exception rout write int type infos size tag field type info ti type infos ti write rout tag deserialize called rti read record input rin string tag throws io exception number elements num elems rin read int tag num elems type infos add generic read type info rin tag generic reader reads next type info object stream returns field type info generic read type info record input rin string tag throws io exception string field name rin read string tag type id id generic read type id rin tag return new field type info field name id generic reader reads next type id object stream returns type id generic read type id record input rin string tag throws io exception byte type val rin read byte tag switch type val case type id rio type bool return type id bool type id case type id rio type buffer return type id buffer type id case type id rio type byte return type id byte type id case type id rio type double return type id double type id case type id rio type float return type id float type id case type id rio type int return type id int type id case type id rio type long return type id long type id case type id rio type map type id id key generic read type id rin tag type id id value generic read type id rin tag return new map type id id key id value case type id rio type string return type id string type id case type id rio type struct struct type id st id new struct type id num elems rin read int tag num elems st id add generic read type info rin tag return st id case type id rio type vector type id id generic read type id rin tag return
481	common\src\java\org\apache\hadoop\record\meta\TypeID.java	unrelated	package org apache hadoop record meta represents type id basic types type id constants representing idl types support rio type byte bool byte buffer byte byte byte double byte float byte int byte long byte map byte string byte struct byte vector constant basic types share type id bool type id new type id rio type bool type id buffer type id new type id rio type buffer type id byte type id new type id rio type byte type id double type id new type id rio type double type id float type id new type id rio type float type id int type id new type id rio type int type id long type id new type id rio type long type id string type id new type id rio type string protected byte type val create type id object type id byte type val type val type val get type value one constants rio type byte get type val return type val serialize type id object write record output rout string tag throws io exception rout write byte type val tag two base type i ds equal refer type boolean equals object return true null return false get class get class return false type id type id type id return type val type id type val we use basic hashcode implementation since likely used hashmap key hash code see effectve java joshua bloch return type val
482	common\src\java\org\apache\hadoop\record\meta\Utils.java	unrelated	package org apache hadoop record meta various utility functions hadooop record i o platform utils cannot create new instance utils utils read skip bytes stream based type skip record input rin string tag type id type id throws io exception switch type id type val case type id rio type bool rin read bool tag break case type id rio type buffer rin read buffer tag break case type id rio type byte rin read byte tag break case type id rio type double rin read double tag break case type id rio type float rin read float tag break case type id rio type int rin read int tag break case type id rio type long rin read long tag break case type id rio type map org apache hadoop record index midx rin start map tag map type id mt id map type id type id midx done midx incr skip rin tag mt id get key type id skip rin tag mt id get value type id rin end map tag break case type id rio type string rin read string tag break case type id rio type struct rin start record tag read past field struct type id st id struct type id type id iterator field type info st id get field type infos iterator next field type info info next skip rin tag info get type id rin end record tag break case type id rio type vector org apache hadoop record index vidx rin start vector tag vector type id vt id vector type id type id vidx done vidx incr skip rin tag vt id get element type id rin end vector tag break default throw new io exception unknown type id skipping bytes
483	common\src\java\org\apache\hadoop\record\meta\VectorTypeID.java	unrelated	package org apache hadoop record meta represents type id vector vector type id extends type id type id type id element vector type id type id type id element super rio type vector type id element type id element type id get element type id return type id element write record output rout string tag throws io exception rout write byte type val tag type id element write rout tag two vector type i ds equal constituent elements type boolean equals object super equals return false vector type id vti vector type id return type id element equals vti type id element we use basic hashcode implementation since likely used hashmap key hash code return type id element hash code
484	common\src\java\org\apache\hadoop\security\AccessControlException.java	unrelated	package org apache hadoop security an exception access control related issues access control exception extends org apache hadoop fs permission access control exception required link java io serializable serial version uid l default constructor needed unwrapping link org apache hadoop ipc remote exception access control exception super permission denied constructs link access control exception specified detail message access control exception string super constructs new exception specified cause detail message tt cause null null cause string tt typically contains detail message tt cause tt link get cause method a tt null tt value permitted indicates cause nonexistent unknown access control exception throwable cause super cause
485	common\src\java\org\apache\hadoop\security\AnnotatedSecurityInfo.java	unrelated	package org apache hadoop security constructs security info annotations provided protocol annotated security info extends security info kerberos info get kerberos info class protocol configuration conf return protocol get annotation kerberos info token info get token info class protocol configuration conf return protocol get annotation token info
486	common\src\java\org\apache\hadoop\security\Credentials.java	unrelated	package org apache hadoop security a provides facilities reading writing secret keys tokens credentials implements writable log log log factory get log credentials map text byte secret keys map new hash map text byte map text token extends token identifier token map new hash map text token extends token identifier returns key bytes alias byte get secret key text alias return secret keys map get alias returns token object alias token extends token identifier get token text alias return token map get alias add token storage memory add token text alias token extends token identifier null token map put alias else log warn null token ignored alias return tokens memory map collection token extends token identifier get all tokens return token map values number of tokens return token map size number of secret keys return secret keys map size set key alias add secret key text alias byte key secret keys map put alias key convenience method reading token storage file loading tokens therein passed ugi credentials read token storage file path filename configuration conf throws io exception fs data input stream null credentials credentials new credentials try filename get file system conf open filename credentials read token storage stream close return credentials catch io exception ioe io utils cleanup log throw new io exception exception reading filename ioe convenience method reading token storage file directly datainputstream read token storage stream data input stream throws io exception byte magic new byte token storage magic length read fully magic arrays equals magic token storage magic throw new io exception bad header found token storage byte version read byte version token storage version throw new io exception unknown version version token storage read fields byte token storage magic hdts get bytes byte token storage version write token storage to stream data output stream os throws io exception os write token storage magic os write token storage version write os write token storage file path filename configuration conf throws io exception fs data output stream os filename get file system conf create filename write token storage to stream os os close stores keys data output write data output throws io exception write tokens first writable utils write v int token map size map entry text token extends token identifier e token map entry set e get key write e get value write write secret keys writable utils write v int secret keys map size map entry text byte e secret keys map entry set e get key write writable utils write v int e get value length write e get value loads keys read fields data input throws io exception secret keys map clear token map clear size writable utils read v int size text alias new text alias read fields token extends token identifier new token token identifier read fields token map put alias size writable utils read v int size text alias new text alias read fields len writable utils read v int byte value new byte len read fully value secret keys map put alias value copy credentials one credential object another
487	common\src\java\org\apache\hadoop\security\GroupMappingServiceProvider.java	unrelated	package org apache hadoop security an implementation user groups mapping service used link groups group mapping service provider get various group memberships given user returns empty list case non existing user list string get groups string user throws io exception refresh cache groups user mapping cache groups refresh throws io exception caches group user information cache groups add list string groups throws io exception
488	common\src\java\org\apache\hadoop\security\Groups.java	unrelated	package org apache hadoop security a user groups mapping service link groups allows server get various group memberships given user via link get groups string call thus ensuring consistent user groups mapping protects vagaries different mappings servers clients hadoop cluster groups log log log factory get log groups group mapping service provider impl map string cached groups user to groups map new concurrent hash map string cached groups cache timeout groups configuration conf impl reflection utils new instance conf get class common configuration keys hadoop security group mapping shell based unix groups mapping group mapping service provider conf cache timeout conf get long common configuration keys hadoop security groups cache secs log debug enabled log debug group mapping impl impl get class get name cache timeout cache timeout get group memberships given user list string get groups string user throws io exception return cached value available cached groups groups user to groups map get user system current time millis cache value expired groups null groups get timestamp cache timeout log debug enabled log debug returning cached groups user return groups get groups create cache user groups groups new cached groups impl get groups user user to groups map put user groups log debug enabled log debug returning fetched groups user return groups get groups refresh user groups mappings refresh log info clearing user to groups map cache try impl cache groups refresh catch io exception e log warn error refreshing groups cache e user to groups map clear add groups cache cache groups add list string groups try impl cache groups add groups catch io exception e log warn error caching groups e class hold cached groups cached groups timestamp list string groups create initialize group cache cached groups list string groups groups groups timestamp system current time millis returns time last cache update get timestamp return timestamp get list cached groups list string get groups return groups groups groups null get groups used map user groups groups get user to groups mapping service return get user to groups mapping service new configuration get groups used map user groups synchronized groups get user to groups mapping service configuration conf groups null log debug enabled log debug creating new groups object groups new groups conf return groups
489	common\src\java\org\apache\hadoop\security\JniBasedUnixGroupsMapping.java	unrelated	package org apache hadoop security a jni based implementation link group mapping service provider invokes lib c calls get group memberships given user jni based unix groups mapping implements group mapping service provider log log log factory get log jni based unix groups mapping native string get group for user string user native code loader native code loaded throw new runtime exception bailing since native library loaded log info using jni based unix groups mapping group resolution list string get groups string user throws io exception string groups new string try groups get group for user user catch exception e log warn error getting groups user e return arrays list groups cache groups refresh throws io exception nothing provider user groups mapping cache groups add list string groups throws io exception nothing provider user groups mapping
490	common\src\java\org\apache\hadoop\security\JniBasedUnixGroupsNetgroupMapping.java	unrelated	package org apache hadoop security a jni based implementation link group mapping service provider invokes lib c calls get group memberships given user jni based unix groups netgroup mapping extends jni based unix groups mapping log log log factory get log jni based unix groups netgroup mapping native string get users for netgroup jni string group native code loader native code loaded throw new runtime exception bailing since native library loaded log info using jni based unix groups netgroup mapping netgroup resolution gets unix groups netgroups user it gets unix groups returned id gn returns netgroups used ac ls way get netgroups given user see documentation getent netgroup list string get groups string user throws io exception parent gets unix groups list string groups new linked list string super get groups user netgroup cache get netgroups user groups return groups refresh netgroup cache cache groups refresh throws io exception list string groups netgroup cache get netgroup names netgroup cache clear cache groups add groups add group cache netgroups cached cache groups add list string groups throws io exception string group groups group length better safe sorry never happen else group char at netgroup cache cached group netgroup cache add group get users for netgroup group else unix group caching calls jni function get users netgroup since c functions reentrant need make synchronized see documentation setnetgrent getnetgrent endnetgrent protected synchronized list string get users for netgroup string netgroup string users null try jni code expect begining group name users get users for netgroup jni netgroup substring catch exception e log warn error getting users netgroup netgroup e users null users length return arrays list users return new linked list string
491	common\src\java\org\apache\hadoop\security\KerberosInfo.java	authenticate	package org apache hadoop security indicates kerberos related information used kerberos info key getting server kerberos principal name configuration string server principal string client principal default
492	common\src\java\org\apache\hadoop\security\KerberosName.java	authenticate	package org apache hadoop security this implements parsing handling kerberos principal names in particular splits apart translates local operating system names kerberos name the first component name string service name the second component name it may null string host name the realm name string realm a pattern matches kerberos name components pattern name parser pattern compile a pattern matches single parameter n pattern parameter pattern pattern compile a pattern parsing auth local rule pattern rule parser pattern compile default rule g a pattern recognizes simple non simple names pattern non simple pattern pattern compile the list translation rules list rule rules string default realm config kerb conf try kerb conf config get instance default realm kerb conf get default realm catch krb exception ke user group information security enabled throw new illegal argument exception can get kerberos configuration ke else default realm create name full kerberos principal name kerberos name string name matcher match name parser matcher name match matches name contains throw new illegal argument exception malformed kerberos name name else service name name host name null realm null else service name match group host name match group realm match group get configured default realm string get default realm return default realm put name back together parts string string string builder result new string builder result append service name host name null result append result append host name realm null result append result append realm return result string get first component name string get service name return service name get second component name string get host name return host name get realm name string get realm return realm an encoding rule translating kerberos names rule boolean default num of components string format pattern match pattern pattern string pattern boolean repeat rule default true num of components format null match null pattern null pattern null repeat false rule num of components string format string match string pattern string pattern boolean repeat default false num of components num of components format format match match null null pattern compile match pattern pattern null null pattern compile pattern pattern pattern repeat repeat string string string builder buf new string builder default buf append default else buf append rule buf append num of components buf append buf append format buf append match null buf append buf append match buf append pattern null buf append buf append pattern buf append buf append pattern buf append repeat buf append g return buf string replace numbered parameters form n n length params normal text copied directly n replaced corresponding parameter string replace parameters string format string params throws bad format string matcher match parameter pattern matcher format start string builder result new string builder start format length match find start result append match group string param num match group param num null try num integer parse int param num num num params length throw new bad format string index num format outside valid range params length result append params num catch number format exception nfe throw new bad format string bad format username mapping param num nfe start match
493	common\src\java\org\apache\hadoop\security\Krb5AndCertsSslSocketConnector.java	authenticate	package org apache hadoop security extend jetty link ssl socket connector optionally also provide kerberos ized ssl sockets the change behavior superclass longer honor requests turn need authentication running kerberos support krb and certs ssl socket connector extends ssl socket connector list string krb cipher suites collections unmodifiable list collections singleton list tls krb with des ede cbc sha system set property https cipher suites krb cipher suites get log log log factory get log krb and certs ssl socket connector string remote principal remote principal enum mode krb certs both support kerberos certificates boolean use krb boolean use certs krb and certs ssl socket connector super use krb true use certs false set passwords krb and certs ssl socket connector mode mode super use krb mode mode krb mode mode both use certs mode mode certs mode mode both set passwords log if debug use kerb use krb use certs use certs if using certs set passwords random gibberish else jetty actually prompt user set passwords use certs random r new random system set property jetty ssl password string value of r next long system set property jetty ssl keypassword string value of r next long protected ssl server socket factory create factory throws exception use certs return super create factory ssl context context super get provider null ssl context get instance super get protocol ssl context get instance super get protocol super get provider context init null null null return context get server socket factory non javadoc protected server socket new server socket string host port backlog throws io exception log if debug creating new krb server socket host ssl server socket ss null use certs get server socket ssl super impl ss ssl server socket super new server socket host port backlog else create default server socket try ss ssl server socket host null create factory create server socket port backlog create factory create server socket port backlog inet address get by name host catch exception e log warn could create krb listener e throw new io exception could create krb listener e string add kerberos ciphers socket server needed use krb ss set need client auth true string combined use certs combine cipher suites string certs ss get enabled cipher suites combined new string certs length krb cipher suites size system arraycopy certs combined certs length system arraycopy krb cipher suites array new string combined certs length krb cipher suites size else just enable kerberos auth combined krb cipher suites array new string ss set enabled cipher suites combined return ss customize end point endpoint request request throws io exception use krb add kerberos specific info ssl socket ssl socket ssl socket endpoint get transport principal remote principal ssl socket get session get peer principal log if debug remote principal remote principal request set scheme http schemes https request set attribute remote principal remote principal use certs add extra info would added super string cipher suite ssl socket get session get cipher suite integer key size integer value of servlet ssl deduce key length cipher suite request set
494	common\src\java\org\apache\hadoop\security\NetgroupCache.java	unrelated	package org apache hadoop security class caches netgroups inverts group user map user group map primarily intented use netgroups returned getent netgrgoup returns group user mapping netgroup cache log log log factory get log netgroup cache boolean netgroup to users map updated true map string set string netgroup to users map new concurrent hash map string set string map string set string user to netgroups map new concurrent hash map string set string get netgroups given user get netgroups string user list string groups netgroup to users map updated netgroup to users map updated false beginning avoid race update user to netgroups map string netgroup netgroup to users map key set string netuser netgroup to users map get netgroup add user to netgroups map user to netgroups map contains key netuser user to netgroups map put netuser new hash set string user to netgroups map get netuser add netgroup user to netgroups map contains key user string netgroup user to netgroups map get user groups add netgroup get list cached netgroups list string get netgroup names return new linked list string netgroup to users map key set returns true given netgroup cached boolean cached string group return netgroup to users map contains key group clear cache clear netgroup to users map clear add group cache add string group list string users cached group netgroup to users map put group new hash set string string user users netgroup to users map get group add user netgroup to users map updated true end avoid race
495	common\src\java\org\apache\hadoop\security\RefreshUserMappingsProtocol.java	unrelated	package org apache hadoop security protocol use server principal common configuration keys hadoop security service user name key refresh user mappings protocol extends versioned protocol version initial version version id l refresh user group mappings refresh user to groups mappings throws io exception refresh superuser proxy group list refresh super user groups configuration throws io exception
496	common\src\java\org\apache\hadoop\security\SaslInputStream.java	unrelated	package org apache hadoop security a sasl input stream composed input stream sasl server sasl client read methods return data read underlying input stream additionally processed sasl server sasl client object the sasl server sasl client object must fully initialized used sasl input stream sasl input stream extends input stream log log log factory get log sasl input stream data input stream stream should wrap communication channel boolean use wrap data read underlying input stream processed sasl byte sasl token sasl client sasl client sasl server sasl server byte length buf new byte buffer holding data processed sasl read byte obuffer position next new byte ostart position last new byte ofinish unsigned bytes to int byte buf buf length throw new illegal argument exception cannot handle byte array bytes result result result buf xff return result read data get processed br entry condition ostart ofinish br exit condition ostart ofinish br return ofinish ostart many bytes data could later absolutely data read more data throws io exception try stream read fully length buf length unsigned bytes to int length buf log debug enabled log debug actual length length sasl token new byte length stream read fully sasl token catch eof exception e return try sasl server null using sasl server obuffer sasl server unwrap sasl token sasl token length else using sasl client obuffer sasl client unwrap sasl token sasl token length catch sasl exception se try dispose sasl catch sasl exception ignored throw se ostart obuffer null ofinish else ofinish obuffer length return ofinish disposes system resources security sensitive information sasl might using sasl error occurs dispose sasl throws sasl exception sasl client null sasl client dispose sasl server null sasl server dispose constructs sasl input stream input stream sasl server br note specified input stream sasl server null null pointer exception may thrown later used input stream processed initialized sasl server object sasl input stream input stream stream sasl server sasl server stream new data input stream stream sasl server sasl server sasl client null string qop string sasl server get negotiated property sasl qop use wrap qop null auth equals ignore case qop constructs sasl input stream input stream sasl client br note specified input stream sasl client null null pointer exception may thrown later used input stream processed initialized sasl client object sasl input stream input stream stream sasl client sasl client stream new data input stream stream sasl server null sasl client sasl client string qop string sasl client get negotiated property sasl qop use wrap qop null auth equals ignore case qop reads next byte data input stream the value byte returned code code range code code code code if byte available end stream reached value code code returned this method blocks input data available end stream detected exception thrown p reached i o error occurs read throws io exception use wrap return stream read ostart ofinish loop new data blocking read more data return return obuffer ostart xff reads code b length code bytes data input stream array bytes p the code read code method
497	common\src\java\org\apache\hadoop\security\SaslOutputStream.java	unrelated	package org apache hadoop security a sasl output stream composed output stream sasl server sasl client write methods first process data writing underlying output stream the sasl server sasl client object must fully initialized used sasl output stream sasl output stream extends output stream output stream stream processed data ready written byte sasl token sasl client sasl client sasl server sasl server buffer holding one byte incoming data byte ibuffer new byte boolean use wrap constructs sasl output stream output stream sasl server br note specified output stream sasl server null null pointer exception may thrown later used output stream processed initialized sasl server object sasl output stream output stream stream sasl server sasl server sasl server sasl server sasl client null string qop string sasl server get negotiated property sasl qop use wrap qop null auth equals ignore case qop use wrap stream new buffered output stream stream else stream stream constructs sasl output stream output stream sasl client br note specified output stream sasl client null null pointer exception may thrown later used output stream processed initialized sasl client object sasl output stream output stream stream sasl client sasl client sasl server null sasl client sasl client string qop string sasl client get negotiated property sasl qop use wrap qop null auth equals ignore case qop use wrap stream new buffered output stream stream else stream stream disposes system resources security sensitive information sasl might using sasl error occurs dispose sasl throws sasl exception sasl client null sasl client dispose sasl server null sasl server dispose writes specified byte output stream code byte code i o error occurs write b throws io exception use wrap stream write b return ibuffer byte b write ibuffer writes code b length code bytes specified byte array output stream p the code write code method code sasl output stream code calls code write code method three arguments three arguments code b code code code code b length code data code b code null i o error occurs write byte b throws io exception write b b length writes code len code bytes specified byte array starting offset code code output stream data start offset data number bytes write i o error occurs write byte buf len throws io exception use wrap stream write buf len return try sasl server null using sasl server sasl token sasl server wrap buf len else using sasl client sasl token sasl client wrap buf len catch sasl exception se try dispose sasl catch sasl exception ignored throw se sasl token null byte array output stream byte out new byte array output stream data output stream dout new data output stream byte out dout write int sasl token length stream write byte out byte array stream write sasl token sasl token length sasl token null flushes output stream i o error occurs flush throws io exception stream flush closes output stream releases system resources associated stream i o error occurs close throws io exception dispose sasl stream close
498	common\src\java\org\apache\hadoop\security\SaslRpcClient.java	authenticate	package org apache hadoop security a utility encapsulates sasl logic rpc client sasl rpc client log log log factory get log sasl rpc client sasl client sasl client create sasl rpc client authentication method requested authentication method token use needed authentication method sasl rpc client auth method method token extends token identifier token string server principal throws io exception switch method case digest log debug enabled log debug creating sasl auth method digest get mechanism name client authenticate service token get service sasl client sasl create sasl client new string auth method digest get mechanism name null null sasl rpc server sasl default realm sasl rpc server sasl props new sasl client callback handler token break case kerberos log debug enabled log debug creating sasl auth method kerberos get mechanism name client server kerberos principal name server principal server principal null server principal length throw new io exception failed specify server kerberos principal name string names sasl rpc server split kerberos name server principal names length throw new io exception kerberos principal name not expected hostname part server principal sasl client sasl create sasl client new string auth method kerberos get mechanism name null names names sasl rpc server sasl props null break default throw new io exception unknown authentication method method sasl client null throw new io exception unable find sasl client implementation read status data input stream stream throws io exception status stream read int read status status sasl status success state throw new remote exception writable utils read string stream writable utils read string stream do client side sasl authentication server via given input stream output stream input stream use output stream use simple auth boolean sasl connect input stream s output stream s throws io exception data input stream stream new data input stream new buffered input stream s data output stream stream new data output stream new buffered output stream s try byte sasl token new byte sasl client initial response sasl token sasl client evaluate challenge sasl token sasl token null stream write int sasl token length stream write sasl token sasl token length stream flush log debug enabled log debug have sent token size sasl token length init sasl context sasl client complete read status stream len stream read int len sasl rpc server switch to simple auth log debug enabled log debug server asks us fall back simple auth sasl client dispose return false sasl token new byte len log debug enabled log debug will read input token size sasl token length processing init sasl context stream read fully sasl token sasl client complete sasl token sasl client evaluate challenge sasl token sasl token null log debug enabled log debug will send token size sasl token length init sasl context stream write int sasl token length stream write sasl token sasl token length stream flush sasl client complete read status stream sasl token new byte stream read int log debug enabled log debug will read input token size sasl token length processing init sasl context stream read fully sasl token log debug enabled log
499	common\src\java\org\apache\hadoop\security\SaslRpcServer.java	authenticate	package org apache hadoop security a utility dealing sasl rpc server sasl rpc server log log log factory get log sasl rpc server string sasl default realm default map string string sasl props new tree map string string switch to simple auth enum quality of protection authentication auth integrity auth privacy auth conf string sasl qop quality of protection string sasl qop sasl qop sasl qop string get sasl qop return sasl qop init configuration conf quality of protection sasl qop quality of protection authentication string rpc protection conf get hadoop rpc protection quality of protection authentication name lower case quality of protection integrity name lower case equals rpc protection sasl qop quality of protection integrity else quality of protection privacy name lower case equals rpc protection sasl qop quality of protection privacy sasl props put sasl qop sasl qop get sasl qop sasl props put sasl server auth true string encode identifier byte identifier return new string base encode base identifier byte decode identifier string identifier return base decode base identifier get bytes t extends token identifier t get identifier string id secret manager t secret manager throws invalid token byte token id decode identifier id t token identifier secret manager create identifier try token identifier read fields new data input stream new byte array input stream token id catch io exception e throw invalid token new invalid token can de serialize token identifier init cause e return token identifier char encode password byte password return new string base encode base password char array splitting fully qualified kerberos name parts string split kerberos name string full name return full name split enum sasl status success error state sasl status state state state authentication method enum auth method simple byte authentication method simple kerberos byte gssapi authentication method kerberos digest byte digest md authentication method token the code method byte code string mechanism name authentication method authentication method auth method byte code string mechanism name authentication method auth method code code mechanism name mechanism name authentication method auth method first code values code return object represented code auth method value of byte code code xff first code return values length null values return sasl mechanism name string get mechanism name return mechanism name read auth method read data input throws io exception return value of read byte write write data output throws io exception write code callback handler sasl digest md mechanism sasl digest callback handler implements callback handler secret manager token identifier secret manager server connection connection sasl digest callback handler secret manager token identifier secret manager server connection connection secret manager secret manager connection connection char get password token identifier tokenid throws invalid token return encode password secret manager retrieve password tokenid inherit doc handle callback callbacks throws invalid token unsupported callback exception name callback nc null password callback pc null authorize callback ac null callback callback callbacks callback instanceof authorize callback ac authorize callback callback else callback instanceof name callback nc name callback callback else callback instanceof password callback pc password callback callback else callback instanceof realm callback continue
500	common\src\java\org\apache\hadoop\security\SecurityInfo.java	unrelated	package org apache hadoop security interface used rpc get security information given protocol security info get kerberos info given protocol kerberos info get kerberos info class protocol configuration conf get token info given protocol token info get token info class protocol configuration conf
501	common\src\java\org\apache\hadoop\security\SecurityUtil.java	authenticate	package org apache hadoop security security util log log log factory get log security util string hostname pattern host find original tgt within current subject credentials cross realm tgt form krbtgt two com one com may present tgt found kerberos ticket get tgt from subject throws io exception subject current subject get subject access controller get context current null throw new io exception can get tgt current subject null set kerberos ticket tickets current get private credentials kerberos ticket kerberos ticket tickets original tgt return throw new io exception failed find tgt current subject tgs must server principal form krbtgt foo foo boolean tgs principal kerberos principal principal principal null return false principal get name equals krbtgt principal get realm principal get realm return true return false check whether server principal tgs principal kinit done protected boolean original tgt kerberos ticket ticket return tgs principal ticket get server explicitly pull service ticket specified host this solves problem java kerberos ssl problem client cannot authenticate cross realm service it necessary clients making kerberized https requests call method target url ensure cross realm environment remote host successfully authenticated this method internal hadoop used applications this method considered stable open removed java behavior changed fetch service ticket url remote host throws io exception user group information security enabled return string service name host remote host get host log debug enabled log debug fetching service ticket host service name credentials service cred null try principal name principal new principal name service name principal name krb nt srv hst service cred credentials acquire service creds principal string krb util ticket to creds get tgt from subject catch exception e throw new io exception can get service ticket service name e service cred null throw new io exception can get service ticket service name subject get subject access controller get context get private credentials add krb util creds to ticket service cred convert kerberos principal name pattern valid kerberos principal names it replaces hostname pattern hostname fully qualified domain name if hostname null uses dynamically looked fqdn current host instead kerberos principal name conf value convert fully qualified domain name used substitution string get server principal string principal config string hostname throws io exception string components get components principal config components null components length components equals hostname pattern return principal config else return replace pattern components hostname convert kerberos principal name pattern valid kerberos principal names this method similar link get server principal string string except reverse dns lookup addr hostname done necessary param addr null default behavior using local hostname addr null kerberos principal name pattern convert inet address host used substitution string get server principal string principal config inet address addr throws io exception string components get components principal config components null components length components equals hostname pattern return principal config else addr null throw new io exception can replace hostname pattern pattern since client address null return replace pattern components addr get canonical host name string get components string principal config principal config null return null return principal config split string replace pattern string components
502	common\src\java\org\apache\hadoop\security\ShellBasedUnixGroupsMapping.java	unrelated	package org apache hadoop security a simple shell based implementation link group mapping service provider exec code groups code shell command fetch group memberships given user shell based unix groups mapping implements group mapping service provider log log log factory get log shell based unix groups mapping returns list groups user list string get groups string user throws io exception return get unix groups user caches groups need provider cache groups refresh throws io exception nothing provider user groups mapping adds groups cache need provider cache groups add list string groups throws io exception nothing provider user groups mapping get current user group list unix running command groups note for non existing user return empty list list string get unix groups string user throws io exception string result try result shell exec command shell get groups for user command user catch exit code exception e get group return empty list log warn got exception trying get groups user user e string tokenizer tokenizer new string tokenizer result list string groups new linked list string tokenizer more tokens groups add tokenizer next token return groups
503	common\src\java\org\apache\hadoop\security\ShellBasedUnixGroupsNetgroupMapping.java	unrelated	package org apache hadoop security a simple shell based implementation link group mapping service provider exec code groups code shell command fetch group memberships given user shell based unix groups netgroup mapping extends shell based unix groups mapping log log log factory get log shell based unix groups netgroup mapping get unix groups parent netgroups given user list string get groups string user throws io exception parent get unix groups list string groups new linked list string super get groups user netgroup cache get netgroups user groups return groups refresh netgroup cache cache groups refresh throws io exception list string groups netgroup cache get netgroup names netgroup cache clear cache groups add groups add group cache netgroups cached cache groups add list string groups throws io exception string group groups group length better safe sorry never happen else group char at netgroup cache cached group netgroup cache add group get users for netgroup group else unix group caching gets users netgroup protected list string get users for netgroup string netgroup throws io exception list string users new linked list string returns similar group user domain user host com string users raw exec shell get user for netgroup netgroup get rid spaces makes splitting much easier users raw users raw replace all remove netgroup name beginning users raw users raw replace first netgroup replace first split user infos string user infos users raw split string user info user infos user info xxx user yyy xxx yyy empty strings get rid everything first last comma string user user info replace first user user replace first voila got username users add user return users calls shell get users netgroup calling getent netgroup low level function returns protected string exec shell get user for netgroup string netgroup throws io exception string result try shell command expect begining group name result shell exec command shell get users for netgroup command netgroup substring catch exit code exception e get group return empty list log warn error getting users netgroup netgroup e return result
504	common\src\java\org\apache\hadoop\security\User.java	authenticate	package org apache hadoop security save full short name user principal this allows us single type always look picking user names user implements principal string full name string short name volatile authentication method auth method null volatile login context login null volatile last login user string name name null null user string name authentication method auth method login context login try short name new kerberos name name get short name catch io exception ioe throw new illegal argument exception illegal principal name name ioe full name name auth method auth method login login get full name user string get name return full name get user name first string get short name return short name boolean equals object return true else null get class get class return false else return full name equals user full name auth method user auth method hash code return full name hash code string string return full name set authentication method authentication method auth method auth method auth method authentication method get authentication method return auth method returns login object login context get login return login set login object set login login context login login login set last login time set last login time last login time get time last login get last login return last login
505	common\src\java\org\apache\hadoop\security\UserGroupInformation.java	authenticate	package org apache hadoop security user group information hadoop this wraps around jaas subject provides methods determine user username groups it supports windows unix kerberos login modules user group information log log log factory get log user group information percentage ticket window use renew ticket ticket renew window f ugi metrics maintains ugi activity statistics publishes metrics interfaces ugi metrics mutable rate login success mutable rate login failure ugi metrics create return default metrics system instance register new ugi metrics a login module looks kerberos unix windows principal adds corresponding user name hadoop login module implements login module subject subject boolean abort throws login exception return true t extends principal t get canonical user class t cls t user subject get principals cls return user return null boolean commit throws login exception already user done subject get principals user empty return true principal user null using kerberos try use kerberos user get canonical user kerberos principal kerberos user use os user user null user get canonical user os principal class found user add principal user null subject get principals add new user user get name return true log error can find user subject throw new login exception can find user name initialize subject subject callback handler callback handler map string shared state map string options subject subject boolean login throws login exception return true boolean logout throws login exception return true metrics track ugi activity ugi metrics metrics ugi metrics create are variables depend configuration initialized boolean initialized false should use kerberos configuration boolean use kerberos server side groups fetching service groups groups the configuration use configuration conf leave minutes relogin attempts min time before relogin l environment variable pointing token cache file string hadoop token file location hadoop token file location a method initialize fields depend configuration must called use kerberos groups used synchronized ensure initialized initialized initialize new configuration initialize ugi related synchronized initialize configuration conf init ugi conf give configuration translate kerberos names try kerberos name set configuration conf catch io exception ioe throw new runtime exception problem kerberos auth local name configuration ioe set configuration values ugi synchronized init ugi configuration conf string value conf get hadoop security authentication value null simple equals value use kerberos false else kerberos equals value use kerberos true else throw new illegal argument exception invalid attribute value hadoop security authentication value if set testing groups use configuration find groups instanceof testing groups groups groups get user to groups mapping service conf set configuration jaas hadoop configuration this done rather initializer avoid circular dependence javax security auth login configuration existing config null try existing config javax security auth login configuration get configuration catch security exception se if security configuration classpath catch exception need delegate anyone existing config instanceof hadoop configuration log info jaas configuration already set hadoop installing else javax security auth login configuration set configuration new hadoop configuration existing config initialized true user group information conf conf set configuration ugi in particular set security authentication mechanism group look service set configuration configuration conf initialize conf determine user group information using kerberos determine
506	common\src\java\org\apache\hadoop\security\authorize\AccessControlList.java	unrelated	package org apache hadoop security authorize class representing configured access control list access control list implements writable register ctor writable factories set factory access control list new writable factory writable new instance return new access control list indicates acl represents access users string wildcard acl value initial capacity set users granted access set string users set groups granted access set string groups whether users granted access boolean allowed groups groups mapping groups get user to groups mapping service new configuration this constructor exists primarily access control list writable access control list construct new acl string representation the string comma separated list users groups the user list comes first separated space followed group list for e g user user group group access control list string acl string build acl acl string build acl given format user user n group group n build acl string acl string users new tree set string groups new tree set string wild card acl value acl string allowed true else allowed false string user group strings acl string split user group strings length list string users list new linked list string arrays list user group strings split cleanup list users list add to set users users list user group strings length list string groups list new linked list string arrays list user group strings split cleanup list groups list add to set groups groups list groups mapping cache groups add groups list checks whether acl contains wildcard boolean wild card acl value string acl string acl string contains wildcard acl value acl string trim equals wildcard acl value return true return false boolean all allowed return allowed add user names users allowed service the user name add user string user wild card acl value user throw new illegal argument exception user user added all allowed users add user add group names groups allowed service the group name add group string group wild card acl value group throw new illegal argument exception group group added all allowed list string groups list new linked list string groups list add group groups mapping cache groups add groups list groups add group remove user names users allowed service the user name remove user string user wild card acl value user throw new illegal argument exception user user removed all allowed users remove user remove group names groups allowed service the group name remove group string group wild card acl value group throw new illegal argument exception group group removed all allowed groups remove group get names users allowed service set string get users return users get names user groups allowed service set string get groups return groups boolean user allowed user group information ugi allowed users contains ugi get short user name return true else string group ugi get group names groups contains group return true return false cleanup list remove empty strings trim leading trailing spaces cleanup list list string list list iterator string list list iterator next string next length remove else trim set add list set add to set set string set list string list string list set add
507	common\src\java\org\apache\hadoop\security\authorize\AuthorizationException.java	unrelated	package org apache hadoop security authorize an exception authorization related issues this em em provide stack trace security purposes authorization exception extends access control exception serial version uid l authorization exception super authorization exception string message super message constructs new exception specified cause detail message tt cause null null cause string tt typically contains detail message tt cause tt link get cause method a tt null tt value permitted indicates cause nonexistent unknown authorization exception throwable cause super cause stack trace element stack trace new stack trace element stack trace element get stack trace do provide stack trace return stack trace print stack trace do provide stack trace print stack trace print stream do provide stack trace print stack trace print writer do provide stack trace
508	common\src\java\org\apache\hadoop\security\authorize\PolicyProvider.java	unrelated	package org apache hadoop security authorize link policy provider provides link service definitions security link policy effect hadoop policy provider configuration key link policy provider implementation string policy provider config hadoop security authorization policyprovider a default link policy provider without defined services policy provider default policy provider new policy provider service get services return null get link service definitions link policy provider service get services
509	common\src\java\org\apache\hadoop\security\authorize\ProxyUsers.java	unrelated	package org apache hadoop security authorize proxy users string conf hosts hosts string conf groups groups string conf hadoop proxyuser hadoop proxyuser string conf hadoop proxyuser re hadoop proxyuser boolean init false list groups hosts per proxyuser map string collection string proxy groups new hash map string collection string map string collection string proxy hosts new hash map string collection string reread conf get new values hadoop proxyuser groups hosts refresh super user groups configuration load server side configuration refresh super user groups configuration new configuration refresh configuration synchronized refresh super user groups configuration configuration conf remove alle existing stuff proxy groups clear proxy hosts clear get new keys groups string regex conf hadoop proxyuser re conf groups map string string match keys conf get val by regex regex entry string string entry match keys entry set proxy groups put entry get key string utils get string collection entry get value hosts regex conf hadoop proxyuser re conf hosts match keys conf get val by regex regex entry string string entry match keys entry set proxy hosts put entry get key string utils get string collection entry get value init true returns configuration key effective user groups allowed superuser string get proxy superuser group conf key string user name return proxy users conf hadoop proxyuser user name proxy users conf groups return configuration key superuser ip addresses string get proxy superuser ip conf key string user name return proxy users conf hadoop proxyuser user name proxy users conf hosts authorize superuser as synchronized authorize user group information user string remote address configuration new conf throws authorization exception init refresh super user groups configuration user get real user null return boolean group authorized false boolean ip authorized false user group information super user user get real user collection string allowed user groups proxy groups get get proxy superuser group conf key super user get short user name wildcard list allowed user groups group authorized true else allowed user groups null allowed user groups empty string group user get group names allowed user groups contains group group authorized true break group authorized throw new authorization exception user super user get user name allowed impersonate user get user name collection string ip list proxy hosts get get proxy superuser ip conf key super user get short user name wildcard list ip list ip authorized true else ip list null ip list empty string allowed host ip list inet address host addr try host addr inet address get by name allowed host catch unknown host exception e continue host addr get host address equals remote address authorization successful ip authorized true ip authorized throw new authorization exception unauthorized connection super user super user get user name ip remote address return true configuration specifies special configuration value indicating group host list allowed use configuration boolean wildcard list collection string list return list null list size list contains
510	common\src\java\org\apache\hadoop\security\authorize\RefreshAuthorizationPolicyProtocol.java	unrelated	package org apache hadoop security authorize protocol used refresh authorization policy use currently server principal common configuration keys hadoop security service user name key refresh authorization policy protocol extends versioned protocol version initial version version id l refresh service level authorization policy effect refresh service acl throws io exception
511	common\src\java\org\apache\hadoop\security\authorize\Service.java	unrelated	package org apache hadoop security authorize an definition em service em related service level authorization hadoop each service defines configuration key also necessary link permission required access service service string key class protocol service string key class protocol key key protocol protocol get configuration key service string get service key return key get protocol service class get protocol return protocol
512	common\src\java\org\apache\hadoop\security\authorize\ServiceAuthorizationManager.java	unrelated	package org apache hadoop security authorize an authorization manager handles service level authorization incoming service requests service authorization manager string hadoop policy file hadoop policy xml map class access control list protocol to acl new identity hash map class access control list configuration key controlling service level authorization hadoop link common configuration keys hadoop security authorization instead string service authorization config hadoop security authorization log auditlog log factory get log security logger service authorization manager get name string authz successfull for authorization successfull string authz failed for authorization failed authorize user access protocol used authorize user group information user class protocol configuration conf inet address addr throws authorization exception access control list acl protocol to acl get protocol acl null throw new authorization exception protocol protocol known get client principal key verify available kerberos info krb info security util get kerberos info protocol conf string client principal null krb info null string client key krb info client principal client key null client key equals try client principal security util get server principal conf get client key addr catch io exception e throw authorization exception new authorization exception can figure kerberos principal name connection addr user user protocol protocol init cause e client principal null client principal equals user get user name acl user allowed user auditlog warn authz failed for user protocol protocol expected client kerberos principal client principal throw new authorization exception user user authorized protocol protocol expected client kerberos principal client principal auditlog info authz successfull for user protocol protocol synchronized refresh configuration conf policy provider provider get system property hadoop policy file string policy file system get property hadoop policy file hadoop policy file make copy original config load policy file configuration policy conf new configuration conf policy conf add resource policy file map class access control list new acls new identity hash map class access control list parse config file service services provider get services services null service service services access control list acl new access control list policy conf get service get service key access control list wildcard acl value new acls put service get protocol acl flip newly parsed permissions protocol to acl new acls package protected use tests set class get protocols with acls return protocol to acl key set
513	common\src\java\org\apache\hadoop\security\token\SecretManager.java	authenticate	package org apache hadoop security token the server side secret manager token type secret manager t extends token identifier the token invalid message explains invalid token extends io exception invalid token string msg super msg create password given identifier identifier may modified inside method protected byte create password t identifier retrieve password given token identifier should check date registry make sure token expired revoked returns relevant password byte retrieve password t identifier throws invalid token create empty token identifier t create identifier the name hashing algorithm string default hmac algorithm hmac sha the length random keys use key length a thread local store macs thread local mac thread local mac new thread local mac protected mac initial value try return mac get instance default hmac algorithm catch no such algorithm exception nsa throw new illegal argument exception can find default hmac algorithm algorithm key generator use key generator key gen try key gen key generator get instance default hmac algorithm key gen init key length catch no such algorithm exception nsa throw new illegal argument exception can find default hmac algorithm algorithm generate new random secret key protected secret key generate secret secret key key synchronized key gen key key gen generate key return key compute hmac identifier using secret key return output password protected byte create password byte identifier secret key key mac mac thread local mac get try mac init key catch invalid key exception ike throw new illegal argument exception invalid key hmac computation ike return mac final identifier convert byte secret key protected secret key create secret key byte key return new secret key spec key default hmac algorithm
514	common\src\java\org\apache\hadoop\security\token\Token.java	authenticate	package org apache hadoop security token the client side form token token t extends token identifier implements writable byte identifier byte password text kind text service construct token given token identifier secret manager type token identifier token t id secret manager t mgr password mgr create password id identifier id get bytes kind id get kind service new text construct token components token byte identifier byte password text kind text service identifier identifier password password kind kind service service default constructor token identifier new byte password new byte kind new text service new text get token identifier byte get identifier return identifier get token password secret byte get password return password get token kind text get kind return kind get service token supposed used text get service return service set service token supposed used set service text new service service new service inherit doc read fields data input throws io exception len writable utils read v int identifier null identifier length len identifier new byte len read fully identifier len writable utils read v int password null password length len password new byte len read fully password kind read fields service read fields inherit doc write data output throws io exception writable utils write v int identifier length write identifier writable utils write v int password length write password kind write service write generate url quoted base encoded serialized form writable string encode writable writable obj throws io exception data output buffer buf new data output buffer obj write buf base encoder new base null true byte raw new byte buf get length system arraycopy buf get data raw buf get length return encoder encode to string raw modify writable value new value decode writable writable obj string new value throws io exception base decoder new base null true data input buffer buf new data input buffer byte decoded decoder decode new value buf reset decoded decoded length obj read fields buf encode token url safe string encode to url string throws io exception return encode writable decode given url safe token decode from url string string new value throws io exception decode writable new value boolean equals object right right return true else right null get class right get class return false else token t r token t right return arrays equals identifier r identifier arrays equals password r password kind equals r kind service equals r service hash code return writable comparator hash bytes identifier identifier length add binary buffer string builder buffer byte bytes idx idx bytes length idx first put blank separator idx buffer append string num integer hex string xff bytes idx one digit add leading num length buffer append buffer append num string string string builder buffer new string builder buffer append ident add binary buffer buffer identifier buffer append pass add binary buffer buffer password buffer append kind buffer append kind string buffer append service buffer append service string return buffer string
515	common\src\java\org\apache\hadoop\security\token\TokenIdentifier.java	authenticate	package org apache hadoop security token an identifier identifies token may contain information token including kind type token identifier implements writable get token kind text get kind get ugi username encoded token identifier empty null user group information get user get bytes token identifier byte get bytes data output buffer buf new data output buffer try write buf catch io exception ie throw new runtime exception error get bytes ie return arrays copy of buf get data buf get length
516	common\src\java\org\apache\hadoop\security\token\TokenInfo.java	authenticate	package org apache hadoop security token indicates token related information used token info the type token selector used class extends token selector extends token identifier value
517	common\src\java\org\apache\hadoop\security\token\TokenSelector.java	authenticate	package org apache hadoop security token select token type t tokens use named service t extends token identifier token selector t extends token identifier token t select token text service collection token extends token identifier tokens
518	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenIdentifier.java	unrelated	package org apache hadoop security token delegation abstract delegation token identifier extends token identifier byte version text owner text renewer text real user issue date max date sequence number master key id abstract delegation token identifier new text new text new text abstract delegation token identifier text owner text renewer text real user owner null owner new text else owner owner renewer null renewer new text else kerberos name renewer krb name new kerberos name renewer string try renewer new text renewer krb name get short name catch io exception e throw new runtime exception e real user null real user new text else real user real user issue date max date text get kind get username encoded token identifier user group information get user owner null equals owner string return null real user null equals real user string real user equals owner return user group information create remote user owner string else user group information real ugi user group information create remote user real user string return user group information create proxy user owner string real ugi text get renewer return renewer set issue date issue date issue date issue date get issue date return issue date set max date max date max date max date get max date return max date set sequence number seq num sequence number seq num get sequence number return sequence number set master key id new id master key id new id get master key id return master key id boolean equal object object b return null b null equals b inherit doc boolean equals object obj obj return true obj instanceof abstract delegation token identifier abstract delegation token identifier abstract delegation token identifier obj return sequence number sequence number issue date issue date max date max date master key id master key id equal owner owner equal renewer renewer equal real user real user return false inherit doc hash code return sequence number read fields data input throws io exception byte version read byte version version throw new io exception unknown version delegation token version owner read fields renewer read fields real user read fields issue date writable utils read v long max date writable utils read v long sequence number writable utils read v int master key id writable utils read v int write data output throws io exception write byte version owner write renewer write real user write writable utils write v long issue date writable utils write v long max date writable utils write v int sequence number writable utils write v int master key id string string string builder buffer new string builder buffer append owner owner renewer renewer real user real user issue date issue date max date max date sequence number sequence number master key id master key id return buffer string
519	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenSecretManager.java	unrelated	package org apache hadoop security token delegation abstract delegation token secret manager token ident extends abstract delegation token identifier extends secret manager token ident log log log factory get log abstract delegation token secret manager cache currently valid tokens mapping delegation token identifier delegation token information protected object lock protected map token ident delegation token information current tokens new hash map token ident delegation token information sequence number create delegation token identifier protected object lock protected delegation token sequence number access keys protected object lock protected map integer delegation key keys new hash map integer delegation key access current id protected object lock protected current id access current key protected object lock delegation key current key key update interval token max lifetime token remover scan interval token renew interval thread token remover thread protected volatile boolean running abstract delegation token secret manager delegation key update interval delegation token max lifetime delegation token renew interval delegation token remover scan interval key update interval delegation key update interval token max lifetime delegation token max lifetime token renew interval delegation token renew interval token remover scan interval delegation token remover scan interval called object used start threads throws io exception update current key synchronized running true token remover thread new daemon new expired token remover token remover thread start add previously used master key cache nn restarts called activate synchronized add key delegation key key throws io exception running safety check throw new io exception can add delegation key running secret manager key get key id current id current id key get key id keys put key get key id key synchronized delegation key get all keys return keys values array new delegation key protected log update master key delegation key key throws io exception return update current master key this called start threads token remover thread created token remover thread afterwards update current key throws io exception log info updating current master key generating delegation tokens create new current key estimated expiry date new current id synchronized new current id current id delegation key new key new delegation key new current id system current time millis key update interval token max lifetime generate secret log must invoked outside lock log update master key new key synchronized current id new key get key id current key new key keys put current key get key id current key update current master key generating delegation tokens it called token remover thread roll master key throws io exception synchronized remove expired keys set expiry date retiring current key current key set expiry date system current time millis token max lifetime current key might removed remove expired keys update master key called expected interval add back keys case keys put current key get key id current key update current key synchronized remove expired keys system current time millis iterator map entry integer delegation key keys entry set iterator next map entry integer delegation key e next e get value get expiry date remove protected synchronized byte create password token ident identifier log info creating password identifier identifier sequence num
520	common\src\java\org\apache\hadoop\security\token\delegation\AbstractDelegationTokenSelector.java	unrelated	package org apache hadoop security token delegation look tokens find first delegation token matches service return abstract delegation token selector token ident extends abstract delegation token identifier implements token selector token ident text kind name protected abstract delegation token selector text kind name kind name kind name token token ident select token text service collection token extends token identifier tokens service null return null token extends token identifier token tokens kind name equals token get kind service equals token get service return token token ident token return null
521	common\src\java\org\apache\hadoop\security\token\delegation\DelegationKey.java	unrelated	package org apache hadoop security token delegation key used generating verifying delegation tokens delegation key implements writable key id expiry date byte key bytes null delegation key l null delegation key key id expiry date secret key key key id key id expiry date expiry date key null key bytes key get encoded get key id return key id get expiry date return expiry date secret key get key key bytes null key bytes length return null else secret key key abstract delegation token secret manager create secret key key bytes return key set expiry date expiry date expiry date expiry date write data output throws io exception writable utils write v int key id writable utils write v long expiry date key bytes null writable utils write v int else writable utils write v int key bytes length write key bytes read fields data input throws io exception key id writable utils read v int expiry date writable utils read v long len writable utils read v int len key bytes null else key bytes new byte len read fully key bytes
522	common\src\java\org\apache\hadoop\tools\GetGroupsBase.java	unrelated	package org apache hadoop tools base hdfs mr implementations tools fetch display groups users belong get groups base extends configured implements tool print stream create instance tool using given configuration protected get groups base configuration conf conf system used exclusively testing protected get groups base configuration conf print stream super conf get groups users given print formatted output link print stream configured earlier run string args throws exception args length args new string user group information get current user get user name string username args string builder sb new string builder sb append username string group get ugm protocol get groups for user username sb append sb append group println sb return must overridden subclasses get address link get user mappings protocol implementation running protected inet socket address get protocol address configuration conf throws io exception get client link get user mappings protocol get user mappings protocol get ugm protocol throws io exception get user mappings protocol user group mapping protocol rpc get proxy get user mappings protocol get user mappings protocol version id get protocol address get conf user group information get current user get conf net utils get socket factory get conf get user mappings protocol return user group mapping protocol
523	common\src\java\org\apache\hadoop\tools\GetUserMappingsProtocol.java	unrelated	package org apache hadoop tools protocol implemented name node job tracker maps users groups get user mappings protocol extends versioned protocol version initial version version id l get groups mapped given user string get groups for user string user throws io exception
524	common\src\java\org\apache\hadoop\util\AsyncDiskService.java	pooling	package org apache hadoop util this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion files we move files to be deleted folder asychronously deleting make sure caller run faster async disk service log log log factory get log async disk service thread pool core pool size core threads per volume thread pool maximum pool size maximum threads per volume thread pool keep alive time threads core pool size threads keep alive seconds thread group thread group new thread group async disk service thread factory thread factory hash map string thread pool executor executors new hash map string thread pool executor create async disk services set volumes specified root directories the async disk services uses one thread pool per volume async disk operations async disk service string volumes throws io exception thread factory new thread factory thread new thread runnable r return new thread thread group r create one thread pool per volume v v volumes length v thread pool executor executor new thread pool executor core threads per volume maximum threads per volume threads keep alive seconds time unit seconds new linked blocking queue runnable thread factory this reduce number running threads executor allow core thread time out true executors put volumes v executor execute task sometime future using thread pools synchronized execute string root runnable task thread pool executor executor executors get root executor null throw new runtime exception cannot find root root execution task task else executor execute task gracefully start shut thread pools synchronized shutdown log info shutting async disk service threads map entry string thread pool executor e executors entry set e get value shutdown wait termination thread pools synchronized boolean await termination milliseconds throws interrupted exception end system current time millis milliseconds map entry string thread pool executor e executors entry set thread pool executor executor e get value executor await termination math max end system current time millis time unit milliseconds log warn async disk service await termination timeout return false log info all async disk service threads terminated return true shut thread pools immediately synchronized list runnable shutdown now log info shutting async disk service threads immediately list runnable list new array list runnable map entry string thread pool executor e executors entry set list add all e get value shutdown now return list
525	common\src\java\org\apache\hadoop\util\CyclicIteration.java	unrelated	package org apache hadoop util provide cyclic link iterator link navigable map the link iterator navigates entries map according map ordering if link iterator hits last entry map continue first entry cyclic iteration k v implements iterable map entry k v navigable map k v navigablemap navigable map k v tailmap construct link iterable object link iterator created iterating given link navigable map the iteration begins starting key exclusively cyclic iteration navigable map k v navigablemap k startingkey navigablemap null navigablemap empty navigablemap null tailmap null else navigablemap navigablemap tailmap navigablemap tail map startingkey false inherit doc iterator map entry k v iterator return new cyclic iterator an link iterator link cyclic iteration cyclic iterator implements iterator map entry k v boolean hasnext iterator map entry k v the first entry begin map entry k v first the next entry map entry k v next cyclic iterator hasnext navigablemap null hasnext tailmap entry set iterator first next entry next first else null first null next null map entry k v next entry next navigablemap entry set iterator return next inherit doc boolean next return hasnext inherit doc map entry k v next hasnext throw new no such element exception map entry k v curr next next next entry hasnext next equals first return curr not supported remove throw new unsupported operation exception not supported
526	common\src\java\org\apache\hadoop\util\Daemon.java	unrelated	package org apache hadoop util a thread called link thread set daemon boolean true daemon extends thread set daemon true always daemon provide factory named daemon threads use executor services constructors daemon factory extends daemon implements thread factory thread new thread runnable runnable return new daemon runnable runnable runnable null construct daemon thread daemon super construct daemon thread daemon runnable runnable super runnable runnable runnable set name object runnable string construct daemon thread part specified thread group daemon thread group group runnable runnable super group runnable runnable runnable set name object runnable string runnable get runnable return runnable
527	common\src\java\org\apache\hadoop\util\DataChecksum.java	unrelated	package org apache hadoop util this provides inteface utilities processing checksums dfs data transfers data checksum implements checksum misc constants header len byte type byte len checksum types checksum null checksum crc checksum crc c checksum null size checksum crc size checksum crc c size data checksum new data checksum type bytes per checksum bytes per checksum return null switch type case checksum null return new data checksum checksum null new checksum null checksum null size bytes per checksum case checksum crc return new data checksum checksum crc new pure java crc checksum crc size bytes per checksum case checksum crc c return new data checksum checksum crc c new pure java crc c checksum crc c size bytes per checksum default return null creates data checksum header len bytes arr offset data checksum new data checksum byte bytes offset offset bytes length offset header len return null like read int bytes per checksum bytes offset xff bytes offset xff bytes offset xff bytes offset xff return new data checksum bytes bytes per checksum this constructucts data checksum reading header len bytes input stream data checksum new data checksum data input stream throws io exception type read byte bpc read int data checksum summer new data checksum type bpc summer null throw new io exception could create data checksum type type bytes per checksum bpc return summer writes checksum header output stream write header data output stream throws io exception write byte type write int bytes per checksum byte get header byte header new byte data checksum header len header byte type xff writing buffer like data output write int header byte bytes per checksum xff header byte bytes per checksum xff header byte bytes per checksum xff header byte bytes per checksum xff return header writes current checksum stream if reset true resets checksum write value data output stream boolean reset throws io exception size return size write int summer get value else throw new io exception unknown checksum type reset reset return size writes current checksum buffer if reset true resets checksum write value byte buf offset boolean reset throws io exception size return size checksum summer get value buf offset byte checksum xff buf offset byte checksum xff buf offset byte checksum xff buf offset byte checksum xff else throw new io exception unknown checksum type reset reset return size compares checksum located buf offset current checksum boolean compare byte buf offset size checksum buf offset xff buf offset xff buf offset xff buf offset xff return checksum summer get value return size type size checksum summer bytes per checksum sum data checksum checksum type checksum checksum sum size chunk size type checksum type summer checksum size sum size bytes per checksum chunk size accessors get checksum type return type get checksum size return size get bytes per checksum return bytes per checksum get num bytes in sum return sum size of integer integer size byte size get checksum header size return size of integer type byte bytes per checksum checksum interface just wrapper around member summer get value
528	common\src\java\org\apache\hadoop\util\DiskChecker.java	unrelated	package org apache hadoop util class provides utility functions checking disk problem disk checker disk error exception extends io exception disk error exception string msg super msg disk out of space exception extends io exception disk out of space exception string msg super msg the semantics mkdirs with exists check method different mkdirs method provided sun java io file following way while creating non existent parent directories method checks existence directories mkdir fails point since directory might created process if mkdir exists check fails seemingly non existent directory signal error sun mkdir would signal error return false directory attempting create already exists mkdir fails boolean mkdirs with exists check file dir dir mkdir dir exists return true file canon dir null try canon dir dir get canonical file catch io exception e return false string parent canon dir get parent return parent null mkdirs with exists check new file parent canon dir mkdir canon dir exists create directory exist check dir file dir throws disk error exception mkdirs with exists check dir throw new disk error exception create directory dir string dir directory throw new disk error exception directory dir string dir read throw new disk error exception directory readable dir string dir write throw new disk error exception directory writable dir string create directory check permissions already exists the semantics mkdirs with exists and permission check method different mkdirs method provided sun java io file following way while creating non existent parent directories method checks existence directories mkdir fails point since directory might created process if mkdir exists check fails seemingly non existent directory signal error sun mkdir would signal error return false directory attempting create already exists mkdir fails mkdirs with exists and permission check local file system local fs path dir fs permission expected throws io exception file directory local fs path to file dir boolean created false directory exists created mkdirs with exists check directory created local fs get file status dir get permission equals expected local fs set permission dir expected create local directory necessary check permissions also ensure read written check dir local file system local fs path dir fs permission expected throws disk error exception io exception mkdirs with exists and permission check local fs dir expected file status stat local fs get file status dir fs permission actual stat get permission stat directory throw new disk error exception directory dir string fs action user actual get user action user implies fs action read throw new disk error exception directory readable dir string user implies fs action write throw new disk error exception directory writable dir string user implies fs action execute throw new disk error exception directory listable dir string
529	common\src\java\org\apache\hadoop\util\GenericOptionsParser.java	unrelated	package org apache hadoop util code generic options parser code utility parse command line arguments generic hadoop framework code generic options parser code recognizes several standarad command line arguments enabling applications easily specify namenode jobtracker additional configuration resources etc id generic options generic options p the supported generic options p p blockquote pre conf lt configuration file gt specify configuration file d lt property value gt use value given property fs lt local namenode port gt specify namenode jt lt local jobtracker port gt specify job tracker files lt comma separated list files gt specify comma separated files copied map reduce cluster libjars lt comma separated list jars gt specify comma separated jar files classpath archives lt comma separated list archives gt specify comma separated archives unarchived compute machines pre blockquote p p the general command line syntax p p tt pre bin hadoop command generic options command options pre tt p p generic command line arguments strong might strong modify code configuration code objects given constructors p p the functionality implemented using commons cli p p examples p p blockquote pre bin hadoop dfs fs darwin ls data list data directory dfs namenode darwin bin hadoop dfs d fs default name darwin ls data list data directory dfs namenode darwin bin hadoop dfs conf core site xml conf hdfs site xml ls data list data directory dfs multiple conf files specified bin hadoop job d mapred job tracker darwin submit job xml submit job job tracker darwin bin hadoop job jt darwin submit job xml submit job job tracker darwin bin hadoop job jt local submit job xml submit job local runner bin hadoop jar libjars testlib jar archives test tgz files file txt inputjar args job submission libjars files archives pre blockquote p generic options parser log log log factory get log generic options parser configuration conf command line command line create options parser given options parse args generic options parser options opts string args throws io exception new configuration opts args create options parser parse args generic options parser string args throws io exception new configuration new options args create code generic options parser code parse generic hadoop arguments the array arguments generic arguments obtained link get remaining args generic options parser configuration conf string args throws io exception conf new options args create code generic options parser code parse given options well generic hadoop options the resulting code command line code object obtained link get command line generic options parser configuration conf options options string args throws io exception parse general options options conf args conf conf returns array strings containing application specific arguments strong empty array strong command line defined string get remaining args return command line null new string command line get args get modified configuration configuration get configuration return conf returns commons cli code command line code object process parsed arguments note if object created link generic options parser configuration string returned object contain parsed generic options parsed options descriptor command line get command line return command line specify properties generic option options build general
530	common\src\java\org\apache\hadoop\util\GenericsUtil.java	unrelated	package org apache hadoop util contains utility methods dealing java generics generics util returns class object type code class lt t gt code argument type code t code t class t get class t class t clazz class t get class return clazz converts given code list lt t gt code array code t code t t array class t c list t list t ta t array new instance c list size list size ta list get return ta converts given code list lt t gt code array code t code use link array class list list may empty t t array list t list return array get class list get list
531	common\src\java\org\apache\hadoop\util\HeapSort.java	unrelated	package org apache hadoop util an implementation core algorithm heap sort heap sort implements indexed sorter heap sort heap indexed sortable b n idx idx n idx idx n compare b idx b idx compare b b idx swap b b idx else return idx else compare b b idx swap b b idx idx else return sort given range items using heap sort inherit doc sort indexed sortable p r sort p r null inherit doc sort indexed sortable p r progressable rep n r p build heap w reverse comparator write place end integer highest one bit n j j j heap p j n null rep rep progress r p swap p heap p p
532	common\src\java\org\apache\hadoop\util\HostsFileReader.java	unrelated	package org apache hadoop util keeps track datanodes tasktrackers allowed connect namenode jobtracker hosts file reader set string includes set string excludes string includes file string excludes file log log log factory get log hosts file reader hosts file reader string file string ex file throws io exception includes new hash set string excludes new hash set string includes file file excludes file ex file refresh read file to set string filename set string set throws io exception file file new file filename file exists return file input stream fis new file input stream file buffered reader reader null try reader new buffered reader new input stream reader fis string line line reader read line null string nodes line split n f r nodes null nodes length nodes trim starts with everything comment break nodes equals log info adding nodes list hosts filename set add nodes might need add canonical name finally reader null reader close fis close synchronized refresh throws io exception log info refreshing hosts exclude list includes file equals set string new includes new hash set string read file to set includes file new includes switch new hosts included includes new includes excludes file equals set string new excludes new hash set string read file to set excludes file new excludes switch excluded hosts excludes new excludes synchronized set string get hosts return includes synchronized set string get excluded hosts return excludes synchronized set includes file string includes file log info setting includes file includes file includes file includes file synchronized set excludes file string excludes file log info setting excludes file excludes file excludes file excludes file synchronized update file names string includes file string excludes file throws io exception set includes file includes file set excludes file excludes file
533	common\src\java\org\apache\hadoop\util\IndexedSortable.java	unrelated	package org apache hadoop util interface collections capable sorted link indexed sorter algorithms indexed sortable compare items given addresses consistent semantics link java util comparator compare object object compare j swap items given addresses swap j
534	common\src\java\org\apache\hadoop\util\IndexedSorter.java	unrelated	package org apache hadoop util interface sort algorithms accepting link indexed sortable items a sort algorithm implementing may link indexed sortable compare link indexed sortable swap items range indices effect sort across range indexed sorter sort items accessed given indexed sortable given range logical indices from perspective sort algorithm index inclusive r exclusive addressable entry sort indexed sortable r same link sort indexed sortable indicate progress periodically sort indexed sortable r progressable rep
535	common\src\java\org\apache\hadoop\util\LineReader.java	unrelated	package org apache hadoop util a provides line reader input stream depending constructor used lines either terminated ul li one following n lf r cr r n cr lf li li em em custom byte sequence delimiter li ul in cases eof also terminates otherwise unterminated line line reader default buffer size buffer size default buffer size input stream byte buffer number bytes real data buffer buffer length current position buffer buffer posn byte cr r byte lf n the line delimiter byte record delimiter bytes create line reader reads given stream using default buffer size k line reader input stream default buffer size create line reader reads given stream using given buffer size line reader input stream buffer size buffer size buffer size buffer new byte buffer size record delimiter bytes null create line reader reads given stream using code io file buffer size code specified given code configuration code line reader input stream configuration conf throws io exception conf get int io file buffer size default buffer size create line reader reads given stream using default buffer size using custom delimiter array bytes line reader input stream byte record delimiter bytes buffer size default buffer size buffer new byte buffer size record delimiter bytes record delimiter bytes create line reader reads given stream using given buffer size using custom delimiter array bytes line reader input stream buffer size byte record delimiter bytes buffer size buffer size buffer new byte buffer size record delimiter bytes record delimiter bytes create line reader reads given stream using code io file buffer size code specified given code configuration code using custom delimiter array bytes line reader input stream configuration conf byte record delimiter bytes throws io exception buffer size conf get int io file buffer size default buffer size buffer new byte buffer size record delimiter bytes record delimiter bytes close underlying stream close throws io exception close read one line input stream given text rest line silently discarded call this hint line cross threshold allow happen it overshoot potentially much one buffer length found read line text str max line length max bytes to consume throws io exception record delimiter bytes null return read custom line str max line length max bytes to consume else return read default line str max line length max bytes to consume read line terminated one cr lf crlf read default line text str max line length max bytes to consume throws io exception we reading data head stream may already buffered buffer several cases no newline characters buffer need copy everything read another buffer stream an unambiguously terminated line buffer copy str ambiguously terminated line buffer e buffer ends cr in case copy everything cr str also need see follows cr lf need consume lf well next call read line read we use flag prev char cr signal previous character cr happens end buffer delay consuming chance look char follows str clear txt length tracks str get length optimization newline length length terminating newline boolean prev char cr false true prev char cr bytes consumed start posn
536	common\src\java\org\apache\hadoop\util\MergeSort.java	unrelated	package org apache hadoop util an implementation core algorithm merge sort merge sort reusable int writables int writable i new int writable int writable j new int writable comparator algo use comparator int writable comparator merge sort comparator int writable comparator comparator comparator merge sort src dest low high length high low insertion sort smallest arrays length low high j j low j i set dest j j set dest j comparator compare i j swap dest j j return recursively sort halves dest src mid low high merge sort dest src low mid merge sort dest src mid high i set src mid j set src mid if list already sorted copy src dest this optimization results faster sorts nearly ordered lists comparator compare i j system arraycopy src low dest low length return merge sorted halves src dest low p low q mid high q high p mid i set src p j set src q q high p mid comparator compare i j dest src p else dest src q swap x b x x x b x b
537	common\src\java\org\apache\hadoop\util\NativeCodeLoader.java	unrelated	package org apache hadoop util a helper load native hadoop code e libhadoop this handles fallback either bundled libhadoop linux default java implementations appropriate native code loader log log log factory get log native code loader boolean native code loaded false try load native hadoop library set fallback flag appropriately log debug enabled log debug trying load custom built native hadoop library try system load library hadoop log info loaded native hadoop library native code loaded true catch throwable ignore failure load log debug enabled log debug failed load native hadoop error log debug java library path system get property java library path native code loaded log warn unable load native hadoop library platform using builtin java applicable check native hadoop code loaded platform else code false code boolean native code loaded return native code loaded return native hadoop libraries present used job used job code false code otherwise boolean get load native libraries configuration conf return conf get boolean common configuration keys io native lib available key common configuration keys io native lib available default set native hadoop libraries present used job set load native libraries configuration conf boolean load native libraries conf set boolean common configuration keys io native lib available key load native libraries
538	common\src\java\org\apache\hadoop\util\Options.java	unrelated	package org apache hadoop util this allows generic access variable length type safe parameter lists options string option string value protected string option string value value value string get value return value class option class value protected class option class value value value class get value return value boolean option boolean value protected boolean option boolean value value value boolean get value return value integer option value protected integer option value value value get value return value long option value protected long option value value value get value return value path option path value protected path option path value value value path get value return value fs data input stream option fs data input stream value protected fs data input stream option fs data input stream value value value fs data input stream get value return value fs data output stream option fs data output stream value protected fs data output stream option fs data output stream value value value fs data output stream get value return value progressable option progressable value protected progressable option progressable value value value progressable get value return value find first option required base t extends base t get option class t cls base opts throws io exception base opts get class cls return t return null prepend new options old options t t prepend options t old opts t new opts copy new options front array t result arrays copy of new opts new opts length old opts length copy old options system arraycopy old opts result new opts length old opts length return result
539	common\src\java\org\apache\hadoop\util\PlatformName.java	unrelated	package org apache hadoop util a helper getting build info java vm platform name the complete platform name identify platform per java vm string platform name system get property os name system get property os arch system get property sun arch data model get complete platform per java vm string get platform name return platform name main string args system println platform name
540	common\src\java\org\apache\hadoop\util\PrintJarMainClass.java	unrelated	package org apache hadoop util a micro application prints main name jar file print jar main class main string args try jar file jar file new jar file args jar file null manifest manifest jar file get manifest manifest null string value manifest get main attributes get value main class value null system println value replace all return catch throwable e ignore system println unknown system exit
541	common\src\java\org\apache\hadoop\util\PriorityQueue.java	scheduler	package org apache hadoop util a priority queue maintains partial ordering elements least element always found constant time put pop require log size time priority queue t t heap size max size determines ordering objects priority queue subclasses must define one method protected boolean less than object object b subclass constructors must call protected initialize max size size heap size max size heap t new object heap size max size max size adds object priority queue log size time if one tries add objects max size initialize runtime exception array index out of bound thrown put t element size heap size element heap adds element priority queue log size time either priority queue full less than element top boolean insert t element size max size put element return true else size less than element top heap element adjust top return true else return false returns least element priority queue constant time t top size return heap else return null removes returns least element priority queue log size time t pop size t result heap save first value heap heap size move last first heap size null permit gc objects size heap adjust heap return result else return null should called object top changes values still log n worst case least twice fast pre pq top change pq adjust top pre instead pre pq pop change pq push pre adjust top heap returns number elements currently stored priority queue size return size removes entries priority queue clear size heap null size heap size t node heap save bottom node j j less than node heap j heap heap j shift parents j j j heap node install saved node heap t node heap save top node j find smaller child k j k size less than heap k heap j j k j size less than heap j node heap heap j shift child j j k j k size less than heap k heap j j k heap node install saved node
542	common\src\java\org\apache\hadoop\util\ProgramDriver.java	unrelated	package org apache hadoop util a driver used run programs added program driver a description program based human readable description map string program description programs program driver programs new tree map string program description program description class param types new class string create description example program program description class main class string description throws security exception no such method exception main main class get method main param types description description invoke example application given arguments invoke string args throws throwable try main invoke null new object args catch invocation target exception except throw except get cause string get description return description method main string description print usage map string program description programs system println valid program names map entry string program description item programs entry set system println item get key item get value get description this method adds classed repository add class string name class main class string description throws throwable programs put name new program description main class description this driver example programs it looks first command line argument tries find example program name if found calls main method rest command line arguments driver string args throws throwable make sure gave us program name args length system println an example program must given first argument print usage programs return and good program description pgm programs get args pgm null system println unknown program args chosen print usage programs return remove leading argument call main string new args new string args length args length new args args pgm invoke new args return
543	common\src\java\org\apache\hadoop\util\Progress.java	unrelated	package org apache hadoop util utility assist generation progress reports applications build hierarchy link progress instances modelling phase execution the root constructed link progress nodes sub phases created calling link add phase progress log log log factory get log progress string status progress current phase array list progress phases new array list progress progress parent each phase different progress weightage for example map task map phase accounts sort phase user needs give weightages parameters phases adding phases progress object wants give weightage phases so nodes added without specifying weightage means fixed weightage phases boolean fixed weightage for all phases false progress per phase f array list float progress weightages for phases new array list float creates new root node progress adds named node tree progress add phase string status progress phase add phase phase set status status return phase adds node tree gives equal weightage phases synchronized progress add phase progress phase add new phase set equal weightage phases progress per phase f phases size fixed weightage for all phases true return phase adds new phase caller needs set progress weightage synchronized progress add new phase progress phase new progress phases add phase phase set parent return phase adds named node specified progress weightage tree progress add phase string status weightage progress phase add phase weightage phase set status status return phase adds node specified progress weightage tree synchronized progress add phase weightage progress phase new progress progress weightages for phases add weightage phases add phase phase set parent ensure sum weightages cross sum phases size sum progress weightages for phases get sum log warn sum weightages but sum sum return phase adds n nodes tree gives equal weightage phases synchronized add phases n n add new phase set equal weightage phases progress per phase f phases size fixed weightage for all phases true returns progress weightage given phase progress weightage get progress weightage phase num fixed weightage for all phases return progress per phase phases equal weightage return progress weightages for phases get phase num synchronized progress get parent return parent synchronized set parent progress parent parent parent called execution move next phase level tree synchronized start next phase current phase returns current sub node executing synchronized progress phase return phases get current phase completes node moving parent node next child complete traverse parent careful locking progress parent synchronized progress f parent parent parent null synchronize parent make sure release lock getting parent since traversing normal traversal direction used get string we need transactional semantics ok parent start next phase called execution leaf node set progress synchronized set progress progress progress returns overall progress root method probably need synchronized get internal synchronized node parent never changes still hurt synchronized get progress node node get parent null find root node parent return node get internal returns progress node get would give overall progress root node given current node synchronized get progress return get internal computes progress node synchronized get internal phase count phases size phase count sub progress f progress from current phase f current phase phase count sub progress phase get
544	common\src\java\org\apache\hadoop\util\Progressable.java	unrelated	package org apache hadoop util a facility reporting progress p clients applications use provided code progressable code explicitly report progress hadoop framework this especially lieu reported progress framework assume error occured time operation p progressable report progress hadoop framework progress
545	common\src\java\org\apache\hadoop\util\ProtoUtil.java	unrelated	package org apache hadoop util proto util read variable length integer format proto bufs encodes read raw varint data input throws io exception byte tmp read byte tmp return tmp result tmp x f tmp read byte result tmp else result tmp x f tmp read byte result tmp else result tmp x f tmp read byte result tmp else result tmp x f result tmp read byte tmp discard upper bits read byte return result throw new io exception malformed varint return result
546	common\src\java\org\apache\hadoop\util\PureJavaCrc32.java	unrelated	package org apache hadoop util a pure java implementation crc checksum uses polynomial built native crc this avoid jni overhead certain uses checksumming many small pieces data checksummed succession the current version x x fast sun native java util zip crc java pure java crc implements checksum current crc value bit flipped crc create new pure java crc object pure java crc reset inherit doc get value return crc xffffffff l inherit doc reset crc xffffffff inherit doc update byte b len local crc crc len c b local crc c b local crc c b local crc c b local crc local crc t c xff t c xff t c xff t c xff local crc t b xff t b xff t b xff t b xff len len local crc local crc t local crc b xff len publish crc object crc local crc inherit doc update b crc crc t crc b xff crc lookup tables generated polynomial x edb see also test pure java crc table t new x x x ee e c x ba x dc x af f x e a x e a x edb x dcb a x e d e e x d d x b c b x eb cbd x e b d x bf d x db x ab f x f b x be de x adad d x ddde eb x f d b x d c x c x ba c x fd f a x a c ec x c f x cd x fa f d x d df x b e c x c e x d e x a x c e d x b d x d d fd x a ab b x b a fa x b c x dbbbc d x acbcf x d ce x df c x dcd dcf x abd d x d ac x de a x c d x bfd x b f b x b c x cfba x b bda f x b e x f x c cd b x b be x f f c x c x c dab x b d d x dc x db x d bc x efd a x b x b b f x fbfe a x e b d x c a x f f x a e x e e x f a dbb x d d d x c x e c x b b f x c c x d x f e x c ed x b a b x f c x f fc x b d c x b e x bbeb ea x fcb c x dd ddf x da d x cd cf x fbd c x db x ab ce x a bc x d bb e x adfa x dd d x a d c d x d d f fb x e a x ed fc x ad x da b d x d x de x aa
547	common\src\java\org\apache\hadoop\util\PureJavaCrc32C.java	unrelated	package org apache hadoop util a pure java implementation crc checksum uses crc c polynomial polynomial used scsi implemented many intel chipsets supporting sse pure java crc c implements checksum current crc value bit flipped crc create new pure java crc object pure java crc c reset inherit doc get value ret crc return ret xffffffff l inherit doc reset crc xffffffff inherit doc update byte b len local crc crc len c b local crc c b local crc c b local crc c b local crc local crc t c xff t c xff t c xff t c xff local crc t b xff t b xff t b xff t b xff len len local crc local crc t local crc b xff len publish crc object crc local crc inherit doc update b crc crc t crc b xff crc polynomial tables generated java cp build test build org apache hadoop util test pure java crc table f b t new x x f b x e b f x f f x c a f x f c x a e e x d ca eb x ad cf x b dbcc x be x ab b x d cfd x bf cd x ac bf x e c x ec f x e c x f b x e b x d c x afd x ff x c a x a fa x ec ca x bbcef x d c x d d bf x af bbc x bc x e dfb b x bd ede x d d ddd x c fe x ed d a x e c x c ac x c x f ea x aa d x f x b fa e x b e x dfe e x f c d x cc f x eaeb fa x e b x c cab x d d x b ba x f deae x dad x ae x e d a x ba a e x d x b x a ae a x da x fcb x c bf x ef x b dbc x b ebf x a d b x bee x e aa x a a x dafa x b x cba x c c x a x d f b x c d c x fe f x ed a b x f x dad x a e d x b eaa x x bf dcc x d cecf x d b x efbe x dbfc c x f x ac f eb x c ac e x c x ee d x fd d f x f e f x c x ad x fde x x a c d x e x a x b cf x eb fcbad x ae x a bb a x f f x c cb x deeedfb x cdbe c x fd af x d x f d e x a fa x c a f x b c x x b e x a e
548	common\src\java\org\apache\hadoop\util\QuickSort.java	unrelated	package org apache hadoop util an implementation core algorithm quick sort quick sort implements indexed sorter indexed sorter alt new heap sort quick sort fix indexed sortable p r compare p r swap p r deepest recursion giving heapsort returns ceil log n protected get max depth x x throw new illegal argument exception undefined x return integer number of leading zeros x sort given range items using quick sort inherit doc if recursion depth falls link get max depth switch link heap sort sort indexed sortable p r sort p r null inherit doc sort indexed sortable p r progressable rep sort internal p r rep get max depth r p sort internal indexed sortable p r progressable rep depth null rep rep progress true r p p r j j p compare j j j swap j j return depth give alt sort p r rep return select move pivot first position fix p r p fix p r r fix p r divide p j r p rr r cr true j cr compare p break cr swap j cr compare p j break cr rr j swap rr j j swap j else break j swap pivot eq values position p swap rr r swap rr j conquer recurse smaller interval first keep stack shallow assert j p r j sort internal p rep depth p j else sort internal j r rep depth r
549	common\src\java\org\apache\hadoop\util\ReflectionUtils.java	unrelated	package org apache hadoop util general reflection utils reflection utils class empty array new class volatile serialization factory serial factory null cache constructors pins garbage collected reflection utils collected map class constructor constructor cache new concurrent hash map class constructor check set configuration necessary set conf object object configuration conf conf null object instanceof configurable configurable object set conf conf set job conf object conf this code support backward compatibility break compile time dependency core mapred this made deprecated along mapred package hadoop should removed mapred package removed set job conf object object configuration conf if job conf job configurable classpath and object type job configurable and conf type job conf invoke configure object try class job conf class conf get class by name org apache hadoop mapred job conf class job configurable class conf get class by name org apache hadoop mapred job configurable job conf class assignable from conf get class job configurable class assignable from object get class method configure method job configurable class get method configure job conf class configure method invoke object conf catch class not found exception e job conf job configurable classpath need configure catch exception e throw new runtime exception error configuring object e create object given initialize conf t t new instance class t class configuration conf t result try constructor t meth constructor t constructor cache get class meth null meth class get declared constructor empty array meth set accessible true constructor cache put class meth result meth new instance catch exception e throw new runtime exception e set conf result conf return result thread mx bean thread bean management factory get thread mx bean set contention tracing boolean val thread bean set thread contention monitoring enabled val string get task name id string name name null return long string id return id name print thread information stack traces print thread info print writer stream string title stack depth boolean contention thread bean thread contention monitoring enabled thread ids thread bean get all thread ids stream println process thread dump title stream println thread ids length active threads tid thread ids thread info info thread bean get thread info tid stack depth info null stream println inactive continue stream println thread get task name info get thread id info get thread name thread state state info get thread state stream println state state stream println blocked count info get blocked count stream println waited count info get waited count contention stream println blocked time info get blocked time stream println waited time info get waited time state thread state waiting stream println waiting info get lock name else state thread state blocked stream println blocked info get lock name stream println blocked get task name info get lock owner id info get lock owner name stream println stack stack trace element frame info get stack trace stream println frame string stream flush previous log time log current thread stacks info level log thread info log log string title min interval boolean dump stack false log info enabled synchronized reflection utils system current
550	common\src\java\org\apache\hadoop\util\RunJar.java	unrelated	package org apache hadoop util run hadoop job jar run jar pattern matches pattern match any pattern compile unpack jar file directory this version unpacks files inside jar regardless filename un jar file jar file file dir throws io exception un jar jar file dir match any unpack matching files jar entries inside jar match given pattern skipped un jar file jar file file dir pattern unpack regex throws io exception jar file jar new jar file jar file try enumeration jar entry entries jar entries entries more elements jar entry entry jar entry entries next element entry directory unpack regex matcher entry get name matches input stream jar get input stream entry try file file new file dir entry get name ensure directory file get parent file output stream new file output stream file try io utils copy bytes finally close finally close finally jar close ensure existence given directory ensure directory file dir throws io exception dir mkdirs dir directory throw new io exception mkdirs failed create dir string run hadoop job jar if main jar manifest must provided command line main string args throws throwable string usage run jar jar file main class args args length system err println usage system exit first arg string file name args first arg file file new file file name string main class name null jar file jar file try jar file new jar file file name catch io exception io throw new io exception error opening job jar file name init cause io manifest manifest jar file get manifest manifest null main class name manifest get main attributes get value main class jar file close main class name null args length system err println usage system exit main class name args first arg main class name main class name replace all file tmp dir new file new configuration get hadoop tmp dir ensure directory tmp dir file work dir file create temp file hadoop unjar tmp dir work dir delete system err println delete failed work dir system exit ensure directory work dir runtime get runtime add shutdown hook new thread run try file util fully delete work dir catch io exception e un jar file work dir array list url path new array list url path add new file work dir uri url path add file uri url path add new file work dir uri url file libs new file work dir lib list files libs null libs length path add libs uri url class loader loader new url class loader path array new url thread current thread set context class loader loader class main class class name main class name true loader method main main class get method main new class array new instance string get class string new args arrays list args sub list first arg args length array new string try main invoke null new object new args catch invocation target exception e throw e get target exception
551	common\src\java\org\apache\hadoop\util\ServicePlugin.java	unrelated	package org apache hadoop util service plug service plug ins may used expose functionality datanodes namenodes using arbitrary rpc protocols plug ins instantiated service instance notified service life cycle events using methods defined service plug ins started service instance started stopped service instance stopped service plugin extends closeable this method invoked service instance started start object service this method invoked service instance shut stop
552	common\src\java\org\apache\hadoop\util\ServletUtil.java	unrelated	package org apache hadoop util servlet util initial html header print writer init html servlet response response string title throws io exception response set content type text html print writer response get writer println html n link rel stylesheet type text css href hadoop css n title title title n body n title n return get parameter servlet request return null parameter contains white spaces string get parameter servlet request request string name string request get parameter name null return null trim return length null string html tail hr n href http hadoop apache org core hadoop calendar get instance get calendar year n body html html footer added jsps string html footer return html tail generate percentage graph returns html representation string percentage graph perc width throws io exception assert perc assert perc string builder builder new string builder builder append table border px width builder append width builder append px tr perc builder append td cellspacing perc filled width builder append perc builder append td perc builder append td cellspacing perc nonfilled width builder append perc builder append td builder append tr table return builder string generate percentage graph returns html representation string percentage graph perc width throws io exception return percentage graph perc width
553	common\src\java\org\apache\hadoop\util\Shell.java	unrelated	package org apache hadoop util a base running unix command code shell code used run unix commands like code du code code df code it also offers facilities gate commands time intervals shell log log log factory get log shell unix command get current user name string user name command whoami unix command get current user groups list string get groups command return new string bash c groups unix command get given user groups list string get groups for user command string user groups username command return non consistent across different unixes return new string bash c id gn user unix command get given netgroup user list string get users for netgroup command string netgroup groups username command return non consistent across different unixes return new string bash c getent netgroup netgroup unix command set permission string set permission command chmod unix command set owner string set owner command chown string set group command chgrp unix command create link string link command ln unix command get link target string read link command readlink return unix command get permission information string get get permission command force bin ls except windows return new string windows ls bin ls ld time executing script would timedout protected time out interval l if script timed atomic boolean timed out unix command get ulimit process string ulimit command ulimit get unix command setting maximum virtual memory available given child process this relevant forking process within mapper reducer implementations also see hadoop pipes hadoop streaming it also checks ensure running nix platform else e g cygwin windows returns code null code code null code running non nix platform limit unspecified string get ulimit memory command memory limit ulimit supported windows windows return null return new string ulimit command v string value of memory limit get unix command setting maximum virtual memory available given child process this relevant forking process within mapper reducer implementations see also hadoop pipes streaming it also checks ensure running nix platform else e g cygwin windows returns code null code code null code running non nix platform limit unspecified string get ulimit memory command configuration conf ulimit supported windows windows return null get memory limit configuration string ulimit conf get mapred child ulimit ulimit null return null parse ensure legal sane memory limit integer value of ulimit return get ulimit memory command memory limit set true windows platforms boolean windows borrowed path windows system get property os name starts with windows interval refresh interval msec last time last time command performed map string string environment env command execution file dir process process sub process used execute command exit code if script finished executing volatile atomic boolean completed shell l command shell interval interval interval last time interval interval set environment command protected set environment map string string env environment env set working directory protected set working directory file dir dir dir check see command needs executed execute needed protected run throws io exception last time interval system current time millis return exit code reset next run run command run command run command throws io exception
554	common\src\java\org\apache\hadoop\util\StringUtils.java	unrelated	package org apache hadoop util general utils string utils decimal format decimal format number format number format number format get number instance locale english decimal format decimal format number format decimal format apply pattern make representation exception string stringify exception throwable e string writer stm new string writer print writer wrt new print writer stm e print stack trace wrt wrt close return stm string given full hostname return word upto first dot string simple hostname string full hostname offset full hostname index of offset return full hostname substring offset return full hostname decimal format one decimal new decimal format given integer return approximate human readable format it uses bases k g string human readable int number abs number math abs number result number string suffix abs number since division occurred format decimal point return string value of number else abs number result number suffix k else abs number result number suffix else result number suffix g return one decimal format result suffix format percentage presentation user string format percent done digits decimal format percent format new decimal format scale math pow digits rounded math floor done scale percent format set decimal separator always shown false percent format set minimum fraction digits digits percent format set maximum fraction digits digits return percent format format rounded scale given array strings return comma separated list elements otherwise string array to string string strs strs length return string builder sbuf new string builder sbuf append strs idx idx strs length idx sbuf append sbuf append strs idx return sbuf string given array bytes convert bytes hex representation bytes string byte to hex string byte bytes start end bytes null throw new illegal argument exception bytes null string builder new string builder start end append string format x bytes return string same byte to hex string bytes bytes length string byte to hex string byte bytes return byte to hex string bytes bytes length given hexstring return byte array corresponding the size byte array therefore hex length byte hex string to byte string hex byte bts new byte hex length bts length bts byte integer parse int hex substring return bts string uri to string uri uris uris null return null string builder ret new string builder uris string uris length ret append ret append uris string return ret string uri to uri string str str null return null uri uris new uri str length str length try uris new uri str catch uri syntax exception ur system println exception specified uri string utils stringify exception ur making sure asssigned null case error uris null return uris path to path string str str null return null path p new path str length str length p new path str return p given finish start time milliseconds returns string format xhrs ymins z sec time difference two times if finish time comes start time negative valeus x y z wil return string format time diff finish time start time time diff finish time start time return format time time diff given time milliseconds returns string format xhrs ymins
555	common\src\java\org\apache\hadoop\util\Tool.java	unrelated	package org apache hadoop util a tool supports handling generic command line options p code tool code standard map reduce tool application the tool application delegate handling href doc root org apache hadoop util generic options parser html generic options standard command line options link tool runner run tool string handle custom arguments p p here typical code tool code implemented p p blockquote pre my app extends configured implements tool run string args throws exception code configuration code processed code tool runner code configuration conf get conf create job conf using processed code conf code job conf job new job conf conf my app process custom command line options path new path args path new path args specify various job specific parameters job set job name app job set input path job set output path job set mapper class my mapper job set reducer class my reducer submit job poll progress job complete job client run job job return main string args throws exception let code tool runner code handle generic command line options res tool runner run new configuration new my app args system exit res pre blockquote p tool extends configurable execute command given arguments run string args throws exception
556	common\src\java\org\apache\hadoop\util\ToolRunner.java	unrelated	package org apache hadoop util a utility help run link tool p code tool runner code used run implementing code tool code it works conjunction link generic options parser parse href doc root org apache hadoop util generic options parser html generic options generic hadoop command line arguments modifies code configuration code code tool code the application specific options passed along without modified p tool runner runs given code tool code link tool run string parsing given generic arguments uses given code configuration code builds one null sets code tool code configuration possibly modified version code conf code run configuration conf tool tool string args throws exception conf null conf new configuration generic options parser parser new generic options parser conf args set configuration back tool configure tool set conf conf get args w generic hadoop args string tool args parser get remaining args return tool run tool args runs code tool code code configuration code equivalent code run tool get conf tool args code run tool tool string args throws exception return run tool get conf tool args prints generic command line argurments usage information print generic command usage print stream generic options parser print generic command usage
557	common\src\java\org\apache\hadoop\util\UTF8ByteArrayUtils.java	unrelated	package org apache hadoop util utf byte array utils find first occurrence given byte b utf encoded find byte byte utf start end byte b start end utf b return return find first occurrence given bytes b utf encoded find bytes byte utf start end byte b match end end b length start match end boolean matched true j j b length j utf j b j matched false break matched return return find nth occurrence given byte b utf encoded find nth byte byte utf start length byte b n pos next start start n pos find byte utf next start length b pos return pos next start pos return pos find nth occurrence given byte b utf encoded find nth byte byte utf byte b n return find nth byte utf utf length b n
558	common\src\java\org\apache\hadoop\util\VersionInfo.java	unrelated	package org apache hadoop util this finds package info hadoop hadoop version annotation information version info log log log factory get log version info package package hadoop version annotation version package hadoop version annotation get package version package get annotation hadoop version annotation get meta data hadoop package package get package return package get hadoop version string get version return version null version version unknown get subversion revision number root directory string get revision return version null version revision unknown get branch originated string get branch return version null version branch unknown the date hadoop compiled string get date return version null version date unknown the user compiled hadoop string get user return version null version user unknown get subversion url root hadoop directory string get url return version null version url unknown get checksum source files hadoop built string get src checksum return version null version src checksum unknown returns build version includes version revision user date string get build version return version info get version version info get revision version info get user source checksum version info get src checksum main string args log debug version version system println hadoop get version system println subversion get url r get revision system println compiled get user get date system println from source checksum get src checksum
559	common\src\java\org\apache\hadoop\util\XMLUtils.java	unrelated	package org apache hadoop util general xml utilities xml utils transform input xml given stylesheet transform input stream style sheet input stream xml writer throws transformer configuration exception transformer exception instantiate transformer factory transformer factory factory transformer factory new instance use transformer factory process stylesheet generate transformer transformer transformer factory new transformer new stream source style sheet use transformer transform xml source send output result object transformer transform new stream source xml new stream result
560	common\src\java\org\apache\hadoop\util\bloom\BloomFilter.java	unrelated	package org apache hadoop util bloom implements bloom filter defined bloom p the bloom filter data structure introduced adopted networking research community past decade thanks bandwidth efficiencies offers transmission set membership information networked hosts a sender encodes information bit vector bloom filter compact conventional representation computation space costs construction linear number elements the receiver uses filter test whether various elements members set though filter occasionally return false positive never return false negative when creating filter sender choose desired point trade false positive rate size p originally created href http www one lab org european commission one lab project bloom filter extends filter byte bitvalues new byte byte x byte x byte x byte x byte x byte x byte x byte x the bit vector bit set bits default constructor use read fields bloom filter super constructor link org apache hadoop util hash hash bloom filter vector size nb hash hash type super vector size nb hash hash type bits new bit set vector size add key key key null throw new null pointer exception key cannot null hash hash key hash clear nb hash bits set filter filter filter null filter instanceof bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot ed bits bloom filter filter bits boolean membership test key key key null throw new null pointer exception key cannot null hash hash key hash clear nb hash bits get return false return true bits flip vector size filter filter filter null filter instanceof bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot ed bits bloom filter filter bits xor filter filter filter null filter instanceof bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot xor ed bits xor bloom filter filter bits string string return bits string get vector size return vector size writable write data output throws io exception super write byte bytes new byte get n bytes byte index bit index vector size bit index bit index bit index byte index bit index bytes byte index bits get bytes byte index bitvalues bit index write bytes read fields data input throws io exception super read fields bits new bit set vector size byte bytes new byte get n bytes read fully bytes byte index bit index vector size bit index bit index bit index byte index bytes byte index bitvalues bit index bits set get n bytes return vector size end
561	common\src\java\org\apache\hadoop\util\bloom\CountingBloomFilter.java	unrelated	package org apache hadoop util bloom implements counting bloom filter defined fan et al to n paper p a counting bloom filter improvement standard bloom filter allows dynamic additions deletions set membership information this achieved use counting vector instead bit vector p originally created href http www one lab org european commission one lab project counting bloom filter extends filter storage counting buckets buckets we using bit buckets bucket count bucket max value default constructor use read fields counting bloom filter constructor link org apache hadoop util hash hash counting bloom filter vector size nb hash hash type super vector size nb hash hash type buckets new buckets words vector size returns number bit words would take hold vector size buckets buckets words vector size return vector size add key key key null throw new null pointer exception key null hash hash key hash clear nb hash find bucket word num div bucket shift x f mod bucket mask l bucket shift bucket value buckets word num bucket mask bucket shift increment count bucket less bucket max value bucket value bucket max value increment buckets word num buckets word num bucket mask bucket value bucket shift removes specified key counting bloom filter p b invariant b nothing happens specified key belong counter bloom filter delete key key key null throw new null pointer exception key may null membership test key throw new illegal argument exception key member hash hash key hash clear nb hash find bucket word num div bucket shift x f mod bucket mask l bucket shift bucket value buckets word num bucket mask bucket shift decrement count bucket bucket max value bucket value bucket value bucket max value decrement buckets word num buckets word num bucket mask bucket value bucket shift filter filter filter null filter instanceof counting bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot ed counting bloom filter cbf counting bloom filter filter size in words buckets words vector size size in words buckets cbf buckets boolean membership test key key key null throw new null pointer exception key may null hash hash key hash clear nb hash find bucket word num div bucket shift x f mod bucket mask l bucket shift buckets word num bucket mask return false return true this method calculates approximate count key e many times key added filter this allows filter used approximate code key gt count code map p note due bucket size filter inserting key times cause overflow filter positions associated key significantly increase error rate keys for reason filter used store small count values code lt n lt lt code returned code v count code probability equal error rate filter code v gt count code otherwise additionally filter experienced underflow result link delete key operation return value may lower code count code probability false negative rate filter approximate count key key res integer max value hash hash key hash clear nb hash find bucket word num div bucket shift x f mod bucket mask l bucket shift
562	common\src\java\org\apache\hadoop\util\bloom\DynamicBloomFilter.java	unrelated	package org apache hadoop util bloom implements dynamic bloom filter defined infocom paper p a dynamic bloom filter dbf makes use code code bit matrix code code rows standard bloom filter the creation process dbf iterative at start dbf code code bit matrix e composed single standard bloom filter it assumes code n sub r sub code elements recorded initial bit vector code n sub r sub n code code n code cardinality set code a code record filter p as size code a code grows execution application several keys must inserted dbf when inserting key dbf one must first get active bloom filter matrix a bloom filter active number recorded keys code n sub r sub code strictly less current cardinality code a code code n code if active bloom filter found key inserted code n sub r sub code incremented one on hand active bloom filter new one created e new row added matrix according current size code a code element added new bloom filter code n sub r sub code value new bloom filter set one a given key said belong dbf code k code positions set one one matrix rows p originally created href http www one lab org european commission one lab project dynamic bloom filter extends filter threshold maximum number key record dynamic bloom filter row nr the number keys recorded current standard active bloom filter current nb record the matrix bloom filter bloom filter matrix zero args constructor serialization dynamic bloom filter constructor p builds empty dynamic bloom filter link org apache hadoop util hash hash dynamic bloom filter row dynamic bloom filter vector size nb hash hash type nr super vector size nb hash hash type nr nr current nb record matrix new bloom filter matrix new bloom filter vector size nb hash hash type add key key key null throw new null pointer exception key null bloom filter bf get active standard bf bf null add row bf matrix matrix length current nb record bf add key current nb record filter filter filter null filter instanceof dynamic bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot ed dynamic bloom filter dbf dynamic bloom filter filter dbf matrix length matrix length dbf nr nr throw new illegal argument exception filters cannot ed matrix length matrix dbf matrix boolean membership test key key key null return true matrix length matrix membership test key return true return false matrix length matrix filter filter filter null filter instanceof dynamic bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot ed dynamic bloom filter dbf dynamic bloom filter filter dbf matrix length matrix length dbf nr nr throw new illegal argument exception filters cannot ed matrix length matrix dbf matrix xor filter filter filter null filter instanceof dynamic bloom filter filter vector size vector size filter nb hash nb hash throw new illegal argument exception filters cannot xor ed dynamic bloom filter dbf dynamic bloom filter filter dbf matrix
563	common\src\java\org\apache\hadoop\util\bloom\Filter.java	unrelated	package org apache hadoop util bloom defines general behavior filter p a filter data structure aims offering lossy summary set code a code the key idea map entries code a code also called keys several positions vector use several hash functions p typically filter implemented bloom filter bloom filter extension p it must extended order define real behavior filter implements writable version negative accommodate old format the vector size filter protected vector size the hash function used map key several positions vector protected hash function hash the number hash function consider protected nb hash type hashing function use protected hash type protected filter constructor protected filter vector size nb hash hash type vector size vector size nb hash nb hash hash type hash type hash new hash function vector size nb hash hash type adds key filter add key key determines wether specified key belongs filter false otherwise boolean membership test key key peforms logical and filter specified filter p b invariant b the result assigned filter filter filter peforms logical or filter specified filter p b invariant b the result assigned filter filter filter peforms logical xor filter specified filter p b invariant b the result assigned filter xor filter filter performs logical not filter p the result assigned filter adds list keys filter add list key keys keys null throw new illegal argument exception array list key may null key key keys add key end add adds collection keys filter add collection key keys keys null throw new illegal argument exception collection key may null key key keys add key end add adds array keys filter add key keys keys null throw new illegal argument exception key may null keys length add keys end add writable write data output throws io exception write int version write int nb hash write byte hash type write int vector size read fields data input throws io exception ver read int ver old unversioned format nb hash ver hash type hash jenkins hash else ver version nb hash read int hash type read byte else throw new io exception unsupported version ver vector size read int hash new hash function vector size nb hash hash type end
564	common\src\java\org\apache\hadoop\util\bloom\HashFunction.java	unrelated	package org apache hadoop util bloom implements hash object returns certain number hashed values hash function the number hashed values nb hash the maximum highest returned value max value hashing algorithm use hash hash function constructor p builds hash function must obey given maximum number returned values highest value hash function max value nb hash hash type max value throw new illegal argument exception max value must nb hash throw new illegal argument exception nb hash must max value max value nb hash nb hash hash function hash get instance hash type hash function null throw new illegal argument exception hash type must known clears hash function a noop clear hashes specified key several integers hash key k byte b k get bytes b null throw new null pointer exception buffer reference null b length throw new illegal argument exception key length must result new nb hash initval nb hash initval hash function hash b initval result math abs initval max value return result
565	common\src\java\org\apache\hadoop\util\bloom\Key.java	unrelated	package org apache hadoop util bloom the general behavior key must stored filter key implements writable comparable key byte value key byte bytes the weight associated key p b invariant b specified instance code key code default weight weight default constructor use read fields key constructor p builds key default weight key byte value value constructor p builds key specified weight key byte value weight set value weight set byte value weight value null throw new illegal argument exception value null bytes value weight weight byte get bytes return bytes get weight return weight increments weight key specified value increment weight weight weight weight increments weight key one increment weight weight boolean equals object instanceof key return false return compare to key hash code result bytes length result byte value of bytes hash code result double value of weight hash code return result writable write data output throws io exception write int bytes length write bytes write double weight read fields data input throws io exception bytes new byte read int read fully bytes weight read double comparable compare to key result bytes length get bytes length result bytes length result bytes bytes result result double value of weight weight value return result
566	common\src\java\org\apache\hadoop\util\bloom\RemoveScheme.java	unrelated	package org apache hadoop util bloom defines different remove scheme retouched bloom filters p originally created href http www one lab org european commission one lab project remove scheme random selection p the idea randomly select bit reset short random minimum fn selection p the idea select bit reset generate minimum number false negative short minimum fn maximum fp selection p the idea select bit reset remove maximum number false positive short maximum fp ratio selection p the idea select bit reset time remove maximum number false positve minimizing amount false negative generated short ratio
567	common\src\java\org\apache\hadoop\util\bloom\RetouchedBloomFilter.java	unrelated	package org apache hadoop util bloom implements retouched bloom filter defined co next paper p it allows removal selected false positives cost introducing random false negatives benefit eliminating random false positives time p originally created href http www one lab org european commission one lab project retouched bloom filter extends bloom filter implements remove scheme key list vector element list vector defined paper false positives list key fp vector key list vector keys recorded filter list key key vector ratio vector ratio random rand default constructor use read fields retouched bloom filter constructor link org apache hadoop util hash hash retouched bloom filter vector size nb hash hash type super vector size nb hash hash type rand null create vector add key key key null throw new null pointer exception key null hash hash key hash clear nb hash bits set key vector add key adds false positive information retouched bloom filter p b invariant b false positive code null code nothing happens add false positive key key key null throw new null pointer exception key null hash hash key hash clear nb hash fp vector add key adds collection false positive information retouched bloom filter add false positive collection key coll coll null throw new null pointer exception collection key null key k coll add false positive k adds list false positive information retouched bloom filter add false positive list key keys keys null throw new null pointer exception array list key null key k keys add false positive k adds array false positive information retouched bloom filter add false positive key keys keys null throw new null pointer exception key null keys length add false positive keys performs selective clearing given key selective clearing key k short scheme k null throw new null pointer exception key null membership test k throw new illegal argument exception key member index hash hash k switch scheme case random index random remove break case minimum fn index minimum fn remove break case maximum fp index maximum fp remove break case ratio index ratio remove break default throw new assertion error undefined selective clearing scheme clear bit index random remove rand null rand new random return rand next int nb hash chooses bit position minimizes number false negative generated minimum fn remove min index integer max value min value double max value nb hash key weight get weight key vector key weight min value min index min value key weight return min index chooses bit position maximizes number false positive removed maximum fp remove max index integer min value max value double min value nb hash fp weight get weight fp vector fp weight max value max value fp weight max index return max index chooses bit position minimizes number false negative generated maximizing number false positive removed ratio remove compute ratio min index integer max value min value double max value nb hash ratio min value min value ratio min index return min index clears specified bit bit vector keeps date key list vectors clear bit index index index vector size throw new array
568	common\src\java\org\apache\hadoop\util\hash\Hash.java	unrelated	package org apache hadoop util hash this represents common api hashing functions hash constant denote invalid hash type invalid hash constant denote link jenkins hash jenkins hash constant denote link murmur hash murmur hash this utility method converts string representation hash function name symbolic constant currently two function types supported jenkins murmur parse hash type string name jenkins equals ignore case name return jenkins hash else murmur equals ignore case name return murmur hash else return invalid hash this utility method converts name configured hash type symbolic constant get hash type configuration conf string name conf get hadoop util hash type murmur return parse hash type name get singleton instance hash function given type hash get instance type switch type case jenkins hash return jenkins hash get instance case murmur hash return murmur hash get instance default return null get singleton instance hash function type defined configuration hash get instance configuration conf type get hash type conf return get instance type calculate hash using bytes input argument seed hash byte bytes return hash bytes bytes length calculate hash using bytes input argument provided seed value hash byte bytes initval return hash bytes bytes length initval calculate hash using bytes code length code provided seed value hash byte bytes length initval
569	common\src\java\org\apache\hadoop\util\hash\JenkinsHash.java	unrelated	package org apache hadoop util hash produces bit hash hash table lookup pre lookup c bob jenkins may public domain you use free purpose it domain it warranty pre function compares others crc md etc dr dobbs article jenkins hash extends hash int mask x ffffffff l byte mask x ff l jenkins hash instance new jenkins hash hash get instance return instance rot val pos return integer rotate left val int mask pos int mask taken hashlittle hash variable length key bit value return value two keys differing one two bits totally different hash values p the best hash table sizes powers there need mod prime mod sooo slow if need less bits use bitmask for example need bits code hashmask code in case hash table hashsize elements p if hashing n strings byte k like n hash k p by bob jenkins bob jenkins burtleburtle net you may use code way wish educational commercial it free p use hash table lookup anything one collision acceptable do not use cryptographic purposes hash byte key nbytes initval length nbytes b c we use longs unsigned ints b c x deadbeef l length initval int mask offset length offset length key offset byte mask int mask key offset byte mask int mask int mask key offset byte mask int mask int mask key offset byte mask int mask int mask b b key offset byte mask int mask b b key offset byte mask int mask int mask b b key offset byte mask int mask int mask b b key offset byte mask int mask int mask c c key offset byte mask int mask c c key offset byte mask int mask int mask c c key offset byte mask int mask int mask c c key offset byte mask int mask int mask mix mix bit values reversibly this reversible information b c mix still b c mix if four pairs b c inputs run mix mix reverse least bits output sometimes one pair different another pair this tested pairs differed one bit two bits combination top bits b c combination bottom bits b c differ defined for i transformed output delta gray code commonly produced subtraction look like single bit difference base values pseudorandom zero one bit set zero plus counter starts zero some k values c rot c k c b arrangement satisfy well quite get bits diffing differ defined one bit base two bit delta i used http burtleburtle net bob hash avalanche html choose operations constants arrangements variables this achieve avalanche there input bits b c fail affect output bits b c especially the thoroughly mixed value c really even achieve avalanche c this allows parallelism read writes good doubling number bits affected goal mixing pulls opposite direction goal parallelism i i could rotates seem cost much shifts every machine i could lay hands rotates much kinder top bottom bits i used rotates define mix b c c rot c c b b b rot c c b c rot b b c rot c c
570	common\src\java\org\apache\hadoop\util\hash\MurmurHash.java	unrelated	package org apache hadoop util hash this fast non cryptographic hash suitable general hash based lookup see http murmurhash googlepages com details p the c version murmur hash found site ported java andrzej bialecki ab getopt org p murmur hash extends hash murmur hash instance new murmur hash hash get instance return instance hash byte data length seed x bd e r seed length len length len k data k k k k data xff k k k k data xff k k k k data xff k k k r k k avoid calculating modulo len len left length len left left data length left data length left data length return
571	hdfs\src\ant\org\apache\hadoop\ant\DfsTask.java	unrelated	package org apache hadoop ant link org apache hadoop fs fs shell fs shell wrapper ant task dfs task extends task default sink link java lang system system link java lang system err system err output stream null out new output stream write b ignore string string return fs shell shell new fs shell protected ant class loader confloader protected output stream null out protected output stream err null out set ant protected string cmd protected linked list string argv new linked list string protected string outprop protected string errprop protected boolean failonerror true saved ant context print stream ant out print stream ant err sets command run link org apache hadoop fs fs shell fs shell set cmd string cmd cmd cmd trim sets argument list string comma separated values set args string args string args trim split argv add sets property system written if property defined task executed updated set out string outprop outprop outprop new byte array output stream outprop equals errprop err sets property system err written if property name property system two interlaced if property defined task executed updated set err string errprop errprop errprop err errprop equals outprop err new byte array output stream sets path parent last class loader intended used link org apache hadoop conf configuration configuration parent class loaders set conf string confpath confloader new ant class loader get class get class loader false confloader set project get project null confpath confloader add path element confpath sets property controlling whether link org apache tools ant build exception build exception thrown command returns value less zero throws exception set failonerror boolean failonerror failonerror failonerror save current values system system err configure output streams fs shell protected push context ant out system ant err system err system set out new print stream system set err err system new print stream err create appropriate output properties respective output restore system system err release resources created class loaders aid garbage collection protected pop context write output property applicable outprop null system check error get project set new property outprop string err errprop null system err check error get project set new property errprop err string system set err ant err system set out ant out confloader cleanup confloader set parent null case dfs task overridden protected post cmd exit code test equals cmd exit code outprop null return exit code invoke link org apache hadoop fs fs shell main fs shell main cursory checks configuration execute throws build exception null cmd throw new build exception missing command cmd argument argv add cmd null confloader set conf get project get property hadoop conf dir exit code try push context configuration conf new hdfs configuration conf set class loader confloader exit code tool runner run conf shell argv array new string argv size exit code post cmd exit code exit code string builder msg new string builder string argv msg append msg append failed exit code throw new exception msg string catch exception e failonerror throw new build exception e finally pop context
572	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsBaseConditional.java	unrelated	package org apache hadoop ant condition this wrapper around link org apache hadoop ant dfs task implements ant gt link org apache tools ant taskdefs condition condition condition hdfs tests so one test conditions like code condition property precond hadoop exists file file a hadoop exists file file b hadoop sizezero file file b condition this define property precond file a exists file b zero length dfs base conditional extends org apache hadoop ant dfs task implements condition protected boolean result string file init args set cmd test set args get flag file set file string file file file protected char get flag protected post cmd exit code exit code super post cmd exit code result exit code return exit code boolean eval init args execute return result
573	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsExists.java	unrelated	package org apache hadoop ant condition dfs exists extends dfs base conditional protected char flag e protected char get flag return flag
574	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsIsDir.java	unrelated	package org apache hadoop ant condition dfs is dir extends dfs base conditional protected char flag protected char get flag return flag
575	hdfs\src\ant\org\apache\hadoop\ant\condition\DfsZeroLen.java	unrelated	package org apache hadoop ant condition dfs zero len extends dfs base conditional protected char flag z protected char get flag return flag
576	hdfs\src\java\org\apache\hadoop\fs\Hdfs.java	unrelated	package org apache hadoop fs hdfs extends abstract file system dfs client dfs boolean verify checksum true hdfs configuration init this constructor signature needed link abstract file system create file system uri configuration must hdfs hdfs uri uri configuration conf throws io exception uri syntax exception super uri fs constants hdfs uri scheme true name node default port uri get scheme equals ignore case fs constants hdfs uri scheme throw new illegal argument exception passed uri scheme hdfs string host uri get host host null throw new io exception incomplete hdfs uri host uri inet socket address namenode name node get address uri get authority dfs new dfs client namenode conf get statistics get uri default port return name node default port fs data output stream create internal path f enum set create flag create flag fs permission absolute permission buffer size short replication block size progressable progress bytes per checksum boolean create parent throws io exception return new fs data output stream dfs primitive create get uri path f absolute permission create flag create parent replication block size progress buffer size bytes per checksum get statistics boolean delete path f boolean recursive throws io exception unresolved link exception return dfs delete get uri path f recursive block location get file block locations path p start len throws io exception unresolved link exception return dfs get block locations get uri path p start len file checksum get file checksum path f throws io exception unresolved link exception return dfs get file checksum get uri path f file status get file status path f throws io exception unresolved link exception hdfs file status fi dfs get file info get uri path f fi null return make qualified fi f else throw new file not found exception file exist f string file status get file link status path f throws io exception unresolved link exception hdfs file status fi dfs get file link info get uri path f fi null return make qualified fi f else throw new file not found exception file exist f file status make qualified hdfs file status f path parent nb symlink made fully qualified file context return new file status f get len f dir f get replication f get block size f get modification time f get access time f get permission f get owner f get group f symlink new path f get symlink null f get full path parent make qualified get uri null fully qualify path located file status make qualified located hdfs located file status f path parent return new located file status f get len f dir f get replication f get block size f get modification time f get access time f get permission f get owner f get group f symlink new path f get symlink null f get full path parent make qualified get uri null fully qualify path dfs util located blocks locations f get block locations fs status get fs status throws io exception return dfs get disk status fs server defaults get server defaults throws io
577	hdfs\src\java\org\apache\hadoop\hdfs\BlockMissingException.java	unrelated	package org apache hadoop hdfs this exception thrown read encounters block locations associated block missing exception extends io exception serial version uid l string filename offset an exception indicates file corrupted block missing exception string filename string description offset super description filename filename offset offset returns name corrupted file string get file return filename returns offset file corrupted get offset return offset
578	hdfs\src\java\org\apache\hadoop\hdfs\BlockReader.java	pooling	package org apache hadoop hdfs this wrapper around connection datanode understands checksum offset etc terminology dl dt block dt dd the hdfs block typically large mb dd dt chunk dt dd a block divided chunks comes checksum we want transfers chunk aligned able verify checksums dd dt packet dt dd a grouping chunks used transport it contains header followed checksum data followed real data dd dl please see data node rpc specification block reader extends fs input checker socket dn sock sending status code e g checksum ok read data input stream data checksum checksum offset block last chunk received last chunk offset last chunk len last seq no offset block reader wants actually read start offset offset block first chunk may less start offset start offset chunk aligned first chunk offset bytes per checksum checksum size the total number bytes need transfer dn this amount user requested plus padding beginning read begin chunk boundary bytes needed to finish boolean eos false boolean sent status code false byte skip buf null byte buffer checksum bytes null amount unread data current received packet data left fs input checker input stream java io input stream read used dfs input stream read this violates one rule checksum error read modify user buffer successful read first reads data user buffer checks checksum synchronized read byte buf len throws io exception this set skip since hit eos skip case entire read smaller checksum chunk boolean eos before eos first read skip extra bytes front last chunk len start offset first chunk offset len skip bytes but call skip skip start offset first chunk offset skip buf null skip buf new byte bytes per checksum super read skip buf skip skip never happen throw new io exception could skip required number bytes n read super read buf len eos set previous read send status code dn eos eos before n read need checksum send read result dn sock status checksum ok else send read result dn sock status success return n read synchronized skip n throws io exception how make sure throw checksum exception least majority cases this one throws skip buf null skip buf new byte bytes per checksum n skipped n skipped n skip math min n n skipped skip buf length ret read skip buf skip ret return n skipped n skipped ret return n skipped read throws io exception throw new io exception read expected invoked use read buf len instead boolean seek to new source target pos throws io exception checksum errors handled outside block reader dfs input stream always call seek to new source in case pread tries different replica without seeking return false seek pos throws io exception throw new io exception seek supported block input checker protected get chunk position pos throw new runtime exception get chunk position supported since seek required makes sure checksum bytes enough capacity limit set number checksum bytes needed read adjust checksum bytes data len required size data len bytes per checksum bytes per checksum checksum size checksum bytes null required size checksum bytes capacity
579	hdfs\src\java\org\apache\hadoop\hdfs\ByteRangeInputStream.java	unrelated	package org apache hadoop hdfs to support http byte streams new connection http server needs created time this hides complexity multiple connections client whenever seek called new connection made successive read the normal input stream functions connected currently active input stream byte range input stream extends fs input stream this wraps url allow easy mocking testing the url cannot easily mocked url opener protected url url url opener url u url u set url url u url u url get url return url http url connection open connection throws io exception return http url connection url open connection enum stream status normal seek protected input stream protected url opener original url protected url opener resolved url protected start pos protected current pos protected filelength stream status status stream status seek byte range input stream url url new url opener url new url opener null byte range input stream url opener url opener r original url resolved url r input stream get input stream throws io exception status stream status normal null close null use original url resolved url exists eg first time request made url opener opener resolved url get url null original url resolved url http url connection connection opener open connection try connection set request method get start pos connection set request property range bytes start pos connection connect string cl connection get header field stream file content length filelength cl null long parse long cl hftp file system log debug enabled hftp file system log debug filelength filelength connection get input stream catch io exception ioe hftp file system throw io exception from connection connection ioe resp code connection get response code start pos resp code http url connection http partial we asked byte range receive partial content response throw new io exception http partial expected received resp code else start pos resp code http url connection http ok we asked bytes beginning receive response none xx codes valid throw new io exception http ok expected received resp code resolved url set url connection get url status stream status normal return update boolean eof n throws io exception eof current pos n else current pos filelength throw new io exception got eof current pos current pos filelength filelength read throws io exception b get input stream read update b return b seek given offset start file the next read location can seek past end file seek pos throws io exception pos current pos start pos pos current pos pos status stream status seek return current offset start file get pos throws io exception return current pos seeks different copy data returns true found new source false otherwise boolean seek to new source target pos throws io exception return false
580	hdfs\src\java\org\apache\hadoop\hdfs\CorruptFileBlockIterator.java	unrelated	package org apache hadoop hdfs provides iterator list corrupt file blocks this used distributed file system hdfs corrupt file block iterator implements remote iterator path dfs client dfs string path string files null file idx string cookie null path next path null calls made corrupt file block iterator dfs client dfs path path throws io exception dfs dfs path path string path load next this debugging testing purposes get calls made return calls made string path string path path return path uri get path path path string return new path load next throws io exception files null file idx files length corrupt file blocks cfb dfs list corrupt file blocks path cookie files cfb get files cookie cfb get cookie file idx calls made file idx files length received empty response corrupt file blocks next path null else next path path files file idx file idx inherit doc boolean next return next path null inherit doc path next throws io exception next throw new no such element exception no corrupt file blocks path result next path load next return result
581	hdfs\src\java\org\apache\hadoop\hdfs\DeprecatedUTF8.java	unrelated	package org apache hadoop hdfs a simple wrapper around link org apache hadoop io utf this used absolutely necessary use link org apache hadoop io utf the difference using require suppress warning annotation avoid javac warning instead deprecation implied name this treated package hdfs deprecated utf extends org apache hadoop io utf deprecated utf super construct given deprecated utf string super construct given deprecated utf deprecated utf utf super utf the following two mostly commonly used methods wrapping editors complain deprecation string read string data input throws io exception return org apache hadoop io utf read string write string data output string throws io exception return org apache hadoop io utf write string
582	hdfs\src\java\org\apache\hadoop\hdfs\DFSClient.java	unrelated	package org apache hadoop hdfs dfs client connect hadoop filesystem perform basic file tasks it uses client protocol communicate name node daemon connects directly data nodes read write block data hadoop dfs users obtain instance distributed file system uses dfs client handle filesystem tasks dfs client implements fs constants java io closeable log log log factory get log dfs client server defaults validity period l hour tcp window size kb client protocol namenode client protocol rpc namenode user group information ugi volatile boolean client running true volatile fs server defaults server defaults volatile server defaults last update string client name configuration conf socket factory socket factory replace datanode on failure dtp replace datanode on failure file system statistics stats hdfs timeout timeout value dfs operation lease renewer leaserenewer socket cache socket cache conf dfs client conf dfs client configuration conf max block acquire failures conf time io buffer size bytes per checksum write packet size socket timeout socket cache capacity wait time window msec block missing exception caught time window n cached conn retry n block write retry n block write locate following retry default block size prefetch size short default replication string task id fs permission u mask conf configuration conf max block acquire failures conf get int dfs client max block acquire failures key dfs client max block acquire failures default conf time conf get int dfs datanode socket write timeout key hdfs constants write timeout io buffer size conf get int common configuration keys public io file buffer size key common configuration keys public io file buffer size default bytes per checksum conf get int dfs bytes per checksum key dfs bytes per checksum default socket timeout conf get int dfs client socket timeout key hdfs constants read timeout dfs write packet size internal config variable write packet size conf get int dfs client write packet size key dfs client write packet size default default block size conf get long dfs block size key default block size default replication short conf get int dfs replication key dfs replication default task id conf get mapreduce task attempt id nonmapreduce socket cache capacity conf get int dfs client socket cache capacity key dfs client socket cache capacity default prefetch size conf get long dfs client read prefetch size key default block size time window conf get int dfs client retry window base n cached conn retry conf get int dfs client cached conn retry key dfs client cached conn retry default n block write retry conf get int dfs client block write retries key dfs client block write retries default n block write locate following retry conf get int dfs client block write locatefollowingblock retries key dfs client block write locatefollowingblock retries default u mask fs permission get u mask conf conf get conf return dfs client conf a map file names link dfs output stream objects currently written client note file written single client map string dfs output stream files being written new hash map string dfs output stream same name node get address conf conf dfs client configuration conf
583	hdfs\src\java\org\apache\hadoop\hdfs\DFSConfigKeys.java	heartbeat	package org apache hadoop hdfs this contains constants configuration keys used hdfs dfs config keys extends common configuration keys string dfs block size key dfs blocksize dfs block size default string dfs replication key dfs replication short dfs replication default string dfs stream buffer size key dfs stream buffer size dfs stream buffer size default string dfs bytes per checksum key dfs bytes per checksum dfs bytes per checksum default string dfs client write packet size key dfs client write packet size dfs client write packet size default string dfs client write replace datanode on failure enable key dfs client block write replace datanode failure enable boolean dfs client write replace datanode on failure enable default true string dfs client write replace datanode on failure policy key dfs client block write replace datanode failure policy string dfs client write replace datanode on failure policy default default string dfs client socket cache capacity key dfs client socketcache capacity dfs client socket cache capacity default string dfs namenode backup address key dfs namenode backup address string dfs namenode backup address default localhost string dfs namenode backup http address key dfs namenode backup http address string dfs namenode backup http address default string dfs namenode backup service rpc address key dfs namenode backup dnrpc address string dfs datanode balance bandwidthpersec key dfs datanode balance bandwidth per sec dfs datanode balance bandwidthpersec default string dfs namenode http address key dfs namenode http address string dfs namenode http address default string dfs namenode rpc address key dfs namenode rpc address string dfs namenode service rpc address key dfs namenode servicerpc address string dfs namenode max objects key dfs namenode max objects dfs namenode max objects default string dfs namenode safemode extension key dfs namenode safemode extension dfs namenode safemode extension default string dfs namenode safemode threshold pct key dfs namenode safemode threshold pct dfs namenode safemode threshold pct default f set slightly smaller value dfs namenode safemode threshold pct default populate needed replication queues exiting safe mode string dfs namenode repl queue threshold pct key dfs namenode replqueue threshold pct string dfs namenode safemode min datanodes key dfs namenode safemode min datanodes dfs namenode safemode min datanodes default string dfs namenode secondary http address key dfs namenode secondary http address string dfs namenode secondary http address default string dfs namenode checkpoint period key dfs namenode checkpoint period dfs namenode checkpoint period default string dfs namenode checkpoint size key dfs namenode checkpoint size dfs namenode checkpoint size default string dfs namenode upgrade permission key dfs namenode upgrade permission dfs namenode upgrade permission default string dfs namenode heartbeat recheck interval key dfs namenode heartbeat recheck interval dfs namenode heartbeat recheck interval default string dfs client https keystore resource key dfs client https keystore resource string dfs client https keystore resource default ssl client xml string dfs client https need auth key dfs client https need auth boolean dfs client https need auth default false string dfs client cached conn retry key dfs client cached conn retry dfs client cached conn retry default string dfs namenode accesstime precision key dfs
584	hdfs\src\java\org\apache\hadoop\hdfs\DFSInputStream.java	unrelated	package org apache hadoop hdfs dfs input stream provides bytes named file it handles negotiation namenode various datanodes necessary dfs input stream extends fs input stream socket cache socket cache dfs client dfs client boolean closed false string src prefetch size block reader block reader null boolean verify checksum located blocks located blocks null last block being written length datanode info current node null located block current located block null pos block end this variable tracks number failures since start recent user facing operation that say reset whenever user makes call stream point retry logic failure count exceeds threshold errors thrown back operation specifically counts number times client gone back namenode get new list block locations capped max block acquire failures failures time window xxx use cocurrent hash map temp fix need fix parallel accesses dfs input stream ptreads properly concurrent hash map datanode info datanode info dead nodes new concurrent hash map datanode info datanode info buffersize byte one byte buf new byte used read n cached conn retry add to dead nodes datanode info dn info dead nodes put dn info dn info dfs input stream dfs client dfs client string src buffersize boolean verify checksum throws io exception unresolved link exception dfs client dfs client verify checksum verify checksum buffersize buffersize src src socket cache dfs client socket cache prefetch size dfs client get conf prefetch size time window dfs client get conf time window n cached conn retry dfs client get conf n cached conn retry open info grab open file info namenode synchronized open info throws io exception unresolved link exception located blocks new info dfs client call get block locations dfs client namenode src prefetch size dfs client log debug enabled dfs client log debug new info new info new info null throw new io exception cannot open filename src located blocks null iterator located block old iter located blocks get located blocks iterator iterator located block new iter new info get located blocks iterator old iter next new iter next old iter next get block equals new iter next get block throw new io exception blocklist src changed located blocks new info last block being written length located blocks last block complete located block last located blocks get last located block last null len read block length last last get block set num bytes len last block being written length len current node null read block length one datanodes read block length located block locatedblock throws io exception locatedblock null locatedblock get locations length return replica not found count locatedblock get locations length datanode info datanode locatedblock get locations client datanode protocol cdp null try cdp dfs util create client datanode protocol proxy datanode dfs client conf dfs client get conf socket timeout locatedblock n cdp get replica visible length locatedblock get block n return n catch io exception ioe ioe instanceof remote exception remote exception ioe unwrap remote exception instanceof replica not found exception special case replica might dn treat length replica not found count dfs client log debug enabled dfs client log debug
585	hdfs\src\java\org\apache\hadoop\hdfs\DFSOutputStream.java	heartbeat	package org apache hadoop hdfs dfs output stream creates files stream bytes the client application writes data cached internally stream data broken packets packet typically k size a packet comprises chunks each chunk typically bytes associated checksum when client application fills current packet enqueued data queue the data streamer thread picks packets data queue sends first datanode pipeline moves data queue ack queue the response processor receives acks datanodes when successful ack packet received datanodes response processor removes corresponding packet ack queue in case error outstanding packets moved ack queue a new pipeline setup eliminating bad datanode original pipeline the data streamer starts sending packets data queue dfs output stream extends fs output summer implements syncable dfs client dfs client max packets packet k total mb socket closed accessed different threads different locks volatile boolean closed false string src block size data checksum checksum data queue ack queue protected data queue lock linked list packet data queue new linked list packet linked list packet ack queue new linked list packet packet current packet null data streamer streamer current seqno last queued seqno last acked seqno bytes cur block bytes writen current block packet size write packet size including header chunks per packet volatile io exception last exception null artificial slowdown last flush offset offset flush invoked persist blocks namenode atomic boolean persist blocks new atomic boolean false volatile boolean append chunk false appending existing partial block initial file size time file open progressable progress short block replication replication factor file packet seqno sequencenumber buffer block offset in block offset block boolean last packet in block last packet block num chunks number chunks currently packet max chunks max chunks packet buffer accumulating packet checksum data byte buffer buffer wraps buf one two may non null byte buf buf pointed like follows c checksum data d payload data hhhhhccccc dddddddddddddddd checksum pos data start data pos checksum start checksum start data start data pos checksum pos heart beat seqno l create heartbeat packet packet last packet in block false num chunks offset in block seqno heart beat seqno buffer null packet size packet header pkt header len dfs client size of integer todo todd strange buf new byte packet size checksum start data start packet size checksum pos checksum start data pos data start max chunks create new packet packet pkt size chunks per pkt offset in block last packet in block false num chunks offset in block offset in block seqno current seqno current seqno buffer null buf new byte pkt size checksum start packet header pkt header len checksum pos checksum start data start checksum start chunks per pkt checksum get checksum size data pos data start max chunks chunks per pkt write data byte inarray len data pos len buf length throw new buffer overflow exception system arraycopy inarray buf data pos len data pos len write checksum byte inarray len checksum pos len data start throw new buffer overflow exception system arraycopy inarray buf checksum pos len checksum pos len returns byte buffer contains one full packet including header byte
586	hdfs\src\java\org\apache\hadoop\hdfs\DFSUtil.java	unrelated	package org apache hadoop hdfs dfs util thread local random random new thread local random protected random initial value return new random random get random return random get compartor sorting data node info based decommissioned states decommissioned nodes moved end array sorting compartor comparator datanode info decom comparator new comparator datanode info compare datanode info datanode info b return decommissioned b decommissioned decommissioned whether pathname valid currently prohibits relative paths names contain boolean valid name string src path must absolute src starts with path separator return false check string tokenizer tokens new string tokenizer src path separator tokens more tokens string element tokens next token element equals element equals element index of element index of return false return true utility facilitate junit test error simulation error simulator boolean simulation null error simulation events initialize error simulation event number of events simulation new boolean number of events number of events simulation false boolean get error simulation index simulation null return false assert index simulation length return simulation index set error simulation index assert index simulation length simulation index true clear error simulation index assert index simulation length simulation index false converts byte array using utf encoding string bytes string byte bytes try return new string bytes utf catch unsupported encoding exception e assert false utf encoding supported return null converts byte array using utf encoding byte bytes string str try return str get bytes utf catch unsupported encoding exception e assert false utf encoding supported return null given list path components returns path utf string string byte array string byte path components path components length return path components length path components length return path separator try string builder result new string builder path components length result append new string path components utf path components length result append path separator char return result string catch unsupported encoding exception ex assert false utf encoding supported return null splits array bytes array arrays bytes byte separator byte bytes byte array byte bytes byte separator return bytes byte array bytes bytes length separator splits first len bytes bytes array arrays bytes byte separator byte bytes byte array byte bytes len byte separator assert len bytes length splits len return new byte null count splits omit multiple separators last one len bytes separator splits last len last bytes last separator splits splits bytes separator return new byte null splits byte result new byte splits start index next index index build splits index splits next index len bytes next index separator next index result index new byte next index start index system arraycopy bytes start index result index next index start index index start index next index next index start index return result convert located blocks block locations block location located blocks locations located blocks blocks blocks null return new block location nr blocks blocks located block count block location blk locations new block location nr blocks nr blocks return blk locations idx located block blk blocks get located blocks assert idx nr blocks incorrect index datanode info locations blk get locations string hosts new string locations length string
587	hdfs\src\java\org\apache\hadoop\hdfs\DistributedFileSystem.java	unrelated	package org apache hadoop hdfs implementation file system dfs system this object way end user code interacts hadoop distributed file system distributed file system extends file system path working dir uri uri dfs client dfs boolean verify checksum true hdfs configuration init distributed file system distributed file system inet socket address namenode configuration conf throws io exception initialize name node get uri namenode conf uri get uri return uri initialize uri uri configuration conf throws io exception super initialize uri conf set conf conf string host uri get host host null throw new io exception incomplete hdfs uri host uri inet socket address namenode name node get address uri get authority dfs new dfs client namenode conf statistics uri uri create fs constants hdfs uri scheme uri get authority working dir get home directory permit paths explicitly specify default port protected check path path path uri uri get uri uri uri path uri string authority uri get authority uri get scheme null uri get scheme equals ignore case uri get scheme uri get port name node default port uri get port uri get port name node default port authority substring authority index of equals ignore case uri get authority return super check path path normalize paths explicitly specify default port path make qualified path path uri uri get uri uri uri path uri string authority uri get authority uri get scheme null uri get scheme equals ignore case uri get scheme uri get port name node default port uri get port authority substring authority index of equals ignore case uri get authority path new path uri get scheme uri get authority uri get path return super make qualified path path get working directory return working dir get default block size return dfs get default block size short get default replication return dfs get default replication path make absolute path f f absolute return f else return new path working dir f set working directory path dir string result make absolute dir uri get path dfs util valid name result throw new illegal argument exception invalid dfs directory name result working dir make absolute dir inherit doc path get home directory return make qualified new path user dfs ugi get short user name string get path name path file check path file string result make absolute file uri get path dfs util valid name result throw new illegal argument exception pathname result file valid dfs filename return result block location get file block locations file status file start len throws io exception file null return null return get file block locations file get path start len block location get file block locations path p start len throws io exception statistics increment read ops return dfs get block locations get path name p start len set verify checksum boolean verify checksum verify checksum verify checksum start lease recovery file boolean recover lease path f throws io exception return dfs recover lease get path name f fs data input stream open path f buffer size throws io exception statistics increment read ops return new dfs
588	hdfs\src\java\org\apache\hadoop\hdfs\HdfsConfiguration.java	heartbeat	package org apache hadoop hdfs adds deprecated keys configuration hdfs configuration extends configuration add deprecated keys adds default resources configuration add default resource hdfs default xml configuration add default resource hdfs site xml hdfs configuration super hdfs configuration boolean load defaults super load defaults hdfs configuration configuration conf super conf this method invoked hdfs configuration loaded already previously loaded upon loading initializer block executed add deprecated keys add default resources it safe method called multiple times initializer block get invoked this replaces previously dangerous practice calling configuration add default resource hdfs default xml directly without loading hdfs configuration first thereby skipping key deprecation init deprecate string old key string new key configuration add deprecation old key new string new key add deprecated keys deprecate dfs backup address dfs config keys dfs namenode backup address key deprecate dfs backup http address dfs config keys dfs namenode backup http address key deprecate dfs balance bandwidth per sec dfs config keys dfs datanode balance bandwidthpersec key deprecate dfs data dir dfs config keys dfs datanode data dir key deprecate dfs http address dfs config keys dfs namenode http address key deprecate dfs https address dfs config keys dfs namenode https address key deprecate dfs max objects dfs config keys dfs namenode max objects key deprecate dfs name dir dfs config keys dfs namenode name dir key deprecate dfs name dir restore dfs config keys dfs namenode name dir restore key deprecate dfs name edits dir dfs config keys dfs namenode edits dir key deprecate dfs read prefetch size dfs config keys dfs client read prefetch size key deprecate dfs safemode extension dfs config keys dfs namenode safemode extension key deprecate dfs safemode threshold pct dfs config keys dfs namenode safemode threshold pct key deprecate dfs secondary http address dfs config keys dfs namenode secondary http address key deprecate dfs socket timeout dfs config keys dfs client socket timeout key deprecate fs checkpoint dir dfs config keys dfs namenode checkpoint dir key deprecate fs checkpoint edits dir dfs config keys dfs namenode checkpoint edits dir key deprecate fs checkpoint period dfs config keys dfs namenode checkpoint period key deprecate fs checkpoint size dfs config keys dfs namenode checkpoint size key deprecate dfs upgrade permission dfs config keys dfs namenode upgrade permission key deprecate heartbeat recheck interval dfs config keys dfs namenode heartbeat recheck interval key deprecate storage id dfs config keys dfs datanode storageid key deprecate dfs https client keystore resource dfs config keys dfs client https keystore resource key deprecate dfs https need client auth dfs config keys dfs client https need auth key deprecate slave host name dfs config keys dfs datanode host name key deprecate session id dfs config keys dfs metrics session id key deprecate dfs access time precision dfs config keys dfs namenode accesstime precision key deprecate dfs replication consider load dfs config keys dfs namenode replication considerload key deprecate dfs replication interval dfs config keys dfs namenode replication interval key deprecate dfs replication min dfs config keys dfs namenode replication min key deprecate dfs replication pending timeout sec dfs config
589	hdfs\src\java\org\apache\hadoop\hdfs\HDFSPolicyProvider.java	unrelated	package org apache hadoop hdfs link policy provider hdfs protocols hdfs policy provider extends policy provider service hdfs services new service new service security client protocol acl client protocol new service security client datanode protocol acl client datanode protocol new service security datanode protocol acl datanode protocol new service security inter datanode protocol acl inter datanode protocol new service security namenode protocol acl namenode protocol new service security refresh policy protocol acl refresh authorization policy protocol new service security refresh user mappings protocol acl refresh user mappings protocol new service security get user mappings protocol acl get user mappings protocol service get services return hdfs services
590	hdfs\src\java\org\apache\hadoop\hdfs\HftpFileSystem.java	unrelated	package org apache hadoop hdfs an implementation protocol accessing filesystems http the following implementation provides limited read filesystem http hftp file system extends file system http url connection set follow redirects true string nn http url uri hdfs uri protected inet socket address nn addr protected user group information ugi string hftp timezone utc string hftp date format yyyy mm dd t hh mm ss z token delegation token identifier delegation token string hftp service name key hdfs service host simple date format get date format simple date format df new simple date format hftp date format df set time zone time zone get time zone hftp timezone return df protected thread local simple date format df new thread local simple date format protected simple date format initial value return get date format renewer thread renewer new renewer thread renewer start protected get default port return dfs config keys dfs https port default string get canonical service name return security util build dt service name hdfs uri get default port string build uri string schema string host port string builder sb new string builder schema return sb append host append append port string initialize uri name configuration conf throws io exception super initialize name conf set conf conf ugi user group information get current user nn addr net utils create socket addr name string case open connection hftp different cluster need know cluster https port set assume cluster port url port conf get int dfs hftp https port url port url port conf get int dfs config keys dfs https port key dfs config keys dfs https port default nn http url build uri https net utils normalize host name name get host url port log debug using url get dt nn http url one uses rpc port different default one one specify setvice name delegation token otherwise hostname rpc port string key hftp file system hftp service name key security util build dt service name name dfs config keys dfs https port default log debug enabled log debug trying find dt name using key key conf conf get key string nn service name conf get key nn port name node default port nn service name null get real port nn port net utils create socket addr nn service name name node default port get port try hdfs uri new uri build uri hdfs nn addr get host name nn port catch uri syntax exception ue throw new io exception bad uri hdfs ue user group information security enabled try finding token namenode esp applicable tasks using hftp if exists one set delegation field string canonical name get canonical service name token extends token identifier ugi get tokens delegation token identifier hdfs delegation kind equals get kind get service string equals canonical name log debug enabled log debug found existing dt name delegation token token delegation token identifier break since already token go get one https delegation token null delegation token token delegation token identifier get delegation token null renewer add token to renew synchronized token get delegation token string renewer throws
591	hdfs\src\java\org\apache\hadoop\hdfs\HsftpFileSystem.java	unrelated	package org apache hadoop hdfs an implementation protocol accessing filesystems https the following implementation provides limited read filesystem https hsftp file system extends hftp file system mm seconds per day volatile exp warn days initialize uri name configuration conf throws io exception super initialize name conf setup ssl conf exp warn days conf get int ssl expiration warn days set ssl resources setup ssl configuration conf throws io exception configuration ssl conf new hdfs configuration false ssl conf add resource conf get dfs config keys dfs client https keystore resource key dfs config keys dfs client https keystore resource default file input stream fis null try ssl context sc ssl context get instance ssl key manager kms null trust manager tms null ssl conf get ssl client keystore location null initialize default key manager keystore file pass key manager factory kmf key manager factory get instance sun x key store ks key store get instance ssl conf get ssl client keystore type jks char ks pass ssl conf get ssl client keystore password changeit char array fis new file input stream ssl conf get ssl client keystore location keystore jks ks load fis ks pass kmf init ks ssl conf get ssl client keystore keypassword changeit char array kms kmf get key managers fis close fis null initialize default trust manager truststore file pass ssl conf get boolean ssl client authenticate server false pass trustmanager validation tms new dummy trust manager new dummy trust manager else trust manager factory tmf trust manager factory get instance pkix key store ts key store get instance ssl conf get ssl client truststore type jks char ts pass ssl conf get ssl client truststore password changeit char array fis new file input stream ssl conf get ssl client truststore location truststore jks ts load fis ts pass tmf init ts tms tmf get trust managers sc init kms tms new java security secure random https url connection set default ssl socket factory sc get socket factory catch exception e throw new io exception could initialize ssl context e finally fis null fis close protected http url connection open connection string path string query throws io exception try query update query query url url new uri https null nn addr get host name nn addr get port path query null url https url connection conn https url connection url open connection bypass hostname verification conn set hostname verifier new dummy hostname verifier conn set request method get conn connect check cert expiration date warn days exp warn days warn days make sure check exp warn days exp time threshold warn days mm seconds per day system current time millis x certificate client certs x certificate conn get local certificates client certs null x certificate cert client certs exp time cert get not after get time exp time exp time threshold string builder sb new string builder sb append n client certificate cert get subject x principal get name day off set exp time system current time millis mm seconds per day sb append day off set days
592	hdfs\src\java\org\apache\hadoop\hdfs\LeaseRenewer.java	unrelated	package org apache hadoop hdfs p used link dfs client renewing file written leases namenode when file opened write create append namenode stores file lease recording identity writer the writer e dfs client required renew lease periodically when lease renewed expires namenode considers writer failed may either let another writer obtain lease close file p p this also provides following functionality ul li it maintains map namenode user pairs lease renewers the link lease renewer instance used renewing lease link dfs client namenode user li li each renewer maintains list link dfs client periodically leases clients renewed a client removed list client closed li li a thread per namenode per user used link lease renewer renew leases li ul p lease renewer log log log factory get log lease renewer lease renewer grace default l lease renewer sleep default l get link lease renewer instance lease renewer get instance string authority user group information ugi dfs client dfsc throws io exception return factory instance get authority ugi dfsc a factory sharing link lease renewer objects among link dfs client instances one renewer per authority per user factory factory instance new factory key namenode info string authority user info user group information ugi key string authority user group information ugi authority null throw new hadoop illegal argument exception authority null else ugi null throw new hadoop illegal argument exception ugi null authority authority ugi ugi hash code return authority hash code ugi hash code boolean equals object obj obj return true obj null obj instanceof key key key obj return authority equals authority ugi equals ugi return false string string return ugi get short user name authority a map per user per namenode renewers map key lease renewer renewers new hash map key lease renewer get renewer synchronized lease renewer get string authority user group information ugi dfs client dfsc key k new key authority ugi lease renewer r renewers get k r null r new lease renewer k renewers put k r r add client dfsc return r remove given renewer synchronized remove lease renewer r lease renewer stored renewers get r factorykey since renewer may expire stored renewer different r stored r clients running renewers remove r factorykey string clien name postfix dfs util get random next int thread current thread get id the time milliseconds map became empty empty time long max value a fixed lease renewal time period milliseconds renewal fs constants lease softlimit period a daemon renewing lease daemon daemon null only daemon current id run current id a period milliseconds lease renewer thread run map became empty in words map empty time period longer grace period renewer terminate grace period the time period milliseconds renewer sleeps iteration sleep period factory key factorykey a list clients corresponding renewer list dfs client dfsclients new array list dfs client a stringified stack trace call stack lease renewer instantiated this generated trace level logging enabled string instantiation trace lease renewer factory key factorykey factorykey factorykey set grace sleep period lease renewer grace default log trace enabled instantiation trace string utils stringify
593	hdfs\src\java\org\apache\hadoop\hdfs\SocketCache.java	unrelated	package org apache hadoop hdfs a cache sockets socket cache log log log factory get log socket cache linked list multimap socket address socket multimap capacity create socket cache given capacity socket cache capacity multimap linked list multimap create capacity capacity get cached socket given address synchronized socket get socket address remote list socket socklist multimap get remote socklist null return null iterator socket iter socklist iterator iter next socket candidate iter next iter remove candidate closed return candidate return null give unused socket cache synchronized put socket sock preconditions check not null sock socket address remote addr sock get remote socket address remote addr null log warn cannot cache unconnected socket remote address sock io utils close socket sock return capacity multimap size evict oldest multimap put remote addr sock synchronized size return multimap size evict oldest entry cache synchronized evict oldest iterator entry socket address socket iter multimap entries iterator iter next throw new illegal state exception cannot evict empty cache entry socket address socket entry iter next iter remove socket sock entry get value io utils close socket sock empty cache close sockets synchronized clear socket sock multimap values io utils close socket sock multimap clear protected finalize clear
594	hdfs\src\java\org\apache\hadoop\hdfs\protocol\AlreadyBeingCreatedException.java	unrelated	package org apache hadoop hdfs protocol the exception happens ask create file already created closed yet already being created exception extends io exception serial version uid x ad l already being created exception string msg super msg
595	hdfs\src\java\org\apache\hadoop\hdfs\protocol\Block.java	unrelated	package org apache hadoop hdfs protocol a block hadoop fs primitive identified block implements writable comparable block string block file prefix blk string metadata extension meta register ctor writable factories set factory block new writable factory writable new instance return new block pattern block file pattern pattern compile block file prefix pattern meta file pattern pattern compile block file prefix metadata extension boolean block filename file f string name f get name return block file pattern matcher name matches filename id string name matcher block file pattern matcher name return matches long parse long group boolean meta filename string name return meta file pattern matcher name matches get generation stamp name metafile name get generation stamp string meta file matcher meta file pattern matcher meta file return matches long parse long group generation stamp grandfather generation stamp get block id name metafile name get block id string meta file matcher meta file pattern matcher meta file return matches long parse long group block id num bytes generation stamp block block blkid len generation stamp set blkid len generation stamp block blkid blkid generation stamp grandfather generation stamp block block blk blk block id blk num bytes blk generation stamp find blockid given filename block file f len genstamp filename id f get name len genstamp set blkid len gen stamp block id blkid num bytes len generation stamp gen stamp get block id return block id set block id bid block id bid string get block name return block file prefix string value of block id get num bytes return num bytes set num bytes len num bytes len get generation stamp return generation stamp set generation stamp stamp generation stamp stamp string string return get block name get generation stamp writable write data output throws io exception write helper read fields data input throws io exception read helper write helper data output throws io exception write long block id write long num bytes write long generation stamp read helper data input throws io exception block id read long num bytes read long generation stamp read long num bytes throw new io exception unexpected block size num bytes write identifier part block write id data output throws io exception write long block id write long generation stamp read identifier part block read id data input throws io exception block id read long generation stamp read long compare to block b return block id b block id block id b block id boolean equals object return true instanceof block return false return compare to block hash code generation stamp irrelevant used return block id block id
596	hdfs\src\java\org\apache\hadoop\hdfs\protocol\BlockListAsLongs.java	unrelated	package org apache hadoop hdfs protocol this provides accessing list blocks implemented this useful block report rather send block reports block send the structure array follows length finalized replica list length construction replica list followed finalized replica list replica represented longs one block id one block length one generation stamp followed invalid replica represented three followed construction replica list replica represented longs three block id length generation stamp forth replica state block list as longs implements iterable block a finalized block longs block id block length generation stamp longs per finalized block an construction block longs block id block length generation stamp replica state longs per uc block number longs header header size returns index first block list belonging specified block the first contains block id index block id block index block index block index get number of blocks return finalized size get number of finalized replicas block index finalized size return header size block index longs per finalized block return header size finalized size longs per finalized block block index finalized size longs per uc block block list create block report finalized construction lists blocks block list as longs list extends block finalized list replica info uc finalized size finalized null finalized size uc size uc null uc size len header size finalized size longs per finalized block uc size longs per uc block block list new len set header block list finalized size block list uc size set finalized blocks finalized size set block finalized get set invalid delimiting block set delimiting block finalized size set construction blocks uc size set block finalized size uc get block list as longs null constructor block list as longs block list block list null block list new header size return block list block list get block list as longs return block list iterates blocks block report avoids object allocation iteration block report iterator implements iterator block current block index block block replica state current replica state block report iterator current block index block new block current replica state null boolean next return current block index get number of blocks block next block set block id current block index block length current block index block generation stamp current block index current replica state block replica state current block index current block index return block remove throw new unsupported operation exception sorry remove get state current replica the state corresponds replica returned latest link next replica state get current replica state return current replica state returns iterator blocks block report iterator block iterator return get block report iterator returns link block report iterator block report iterator get block report iterator return new block report iterator the number blocks get number of blocks assert block list length header size block list longs per finalized block block list longs per uc block number blocks inconcistent array length return get number of finalized replicas get number of uc replicas returns number finalized replicas block report get number of finalized replicas return block list returns number construction replicas block report get number of uc replicas return block list returns
597	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ClientDatanodeProtocol.java	pooling	package org apache hadoop hdfs protocol an client datanode protocol block recovery server principal dfs config keys dfs datanode user name key client datanode protocol extends versioned protocol log log log factory get log client datanode protocol added delete block pool method version id l return visible length replica get replica visible length extended block b throws io exception refresh list federated namenodes updated configuration adds new namenodes stops deleted namenodes refresh namenodes throws io exception delete block pool directory if force false deleted empty otherwise deleted along contents e contain block files otherwise deleted along contents delete block pool string bpid boolean force throws io exception
598	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ClientProtocol.java	pooling	package org apache hadoop hdfs protocol client protocol used user code via link org apache hadoop hdfs distributed file system communicate name node user code manipulate directory namespace well open close file streams etc server principal dfs config keys dfs namenode user name key client protocol extends versioned protocol compared previous version following changes introduced only latest change reflected the log historical changes retrieved svn add block pool id block version id l file contents get locations blocks specified file within specified range data node locations block sorted proximity client p return link located blocks contains file length blocks locations data node locations block sorted distance client address p the client contact one indicated data nodes obtain actual data located blocks get block locations string src offset length throws access control exception file not found exception unresolved link exception io exception get server default values number configuration params fs server defaults get server defaults throws io exception create new file entry namespace p this create empty file specified source path the path reflect full path originated root the name node notion current directory client p once created file visible available read clients although clients cannot link delete string boolean create link rename string string file completed explicitly result lease expiration p blocks maximum size clients intend create multi block files must also use link add block string string extended block datanode info overwritten already exists create exist append quota restriction code create parent code false directory quota restriction runtime exceptions create string src fs permission masked string client name enum set writable create flag flag boolean create parent short replication block size throws access control exception already being created exception ds quota exceeded exception file already exists exception file not found exception ns quota exceeded exception parent not directory exception safe mode exception unresolved link exception io exception append end file denied system as usually client side exception wrapped link org apache hadoop ipc remote exception allows appending existing file server configured parameter dfs support append set true otherwise throws io exception restriction runtime exceptions located block append string src string client name throws access control exception ds quota exceeded exception file not found exception safe mode exception unresolved link exception io exception set replication existing file p the name node sets replication new value returns the actual block replication expected performed method call the blocks populated removed background result routine block maintenance procedures false file exist directory quota restriction boolean set replication string src short replication throws access control exception ds quota exceeded exception file not found exception safe mode exception unresolved link exception io exception set permissions existing file directory set permission string src fs permission permission throws access control exception file not found exception safe mode exception unresolved link exception io exception set owner path e file directory the parameters username groupname cannot null set owner string src string username string groupname throws access control exception file not found exception safe mode exception unresolved link exception io exception the client give blcok calling abandon block the client either obtain
599	hdfs\src\java\org\apache\hadoop\hdfs\protocol\CorruptFileBlocks.java	unrelated	package org apache hadoop hdfs protocol contains list paths corresponding corrupt files cookie used iterative calls name node list corrupt file blocks corrupt file blocks implements writable used hash code prime string files string cookie corrupt file blocks new string corrupt file blocks string files string cookie files files cookie cookie string get files return files string get cookie return cookie inherit doc read fields data input throws io exception file count read int files new string file count file count files text read string cookie text read string inherit doc write data output throws io exception write int files length files length text write string files text write string cookie inherit doc boolean equals object obj obj return true obj instanceof corrupt file blocks return false corrupt file blocks corrupt file blocks obj return cookie equals cookie arrays equals files files inherit doc hash code result cookie hash code string file files result prime result file hash code return result
600	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DatanodeID.java	unrelated	package org apache hadoop hdfs protocol datanode id composed data node name hostname port number data storage id currently represents datanode id implements writable comparable datanode id datanode id empty array string name hostname port number string storage id unique per cluster storage id protected info port port infoserver running ipc port port ipc server running equivalent datanode id datanode id equivalent datanode id node name datanode id string node name node name datanode id copy constructor datanode id datanode id get name get storage id get info port get ipc port create datanode id datanode id string node name string storage id info port ipc port name node name storage id storage id info port info port ipc port ipc port set name string name name name set info port info port info port info port set ipc port ipc port ipc port ipc port string get name return name string get storage id return storage id get info port return info port get ipc port return ipc port sets data storage id set storage id string storage id storage id storage id string get host colon name index of colon return name else return name substring colon get port colon name index of colon return default port return integer parse int name substring colon boolean equals object return true instanceof datanode id return false return name equals datanode id get name storage id equals datanode id get storage id hash code return name hash code storage id hash code string string return name update fields new registration request comes note update storage id update reg info datanode id node reg name node reg get name info port node reg get info port ipc port node reg get ipc port update fields added future comparable basis compare string name host port number compare to datanode id return name compare to get name writable inherit doc write data output throws io exception deprecated utf write string name deprecated utf write string storage id write short info port inherit doc read fields data input throws io exception name deprecated utf read string storage id deprecated utf read string info port read could negative port large number bits storage size less bits so chop first two bytes hence signed bits setting field info port read short x ffff
601	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DatanodeInfo.java	pooling	package org apache hadoop hdfs protocol datanode info represents status data node this object used communication datanode protocol client protocol datanode info extends datanode id implements node protected capacity protected dfs used protected remaining protected block pool used protected last update protected xceiver count protected string location network topology default rack host name supplied datanode registration name namenode uses datanode ip address name protected string host name null administrative states datanode enum admin states normal in service decommission inprogress decommission in progress decommissioned decommissioned string value admin states string v value v string string return value protected admin states admin state datanode info super admin state null datanode info datanode info super capacity get capacity dfs used get dfs used remaining get remaining block pool used get block pool used last update get last update xceiver count get xceiver count location get network location admin state admin state host name host name datanode info datanode id node id super node id capacity l dfs used l remaining l block pool used l last update l xceiver count admin state null protected datanode info datanode id node id string location string host name node id location location host name host name the raw capacity get capacity return capacity the used space data node get dfs used return dfs used the used space block pool data node get block pool used return block pool used the used space data node get non dfs used non dfs used capacity dfs used remaining return non dfs used non dfs used the used space data node percentage present capacity get dfs used percent return dfs util get percent used dfs used capacity the raw free space get remaining return remaining used space block pool percentage present capacity get block pool used percent return dfs util get percent used block pool used capacity the remaining space percentage configured capacity get remaining percent return dfs util get percent remaining remaining capacity the time information accurate get last update return last update number active connections get xceiver count return xceiver count sets raw capacity set capacity capacity capacity capacity sets used space datanode set dfs used dfs used dfs used dfs used sets raw free space set remaining remaining remaining remaining sets block pool used space set block pool used bp used block pool used bp used sets time information accurate set last update last update last update last update sets number active connections set xceiver count xceiver count xceiver count xceiver count rack name synchronized string get network location return location sets rack name synchronized set network location string location location node base normalize location string get host name return host name null host name length get host host name set host name string host host name host a formatted reporting status data node string get datanode report string builder buffer new string builder c get capacity r get remaining u get dfs used non dfs used get non dfs used used percent get dfs used percent remaining percent get remaining percent string host name net utils get host
602	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DirectoryListing.java	unrelated	package org apache hadoop hdfs protocol this defines partial listing directory support iterative directory listing directory listing implements writable register ctor writable factories set factory directory listing new writable factory writable new instance return new directory listing hdfs file status partial listing remaining entries default constructor directory listing constructor directory listing hdfs file status partial listing remaining entries partial listing null throw new illegal argument exception partial listing null partial listing length remaining entries throw new illegal argument exception partial listing empty number remaining entries zero partial listing partial listing remaining entries remaining entries get partial listing file status hdfs file status get partial listing return partial listing get number remaining entries left listed get remaining entries return remaining entries check entries left listed return false otherwise boolean more return remaining entries get last name list byte get last name partial listing length return null return partial listing partial listing length get local name in bytes writable read fields data input throws io exception num entries read int partial listing new hdfs file status num entries num entries boolean location read boolean num entries location partial listing new hdfs located file status else partial listing new hdfs file status partial listing read fields remaining entries read int write data output throws io exception write int partial listing length partial listing length partial listing instanceof hdfs located file status write boolean true else write boolean false hdfs file status file status partial listing file status write write int remaining entries
603	hdfs\src\java\org\apache\hadoop\hdfs\protocol\DSQuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol ds quota exceeded exception extends quota exceeded exception protected serial version uid l ds quota exceeded exception ds quota exceeded exception string msg super msg ds quota exceeded exception quota count super quota count string get message string msg super get message msg null return the disk space quota path name null path name exceeded quota string utils human readable int quota diskspace consumed string utils human readable int count else return msg
604	hdfs\src\java\org\apache\hadoop\hdfs\protocol\ExtendedBlock.java	pooling	package org apache hadoop hdfs protocol identifies block uniquely across block pools extended block implements writable string pool id block block register ctor writable factories set factory extended block new writable factory writable new instance return new extended block extended block null extended block extended block b b pool id new block b block extended block string pool id block id pool id block id extended block string pool id block b pool id pool id block b extended block string pool id blkid len genstamp pool id pool id block new block blkid len genstamp write data output throws io exception deprecated utf write string pool id block write helper read fields data input throws io exception pool id deprecated utf read string block read helper write identifier part block write id data output throws io exception deprecated utf write string pool id block write id read identifier part block read id data input throws io exception pool id deprecated utf read string block read id string get block pool id return pool id returns block file name block string get block name return block get block name get num bytes return block get num bytes get block id return block get block id get generation stamp return block get generation stamp set block id bid block set block id bid set generation stamp gen stamp block set generation stamp gen stamp set num bytes len block set num bytes len set string pool id block blk pool id pool id block blk block get local block extended block b return b null null b get local block block get local block return block boolean equals object return true instanceof extended block return false extended block b extended block return b block equals block b pool id equals pool id hash code return block hash code string string return pool id block
605	hdfs\src\java\org\apache\hadoop\hdfs\protocol\FSConstants.java	heartbeat	package org apache hadoop hdfs protocol some handy constants fs constants min blocks for write long indicates leave current quota unchanged quota dont set long max value quota reset l timeouts constants heartbeat interval blockreport interval blockreport initial delay lease softlimit period lease hardlimit period lease softlimit period lease recover period ms we need limit length depth path filesystem hadoop currently set maximum length k characters maximum depth k max path length max path depth buffer size new hdfs configuration get int io file buffer size used writing header etc small buffer size math min buffer size todo mb media style com conf injected default block size default bytes per checksum default write packet size short default replication factor default file buffer size default data socket size size of integer integer size byte size safe mode actions enum safe mode action safemode leave safemode enter safemode get type datanode report enum datanode report type all live dead distributed upgrade actions get upgrade status get detailed upgrade status proceed upgrade stuck matter status enum upgrade action get status detailed status force proceed uri scheme hdfs namenode ur is string hdfs uri scheme hdfs please see link layout version adding new layout version layout version layout version get current layout version
606	hdfs\src\java\org\apache\hadoop\hdfs\protocol\FSLimitException.java	unrelated	package org apache hadoop hdfs protocol abstract deriving exceptions related filesystem constraints fs limit exception extends quota exceeded exception protected serial version uid l protected fs limit exception protected fs limit exception string msg super msg protected fs limit exception quota count super quota count path component length path component too long exception extends fs limit exception protected serial version uid l protected path component too long exception protected path component too long exception string msg super msg path component too long exception quota count super quota count string get message path violator new path path name return the maximum path component name limit violator get name directory violator get parent exceeded limit quota length count directory many items max directory items exceeded exception extends fs limit exception protected serial version uid l protected max directory items exceeded exception protected max directory items exceeded exception string msg super msg max directory items exceeded exception quota count super quota count string get message return the directory item limit path name exceeded limit quota items count
607	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsFileStatus.java	unrelated	package org apache hadoop hdfs protocol interface represents wire information file hdfs file status implements writable register ctor writable factories set factory hdfs file status new writable factory writable new instance return new hdfs file status byte path local name inode encoded java utf byte symlink symlink target encoded java utf null length boolean isdir short block replication blocksize modification time access time fs permission permission string owner string group byte empty name new byte default constructor hdfs file status false null null null null null constructor hdfs file status length boolean isdir block replication blocksize modification time access time fs permission permission string owner string group byte symlink byte path length length isdir isdir block replication short block replication blocksize blocksize modification time modification time access time access time permission permission null fs permission get default permission owner owner null owner group group null group symlink symlink path path get length file bytes get len return length is directory boolean dir return isdir is symbolic link boolean symlink return symlink null get block size file get block size return blocksize get replication factor file short get replication return block replication get modification time file get modification time return modification time get access time file get access time return access time get fs permission associated file fs permission get permission return permission get owner file string get owner return owner get group associated file string get group return group check local name empty boolean empty local name return path length get representation local name string get local name return dfs util bytes string path get java utf representation local name byte get local name in bytes return path get representation full path name string get full name string parent empty local name return parent string builder full name new string builder parent parent ends with path separator full name append path separator full name append get local name return full name string get full path path get full path path parent empty local name return parent return new path parent get local name get representation symlink string get symlink return dfs util bytes string symlink writable write data output throws io exception write int path length write path write long length write boolean isdir write short block replication write long blocksize write long modification time write long access time permission write text write string owner text write string group write boolean symlink symlink write int symlink length write symlink read fields data input throws io exception num of bytes read int num of bytes path empty name else path new byte num of bytes read fully path length read long isdir read boolean block replication read short blocksize read long modification time read long access time read long permission read fields owner text read string group text read string read boolean num of bytes read int symlink new byte num of bytes read fully symlink
608	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsLocatedFileStatus.java	unrelated	package org apache hadoop hdfs protocol interface represents wire information including block locations file hdfs located file status extends hdfs file status located blocks locations default constructor hdfs located file status constructor hdfs located file status length boolean isdir block replication blocksize modification time access time fs permission permission string owner string group byte symlink byte path located blocks locations super length isdir block replication blocksize modification time access time permission owner group symlink path locations locations located blocks get block locations return locations writable write data output throws io exception super write dir symlink locations write read fields data input throws io exception super read fields dir symlink locations new located blocks locations read fields
609	hdfs\src\java\org\apache\hadoop\hdfs\protocol\HdfsProtoUtil.java	pooling	package org apache hadoop hdfs protocol utilities converting protocol buffers used hdfs wire protocol well generic utilities useful dealing protocol buffers hdfs proto util block token hdfs protos block token identifier proto proto token block token return hdfs protos block token identifier proto new builder set identifier byte string copy from block token get identifier set password byte string copy from block token get password set kind block token get kind string set service block token get service string build token block token identifier proto hdfs protos block token identifier proto proto return new token block token identifier proto get identifier byte array proto get password byte array new text proto get kind new text proto get service extended block hdfs protos extended block proto proto extended block block return hdfs protos extended block proto new builder set block id block get block id set pool id block get block pool id set num bytes block get num bytes set generation stamp block get generation stamp build extended block proto hdfs protos extended block proto proto return new extended block proto get pool id proto get block id proto get num bytes proto get generation stamp datanode id hdfs protos datanode id proto proto datanode id dni return hdfs protos datanode id proto new builder set name dni get name set storage id dni get storage id set info port dni get info port build datanode id proto hdfs protos datanode id proto id proto return new datanode id id proto get name id proto get storage id id proto get info port ipc port serialized writables either datanode info hdfs protos datanode info proto proto datanode info dni return hdfs protos datanode info proto new builder set id proto datanode id dni set capacity dni get capacity set dfs used dni get dfs used set remaining dni get remaining set block pool used dni get block pool used set last update dni get last update set xceiver count dni get xceiver count set location dni get network location set host name dni get host name set admin state hdfs protos datanode info proto admin state value of dni get admin state name build datanode info proto hdfs protos datanode info proto dni proto datanode info dni obj new datanode info proto dni proto get id dni proto get location dni proto get host name dni obj set capacity dni proto get capacity dni obj set dfs used dni proto get dfs used dni obj set remaining dni proto get remaining dni obj set block pool used dni proto get block pool used dni obj set last update dni proto get last update dni obj set xceiver count dni proto get xceiver count dni obj set admin state datanode info admin states value of dni proto get admin state name return dni obj array list extends hdfs protos datanode info proto protos datanode info dn infos start idx array list hdfs protos datanode info proto protos lists new array list with capacity dn infos length start idx dn infos length protos add
610	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LayoutVersion.java	unrelated	package org apache hadoop hdfs protocol this tracks changes layout version hdfs layout version changed following reasons ol li the layout namenode datanode stores information disk changes li li a new operation code added editlog li li modification format record content record editlog fsimage li ol br b how update layout version br b when change requires new layout version please add entry link feature short enum name new layout version description change please see link feature details br layout version enums features change layout version br br to add new layout version ul li define new enum constant short enum name new layout version description added feature li li when adding layout version ancestor immediate predecessor use constructor spacific ancestor passed li ul enum feature namespace quota support namespace quotas file access time support access time files diskspace quota support disk space quotas sticky bit support sticky bits append rbw dir datanode rbw subdirectory append atomic rename support atomic rename concat support concat operation symlinks support symbolic links delegation token support delegation tokens security fsimage compression support fsimage compression fsimage checksum support checksum fsimage remove rel disk layout support remove support disk layout edits cheskum support checksum editlog unused skipped version fsimage name optimization store last part path fsimage reserved rel reserved release reserved rel reserved release reserved rel reserved release reserved rel reserved release federation support namenode federation lease reassignment support persisting lease holder reassignment lv ancestor lv string description feature added code current lv feature lv string description lv lv description feature added code current lv feature lv ancestor lv string description lv lv ancestor lv ancestor lv description description accessor method feature layout version get layout version return lv accessor method feature ancestor layout version get ancestor layout version return ancestor lv accessor method feature description string get description return description build layout version corresponding feature matrix map integer enum set feature map new hash map integer enum set feature static initialization init map initialize map layout version enum set link feature supported init map go enum constants build map layout version enum set supported features layout version feature f feature values enum set feature ancestor set map get f ancestor lv ancestor set null ancestor set enum set none of feature empty enum set map put f ancestor lv ancestor set enum set feature feature set enum set copy of ancestor set feature set add f map put f lv feature set special initialization add feature delegation token special init feature reserved rel lv feature delegation token special init feature reserved rel lv feature delegation token special init lv feature f enum set feature set map get lv set add f gets formatted describes link layout version information string get string string builder buf new string builder buf append feature list n feature f feature values buf append f append introduced layout version append f lv append append f description append n buf append n n layout version supported features n feature f feature values buf append f lv append append map get f lv append n
611	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LocatedBlock.java	unrelated	package org apache hadoop hdfs protocol a located block pair block datanode info objects it tells find block located block implements writable register ctor writable factories set factory located block new writable factory writable new instance return new located block extended block b offset offset first byte block file datanode info locs corrupt flag true replicas block corrupt else false if block corrupt replicas filtered locations part object boolean corrupt token block token identifier block token new token block token identifier located block new extended block new datanode info l false located block string bpid block b datanode info locs new extended block bpid b locs false start offset unknown located block extended block b datanode info locs b locs false start offset unknown located block extended block b datanode info locs start offset b locs start offset false located block extended block b datanode info locs start offset boolean corrupt b b offset start offset corrupt corrupt locs null locs new datanode info else locs locs token block token identifier get block token return block token set block token token block token identifier token block token token extended block get block return b datanode info get locations return locs get start offset return offset get block size return b get num bytes set start offset value offset value set corrupt boolean corrupt corrupt corrupt boolean corrupt return corrupt writable write data output throws io exception block token write write boolean corrupt write long offset b write write int locs length locs length locs write read fields data input throws io exception block token read fields corrupt read boolean offset read long b new extended block b read fields count read int locs new datanode info count locs length locs new datanode info locs read fields read located block located block read data input throws io exception located block lb new located block lb read fields return lb inherit doc string string return get class get simple name b get block size get block size corrupt corrupt offset offset locs java util arrays list locs
612	hdfs\src\java\org\apache\hadoop\hdfs\protocol\LocatedBlocks.java	unrelated	package org apache hadoop hdfs protocol collection blocks locations file length located blocks implements writable file length list located block blocks array blocks prioritized locations boolean construction located block last located block null boolean last block complete false located blocks file length blocks null construction false constructor located blocks flength boolean under constuction list located block blks located block last block boolean last block completed file length flength blocks blks construction under constuction last located block last block last block complete last block completed get located blocks list located block get located blocks return blocks get last located block located block get last located block return last located block is last block completed boolean last block complete return last block complete get located block located block get index return blocks get index get number located blocks located block count return blocks null blocks size get file length return file length return ture file construction located blocks constructed false otherwise boolean under construction return construction find block containing specified offset find block offset create fake block size key located block key new located block key set start offset offset key get block set num bytes comparator located block comp new comparator located block returns iff inside b b inside compare located block located block b beg get start offset b beg b get start offset end beg get block size b end b beg b get block size beg b beg b end end b beg beg end b end return one blocks inside beg b beg return left bound left b return return collections binary search blocks key comp insert range block idx list located block new blocks old idx block idx ins start ins end new idx new idx new blocks size old idx blocks size new idx new off new blocks get new idx get start offset old off blocks get old idx get start offset new off old off ins end else new off old off replace old cached block new one blocks set old idx new blocks get new idx ins start ins end insert new blocks blocks add all old idx new blocks sub list ins start ins end old idx ins end ins start ins start ins end new idx old idx else new off old off assert false list located block must sorted start offset ins end new blocks size ins start ins end insert new blocks blocks add all old idx new blocks sub list ins start ins end get insert index bin search result return bin search result bin search result bin search result writable register ctor writable factories set factory located blocks new writable factory writable new instance return new located blocks write data output throws io exception write long file length write boolean construction write last located block boolean null last located block null write boolean null null last located block write write boolean last block complete write located blocks nr blocks located block count write int nr blocks nr blocks return located block blk blocks blk write read fields data
613	hdfs\src\java\org\apache\hadoop\hdfs\protocol\NSQuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol ns quota exceeded exception extends quota exceeded exception protected serial version uid l ns quota exceeded exception ns quota exceeded exception string msg super msg ns quota exceeded exception quota count super quota count string get message string msg super get message msg null return the name space quota directories files path name null directory path name exceeded quota quota file count count else return msg
614	hdfs\src\java\org\apache\hadoop\hdfs\protocol\QuotaExceededException.java	unrelated	package org apache hadoop hdfs protocol this exception thrown modification hdfs results violation directory quota a directory quota might namespace quota limit number files directories diskspace quota limit space taken file directory tree br br the message exception specifies directory quota violated actual quotas specific message generated corresponding exception ds quota exceeded exception ns quota exceeded exception quota exceeded exception extends io exception protected serial version uid l protected string path name null protected quota quota protected count actual value protected quota exceeded exception protected quota exceeded exception string msg super msg protected quota exceeded exception quota count quota quota count count set path name string path path name path string get message return super get message
615	hdfs\src\java\org\apache\hadoop\hdfs\protocol\RecoveryInProgressException.java	unrelated	package org apache hadoop hdfs protocol exception indicating replica already recovery recovery in progress exception extends io exception serial version uid l recovery in progress exception string msg super msg
616	hdfs\src\java\org\apache\hadoop\hdfs\protocol\UnregisteredNodeException.java	unrelated	package org apache hadoop hdfs protocol this exception thrown node previously registered trying access name node unregistered node exception extends io exception serial version uid l unregistered node exception node registration node reg super unregistered server node reg string the exception thrown different data node claims storage id existing one unregistered node exception datanode id node id datanode info stored node super data node node id get name attempting report storage id node id get storage id node stored node get name expected serve storage
617	hdfs\src\java\org\apache\hadoop\hdfs\protocol\UnresolvedPathException.java	unrelated	package org apache hadoop hdfs protocol thrown symbolic link encountered path unresolved path exception extends unresolved link exception serial version uid l string original path the original path containing link string link target the target link string remaining path the path part following link used remote exception instantiate unresolved path exception unresolved path exception string msg super msg unresolved path exception string original path string remaining path string link target original path original path remaining path remaining path link target link target path get unresolved path throws io exception return new path original path path get resolved path throws io exception remaining path null equals remaining path return new path link target return new path link target remaining path string get message string msg super get message msg null return msg string msg unresolved path original path try return get resolved path string catch io exception e ignore return msg
618	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\BlockConstructionStage.java	unrelated	package org apache hadoop hdfs protocol datatransfer block construction stage enum block construction stage the enumerates always listed regular stage followed recovery stage changing order make get recovery stage working pipeline set block append pipeline setup append pipeline set failed pipeline setup append recovery pipeline setup append recovery data streaming data streaming pipeline setup failed data streaming recovery pipeline setup streaming recovery close block pipeline pipeline close recover failed pipeline close pipeline close recovery pipeline set block creation pipeline setup create transfer rbw adding datanodes transfer rbw transfer finalized adding datanodes transfer finalized byte recovery bit byte get recovery stage stage block construction stage get recovery stage pipeline setup create throw new illegal argument exception unexpected block stage else return values ordinal recovery bit
619	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\DataTransferProtocol.java	unrelated	package org apache hadoop hdfs protocol datatransfer transfer data datanode using streaming protocol data transfer protocol log log log factory get log data transfer protocol version data transfers clients datanodes this change serialization datanode info protocol changes it obvious version declare methods data transfer protocol data transfer version read block read block extended block blk token block token identifier block token string client name block offset length throws io exception write block datanode pipeline write block extended block blk token block token identifier block token string client name datanode info targets datanode info source block construction stage stage pipeline size min bytes rcvd max bytes rcvd latest generation stamp throws io exception transfer block another datanode the block stage must either link block construction stage transfer rbw link block construction stage transfer finalized transfer block extended block blk token block token identifier block token string client name datanode info targets throws io exception receive block source datanode notifies namenode remove copy original datanode note source datanode original datanode different it used balancing purpose replace block extended block blk token block token identifier block token string del hint datanode info source throws io exception copy block it used balancing purpose copy block extended block blk token block token identifier block token throws io exception get block checksum md crc block checksum extended block blk token block token identifier block token throws io exception
620	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\DataTransferProtoUtil.java	unrelated	package org apache hadoop hdfs protocol datatransfer static utilities dealing protocol buffers used data transfer protocol data transfer proto util block construction stage proto op write block proto block construction stage stage return block construction stage value of block construction stage stage name op write block proto block construction stage proto block construction stage stage return op write block proto block construction stage value of stage name client operation header proto build client header extended block blk string client token block token identifier block token client operation header proto header client operation header proto new builder set base header build base header blk block token set client name client build return header base header proto build base header extended block blk token block token identifier block token return base header proto new builder set block hdfs proto util proto blk set token hdfs proto util proto block token build
621	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Op.java	unrelated	package org apache hadoop hdfs protocol datatransfer operation enum op write block byte read block byte read metadata byte replace block byte copy block byte block checksum byte transfer block byte the code operation byte code op byte code code code first code values code return object represented code op value of byte code code xff first code return values length null values read op read data input throws io exception return value of read byte write write data output throws io exception write code
622	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\PacketHeader.java	unrelated	package org apache hadoop hdfs protocol datatransfer header data packet goes read write pipelines packet header header size packet proto size packet header proto new builder set offset in block set seqno set last packet in block false set data len build get serialized size pkt header len proto size packet len packet header proto proto packet header packet header packet len offset in block seqno boolean last packet in block data len packet len packet len proto packet header proto new builder set offset in block offset in block set seqno seqno set last packet in block last packet in block set data len data len build get data len return proto get data len boolean last packet in block return proto get last packet in block get seqno return proto get seqno get offset in block return proto get offset in block get packet len return packet len string string return packet header packet len packet len header data proto string read fields byte buffer buf throws io exception packet len buf get int short proto len buf get short byte data new byte proto len buf get data proto packet header proto parse from data read fields data input stream throws io exception packet len read int short proto len read short byte data new byte proto len read fully data proto packet header proto parse from data write header buffer this requires pkt header len bytes available put in buffer byte buffer buf assert proto get serialized size proto size expected proto size got proto get serialized size try buf put int packet len buf put short short proto get serialized size proto write to new byte buffer output stream buf catch io exception e throw new runtime exception e write data output stream throws io exception assert proto get serialized size proto size expected proto size got proto get serialized size write int packet len write short proto get serialized size proto write to perform sanity check packet returning true sane sequence number larger boolean sanity check last seq no we non positive data length last packet proto get data len proto get last packet in block return false the last packet contain data proto get last packet in block proto get data len return false seqnos always increase packet received proto get seqno last seq no return false return true boolean equals object instanceof packet header return false packet header packet header return proto equals proto hash code return proto get seqno
623	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\PipelineAck.java	unrelated	package org apache hadoop hdfs protocol datatransfer pipeline acknowledgment pipeline ack pipeline ack proto proto unkown seqno default constructor pipeline ack constructor pipeline ack seqno status replies proto pipeline ack proto new builder set seqno seqno add all status arrays list replies build get sequence number get seqno return proto get seqno get number replies short get num of replies return short proto get status count get ith reply status get reply return proto get status check ack contains error status boolean success data transfer protos status reply proto get status list reply data transfer protos status success return false return true writable read fields input stream throws io exception proto pipeline ack proto parse from vint prefixed write output stream throws io exception proto write delimited to string string return proto string
624	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Receiver.java	unrelated	package org apache hadoop hdfs protocol datatransfer receiver receiver implements data transfer protocol protected data input stream create receiver data transfer protocol socket protected receiver data input stream read op it also checks protocol version protected op read op throws io exception short version read short version data transfer protocol data transfer version throw new io exception version mismatch expected data transfer protocol data transfer version received version return op read process op corresponding method protected process op op op throws io exception switch op case read block op read block break case write block op write block break case replace block op replace block break case copy block op copy block break case block checksum op block checksum break case transfer block op transfer block break default throw new io exception unknown op op data stream receive op read block op read block throws io exception op read block proto proto op read block proto parse from vint prefixed read block proto proto get header get base header get block proto proto get header get base header get token proto get header get client name proto get offset proto get len receive op write block op write block data input stream throws io exception op write block proto proto op write block proto parse from vint prefixed write block proto proto get header get base header get block proto proto get header get base header get token proto get header get client name protos proto get targets list proto proto get source proto proto get stage proto get pipeline size proto get min bytes rcvd proto get max bytes rcvd proto get latest generation stamp receive link op transfer block op transfer block data input stream throws io exception op transfer block proto proto op transfer block proto parse from vint prefixed transfer block proto proto get header get base header get block proto proto get header get base header get token proto get header get client name protos proto get targets list receive op replace block op replace block data input stream throws io exception op replace block proto proto op replace block proto parse from vint prefixed replace block proto proto get header get block proto proto get header get token proto get del hint proto proto get source receive op copy block op copy block data input stream throws io exception op copy block proto proto op copy block proto parse from vint prefixed copy block proto proto get header get block proto proto get header get token receive op block checksum op block checksum data input stream throws io exception op block checksum proto proto op block checksum proto parse from vint prefixed block checksum proto proto get header get block proto proto get header get token
625	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\ReplaceDatanodeOnFailure.java	unrelated	package org apache hadoop hdfs protocol datatransfer the setting replace datanode failure feature enum replace datanode on failure the feature disabled entire site disable never add new datanode never default policy let r replication number let n number existing datanodes add new datanode r either floor r n r n block hflushed appended default always add new datanode existing datanode removed always check feature enabled check enabled disable throw new unsupported operation exception this feature disabled please refer dfs config keys dfs client write replace datanode on failure enable key configuration property is policy satisfied boolean satisfy short replication datanode info existings boolean append boolean hflushed n existings null existings length n n replication need add datanode policy return false else disable never return false else always return true else default replication return false else n replication return true else return append hflushed get setting configuration replace datanode on failure get configuration conf boolean enabled conf get boolean dfs config keys dfs client write replace datanode on failure enable key dfs config keys dfs client write replace datanode on failure enable default enabled return disable string policy conf get dfs config keys dfs client write replace datanode on failure policy key dfs config keys dfs client write replace datanode on failure policy default values length replace datanode on failure rdof values rdof name equals ignore case policy return rdof throw new hadoop illegal argument exception illegal configuration value dfs config keys dfs client write replace datanode on failure policy key policy write setting configuration write configuration conf conf set boolean dfs config keys dfs client write replace datanode on failure enable key disable conf set dfs config keys dfs client write replace datanode on failure policy key name
626	hdfs\src\java\org\apache\hadoop\hdfs\protocol\datatransfer\Sender.java	unrelated	package org apache hadoop hdfs protocol datatransfer sender sender implements data transfer protocol data output stream create sender data transfer protocol output stream sender data output stream initialize operation op data output op op throws io exception write short data transfer protocol data transfer version op write send data output stream op opcode message proto throws io exception op opcode proto write delimited to flush read block extended block blk token block token identifier block token string client name block offset length throws io exception op read block proto proto op read block proto new builder set header data transfer proto util build client header blk client name block token set offset block offset set len length build send op read block proto write block extended block blk token block token identifier block token string client name datanode info targets datanode info source block construction stage stage pipeline size min bytes rcvd max bytes rcvd latest generation stamp throws io exception client operation header proto header data transfer proto util build client header blk client name block token op write block proto builder proto op write block proto new builder set header header add all targets protos targets set stage proto stage set pipeline size pipeline size set min bytes rcvd min bytes rcvd set max bytes rcvd max bytes rcvd set latest generation stamp latest generation stamp source null proto set source proto source send op write block proto build transfer block extended block blk token block token identifier block token string client name datanode info targets throws io exception op transfer block proto proto op transfer block proto new builder set header data transfer proto util build client header blk client name block token add all targets protos targets build send op transfer block proto replace block extended block blk token block token identifier block token string del hint datanode info source throws io exception op replace block proto proto op replace block proto new builder set header data transfer proto util build base header blk block token set del hint del hint set source proto source build send op replace block proto copy block extended block blk token block token identifier block token throws io exception op copy block proto proto op copy block proto new builder set header data transfer proto util build base header blk block token build send op copy block proto block checksum extended block blk token block token identifier block token throws io exception op block checksum proto proto op block checksum proto new builder set header data transfer proto util build base header blk block token build send op block checksum proto
627	hdfs\src\java\org\apache\hadoop\hdfs\protocol\proto\DataTransferProtos.java	unrelated	package org apache hadoop hdfs protocol proto data transfer protos data transfer protos register all extensions com google protobuf extension registry registry enum status implements com google protobuf protocol message enum success error error checksum error invalid error exists error access token checksum ok success value error value error checksum value error invalid value error exists value error access token value checksum ok value get number return value status value of value switch value case return success case return error case return error checksum case return error invalid case return error exists case return error access token case return checksum ok default return null com google protobuf internal enum lite map status internal get value map return internal value map com google protobuf internal enum lite map status internal value map new com google protobuf internal enum lite map status status find value by number number return status value of number com google protobuf descriptors enum value descriptor get value descriptor return get descriptor get values get index com google protobuf descriptors enum descriptor get descriptor for type return get descriptor com google protobuf descriptors enum descriptor get descriptor return org apache hadoop hdfs protocol proto data transfer protos get descriptor get enum types get status values success error error checksum error invalid error exists error access token checksum ok status value of com google protobuf descriptors enum value descriptor desc desc get type get descriptor throw new java lang illegal argument exception enum value descriptor type return values desc get index index value status index value index index value value base header proto or builder extends com google protobuf message or builder required extended block proto block boolean block org apache hadoop hdfs protocol proto hdfs protos extended block proto get block org apache hadoop hdfs protocol proto hdfs protos extended block proto or builder get block or builder optional block token identifier proto token boolean token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto get token org apache hadoop hdfs protocol proto hdfs protos block token identifier proto or builder get token or builder base header proto extends com google protobuf generated message implements base header proto or builder use base header proto new builder construct base header proto builder builder super builder base header proto boolean init base header proto default instance base header proto get default instance return default instance base header proto get default instance for type return default instance com google protobuf descriptors descriptor get descriptor return org apache hadoop hdfs protocol proto data transfer protos internal base header proto descriptor protected com google protobuf generated message field accessor table internal get field accessor table return org apache hadoop hdfs protocol proto data transfer protos internal base header proto field accessor table bit field required extended block proto block block field number org apache hadoop hdfs protocol proto hdfs protos extended block proto block boolean block return bit field x x org apache hadoop hdfs protocol proto hdfs protos extended block proto get block return block org apache hadoop hdfs protocol proto
628	hdfs\src\java\org\apache\hadoop\hdfs\protocol\proto\HdfsProtos.java	pooling	package org apache hadoop hdfs protocol proto hdfs protos hdfs protos register all extensions com google protobuf extension registry registry extended block proto or builder extends com google protobuf message or builder required pool id boolean pool id string get pool id required uint block id boolean block id get block id required uint num bytes boolean num bytes get num bytes required uint generation stamp boolean generation stamp get generation stamp extended block proto extends com google protobuf generated message implements extended block proto or builder use extended block proto new builder construct extended block proto builder builder super builder extended block proto boolean init extended block proto default instance extended block proto get default instance return default instance extended block proto get default instance for type return default instance com google protobuf descriptors descriptor get descriptor return org apache hadoop hdfs protocol proto hdfs protos internal extended block proto descriptor protected com google protobuf generated message field accessor table internal get field accessor table return org apache hadoop hdfs protocol proto hdfs protos internal extended block proto field accessor table bit field required pool id poolid field number java lang object pool id boolean pool id return bit field x x string get pool id java lang object ref pool id ref instanceof string return string ref else com google protobuf byte string bs com google protobuf byte string ref string bs string utf com google protobuf internal valid utf bs pool id return com google protobuf byte string get pool id bytes java lang object ref pool id ref instanceof string com google protobuf byte string b com google protobuf byte string copy from utf string ref pool id b return b else return com google protobuf byte string ref required uint block id blockid field number block id boolean block id return bit field x x get block id return block id required uint num bytes numbytes field number num bytes boolean num bytes return bit field x x get num bytes return num bytes required uint generation stamp generationstamp field number generation stamp boolean generation stamp return bit field x x get generation stamp return generation stamp init fields pool id block id l num bytes l generation stamp l byte memoized is initialized boolean initialized byte initialized memoized is initialized initialized return initialized pool id memoized is initialized return false block id memoized is initialized return false num bytes memoized is initialized return false generation stamp memoized is initialized return false memoized is initialized return true write to com google protobuf coded output stream output throws java io io exception get serialized size bit field x x output write bytes get pool id bytes bit field x x output write u int block id bit field x x output write u int num bytes bit field x x output write u int generation stamp get unknown fields write to output memoized serialized size get serialized size size memoized serialized size size return size size bit field x x size com google protobuf coded output stream compute
629	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockKey.java	unrelated	package org apache hadoop hdfs security token block key used generating verifying block tokens block key extends delegation key block key super block key key id expiry date secret key key super key id expiry date key
630	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockPoolTokenSecretManager.java	pooling	package org apache hadoop hdfs security token block manages link block token secret manager per block pool routes requests given block pool id corresponding link block token secret manager block pool token secret manager extends secret manager block token identifier map string block token secret manager map new hash map string block token secret manager add block pool id corresponding link block token secret manager map synchronized add block pool string bpid block token secret manager secret mgr map put bpid secret mgr synchronized block token secret manager get string bpid block token secret manager secret mgr map get bpid secret mgr null throw new illegal argument exception block pool bpid found return secret mgr return empty block token identifer block token identifier create identifier return new block token identifier byte create password block token identifier identifier return get identifier get block pool id create password identifier byte retrieve password block token identifier identifier throws invalid token return get identifier get block pool id retrieve password identifier see link block token secret manager check access block token identifier string extended block access mode check access block token identifier id string user id extended block block access mode mode throws invalid token get block get block pool id check access id user id block mode see link block token secret manager check access token string extended block access mode check access token block token identifier token string user id extended block block access mode mode throws invalid token get block get block pool id check access token user id block mode see link block token secret manager set keys exported block keys set keys string bpid exported block keys exported keys throws io exception get bpid set keys exported keys see link block token secret manager generate token extended block enum set token block token identifier generate token extended block b enum set access mode throws io exception return get b get block pool id generate token b
631	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenIdentifier.java	pooling	package org apache hadoop hdfs security token block block token identifier extends token identifier text kind name new text hdfs block token expiry date key id string user id string block pool id block id enum set access mode modes byte cache block token identifier null null enum set none of access mode block token identifier string user id string bpid block id enum set access mode modes cache null user id user id block pool id bpid block id block id modes modes null enum set none of access mode modes text get kind return kind name user group information get user user id null equals user id string user block pool id long string block id return user group information create remote user user return user group information create remote user user id get expiry date return expiry date set expiry date expiry date cache null expiry date expiry date get key id return key id set key id key id cache null key id key id string get user id return user id string get block pool id return block pool id get block id return block id enum set access mode get access modes return modes string string return block token identifier expiry date get expiry date key id get key id user id get user id block pool id get block pool id block id get block id access modes get access modes boolean equal object object b return null b null equals b inherit doc boolean equals object obj obj return true obj instanceof block token identifier block token identifier block token identifier obj return expiry date expiry date key id key id equal user id user id equal block pool id block pool id block id block id equal modes modes return false inherit doc hash code return expiry date key id block id modes hash code user id null user id hash code block pool id null block pool id hash code read fields data input throws io exception cache null expiry date writable utils read v long key id writable utils read v int user id writable utils read string block pool id writable utils read string block id writable utils read v long length writable utils read v int length modes add writable utils read enum access mode write data output throws io exception writable utils write v long expiry date writable utils write v int key id writable utils write string user id writable utils write string block pool id writable utils write v long block id writable utils write v int modes size access mode mode modes writable utils write enum mode byte get bytes cache null cache super get bytes return cache
632	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenSecretManager.java	pooling	package org apache hadoop hdfs security token block block token secret manager instantiated modes master mode slave mode master generate new block keys export block keys slaves slaves import use block keys received master both master slave generate verify block tokens typically master mode used nn slave mode used dn block token secret manager extends secret manager block token identifier log log log factory get log block token secret manager token block token identifier dummy token new token block token identifier boolean master key update interval interval nn updates block keys it set enough live dn balancer sync ed block keys nn least interval key update interval volatile token lifetime serial no new secure random next int block key current key block key next key map integer block key keys enum access mode read write copy replace constructor block token secret manager boolean master key update interval token lifetime throws io exception master master key update interval key update interval token lifetime token lifetime keys new hash map integer block key generate keys initialize block keys synchronized generate keys master return need set estimated expiry dates current key next key nn crashes dn still expire keys nn stop using newly generated current key first key update interval however may still used dn balancer generate new tokens get chance sync keys nn since require key upd interval enough live dn balancer sync keys nn least period estimated expiry date current key set key update interval token lifetime similarly estimated expiry date next key one key update interval serial no current key new block key serial no system current time millis key update interval token lifetime generate secret serial no next key new block key serial no system current time millis key update interval token lifetime generate secret keys put current key get key id current key keys put next key get key id next key export block keys used master mode synchronized exported block keys export keys master return null log debug enabled log debug exporting access keys return new exported block keys true key update interval token lifetime current key keys values array new block key synchronized remove expired keys system current time millis iterator map entry integer block key keys entry set iterator next map entry integer block key e next e get value get expiry date remove set block keys used slave mode synchronized set keys exported block keys exported keys throws io exception master exported keys null return log info setting block keys remove expired keys current key exported keys get current key block key received keys exported keys get all keys received keys length received keys null continue keys put received keys get key id received keys update block keys used master mode synchronized update keys throws io exception master return log info updating block keys remove expired keys set expiry date retiring current key keys put current key get key id new block key current key get key id system current time millis key update interval token lifetime current key get key update estimated expiry date new
633	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\BlockTokenSelector.java	unrelated	package org apache hadoop hdfs security token block a block token selector hdfs block token selector implements token selector block token identifier token block token identifier select token text service collection token extends token identifier tokens service null return null token extends token identifier token tokens block token identifier kind name equals token get kind return token block token identifier token return null
634	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\ExportedBlockKeys.java	unrelated	package org apache hadoop hdfs security token block object passing block keys exported block keys implements writable exported block keys dummy keys new exported block keys boolean block token enabled key update interval token lifetime block key current key block key keys exported block keys false new block key new block key exported block keys boolean block token enabled key update interval token lifetime block key current key block key keys block token enabled block token enabled key update interval key update interval token lifetime token lifetime current key current key null new block key current key keys keys null new block key keys boolean block token enabled return block token enabled get key update interval return key update interval get token lifetime return token lifetime block key get current key return current key block key get all keys return keys writable register ctor writable factories set factory exported block keys new writable factory writable new instance return new exported block keys write data output throws io exception write boolean block token enabled write long key update interval write long token lifetime current key write write int keys length keys length keys write read fields data input throws io exception block token enabled read boolean key update interval read long token lifetime read long current key read fields keys new block key read int keys length keys new block key keys read fields
635	hdfs\src\java\org\apache\hadoop\hdfs\security\token\block\InvalidBlockTokenException.java	unrelated	package org apache hadoop hdfs security token block access token verification failed invalid block token exception extends io exception serial version uid l invalid block token exception super invalid block token exception string msg super msg
636	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenIdentifier.java	unrelated	package org apache hadoop hdfs security token delegation a delegation token identifier specific hdfs delegation token identifier extends abstract delegation token identifier text hdfs delegation kind new text hdfs delegation token create empty delegation token identifier reading delegation token identifier create new delegation token identifier delegation token identifier text owner text renewer text real user super owner renewer real user text get kind return hdfs delegation kind string string return get kind token get sequence number get user get short user name string stringify token token token throws io exception delegation token identifier ident new delegation token identifier byte array input stream buf new byte array input stream token get identifier data input stream new data input stream buf ident read fields token get service get length return ident token get service else return ident string
637	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenSecretManager.java	unrelated	package org apache hadoop hdfs security token delegation a hdfs specific delegation token secret manager the secret manager responsible generating accepting password token delegation token secret manager extends abstract delegation token secret manager delegation token identifier log log log factory get log delegation token secret manager fs namesystem namesystem create secret manager secret keys tokens expired tokens delegation token secret manager delegation key update interval delegation token max lifetime delegation token renew interval delegation token remover scan interval fs namesystem namesystem super delegation key update interval delegation token max lifetime delegation token renew interval delegation token remover scan interval namesystem namesystem delegation token identifier create identifier return new delegation token identifier returns expiry time token given identifier synchronized get token expiry time delegation token identifier dt id throws io exception delegation token information info current tokens get dt id info null return info get renew date else throw new io exception no delegation token found identifier load secret manager state fsimage synchronized load secret manager state data input stream throws io exception running safety check throw new io exception can load state image running secret manager current id read int load all keys delegation token sequence number read int load current tokens store current state secret manager persistence synchronized save secret manager state data output stream throws io exception write int current id save all keys write int delegation token sequence number save current tokens this method intended used reading edit logs fsimage synchronized add persisted delegation token delegation token identifier identifier expiry time throws io exception running safety check throw new io exception can add persisted delegation token running secret manager key id identifier get master key id delegation key key keys get key id key null log warn no key found persisted identifier identifier string return byte password create password identifier get bytes key get key identifier get sequence number delegation token sequence number delegation token sequence number identifier get sequence number current tokens get identifier null current tokens put identifier new delegation token information expiry time password else throw new io exception same delegation token added twice invalid entry fsimage editlogs add master key list keys synchronized update persisted master key delegation key key throws io exception add key key update token cache renewal record edit logs synchronized update persisted token renewal delegation token identifier identifier expiry time throws io exception running safety check throw new io exception can update persisted delegation token renewal running secret manager delegation token information info null info current tokens get identifier info null key id identifier get master key id byte password create password identifier get bytes keys get key id get key current tokens put identifier new delegation token information expiry time password update token cache cancel record edit logs synchronized update persisted token cancellation delegation token identifier identifier throws io exception running safety check throw new io exception can update persisted delegation token renewal running secret manager current tokens remove identifier returns number delegation keys currently stored synchronized get number of keys return keys size private helper methods save delegation keys
638	hdfs\src\java\org\apache\hadoop\hdfs\security\token\delegation\DelegationTokenSelector.java	unrelated	package org apache hadoop hdfs security token delegation a delegation token specialized hdfs delegation token selector extends abstract delegation token selector delegation token identifier delegation token selector super delegation token identifier hdfs delegation kind
639	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\Balancer.java	heartbeat	package org apache hadoop hdfs server balancer p the balancer tool balances disk space usage hdfs cluster datanodes become full new empty nodes join cluster the tool deployed application program run cluster administrator live hdfs cluster applications adding deleting files p synopsis pre to start bin start balancer sh threshold threshold example bin start balancer sh start balancer default threshold bin start balancer sh threshold start balancer threshold to stop bin stop balancer sh pre p description p the threshold parameter fraction range default value the threshold sets target whether cluster balanced a cluster balanced datanode utilization node ratio used space node total capacity node differs utilization ratio used space cluster total capacity cluster threshold value the smaller threshold balanced cluster become it takes time run balancer small threshold values also small threshold cluster may able reach balanced state applications write delete files concurrently p the tool moves blocks highly utilized datanodes poorly utilized datanodes iteratively in iteration datanode moves receives lesser g bytes threshold fraction capacity each iteration runs minutes at end iteration balancer obtains updated datanodes information namenode p a system property limits balancer use bandwidth defined default configuration file pre property name dfs balance bandwidth per sec name value value description specifies maximum bandwidth datanode utilize balancing purpose term number bytes per second description property pre p this property determines maximum speed block moved one datanode another the default value mb the higher bandwidth faster cluster reach balanced state greater competition application processes if administrator changes value property configuration file change observed hdfs next restarted p monitering balancer progress p after balancer started output file name balancer progress recorded printed screen the administrator monitor running balancer reading output file the output shows balancer status iteration iteration in iteration prints starting time iteration number total number bytes moved previous iterations total number bytes left move order cluster balanced number bytes moved iteration normally bytes already moved increasing bytes left to move decreasing p running multiple instances balancer hdfs cluster prohibited tool p the balancer automatically exits following five conditions satisfied ol li the cluster balanced li no block moved li no block moved five consecutive iterations li an io exception occurs communicating namenode li another balancer running ol p upon exit balancer returns exit code prints one following messages output file corresponding exit reasons ol li the cluster balanced exiting li no block moved exiting li no block moved iterations exiting li received io exception failure reason exiting li another balancer running exiting ol p the administrator interrupt execution balancer time running command stop balancer sh machine balancer running balancer log log log factory get log balancer max blocks size to fetch l gb win width l hour the maximum number concurrent blocks moves balancing purpose datanode max num concurrent moves name node connector nnc balancing policy policy threshold data node lists collection source utilized datanodes new linked list source collection source avg utilized datanodes new linked list source collection balancer datanode avg utilized datanodes new linked list balancer datanode collection balancer datanode utilized datanodes new linked list balancer datanode
640	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\BalancingPolicy.java	pooling	package org apache hadoop hdfs server balancer balancing policy since datanode may contain multiple block pools link pool implies link node not way around balancing policy total capacity total used space avg utilization reset total capacity l total used space l avg utilization get policy name string get name accumulate used space capacity accumulate spaces datanode info init avg utilization avg utilization total used space total capacity get avg utilization return avg utilization return utilization datanode get utilization datanode info string string return balancing policy get simple name get class get simple name get link balancing policy instances balancing policy parse string balancing policy balancing policy node instance balancing policy pool instance balancing policy p p get name equals ignore case return p throw new illegal argument exception cannot parse cluster balanced node balanced node extends balancing policy node instance new node node string get name return datanode accumulate spaces datanode info total capacity get capacity total used space get dfs used get utilization datanode info return get dfs used get capacity cluster balanced pool node balanced pool extends balancing policy pool instance new pool pool string get name return blockpool accumulate spaces datanode info total capacity get capacity total used space get block pool used get utilization datanode info return get block pool used get capacity
641	hdfs\src\java\org\apache\hadoop\hdfs\server\balancer\NameNodeConnector.java	pooling	package org apache hadoop hdfs server balancer the provides utilities link balancer access name node name node connector log log balancer log path balancer id path new path system balancer id inet socket address namenode address string blockpool id namenode protocol namenode client protocol client file system fs output stream boolean block token enabled boolean run key updater interval block token secret manager block token secret manager daemon keyupdaterthread access key updater thread name node connector inet socket address namenode address configuration conf throws io exception namenode address namenode address namenode create namenode namenode address conf client dfs util create namenode conf fs file system get name node get uri namenode address conf namespace info namespaceinfo namenode version request blockpool id namespaceinfo get block pool id exported block keys keys namenode get block keys block token enabled keys block token enabled block token enabled block key update interval keys get key update interval block token lifetime keys get token lifetime log info block token params received nn key update interval block key update interval min token lifetime block token lifetime min block token secret manager new block token secret manager false block key update interval block token lifetime block token secret manager set keys keys balancer sync block keys nn frequently nn updates block keys key updater interval block key update interval log info balancer update block keys every key updater interval minute keyupdaterthread new daemon new block key updater run true keyupdaterthread start check another balancer running exit another one running check and mark running balancer null throw new io exception another balancer running get access token block token block token identifier get access token extended block eb throws io exception block token enabled return block token secret manager dummy token else return block token secret manager generate token null eb enum set block token secret manager access mode replace block token secret manager access mode copy the idea making sure one balancer running hdfs create file hdfs writes ip address machine balancer running file close file balancer exits this prevents second balancer running creates file first one running this method checks running balancer mark yes note atomic operation return null running balancer otherwise output stream newly created file output stream check and mark running balancer throws io exception try data output stream fs create balancer id path write bytes inet address get local host get host name flush return catch remote exception e already being created exception get name equals e get class name return null else throw e close connection close run false try keyupdaterthread null keyupdaterthread interrupt catch exception e log warn exception shutting access key updater thread e close output file io utils close stream fs null try fs delete balancer id path true catch io exception ioe log warn failed delete balancer id path ioe string string return get class get simple name namenode address namenode address id blockpool id build namenode protocol connection namenode set retry policy namenode protocol create namenode inet socket address address configuration conf throws io exception retry policy timeout policy retry
642	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockInfo.java	unrelated	package org apache hadoop hdfs server blockmanagement internal block metadata block info extends block implements light weight g set linked element i node file inode for implementing link light weight g set linked element light weight g set linked element next linked element this array contains triplets references for th datanode block belongs triplets reference datanode descriptor triplets triplets references previous next blocks respectively list blocks belonging data node object triplets construct entry blocksmap block info replication triplets new object replication inode null block info block blk replication super blk triplets new object replication inode null copy construction this used convert block info under construction protected block info block info inode get replication inode inode i node file get i node return inode set i node i node file inode inode inode datanode descriptor get datanode index assert triplets null block info initialized assert index index triplets length index bound datanode descriptor node datanode descriptor triplets index assert node null datanode descriptor get name equals node get class get name datanode descriptor expected index return node block info get previous index assert triplets null block info initialized assert index index triplets length index bound block info info block info triplets index assert info null info get class get name starts with block info get name block info expected index return info block info get next index assert triplets null block info initialized assert index index triplets length index bound block info info block info triplets index assert info null info get class get name starts with block info get name block info expected index return info set datanode index datanode descriptor node assert triplets null block info initialized assert index index triplets length index bound triplets index node set previous index block info assert triplets null block info initialized assert index index triplets length index bound triplets index set next index block info assert triplets null block info initialized assert index index triplets length index bound triplets index get capacity assert triplets null block info initialized assert triplets length malformed block info return triplets length ensure enough space num triplets ensure capacity num assert triplets null block info initialized last num nodes triplets length last num return last not enough space left create new array should normally happen replication manually increased user object old triplets triplets new object last num last triplets old return last count number data nodes block belongs num nodes assert triplets null block info initialized assert triplets length malformed block info idx get capacity idx idx get datanode idx null return idx return add data node block belongs boolean add node datanode descriptor node find datanode node node already return false find last null node last node ensure capacity set datanode last node node set next last node null set previous last node null return true remove data node block boolean remove node datanode descriptor node dn index find datanode node dn index node found return false assert get previous dn index null get next dn index null block still list must removed first find last null node
643	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockInfoUnderConstruction.java	unrelated	package org apache hadoop hdfs server blockmanagement represents block currently constructed br this usually last block file opened write append block info under construction extends block info block state see link block uc state block uc state block uc state block replicas assigned block allocated this defines pipeline order list replica under construction replicas a data node responsible block recovery primary node index the new generation stamp block recovery succeeds also used recovery id identify right recovery abandoned recoveries appear block recovery id replica under construction contains information replicas construction the gs length state replica reported data node it guaranteed expected data nodes actually corresponding replicas replica under construction extends block datanode descriptor expected location replica state state replica under construction block block datanode descriptor target replica state state super block expected location target state state expected block replica location assigned block allocated this defines pipeline order it guaranteed expected data node actually replica datanode descriptor get expected location return expected location get replica state reported data node replica state get state return state set replica state set state replica state state is data node replica belongs alive boolean alive return expected location alive hash code return super hash code boolean equals object obj sufficient rely super implementation return obj super equals obj inherit doc string string string builder b new string builder get class get simple name b append append expected location append append state append return b string create block set state link block uc state under construction block info under construction block blk replication blk replication block uc state under construction null create block currently constructed block info under construction block blk replication block uc state state datanode descriptor targets super blk replication assert get block uc state block uc state complete block info under construction cannot complete state block uc state state set expected locations targets convert construction block complete block generation stamp length committed client least minimal number replicas reported data nodes block info convert to complete block throws io exception assert get block uc state block uc state complete trying convert complete block get block uc state block uc state committed throw new io exception cannot complete block block committed client return new block info set expected locations set expected locations datanode descriptor targets num locations targets null targets length replicas new array list replica under construction num locations num locations replicas add new replica under construction targets replica state rbw create array expected replica locations assigned choose targets datanode descriptor get expected locations num locations replicas null replicas size datanode descriptor locations new datanode descriptor num locations num locations locations replicas get get expected location return locations get number expected locations get num expected locations return replicas null replicas size return state block construction block uc state get block uc state return block uc state set block uc state block uc state block uc state get block recovery id get block recovery id return block recovery id commit block length generation stamp reported client set block state link block uc state committed commit block block
644	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockManager.java	unrelated	package org apache hadoop hdfs server blockmanagement keeps information related blocks stored hadoop cluster this helper link fs namesystem requires several methods called lock held link fs namesystem block manager log log log factory get log block manager default load factor map default map load factor f fs namesystem namesystem volatile pending replication blocks count l volatile corrupt replica blocks count l volatile replicated blocks count l volatile scheduled replication blocks count l volatile excess blocks count l volatile pending deletion blocks count l used metrics get pending replication blocks count return pending replication blocks count used metrics get under replicated blocks count return replicated blocks count used metrics get corrupt replica blocks count return corrupt replica blocks count used metrics get scheduled replication blocks count return scheduled replication blocks count used metrics get pending deletion blocks count return pending deletion blocks count used metrics get excess blocks count return excess blocks count replication recheck interval often namenode checks new replication work replication recheck interval mapping block i node datanodes self ref updated response client sent information blocks map blocks map datanode manager datanode manager replication thread daemon replication thread new daemon new replication monitor store blocks datanodedescriptor map corrupt replicas corrupt replicas map corrupt replicas new corrupt replicas map keeps collection every named machine containing blocks recently invalidated thought live machine question mapping storage id array list block map string collection block recent invalidate sets new tree map string collection block keeps tree set every named node each treeset contains list blocks extra location we eventually remove extras mapping storage id tree set block map string collection block excess replicate map new tree map string collection block store set blocks need replicated times we also store pending replication orders under replicated blocks needed replications new under replicated blocks pending replication blocks pending replications the maximum number replicas allowed block max replication the maximum number outgoing replication streams given node one time max replication streams minimum copies needed else write disallowed min replication default number replicas default replication the maximum number entries returned get corrupt inodes max corrupt files returned variable enable check enough racks boolean check for enough racks last block index used replication work repl index block replicas placement block placement policy blockplacement block manager fs namesystem fsn configuration conf throws io exception namesystem fsn datanode manager new datanode manager fsn conf blocks map new blocks map default map load factor blockplacement block placement policy get instance conf namesystem datanode manager get network topology pending replications new pending replication blocks conf get int dfs config keys dfs namenode replication pending timeout sec key dfs config keys dfs namenode replication pending timeout sec default l max corrupt files returned conf get int dfs config keys dfs default max corrupt files returned key dfs config keys dfs default max corrupt files returned default replication conf get int dfs config keys dfs replication key dfs config keys dfs replication default max replication conf get int dfs config keys dfs replication max key dfs config keys dfs replication max default min replication conf get
645	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicy.java	unrelated	package org apache hadoop hdfs server blockmanagement this used choosing desired number targets placing block replicas block placement policy log log log factory get log block placement policy not enough replicas exception extends exception serial version uid l not enough replicas exception string msg super msg choose num of replicas data nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes blocksize same link choose target string datanode descriptor list boolean hash map return chosen nodes equal false datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes hash map node node excluded nodes blocksize return choose target src path num of replicas writer chosen nodes false excluded nodes blocksize choose num of replicas data nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes boolean return chosen nodes hash map node node excluded nodes blocksize choose num of replicas data nodes writer if return many the base implemenatation extracts pathname file specified src inode could costly operation depending file system implementation concrete implementations method avoid overhead sorted pipeline datanode descriptor choose target fs inode info src inode num of replicas datanode descriptor writer list datanode descriptor chosen nodes blocksize return choose target src inode get full path name num of replicas writer chosen nodes blocksize verify block replicated least min racks different racks min racks rack system block replicated verify block placement string src path located block blk min racks decide whether deleting specified replica block still makes block conform configured block placement policy least two unique racks listed previous parameter datanode descriptor choose replica to delete fs inode info src inode block block short replication factor collection datanode descriptor existing replicas collection datanode descriptor existing replicas used setup block placement policy object this defined implementations block placement policy protected initialize configuration conf fs cluster stats stats network topology cluster map get instance configured block placement policy based value configuration paramater dfs block replicator classname block placement policy get instance configuration conf fs cluster stats stats network topology cluster map class extends block placement policy replicator class conf get class dfs block replicator classname block placement policy default block placement policy block placement policy replicator block placement policy reflection utils new instance replicator class conf replicator initialize conf stats cluster map return replicator choose num of replicas nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path num of replicas datanode descriptor writer blocksize return choose target src path num of replicas writer new array list datanode descriptor blocksize choose num of replicas nodes writer replicate block size blocksize if return many sorted pipeline datanode descriptor choose target string src path num of replicas datanode descriptor writer hash map node node excluded nodes blocksize return choose target src path num of replicas writer new array list datanode
646	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicyDefault.java	unrelated	package org apache hadoop hdfs server blockmanagement the responsible choosing desired number targets placing block replicas the replica placement strategy writer datanode st replica placed local machine otherwise random datanode the nd replica placed datanode different rack the rd replica placed datanode different node rack second replica block placement policy default extends block placement policy boolean consider load network topology cluster map fs cluster stats stats string enable debug logging for information please enable debug level logging org apache hadoop hdfs server namenode fs namesystem logger block placement policy default configuration conf fs cluster stats stats network topology cluster map initialize conf stats cluster map block placement policy default inherit doc initialize configuration conf fs cluster stats stats network topology cluster map consider load conf get boolean dfs config keys dfs namenode replication considerload key true stats stats cluster map cluster map thread local string builder thread local builder new thread local string builder protected string builder initial value return new string builder inherit doc datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes blocksize return choose target num of replicas writer chosen nodes false null blocksize inherit doc datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes boolean return chosen nodes hash map node node excluded nodes blocksize return choose target num of replicas writer chosen nodes return chosen nodes excluded nodes blocksize inherit doc datanode descriptor choose target fs inode info src inode num of replicas datanode descriptor writer list datanode descriptor chosen nodes blocksize return choose target num of replicas writer chosen nodes false null blocksize this implementation datanode descriptor choose target num of replicas datanode descriptor writer list datanode descriptor chosen nodes boolean return chosen nodes hash map node node excluded nodes blocksize num of replicas cluster map get num of leaves return new datanode descriptor excluded nodes null excluded nodes new hash map node node cluster size cluster map get num of leaves total num of replicas chosen nodes size num of replicas total num of replicas cluster size num of replicas total num of replicas cluster size total num of replicas cluster size max nodes per rack total num of replicas cluster map get num of racks list datanode descriptor results new array list datanode descriptor chosen nodes node node chosen nodes excluded nodes put node node cluster map contains writer writer null datanode descriptor local node choose target num of replicas writer excluded nodes blocksize max nodes per rack results return chosen nodes results remove all chosen nodes sorting nodes form pipeline return get pipeline writer null local node writer results array new datanode descriptor results size choose num of replicas data nodes datanode descriptor choose target num of replicas datanode descriptor writer hash map node node excluded nodes blocksize max nodes per rack list datanode descriptor results num of replicas cluster map get num of leaves return writer total replicas expected num of replicas num of results results size boolean new block num of results writer
647	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlocksMap.java	unrelated	package org apache hadoop hdfs server blockmanagement this maintains map block metadata block metadata currently includes i node belongs datanodes store block blocks map node iterator implements iterator datanode descriptor block info block info next idx node iterator block info blk info block info blk info boolean next return block info null next idx block info get capacity block info get datanode next idx null datanode descriptor next return block info get datanode next idx remove throw new unsupported operation exception sorry remove constant link light weight g set capacity capacity g set block block info blocks blocks map load factor capacity compute capacity blocks new light weight g set block block info capacity let max memory let e round log then choose capacity e size reference unless outside close interval compute capacity vm detection see http java sun com docs hotspot hot spot faq html bit detection string vm bit system get property sun arch data model max memory two pc runtime get runtime max memory compute capacity e math log two pc math log e e equals vm bit exponent e e e c exponent light weight g set log info vm type vm bit bit light weight g set log info max memory two pc mb light weight g set log info capacity exponent c entries return c close blocks null i node file get i node block b block info info blocks get b return info null info get i node null add block b belonging specified file inode map block info add i node block info b i node file node block info info blocks get b info b info b blocks put info info set i node node return info remove block block map remove data node lists belongs remove data node locations associated block remove block block block block info block info blocks remove block block info null return block info set i node null idx block info num nodes idx idx datanode descriptor dn block info get datanode idx dn remove block block info remove list wipe location returns block object exists map block info get stored block block b return blocks get b searches block blocks map returns iterator iterates nodes block belongs iterator datanode descriptor node iterator block b return node iterator blocks get b for block already retrieved blocks map returns iterator iterates nodes block belongs iterator datanode descriptor node iterator block info stored block return new node iterator stored block counts number containing nodes better using iterator num nodes block b block info info blocks get b return info null info num nodes remove data node reference block remove block block map belong file data nodes boolean remove node block b datanode descriptor node block info info blocks get b info null return false remove block data node list node block info boolean removed node remove block info info get datanode null datanodes left info get i node null belong file blocks remove b remove block map return removed size return blocks size iterable block info get blocks return blocks
648	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\CorruptReplicasMap.java	unrelated	package org apache hadoop hdfs server blockmanagement stores information corrupt blocks file system a block considered corrupt replicas corrupt while reporting replicas block hide corrupt copies these copies removed block found expected number good replicas mapping block tree set datanode descriptor corrupt replicas map sorted map block collection datanode descriptor corrupt replicas map new tree map block collection datanode descriptor mark block belonging datanode corrupt add to corrupt replicas map block blk datanode descriptor dn collection datanode descriptor nodes get nodes blk nodes null nodes new tree set datanode descriptor corrupt replicas map put blk nodes nodes contains dn nodes add dn name node state change log info block name system add to corrupt replicas map blk get block name added corrupt dn get name server get remote ip else name node state change log info block name system add to corrupt replicas map duplicate requested blk get block name add corrupt dn get name server get remote ip remove block corrupt blocks map remove from corrupt replicas map block blk corrupt replicas map null corrupt replicas map remove blk remove block given datanode corrupt block map false replica map boolean remove from corrupt replicas map block blk datanode descriptor datanode collection datanode descriptor datanodes corrupt replicas map get blk datanodes null return false datanodes remove datanode remove replicas datanodes empty remove block corrupted replicas corrupt replicas map remove blk return true return false get nodes corrupt replicas block collection datanode descriptor get nodes block blk return corrupt replicas map get blk check replica belonging datanode corrupt boolean replica corrupt block blk datanode descriptor node collection datanode descriptor nodes get nodes blk return nodes null nodes contains node num corrupt replicas block blk collection datanode descriptor nodes get nodes blk return nodes null nodes size size return corrupt replicas map size return range corrupt replica block ids up num expected blocks blocks starting next block starting block id returned fewer num expected blocks blocks unavailable if starting block id null num expected blocks blocks returned beginning if starting block id cannot found null returned num expected blocks beginning get corrupt replica block ids num expected blocks long starting block id num expected blocks num expected blocks return null iterator block block it corrupt replicas map key set iterator starting block id specified iterate keys find matching block if find matching block break leave iterator next block specified block starting block id null boolean block found false block it next block b block it next b get block id starting block id block found true break block found return null array list long corrupt replica block ids new array list long append num expected blocks block ids list num expected blocks block it next corrupt replica block ids add block it next get block id ret new corrupt replica block ids size ret length ret corrupt replica block ids get return ret
649	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DatanodeDescriptor.java	unrelated	package org apache hadoop hdfs server blockmanagement datanode descriptor tracks stats given data node available storage capacity last update time etc maintains set blocks stored datanode this data structure internal namenode it sent wire client datanodes neither stored persistently fs image datanode descriptor extends datanode info stores status decommissioning if node decommissioning use object anything decommissioning status decommissioning status new decommissioning status block targets pair block target pair block block datanode descriptor targets block target pair block block datanode descriptor targets block block targets targets a block target pair queue block queue e queue e blockq new linked list e size queue synchronized size return blockq size enqueue synchronized boolean offer e e return blockq offer e dequeue synchronized list e poll num blocks num blocks blockq empty return null list e results new array list e blockq empty num blocks num blocks results add blockq poll return results returns tt true tt queue contains specified element boolean contains e e return blockq contains e volatile block info block list null num blocks alive heartbeats contains this optimization contains takes o n time arraylist boolean alive false boolean need key update false a queue blocks replicated datanode block queue block target pair replicate blocks new block queue block target pair a queue blocks recovered datanode block queue block info under construction recover blocks new block queue block info under construction a set blocks invalidated datanode set block invalidate blocks new tree set block variables maintaining number blocks scheduled written datanode this count approximate might slightly bigger case errors e g datanode report error occurs writing block curr approx blocks scheduled prev approx blocks scheduled last blocks scheduled roll time blocks scheduled roll interval min volume failures when set true node list allowed communicate namenode boolean disallowed false default constructor datanode descriptor datanode descriptor constructor datanode descriptor datanode id node id node id l l l l datanode descriptor constructor datanode descriptor datanode id node id string network location node id network location null datanode descriptor constructor datanode descriptor datanode id node id string network location string host name node id network location host name l l l l datanode descriptor constructor datanode descriptor datanode id node id capacity dfs used remaining bpused xceiver count failed volumes super node id update heartbeat capacity dfs used remaining bpused xceiver count failed volumes datanode descriptor constructor datanode descriptor datanode id node id string network location string host name capacity dfs used remaining bpused xceiver count failed volumes super node id network location host name update heartbeat capacity dfs used remaining bpused xceiver count failed volumes add datanode block add block head list blocks belonging data node boolean add block block info b b add node return false add head data node list block list b list insert block list num blocks return true remove block list blocks belonging data node remove datanode block boolean remove block block info b block list b list remove block list b remove node num blocks return true else return false move block head list blocks belonging data node move
650	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DatanodeManager.java	heartbeat	package org apache hadoop hdfs server blockmanagement manage datanodes decommission activities datanode manager log log log factory get log datanode manager fs namesystem namesystem stores datanode block map p done storing set link datanode descriptor objects sorted storage id in order keep storage map consistent tracks storages ever registered namenode a descriptor corresponding specific storage id ul li added map new storage id li li updated new datanode started replacement old one storage id li li removed existing datanode restarted serve different storage id li ul br the list link datanode descriptor map checkpointed namespace image file only link datanode info part persistent list blocks restored datanode block reports p mapping storage id datanode descriptor navigable map string datanode descriptor datanode map new tree map string datanode descriptor cluster network topology network topology networktopology new network topology host names datanode descriptors mapping host nodes map host datanode map new host nodes map dns to switch mapping dns to switch mapping read exclude files hosts file reader hosts reader the period wait datanode heartbeat heartbeat expire interval ask datanode many blocks delete block invalidate limit datanode manager fs namesystem namesystem configuration conf throws io exception namesystem namesystem hosts reader new hosts file reader conf get dfs config keys dfs hosts conf get dfs config keys dfs hosts exclude dns to switch mapping reflection utils new instance conf get class dfs config keys net topology node switch mapping impl key script based mapping dns to switch mapping conf if dns switch mapping supports cache resolve network locations hosts list store mapping cache future calls resolve fast dns to switch mapping instanceof cached dns to switch mapping dns to switch mapping resolve new array list string hosts reader get hosts heartbeat interval seconds conf get long dfs config keys dfs heartbeat interval key dfs config keys dfs heartbeat interval default heartbeat recheck interval conf get int dfs config keys dfs namenode heartbeat recheck interval key dfs config keys dfs namenode heartbeat recheck interval default minutes heartbeat expire interval heartbeat recheck interval heartbeat interval seconds block invalidate limit math max heartbeat interval seconds dfs config keys dfs block invalidate limit default log info dfs config keys dfs block invalidate limit key block invalidate limit daemon decommissionthread null activate configuration conf decommissionthread new daemon new decommission manager namesystem new monitor conf get int dfs config keys dfs namenode decommission interval key dfs config keys dfs namenode decommission interval default conf get int dfs config keys dfs namenode decommission nodes per interval key dfs config keys dfs namenode decommission nodes per interval default decommissionthread start close decommissionthread null decommissionthread interrupt network topology get network topology return networktopology sort located blocks distance target host sort located blocks string targethost list located block locatedblocks sort blocks datanode descriptor client get datanode by host targethost located block b locatedblocks networktopology pseudo sort by distance client b get locations move decommissioned datanodes bottom arrays sort b get locations dfs util decom comparator cyclic iteration string datanode descriptor get datanode cyclic iteration string firstkey return new cyclic iteration string datanode descriptor datanode map
651	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\DecommissionManager.java	unrelated	package org apache hadoop hdfs server blockmanagement manage node decommissioning decommission manager log log log factory get log decommission manager fs namesystem fsnamesystem block manager block manager decommission manager fs namesystem namesystem fsnamesystem namesystem block manager fsnamesystem get block manager periodically check decommission status monitor implements runnable recheck interval often namenode checks node finished decommission recheck interval the number decommission nodes check interval num nodes per check firstkey initialized anything string firstkey monitor recheck interval in second num nodes per check recheck interval recheck interval in second l num nodes per check num nodes per check check decommission status num nodes per check nodes every recheck interval milliseconds run fsnamesystem running fsnamesystem write lock try check finally fsnamesystem write unlock try thread sleep recheck interval catch interrupted exception ie log warn get class get simple name interrupted ie check count map entry string datanode descriptor entry block manager get datanode manager get datanode cyclic iteration firstkey datanode descriptor entry get value firstkey entry get key decommission in progress try block manager check decommission state internal catch exception e log warn entry entry e count num nodes per check return
652	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\Host2NodesMap.java	unrelated	package org apache hadoop hdfs server blockmanagement a map host names datanode descriptors host nodes map hash map string datanode descriptor map new hash map string datanode descriptor read write lock hostmap lock new reentrant read write lock check node already map boolean contains datanode descriptor node node null return false string host node get host hostmap lock read lock lock try datanode descriptor nodes map get host nodes null datanode descriptor contained node nodes node contained node return true finally hostmap lock read lock unlock return false add node map return true node added false otherwise boolean add datanode descriptor node hostmap lock write lock lock try node null contains node return false string host node get host datanode descriptor nodes map get host datanode descriptor new nodes nodes null new nodes new datanode descriptor new nodes node else rare case one datanode host new nodes new datanode descriptor nodes length system arraycopy nodes new nodes nodes length new nodes nodes length node map put host new nodes return true finally hostmap lock write lock unlock remove node map return true node removed false otherwise boolean remove datanode descriptor node node null return false string host node get host hostmap lock write lock lock try datanode descriptor nodes map get host nodes null return false nodes length nodes node map remove host return true else return false rare case nodes length nodes node break nodes length return false else datanode descriptor new nodes new nodes new datanode descriptor nodes length system arraycopy nodes new nodes system arraycopy nodes new nodes nodes length map put host new nodes return true finally hostmap lock write lock unlock get data node host datanode descriptor get datanode by host string host host null return null hostmap lock read lock lock try datanode descriptor nodes map get host entry nodes null return null one node nodes length return nodes one node return nodes dfs util get random next int nodes length finally hostmap lock read lock unlock find data node name datanode descriptor get datanode by name string name name null return null colon name index of string host colon host name else host name substring colon hostmap lock read lock lock try datanode descriptor nodes map get host entry nodes null return null datanode descriptor contained node nodes name equals contained node get name return contained node return null finally hostmap lock read lock unlock
653	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\NumberReplicas.java	unrelated	package org apache hadoop hdfs server blockmanagement a immutable object stores number live replicas number decommissined replicas number replicas live replicas decommissioned replicas corrupt replicas excess replicas number replicas initialize number replicas live decommissioned corrupt excess initialize live decommissioned corrupt excess initialize live decommissioned corrupt excess live replicas live decommissioned replicas decommissioned corrupt replicas corrupt excess replicas excess live replicas return live replicas decommissioned replicas return decommissioned replicas corrupt replicas return corrupt replicas excess replicas return excess replicas
654	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\PendingReplicationBlocks.java	unrelated	package org apache hadoop hdfs server blockmanagement pending replication blocks bookkeeping blocks getting replicated it following record blocks getting replicated instant coarse grain timer track age replication request thread periodically identifies replication requests never made pending replication blocks log log block manager log map block pending block info pending replications array list block timed out items daemon timer thread null volatile boolean fs running true it might take anywhere minutes request timed timeout default recheck interval pending replication blocks timeout period timeout period timeout timeout period pending replications new hash map block pending block info timed out items new array list block start timer thread new daemon new pending replication monitor timer thread start add block list pending replications add block block num replicas synchronized pending replications pending block info found pending replications get block found null pending replications put block new pending block info num replicas else found increment replicas num replicas found set time stamp one replication request block finished decrement number pending replication requests block remove block block synchronized pending replications pending block info found pending replications get block found null log debug enabled log debug removing pending replication block found decrement replicas found get num replicas pending replications remove block the total number blocks undergoing replication size return pending replications size how many copies block pending replication get num replicas block block synchronized pending replications pending block info found pending replications get block found null return found get num replicas return returns list blocks timed replication requests returns null blocks timed block get timed out blocks synchronized timed out items timed out items size return null block block list timed out items array new block timed out items size timed out items clear return block list an object contains information block replicated it records timestamp system started replicating recent copy block it also records number replication requests progress pending block info time stamp num replicas in progress pending block info num replicas time stamp num replicas in progress num replicas get time stamp return time stamp set time stamp time stamp increment replicas increment num replicas in progress increment decrement replicas num replicas in progress assert num replicas in progress get num replicas return num replicas in progress a periodic thread scans blocks never finished replication request pending replication monitor implements runnable run fs running period math min default recheck interval timeout try pending replication check thread sleep period catch interrupted exception ie log debug enabled log debug pending replication monitor thread interrupted ie iterate items detect timed items pending replication check synchronized pending replications iterator map entry block pending block info iter pending replications entry set iterator log debug enabled log debug pending replication monitor checking q iter next map entry block pending block info entry iter next pending block info pending block entry get value pending block get time stamp timeout block block entry get key synchronized timed out items timed out items add block log warn pending replication monitor timed block iter remove shuts pending replication monitor thread waits thread exit stop fs running false
655	hdfs\src\java\org\apache\hadoop\hdfs\server\blockmanagement\UnderReplicatedBlocks.java	unrelated	package org apache hadoop hdfs server blockmanagement class keeping track replication blocks blocks replication priority priority indicating highest blocks one replicas highest under replicated blocks implements iterable block level queue with corrupt blocks list tree set block priority queues new array list tree set block constructor under replicated blocks level priority queues add new tree set block empty queues clear level priority queues get clear return total number replication blocks synchronized size size level size priority queues get size return size return number replication blocks excluding corrupt blocks synchronized get under replicated block count size queue with corrupt blocks size priority queues get size return size return number corrupt blocks synchronized get corrupt block size return priority queues get queue with corrupt blocks size check block needed replication queue synchronized boolean contains block block tree set block set priority queues set contains block return true return false return priority block get priority block block cur replicas decommissioned replicas expected replicas assert cur replicas negative replicas cur replicas expected replicas return block enough racks else cur replicas if zero non decommissioned replica decommissioned replicas assign highest priority decommissioned replicas return return queue with corrupt blocks keep blocks needed replication else cur replicas return highest priority else cur replicas expected replicas return else return add block replication queue according priority synchronized boolean add block block cur replicas decomissioned replicas expected replicas assert cur replicas negative replicas pri level get priority block cur replicas decomissioned replicas expected replicas pri level level priority queues get pri level add block name node state change log debug enabled name node state change log debug block name system under replication block add block cur replicas replicas need expected replicas replicas added needed replications priority level pri level return true return false remove block replication queue synchronized boolean remove block block old replicas decommissioned replicas old expected replicas pri level get priority block old replicas decommissioned replicas old expected replicas return remove block pri level remove block replication queue given priority boolean remove block block pri level pri level pri level level priority queues get pri level remove block name node state change log debug enabled name node state change log debug block name system under replication block remove removing block block priority queue pri level return true else try remove block queues block found queue given priority level level priority queues get remove block name node state change log debug enabled name node state change log debug block name system under replication block remove removing block block priority queue return true return false update priority level block synchronized update block block cur replicas decommissioned replicas cur expected replicas cur replicas delta expected replicas delta old replicas cur replicas cur replicas delta old expected replicas cur expected replicas expected replicas delta cur pri get priority block cur replicas decommissioned replicas cur expected replicas old pri get priority block old replicas decommissioned replicas old expected replicas name node state change log debug enabled name node state change log debug under replication blocks update block cur replicas cur replicas cur expected replicas
656	hdfs\src\java\org\apache\hadoop\hdfs\server\common\GenerationStamp.java	unrelated	package org apache hadoop hdfs server common a generation stamp hadoop fs primitive identified generation stamp implements comparable generation stamp the first valid generation stamp first valid stamp l generation stamp blocks pre date introduction generation stamp grandfather generation stamp volatile genstamp create new instance initialized first valid stamp generation stamp generation stamp first valid stamp create new instance initialized specified value generation stamp stamp genstamp stamp returns current generation stamp get stamp return genstamp sets current generation stamp set stamp stamp genstamp stamp first increments counter returns stamp synchronized next stamp genstamp return genstamp compare to generation stamp return genstamp genstamp genstamp genstamp boolean equals object instanceof generation stamp return false return compare to generation stamp hash code return genstamp genstamp
657	hdfs\src\java\org\apache\hadoop\hdfs\server\common\HdfsConstants.java	unrelated	package org apache hadoop hdfs server common some handy internal hdfs constants hdfs constants type node enum node type name node data node startup options enum startup option format format clusterid clusterid genclusterid genclusterid regular regular backup backup checkpoint checkpoint upgrade upgrade rollback rollback finalize finalize import import checkpoint string name null used format upgrade options string cluster id null startup option string arg name arg string get name return name namenode role node role switch case backup return namenode role backup case checkpoint return namenode role checkpoint default return namenode role namenode set cluster id string cid cluster id cid string get cluster id return cluster id timeouts communicating data node streaming writes reads read timeout read timeout extension write timeout write timeout extension write pipeline dn keepalive timeout defines name node role enum namenode role namenode name node backup backup node checkpoint checkpoint node string description null namenode role string arg description arg string string return description block replica states go constructed enum replica state replica finalized the state replica modified finalized replica written rbw replica waiting recovered rwr replica recovery rur temporary replica created replication relocation temporary value replica state v value v get value return value replica state get state v return replica state values v read replica state read data input throws io exception return values read byte write write data output throws io exception write byte ordinal states block go construction enum block uc state block construction completed br the block least one link replica state finalized replica going modified complete the block construction br it recently allocated write append under construction the block recovery br when file lease expires last block may link complete needs go recovery procedure synchronizes existing replicas contents under recovery the block committed br the client reported bytes written data nodes given generation stamp block length link replica state finalized replicas yet reported data nodes committed string namenode lease holder hdfs name node namenode lease recheck interval
658	hdfs\src\java\org\apache\hadoop\hdfs\server\common\InconsistentFSStateException.java	unrelated	package org apache hadoop hdfs server common the exception thrown file system state inconsistent recoverable inconsistent fs state exception extends io exception serial version uid l inconsistent fs state exception file dir string descr super directory get file path dir inconsistent state descr inconsistent fs state exception file dir string descr throwable ex dir descr n string utils stringify exception ex string get file path file dir try return dir get canonical path catch io exception e return dir get path
659	hdfs\src\java\org\apache\hadoop\hdfs\server\common\IncorrectVersionException.java	unrelated	package org apache hadoop hdfs server common the exception thrown external version match current version application incorrect version exception extends io exception serial version uid l incorrect version exception version reported string what version reported what fs constants layout version incorrect version exception version reported string what version expected super unexpected version what null what reported version reported expecting version expected incorrect version exception string version reported string what string version expected super unexpected version what null what reported version reported expecting version expected
660	hdfs\src\java\org\apache\hadoop\hdfs\server\common\JspHelper.java	authenticate	package org apache hadoop hdfs server common jsp helper string current conf current conf string web ugi property name dfs config keys dfs web ugi key string delegation parameter name delegation string namenode address nnaddr string set delegation delegation parameter name log log log factory get log jsp helper private constructor preventing creating jsp helper object jsp helper data structure count number blocks datanodes node record extends datanode info frequency node record frequency node record datanode info info count super info frequency count boolean equals object obj sufficient use super equality datanodes uniquely identified datanode id return obj super equals obj hash code super implementation sufficient return super hash code compare two records based frequency node record comparator implements comparator node record compare node record node record frequency frequency return else frequency frequency return return datanode info best node located blocks blks throws io exception hash map datanode info node record map new hash map datanode info node record located block block blks get located blocks datanode info nodes block get locations datanode info node nodes node record record map get node record null map put node new node record node else record frequency node record nodes map values array new node record map size arrays sort nodes new node record comparator return best node nodes false datanode info best node located block blk throws io exception datanode info nodes blk get locations return best node nodes true datanode info best node datanode info nodes boolean random throws io exception tree set datanode info dead nodes new tree set datanode info datanode info chosen node null failures socket null index nodes null nodes length throw new io exception no nodes contain block null chosen node null random index dfs util get random next int nodes length else index chosen node nodes index dead nodes contains chosen node chosen node nodes index ping check whether node alive inet socket address target addr net utils create socket addr chosen node get host chosen node get info port try new socket connect target addr hdfs constants read timeout set so timeout hdfs constants read timeout catch io exception e dead nodes add chosen node close null failures failures nodes length throw new io exception could reach block containing data please try close return chosen node stream block in ascii inet socket address addr string pool id block id token block token identifier block token gen stamp block size offset into block chunk size to view jsp writer configuration conf throws io exception chunk size to view return socket new socket connect addr hdfs constants read timeout set so timeout hdfs constants read timeout amt to read math min chunk size to view block size offset into block use block name file name string file block reader get file name addr pool id block id block reader block reader block reader new block reader file new extended block pool id block id gen stamp block token offset into block amt to read conf get int io file buffer size byte buf new byte amt to read
661	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Storage.java	unrelated	package org apache hadoop hdfs server common storage information file p local storage information stored separate file version it contains type node storage layout version namespace id fs state creation time p local storage reside multiple directories each directory contain version file others during startup hadoop servers name node data nodes read local storage information p the servers hold lock storage directory run nodes able startup sharing storage the locks released servers stop normally abnormally storage extends storage info log log log factory get log storage get name constants last layout version support upgrades last pre upgrade layout version corresponds hadoop last upgradable layout version protected string last upgradable hadoop version hadoop removed last upgradable lv goes beyond upgrade code uses constant also removed pre generationstamp layout version layout versions release layout versions string storage file lock use lock protected string storage file version version string storage dir current current string storage dir previous previous string storage tmp removed removed tmp string storage tmp previous previous tmp string storage tmp finalized finalized tmp string storage tmp last ckpt lastcheckpoint tmp string storage previous ckpt previous checkpoint enum storage state non existent not formatted complete upgrade recover upgrade complete finalize complete rollback recover rollback complete checkpoint recover checkpoint normal an denote storage directory type implementations define type storage directory implementing storage dir type storage dir type get storage dir type boolean of type storage dir type type protected node type storage type type node using storage protected list storage directory storage dirs new array list storage directory dir iterator implements iterator storage directory storage dir type dir type prev index remove next index next dir iterator storage dir type dir type dir type dir type next index prev index boolean next storage dirs empty next index storage dirs size return false dir type null next index storage dirs size get storage dir next index get storage dir type of type dir type break next index next index storage dirs size return false return true storage directory next storage directory sd get storage dir next index prev index next index next index dir type null next index storage dirs size get storage dir next index get storage dir type of type dir type break next index return sd remove next index prev index restore previous state storage dirs remove prev index remove last returned element next reset next index correct place return default iterator this iterator returns entries storage dirs iterator storage directory dir iterator return dir iterator null return iterator based storage directory type this iterator selects entries storage dirs type dir type returns via iterator iterator storage directory dir iterator storage dir type dir type return new dir iterator dir type generate storage list debug line string list storage directories string builder buf new string builder storage directory sd storage dirs buf append sd get root sd get storage dir type return buf string one storage directories storage directory file root root directory boolean use lock flag enable storage lock storage dir type dir type storage dir type file lock lock storage
662	hdfs\src\java\org\apache\hadoop\hdfs\server\common\StorageInfo.java	unrelated	package org apache hadoop hdfs server common common storage information todo namespace id computed hash address port storage info implements writable layout version layout version storage data namespace id id file system string cluster id id cluster c time creation time file system state storage info l storage info layout v ns id string cid c t layout version layout v cluster id cid namespace id ns id c time c t storage info storage info set storage info layout version storage data get layout version return layout version namespace id file system p assigned file system formatting never changes shared file system components get namespace id return namespace id cluster id file system p string get cluster id return cluster id creation time file system state p modified upgrades get c time return c time set storage info storage info layout version layout version cluster id cluster id namespace id namespace id c time c time writable write data output throws io exception write int get layout version write int get namespace id writable utils write string cluster id write long get c time read fields data input throws io exception layout version read int namespace id read int cluster id writable utils read string c time read long string string string builder sb new string builder sb append lv append layout version append cid append cluster id append nsid append namespace id append c append c time return sb string
663	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Upgradeable.java	unrelated	package org apache hadoop hdfs server common common distributed upgrade objects each upgrade object corresponds layout version latest version upgraded using object that components whose layout version greater equal one returned link get version must upgraded object upgradeable extends comparable upgradeable get layout version upgrade object get version get type software component object upgrading hdfs constants node type get type description upgrade object displaying string get description upgrade status determines percentage work done total amount required upgrade means upgrade completed any value means complete the return value provide least values e g short get upgrade status prepare upgrade e g initialize upgrade data structures set status returns upgrade command used broadcasting cluster components e g name node informs data nodes must perform distributed upgrade upgrade command start upgrade throws io exception complete upgrade e g cleanup upgrade data structures write metadata disk returns upgrade command used broadcasting cluster components e g data nodes inform name node completed upgrade data nodes still upgrading upgrade command complete upgrade throws io exception get status report upgrade false otherwise upgrade status report get upgrade status report boolean details throws io exception
664	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeManager.java	unrelated	package org apache hadoop hdfs server common generic upgrade manager link broadcast command command upgrade manager protected sorted set upgradeable current upgrades null protected boolean upgrade state false true upgrade progress protected upgrade version protected upgrade command broadcast command null synchronized upgrade command get broadcast command return broadcast command synchronized boolean get upgrade state return upgrade state synchronized get upgrade version return upgrade version synchronized set upgrade state boolean u state u version upgrade state u state upgrade version u version sorted set upgradeable get distributed upgrades throws io exception return upgrade object collection get distributed upgrades get upgrade version get type synchronized short get upgrade status current upgrades null return return current upgrades first get upgrade status synchronized boolean initialize upgrade throws io exception current upgrades get distributed upgrades current upgrades null set new upgrade state set upgrade state false fs constants layout version return false upgradeable cur uo current upgrades first set write new upgrade state disk set upgrade state true cur uo get version return true synchronized boolean upgrade completed current upgrades null return true return false hdfs constants node type get type boolean start upgrade throws io exception complete upgrade throws io exception
665	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeObject.java	unrelated	package org apache hadoop hdfs server common abstract upgrade object contains default implementation common methods link upgradeable upgrade object implements upgradeable protected short status short get upgrade status return status string get description return upgrade object get type layout version get version upgrade status report get upgrade status report boolean details throws io exception return new upgrade status report get version get upgrade status false compare to upgradeable get version get version return get version get version res get type string compare to get type string res return res return get class get canonical name compare to get class get canonical name boolean equals object instanceof upgrade object return false return compare to upgrade object hash code return new uo signature hash code
666	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeObjectCollection.java	unrelated	package org apache hadoop hdfs server common collection upgrade objects upgrade objects registered used upgrade object collection initialize registered distributed upgrade objects register upgrade new upgrade object uo signature implements comparable uo signature version hdfs constants node type type string name uo signature upgradeable uo version uo get version type uo get type name uo get class get canonical name get version return version hdfs constants node type get type return type string get class name return name upgradeable instantiate throws io exception try return upgradeable class name get class name new instance catch class not found exception e throw new io exception string utils stringify exception e catch instantiation exception e throw new io exception string utils stringify exception e catch illegal access exception e throw new io exception string utils stringify exception e compare to uo signature version version return version version res get type string compare to get type string res return res return name compare to name boolean equals object instanceof uo signature return false return compare to uo signature hash code return version type null type hash code name null name hash code static collection upgrade objects sorted version layout versions negative therefore newer versions go first sorted set uo signature upgrade table initialize upgrade table new tree set uo signature register upgrade upgradeable uo registered distributed upgrade objects upgrade table add new uo signature uo sorted set upgradeable get distributed upgrades version from hdfs constants node type type throws io exception assert fs constants layout version version from incorrect version version from expected fs constants layout version sorted set upgradeable upgrade objects new tree set upgradeable uo signature sig upgrade table sig get version fs constants layout version continue sig get version version from break sig get type type continue upgrade objects add sig instantiate upgrade objects size return null return upgrade objects
667	hdfs\src\java\org\apache\hadoop\hdfs\server\common\UpgradeStatusReport.java	unrelated	package org apache hadoop hdfs server common base upgrade upgrade status overload specific status fields need reported describes status current upgrade upgrade status report implements writable protected version protected short upgrade status protected boolean finalized upgrade status report version upgrade status finalized false upgrade status report version short status boolean finalized version version upgrade status status finalized finalized get layout version currently running upgrade get version return version get upgrade upgrade status percentage total upgrade done short get upgrade status return upgrade status is current upgrade finalized boolean finalized return finalized get upgrade status data text reporting should overloaded particular upgrade specific upgrade status data false otherwise string get status text boolean details return upgrade version get version upgrade status progress status upgrade status completed n upgrade finalized finalized print basic upgrade status details string string return get status text false writable register ctor writable factories set factory upgrade status report new writable factory writable new instance return new upgrade status report write data output throws io exception write int version write short upgrade status read fields data input throws io exception version read int upgrade status read short
668	hdfs\src\java\org\apache\hadoop\hdfs\server\common\Util.java	unrelated	package org apache hadoop hdfs server common util log log log factory get log util get name current system time return system current time millis interprets passed uri in case error assumes specified file uri as uri string throws io exception uri u null try make uri try u new uri catch uri syntax exception e log error syntax error uri please check hdfs configuration e uri null scheme undefined assume file u null u get scheme null log warn path specified uri configuration files please update hdfs configuration u file as uri new file return u converts passed file uri uri file as uri file f throws io exception return f get canonical file uri converts collection strings collection ur is collection uri collection as ur is collection string names collection uri uris new array list uri names size string name names try uris add as uri name catch io exception e log error error processing uri name e return uris
669	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockMetadataHeader.java	unrelated	package org apache hadoop hdfs server datanode block metadata header manages metadata data blocks datanodes this related block related functionality namenode the biggest part data block metadata crc block block metadata header short metadata version fs dataset metadata version header includes everything except checksum version two bytes following data checksum occupies bytes short version data checksum checksum null block metadata header short version data checksum checksum checksum checksum version version short get version return version data checksum get checksum return checksum this reads fields till beginning checksum block metadata header read header data input stream throws io exception return read header read short reads header top metadata file returns header block metadata header read header file file throws io exception data input stream null try new data input stream new buffered input stream new file input stream file return read header finally io utils close stream version already read block metadata header read header short version data input stream throws io exception data checksum checksum data checksum new data checksum return new block metadata header version checksum this writes fields till beginning checksum write header data output stream block metadata header header throws io exception write short header get version header get checksum write header writes fields till beginning checksum write header data output stream data checksum checksum throws io exception write header new block metadata header metadata version checksum returns size header get header size return short size byte size data checksum get checksum header size
670	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockPoolSliceScanner.java	unrelated	package org apache hadoop hdfs server datanode performs two types scanning li gets block files data directories reconciles difference blocks disk memory link fs dataset li li scans data directories block files block pool verifies files corrupt li this keeps track blocks last verification times currently modify metadata block block pool slice scanner log log log factory get log block pool slice scanner max scan rate mb per sec min scan rate mb per sec default scan period hours l three weeks string block pool id string date format string yyyy mm dd hh mm ss sss string verification log file dncp block verification log verfication log limit num blocks scan period default scan period hours data node datanode fs dataset dataset sorted set tree set block scan info block info set hash map block block scan info block map processed blocks keeps track blocks scanned since last run hash map long integer processed blocks total scans total scan errors total transient errors total blocks scanned in last run used test current period start system current time millis bytes left bytes scan period total bytes to scan log file handler verification log data transfer throttler throttler null enum scan type verification scan scanned part periodic verfication none block scan info implements comparable block scan info block block last scan time scan type last scan type scan type none boolean last scan ok true block scan info block block block block hash code return block hash code boolean equals object return instanceof block scan info compare to block scan info get last scan time return last scan type scan type none last scan time compare to block scan info last scan time last scan time return block compare to block block pool slice scanner data node datanode fs dataset dataset configuration conf string bpid datanode datanode dataset dataset block pool id bpid scan period conf get int dfs config keys dfs datanode scan period hours key dfs config keys dfs datanode scan period hours default scan period scan period default scan period hours scan period log info periodic block verification scan initialized interval scan period string get block pool id return block pool id synchronized boolean initialized return throttler null update bytes to scan len last scan time len could negative block deleted total bytes to scan len last scan time current period start bytes left len should change throttler bandwidth every time bytes left changes really required synchronized add block info block scan info info boolean added block info set add info block map put info block info added update bytes to scan info block get num bytes info last scan time synchronized del block info block scan info info boolean exists block info set remove info block map remove info block exists update bytes to scan info block get num bytes info last scan time update block map given log entry synchronized update block info log entry e block scan info info block map get new block e block id e gen stamp info null e verification time info last scan
671	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockPoolSliceStorage.java	pooling	package org apache hadoop hdfs server datanode manages storage set block pool slices share particular block pool id data node this supports following functionality ol li formatting new block pool storage li li recovering storage state consistent state possible li li taking snapshot block pool upgrade li li rolling back block pool previous snapshot li li finalizing block storage deletion snapshot li ul block pool slice storage extends storage pattern block pool path pattern pattern compile bp string blockpool id id blockpool block pool slice storage storage info storage info string bpid super node type data node storage info blockpool id bpid block pool slice storage namespace id string bp id c time string cluster id super node type data node namespace id namespace id blockpool id bp id c time c time cluster id cluster id analyze storage directories recover previous transitions required recover transition read data node datanode namespace info ns info collection file data dirs startup option start opt throws io exception assert fs constants layout version ns info get layout version block pool name node layout versions must for bp data directory analyze state check whether consistent transitioning storage dirs new array list storage directory data dirs size array list storage state data dir states new array list storage state data dirs size iterator file data dirs iterator next file data dir next storage directory sd new storage directory data dir null false storage state cur state try cur state sd analyze storage start opt sd locked opened switch cur state case normal break case non existent ignore storage log info storage directory data dir exist remove continue case not formatted format log info storage directory data dir formatted log info formatting format sd ns info break default recovery part common sd recover cur state catch io exception ioe sd unlock throw ioe add storage list this inherited parent storage add storage dir sd data dir states add cur state data dirs size none data dirs exist throw new io exception all specified directories accessible exist do transitions each storage directory treated individually during startup upgrade roll back others could date regular startup idx idx get num storage dirs idx transition datanode get storage dir idx ns info start opt assert get layout version ns info get layout version data node name node layout versions must assert get c time ns info get c time data node name node c times must update storages some might formatted write all format block pool slice storage format file dn cur dir namespace info ns info throws io exception file cur bp dir get bp root ns info get block pool id dn cur dir storage directory bp sdir new storage directory cur bp dir format bp sdir ns info format block pool slice storage format storage directory bp sdir namespace info ns info throws io exception log info formatting block pool blockpool id directory bp sdir get current dir bp sdir clear directory create directory layout version fs constants layout version c time ns info get c time namespace id
672	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockReceiver.java	pooling	package org apache hadoop hdfs server datanode a receives block writes disk meanwhile may copies another site if throttler provided streaming throttling also supported block receiver implements closeable fs constants log log data node log log client trace log data node client trace log data input stream null data read data checksum checksum chunks block read output stream null block file local disk output stream cout null output stream cehcksum file data output stream checksum out null crc file local disk bytes per checksum checksum size byte buffer buf contains one full packet buf read amount valid data buf max packet read len protected string addr protected string addr string mirror addr data output stream mirror out daemon responder null data transfer throttler throttler fs dataset block write streams streams datanode info src data node null checksum partial crc null data node datanode volatile boolean mirror error the client name it empty datanode client string clientname boolean client boolean datanode block receive extended block block replica write replica in pipeline interface replica info pipeline stage block construction stage stage boolean transfer block receiver extended block block data input stream string addr string addr block construction stage stage new gs min bytes rcvd max bytes rcvd string clientname datanode info src data node data node datanode throws io exception try block block addr addr addr addr src data node src data node datanode datanode clientname clientname datanode clientname length client datanode datanode client name length stage null pipeline setup create stage stage transfer stage block construction stage transfer rbw stage block construction stage transfer finalized log debug enabled log debug get class get simple name block n client client clientname clientname n datanode datanode src data node src data node n addr addr addr addr open local disk datanode replication move replica info datanode data create temporary block else switch stage case pipeline setup create replica info datanode data create rbw block break case pipeline setup streaming recovery replica info datanode data recover rbw block new gs min bytes rcvd max bytes rcvd block set generation stamp new gs break case pipeline setup append replica info datanode data append block new gs min bytes rcvd datanode block scanner null remove block scanner datanode block scanner delete block block get block pool id block get local block block set generation stamp new gs break case pipeline setup append recovery replica info datanode data recover append block new gs min bytes rcvd datanode block scanner null remove block scanner datanode block scanner delete block block get block pool id block get local block block set generation stamp new gs break case transfer rbw case transfer finalized transfer destination replica info datanode data create temporary block break default throw new io exception unsupported stage stage receiving block block addr read checksum meta information checksum data checksum new data checksum bytes per checksum checksum get bytes per checksum checksum size checksum get checksum size boolean create datanode transfer stage block construction stage pipeline setup create streams replica info create streams create bytes per checksum checksum size streams
673	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockSender.java	pooling	package org apache hadoop hdfs server datanode reads block disk sends recipient block sender implements java io closeable fs constants log log data node log log client trace log data node client trace log extended block block block read replica read replica replica the visible length replica replica visible length boolean bit system get property sun arch data model equals input stream block in data stream block in position updated using transfer to data input stream checksum in checksum datastream data checksum checksum checksum stream offset starting position read end offset ending position bytes per checksum chunk size checksum size checksum size boolean corrupt checksum ok need verify checksum boolean chunk offset ok need send chunk offset seqno sequence number packet boolean transfer to allowed true set entire requested byte range sent client boolean sent entire byte range boolean verify checksum true check verified reading data transfer throttler throttler string client trace fmt format client trace log message minimum buffer used sending data clients used transfer to enabled kb large it could larger sure much improvement min buffer with transferto volatile chunk checksum last chunk checksum null block sender extended block block start offset length boolean corrupt checksum ok boolean chunk offset ok boolean verify checksum data node datanode throws io exception block start offset length corrupt checksum ok chunk offset ok verify checksum datanode null block sender extended block block start offset length boolean corrupt checksum ok boolean chunk offset ok boolean verify checksum data node datanode string client trace fmt throws io exception try block block synchronized datanode data replica datanode data get replica block get block pool id block get block id replica null throw new replica not found exception block replica visible length replica get visible length min end offset start offset length write progress chunk checksum chunk checksum null replica instanceof replica being written replica get bytes on disk min end offset try thread sleep catch interrupted exception ie throw new io exception ie current bytes on disk replica get bytes on disk current bytes on disk min end offset throw new io exception string format need bytes bytes available min end offset current bytes on disk replica in pipeline rip replica in pipeline replica chunk checksum rip get last checksum and data len replica get generation stamp block get generation stamp throw new io exception replica get generation stamp block get generation stamp block block replica replica replica visible length throw new io exception the replica readable block block replica replica data node log debug enabled data node log debug block block replica replica chunk offset ok chunk offset ok corrupt checksum ok corrupt checksum ok verify checksum verify checksum transfer to fully fails bit platforms block sizes gb use normal transfer cases transfer to allowed datanode transfer to allowed bit length integer max value client trace fmt client trace fmt corrupt checksum ok datanode data meta file exists block checksum in new data input stream new buffered input stream datanode data get meta data input stream block buffer size read handle common header for version
674	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\BlockVolumeChoosingPolicy.java	unrelated	package org apache hadoop hdfs server datanode block volume choosing policy allows data node specify policy used choosing volume block request block volume choosing policy returns specific fs volume applying suitable choice algorithm place given block given list fs volumes block size sought storage policies maintain state must thread safe fs volume choose volume list fs volume volumes block size throws io exception
675	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ChunkChecksum.java	unrelated	package org apache hadoop hdfs server datanode holder holds checksum bytes length block checksum bytes end ex length checksum bytes bytes checksum applies last chunk bytes chunk checksum data length null available byte checksum chunk checksum data length byte checksum data length data length checksum checksum get data length return data length byte get checksum return checksum
676	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataBlockScanner.java	pooling	package org apache hadoop hdfs server datanode data block scanner manages block scanning block pools for block pool link block pool slice scanner created runs separate thread scan blocks block pool when link bp offer service becomes alive dies block pool scanner map updated data block scanner implements runnable log log log factory get log data block scanner data node datanode fs dataset dataset configuration conf map find block pool scanner given block pool id this updated bp offer service becomes alive dies tree map string block pool slice scanner block pool scanner map new tree map string block pool slice scanner thread block scanner thread null data block scanner data node datanode fs dataset dataset configuration conf datanode datanode dataset dataset conf conf run string current bp id boolean first run true datanode run thread interrupted sleep everytime except first interation first run try thread sleep catch interrupted exception ex interrupt set interrupt status block scanner thread interrupt continue else first run false block pool slice scanner bp scanner get next bp scanner current bp id bp scanner null possible thread interrupted continue current bp id bp scanner get block pool id if bp offer service pool alive process datanode bp service alive current bp id log warn block pool current bp id alive remove case bp service died abruptly without proper shutdown remove block pool current bp id continue bp scanner scan block pool slice wait least one block pool wait for init string bpid upgrade manager datanode um null bpid null bpid equals um datanode get upgrade manager datanode bpid um null um upgrade completed get block pool set size datanode get all bp os length get block pool set size try thread sleep catch interrupted exception e block scanner thread interrupt return find next block pool id scan there one current verification log file find block pool contains current verification log file used starting block pool id if current files found start first block pool block pool set however one current files found one latest modification time used find next block pool id block pool slice scanner get next bp scanner string current bp id string next bp id null next bp id null datanode run block scanner thread interrupted wait for init current bp id synchronized get block pool set size find next bp id finding last modified current log file last scan time iterator string bpid iterator block pool scanner map key set iterator bpid iterator next string bpid bpid iterator next fs dataset fs volume vol dataset volumes get volumes try file curr file block pool slice scanner get current file vol bpid curr file exists last modified curr file last modified last scan time last modified last scan time last modified next bp id bpid catch io exception e log warn received exception e next bp id still null current log found find next bp id sequentially next bp id null equals current bp id next bp id block pool scanner map first key else next bp id block pool scanner map higher key
677	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataNode.java	heartbeat	package org apache hadoop hdfs server datanode data node program stores set blocks dfs deployment a single deployment one many data nodes each data node communicates regularly single name node it also communicates client code data nodes time time data nodes store series named blocks the data node allows client code read blocks write new block data the data node may also response instructions name node delete blocks copy blocks data nodes the data node maintains one critical table block stream bytes block size less this info stored local disk the data node reports table contents name node upon startup every often afterwards data nodes spend lives endless loop asking name node something a name node cannot connect data node directly name node simply returns values functions invoked data node data nodes maintain open server socket client code data nodes read write data the host port server reported name node sends information clients data nodes might interested data node extends configured implements inter datanode protocol client datanode protocol fs constants data node mx bean log log log factory get log data node hdfs configuration init string dn clienttrace format src src ip dest dst ip bytes byte count op operation cli id dfs client id offset offset srv id datanode registration blockid block id duration duration time log client trace log log factory get log data node get name clienttrace use link net utils create socket addr string instead inet socket address create socket addr string target throws io exception return net utils create socket addr target manages bp offer service objects data node creation removal starting stopping shutdown bp offer service objects must done via ap is block pool manager map string bp offer service bp mapping map inet socket address bp offer service name node threads this lock used ensure exclusion refresh namenodes object refresh namenodes lock new object block pool manager configuration conf throws io exception bp mapping new hash map string bp offer service name node threads new hash map inet socket address bp offer service list inet socket address isas dfs util get nn service rpc addresses conf inet socket address isa isas bp offer service bpos new bp offer service isa name node threads put bpos get nn socket address bpos synchronized add block pool bp offer service name node threads get get nn socket address null throw new illegal argument exception unknown bp offer service thread namenode address get nn socket address get block pool id null throw new illegal argument exception null blockpool id bp mapping put get block pool id returns array bp offer service objects caution the bp offer service returned could shutdown time synchronized bp offer service get all namenode threads bp offer service bpos array new bp offer service name node threads values size return name node threads values array bpos array synchronized bp offer service get inet socket address addr return name node threads get addr synchronized bp offer service get string bpid return bp mapping get bpid synchronized remove bp offer service name node threads remove get nn
678	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DatanodeJspHelper.java	pooling	package org apache hadoop hdfs server datanode datanode jsp helper dfs client get dfs client user group information user inet socket address addr configuration conf throws io exception interrupted exception return user as new privileged exception action dfs client dfs client run throws io exception return new dfs client addr conf simple date format ls date format new simple date format yyyy mm dd hh mm get default chunk size get default chunk size configuration conf return conf get int dfs config keys dfs default chunk view size key dfs config keys dfs default chunk view size default generate directory structure jsp writer http servlet request req http servlet response resp configuration conf throws io exception interrupted exception string dir jsp helper validate path string escape utils unescape html req get parameter dir dir null print invalid input return string token string req get parameter jsp helper delegation parameter name user group information ugi jsp helper get ugi req conf string namenode info port str req get parameter namenode info port namenode info port namenode info port str null namenode info port integer parse int namenode info port str string nn addr req get parameter jsp helper namenode address nn addr null print jsp helper namenode address url param null return inet socket address namenode address dfs util get socket address nn addr dfs client dfs get dfs client ugi namenode address conf string target dir hdfs file status target status dfs get file info target target status null exists print file directory target exist jsp helper print goto form namenode info port token string target nn addr else target status dir file list located block blocks dfs get namenode get block locations dir get located blocks located block first block null datanode info locations null blocks size first block blocks get locations first block get locations locations null locations length print empty file else datanode info chosen node jsp helper best node first block string fqdn inet address get by name chosen node get host get canonical host name string datanode addr chosen node get name datanode port integer parse int datanode addr substring datanode addr index of datanode addr length string redirect location http fqdn chosen node get info port browse block jsp block id first block get block get block id block size first block get block get num bytes genstamp first block get block get generation stamp filename url encoder encode dir utf datanode port datanode port namenode info port namenode info port jsp helper get delegation token url param token string jsp helper get url param jsp helper namenode address nn addr resp send redirect redirect location return directory generate table dump info string headings name type size replication block size modification time permission owner group print contents directory jsp helper print path with links dir namenode info port token string nn addr print hr jsp helper print goto form namenode info port token string dir nn addr print hr file f new file dir string parent parent f get parent null print href req get request
679	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataNodeMXBean.java	unrelated	package org apache hadoop hdfs server datanode this jmx management data node information data node mx bean gets version hadoop string get version gets rpc port string get rpc port gets http port string get http port gets namenode ip addresses string get namenode addresses gets information volume datanode please see implementation format returned information string get volume info gets cluster id string get cluster id
680	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataStorage.java	pooling	package org apache hadoop hdfs server datanode data storage information file p data storage extends storage constants string block subdir prefix subdir string block file prefix blk string copy file prefix dncp string storage dir detached detach string storage dir rbw rbw string storage dir finalized finalized string storage dir tmp tmp pattern pre genstamp meta file pattern pattern compile blk meta access variable guarded string storage id flag ensure initialzing storage occurs boolean initilized false block pool storage map block pool id block pool storage map string block pool slice storage bp storage map new hash map string block pool slice storage data storage super node type data node storage id storage info get bp storage string bpid return bp storage map get bpid data storage storage info storage info string strg id super node type data node storage info storage id strg id synchronized string get storage id return storage id synchronized set storage id string new storage id storage id new storage id synchronized create storage id datanode port storage id null storage id empty return storage id data node create new storage id datanode port analyze storage directories recover previous transitions required perform fs state transition necessary depending namespace info read storage info br this method synchronized multiple dn threads only first dn thread dn level storage dir recover transition read synchronized recover transition read data node datanode namespace info ns info collection file data dirs startup option start opt throws io exception initilized dn storage initialized need anything return assert fs constants layout version ns info get layout version data node version fs constants layout version name node layout version ns info get layout version must for data directory calculate state check whether consistent transitioning format recover storage dirs new array list storage directory data dirs size array list storage state data dir states new array list storage state data dirs size iterator file data dirs iterator next file data dir next storage directory sd new storage directory data dir storage state cur state try cur state sd analyze storage start opt sd locked opened switch cur state case normal break case non existent ignore storage log info storage directory data dir exist remove continue case not formatted format log info storage directory data dir formatted log info formatting format sd ns info break default recovery part common sd recover cur state catch io exception ioe sd unlock log warn ignoring storage directory data dir due exception ioe continue good dirs continue add storage list add storage dir sd data dir states add cur state data dirs size data dir states size none data dirs exist throw new io exception all specified directories accessible exist do transitions each storage directory treated individually during startup upgrade rollback others could uptodate regular startup idx idx get num storage dirs idx transition datanode get storage dir idx ns info start opt assert get layout version ns info get layout version data node name node layout versions must make sure storage id set generate new one create storage id datanode get
681	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataXceiver.java	pooling	package org apache hadoop hdfs server datanode thread processing incoming outgoing data stream data xceiver extends receiver implements runnable fs constants log log data node log log client trace log data node client trace log socket boolean local local connection string remote address address remote side string local address local address daemon data node datanode data xceiver server data xceiver server socket keepalive timeout op start time start time receiving op data xceiver socket data node datanode data xceiver server data xceiver server throws io exception super new data input stream new buffered input stream net utils get input stream fs constants small buffer size local get inet address equals get local address datanode datanode data xceiver server data xceiver server data xceiver server child sockets put remote address get remote socket address string local address get local socket address string socket keepalive timeout datanode get conf get int dfs config keys dfs datanode socket reuse keepalive key dfs config keys dfs datanode socket reuse keepalive default log debug enabled log debug number active connections datanode get xceiver count update current thread name contain current status use receiver started thread e outside constructor update current thread name string status string builder sb new string builder sb append data xceiver client append remote address status null sb append append status append thread current thread set name sb string return datanode object data node get data node return datanode read write data data xceiver server run update current thread name waiting operation ops processed op op null try std timeout get so timeout we process requests loop stay around short timeout this optimistic behaviour allows end reuse connections setting keepalive timeout disable behavior try ops processed assert socket keepalive timeout set so timeout socket keepalive timeout op read op catch interrupted io exception ignored time wait client rpc break catch io exception err since optimistically expect next op quite normal get eof ops processed err instanceof eof exception err instanceof closed channel exception log debug enabled log debug cached string closing ops processed ops else throw err break restore normal timeout ops processed set so timeout std timeout make sure xceiver count exceeded cur xceiver count datanode get xceiver count cur xceiver count data xceiver server max xceiver count throw new io exception xceiver count cur xceiver count exceeds limit concurrent xcievers data xceiver server max xceiver count op start time process op op ops processed closed socket keepalive timeout catch throwable log error datanode get machine name data xceiver error processing op null unknown op name operation src remote address dest local address finally log debug enabled log debug datanode get machine name number active connections datanode get xceiver count update current thread name cleaning io utils close stream io utils close socket data xceiver server child sockets remove read block extended block block token block token identifier block token string client name block offset length throws io exception output stream base stream net utils get output stream datanode socket write timeout data output stream new data output stream new buffered output stream
682	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DataXceiverServer.java	unrelated	package org apache hadoop hdfs server datanode server used receiving sending block data this created listen requests clients data nodes this small server use hadoop ipc mechanism data xceiver server implements runnable fs constants log log data node log server socket ss data node datanode record sockets opened data transfer map socket socket child sockets collections synchronized map new hash map socket socket maximal number concurrent xceivers per node enforcing limit required order avoid data node running memory max xceiver count dfs config keys dfs datanode max receiver threads default a manager make sure cluster balancing take much resources it limits number block moves balancing total amount bandwidth use block balance throttler extends data transfer throttler num threads constructor block balance throttler bandwidth super bandwidth log info balancing bandwith bandwidth bytes check block move start return true thread quota exceeded counter incremented false otherwise synchronized boolean acquire num threads balancer max num concurrent moves return false num threads return true mark move completed the thread counter decremented synchronized release num threads block balance throttler balance throttler we need estimate block size check disk partition enough space for set default block size set server side configuration ideal default block size client size configuration a better solution header estimated block size e either actual block size default block size estimate block size data xceiver server server socket ss configuration conf data node datanode ss ss datanode datanode max xceiver count conf get int dfs config keys dfs datanode max receiver threads key dfs config keys dfs datanode max receiver threads default estimate block size conf get long dfs config keys dfs block size key default block size set parameter cluster balancing balance throttler new block balance throttler conf get long dfs config keys dfs datanode balance bandwidthpersec key dfs config keys dfs datanode balance bandwidthpersec default run datanode run try socket ss accept set tcp no delay true data xceiver exciver try exciver new data xceiver datanode catch io exception e io utils close socket throw e new daemon datanode thread group exciver start catch socket timeout exception ignored wake see continue run catch io exception ie log warn datanode get machine name data xceiver server ie catch throwable te log error datanode get machine name data xceiver server exiting due te datanode run false try ss close catch io exception ie log warn datanode get machine name data xceiver server close exception ie kill assert datanode run false shoud run set false killing try ss close catch io exception ie log warn datanode get machine name data xceiver server kill ie close sockets accepted earlier synchronized child sockets iterator socket child sockets values iterator next socket thissock next try thissock close catch io exception e
683	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\DirectoryScanner.java	pooling	package org apache hadoop hdfs server datanode periodically scans data directories block block metadata files reconciles differences block information maintained link fs dataset directory scanner implements runnable log log log factory get log directory scanner data node datanode fs dataset dataset executor service report compile thread pool scheduled executor service master thread scan period msecs volatile boolean run false boolean retain diffs false scan info per block pool diffs new scan info per block pool map string stats stats new hash map string stats allow retaining diffs unit test analysis set retain diffs boolean b retain diffs b stats tracked reporting testing per blockpool stats string bpid total blocks missing meta file missing block file missing memory blocks mismatch blocks stats string bpid bpid bpid string string return block pool bpid total blocks total blocks missing metadata files missing meta file missing block files missing block file missing blocks memory missing memory blocks mismatched blocks mismatch blocks scan info per block pool extends hash map string linked list scan info serial version uid l scan info per block pool super scan info per block pool sz super sz merges scan info per block pool one add all scan info per block pool null return entry string linked list scan info entry entry set string bpid entry get key linked list scan info list entry get value contains key bpid merge per bpid linked list one get bpid add all list else add new bpid linked list put bpid list convert linked list values scan info per block pool map sorted arrays return new map arrays per blockpool map string scan info sorted arrays map string scan info result new hash map string scan info size entry string linked list scan info entry entry set string bpid entry get key linked list scan info list entry get value convert list array scan info record list array new scan info list size sort array based block id arrays sort record result put bpid record return result tracks files information related block disk missing file indicated setting corresponding member null scan info implements comparable scan info block id file meta file file block file fs volume volume scan info block id block id null null null scan info block id file block file file meta file fs volume vol block id block id meta file meta file block file block file volume vol file get meta file return meta file file get block file return block file get block id return block id fs volume get volume return volume compare to scan info b block id b block id return else block id b block id return else return boolean equals object return true instanceof scan info return false return block id scan info block id hash code return block id block id get gen stamp return meta file null block get generation stamp meta file get name generation stamp grandfather generation stamp directory scanner data node dn fs dataset dataset configuration conf datanode dn dataset dataset interval conf get int dfs config keys dfs
684	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FinalizedReplica.java	unrelated	package org apache hadoop hdfs server datanode this describes replica finalized finalized replica extends replica info boolean unlinked copy write done block constructor finalized replica block id len gen stamp fs volume vol file dir super block id len gen stamp vol dir constructor finalized replica block block fs volume vol file dir super block vol dir copy constructor finalized replica finalized replica super unlinked unlinked replica state get state return replica state finalized boolean unlinked return unlinked set unlinked unlinked true get visible length return get num bytes bytes visible get bytes on disk return get num bytes boolean equals object return super equals hash code return super hash code string string return super string n unlinked unlinked
685	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDataset.java	pooling	package org apache hadoop hdfs server datanode fs dataset manages set data blocks each block unique name extent disk fs dataset implements fs constants fs dataset interface a node type built tree reflecting hierarchy blocks local disk fs dir file dir num blocks fs dir children last child idx fs dir file dir throws io exception dir dir children null dir exists dir mkdirs throw new io exception mkdirs failed create dir string else file files file util list files dir list fs dir dir list new array list fs dir idx idx files length idx files idx directory dir list add new fs dir files idx else block block filename files idx num blocks dir list size children dir list array new fs dir dir list size file add block block b file src throws io exception first try without creating subdirectories file file add block b src false false return file null file add block b src true true file add block block b file src boolean create ok boolean reset idx throws io exception num blocks max blocks per dir file dest move block files b src dir num blocks return dest last child idx reset idx reset children checked last child idx dfs util get random next int children length last child idx children null check child tree room block children length idx last child idx children length file file children idx add block b src false reset idx file null last child idx idx return file last child idx create ok return null children null children length children new fs dir max blocks per dir idx idx max blocks per dir idx children idx new fs dir new file dir data storage block subdir prefix idx pick child randomly creating new set subdirs last child idx dfs util get random next int children length return children last child idx add block b src true false get volume map string bpid replicas map volume map fs volume volume throws io exception children null children length children get volume map bpid volume map volume recover temp unlinked block volume add to replicas map bpid volume map dir true recover unlinked tmp files datanode restart if original block exist tmp file renamed original file name otherwise tmp file deleted recover temp unlinked block throws io exception file files file util list files dir file file files fs dataset unlink tmp file file continue file block file get orig file file block file exists if original block file still exists recovery needed file delete throw new io exception unable cleanup unlinked tmp file file else file rename to block file throw new io exception unable cleanup detached file file check data diretory healthy check dir tree throws disk error exception disk checker check dir dir children null children length children check dir tree clear path file f string root dir get absolute path string dir f get absolute path dir starts with root string dir names dir substring root length split file separator subdir clear path f dir names return clear
686	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDatasetAsyncDiskService.java	pooling	package org apache hadoop hdfs server datanode this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion block files fs dataset we want create new thread deletion request want deletions heartbeat thread since deletion slow want use single thread pool inefficient volume async disk service solution this used inside fs dataset in future extract async disk service put common the fs dataset specific logic reside fs dataset async disk service log log log factory get log fs dataset async disk service thread pool core pool size core threads per volume thread pool maximum pool size maximum threads per volume thread pool keep alive time threads core pool size threads keep alive seconds thread group thread group new thread group async disk service hash map file thread pool executor executors new hash map file thread pool executor create async disk services set volumes specified root directories the async disk services uses one thread pool per volume async disk operations fs dataset async disk service file volumes create one thread pool per volume v v volumes length v file vol volumes v thread factory thread factory new thread factory counter thread new thread runnable r index synchronized index counter thread new thread thread group r set name async disk worker index volume vol return thread pool executor executor new thread pool executor core threads per volume maximum threads per volume threads keep alive seconds time unit seconds new linked blocking queue runnable thread factory this reduce number running threads executor allow core thread time out true executors put vol executor execute task sometime future using thread pools synchronized execute file root runnable task executors null throw new runtime exception async disk service already shutdown thread pool executor executor executors get root executor null throw new runtime exception cannot find root root execution task task else executor execute task gracefully shut thread pool will wait deletion tasks finish synchronized shutdown executors null log warn async disk service already shut else log info shutting async disk service threads map entry file thread pool executor e executors entry set e get value shutdown clear executor map calling execute fail executors null log info all async disk service threads shut delete block file meta file disk asynchronously adjust dfs used statistics accordingly delete async fs dataset fs volume volume string bpid file block file file meta file dfs bytes string block name data node log info scheduling block block name file block file deletion replica file delete task deletion task new replica file delete task volume bpid block file meta file dfs bytes block name execute volume get current dir deletion task a task deleting block file associated meta file well decrement dfs usage volume replica file delete task implements runnable fs dataset fs volume volume string block pool id file block file file meta file dfs bytes string block name replica file delete task fs dataset fs volume volume string bpid file block file file meta file dfs bytes string block name volume volume block pool id bpid block file block
687	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\FSDatasetInterface.java	pooling	package org apache hadoop hdfs server datanode this underlying storage stores blocks data node examples fs dataset stores blocks dirs simulated fs dataset simulates data fs dataset interface extends fs dataset m bean returns length metadata file specified block get meta data length extended block b throws io exception this provides input stream length metadata block meta data input stream extends filter input stream meta data input stream input stream stream len super stream length len length get length return length returns meta data block b input stream length meta data input stream get meta data input stream extended block b throws io exception does meta file exist block boolean meta file exists extended block b throws io exception returns specified block disk length excluding metadata get length extended block b throws io exception get reference replica meta info replicas map to called methods synchronized link fs dataset replica get replica string bpid block id string get replica string string bpid block id block get stored block string bpid blkid throws io exception returns input stream read contents specified block input stream get block input stream extended block b throws io exception returns input stream specified offset specified block starting offset input stream get block input stream extended block b seek offset throws io exception returns input stream specified offset specified block the block still tmp directory finalized starting offset block input streams get tmp input streams extended block b blkoff ckoff throws io exception this contains output streams data checksum block block write streams output stream data out output stream checksum out block write streams output stream out output stream c out data out out checksum out c out close throws io exception io utils close stream data out io utils close stream checksum out this contains input streams data checksum block block input streams implements closeable input stream data in input stream checksum in block input streams input stream data in input stream checksum in data in data in checksum in checksum in inherit doc close io utils close stream data in io utils close stream checksum in creates temporary replica returns meta information replica replica in pipeline interface create temporary extended block b throws io exception creates rbw replica returns meta info replica replica in pipeline interface create rbw extended block b throws io exception recovers rbw replica returns meta info replica replica in pipeline interface recover rbw extended block b new gs min bytes rcvd max bytes rcvd throws io exception covert temporary replica rbw replica in pipeline interface convert temporary to rbw extended block temporary throws io exception append finalized replica returns meta info replica replica in pipeline interface append extended block b new gs expected block len throws io exception recover failed append finalized replica returns meta info replica replica in pipeline interface recover append extended block b new gs expected block len throws io exception recover failed pipeline close it bumps replica generation stamp finalize rbw replica recover close extended block b new gs expected block len throws io exception finalizes block previously opened writing
688	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\Replica.java	unrelated	package org apache hadoop hdfs server datanode this represents block replicas stored data node replica get block id get block id get generation stamp get generation stamp get replica state replica state get state get number bytes received get num bytes get number bytes written disk get bytes on disk get number bytes visible readers get visible length
689	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaAlreadyExistsException.java	unrelated	package org apache hadoop hdfs server datanode exception indicating target block already exists set recovered overwritten replica already exists exception extends io exception serial version uid l replica already exists exception super replica already exists exception string msg super msg
690	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaBeingWritten.java	unrelated	package org apache hadoop hdfs server datanode this represents replicas written those replicas created pipeline initiated dfs client replica being written extends replica in pipeline constructor zero length replica replica being written block id gen stamp fs volume vol file dir super block id gen stamp vol dir constructor replica being written block block fs volume vol file dir thread writer super block vol dir writer constructor replica being written block id len gen stamp fs volume vol file dir thread writer super block id len gen stamp vol dir writer copy constructor replica being written replica being written super get visible length return get bytes acked acked bytes visible replica state get state return replica state rbw boolean equals object return super equals hash code return super hash code
691	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInfo.java	unrelated	package org apache hadoop hdfs server datanode this used datanodes maintain meta data replicas it provides general meta information replica replica info extends block implements replica fs volume volume volume replica belongs file dir directory block meta files belong constructor zero length replica replica info block id gen stamp fs volume vol file dir block id l gen stamp vol dir constructor replica info block block fs volume vol file dir block get block id block get num bytes block get generation stamp vol dir constructor replica info block id len gen stamp fs volume vol file dir super block id len gen stamp volume vol dir dir copy constructor replica info replica info get volume get dir get replica meta file name string get meta file name return get block name get generation stamp metadata extension get full path replica data file file get block file return new file get dir get block name get full path replica meta file file get meta file return new file get dir get meta file name get volume replica located disk fs volume get volume return volume set volume replica located disk set volume fs volume vol volume vol return parent directory path replica located file get dir return dir set parent directory replica located set dir file dir dir dir check replica already unlinked need detached false otherwise boolean unlinked return true need unlinked set replica unlinked set unlinked need unlinked copy specified file temporary file then rename temporary file original name this cause hardlinks original file removed the temporary files created directory the temporary files recovered especially windows datanode restart unlink file file file block b throws io exception file tmp file fs dataset create tmp file b fs dataset get unlink tmp file file try file input stream new file input stream file try file output stream new file output stream tmp file try io utils copy bytes finally close finally close file length tmp file length throw new io exception copy file file size file length file tmp file resulted size tmp file length file util replace file tmp file file catch io exception e boolean done tmp file delete done data node log info detach file failed delete temporary file tmp file throw e remove hard link copying block temporary place moving back false already detached need detached boolean unlink block num links throws io exception unlinked return false file file get block file file null get volume null throw new io exception detach block block found file meta get meta file meta null throw new io exception meta file found block hard link get link count file num links data node log info copy on write block unlink file file hard link get link count meta num links unlink file meta set unlinked return true set replica generation stamp newer one set newer generation stamp new gs throws io exception cur gs get generation stamp new gs cur gs throw new io exception new generation stamp new gs must greater current one cur gs set generation stamp new
692	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInPipeline.java	unrelated	package org apache hadoop hdfs server datanode this defines replica pipeline includes persistent replica written dfs client temporary replica replicated source datanode copied balancing purpose the base implements temporary replica replica in pipeline extends replica info implements replica in pipeline interface bytes acked bytes on disk byte last checksum thread writer constructor zero length replica replica in pipeline block id gen stamp fs volume vol file dir block id l gen stamp vol dir thread current thread constructor replica in pipeline block block fs volume vol file dir thread writer block get block id block get num bytes block get generation stamp vol dir writer constructor replica in pipeline block id len gen stamp fs volume vol file dir thread writer super block id len gen stamp vol dir bytes acked len bytes on disk len writer writer copy constructor replica in pipeline replica in pipeline super bytes acked get bytes acked bytes on disk get bytes on disk writer writer get visible length return replica state get state return replica state temporary get bytes acked return bytes acked set bytes acked bytes acked bytes acked bytes acked get bytes on disk return bytes on disk synchronized set last checksum and data len data length byte last checksum bytes on disk data length last checksum last checksum synchronized chunk checksum get last checksum and data len return new chunk checksum get bytes on disk last checksum set thread writing replica set writer thread writer writer writer boolean equals object return super equals interrupt writing thread wait dies stop writer throws io exception writer null writer thread current thread writer alive writer interrupt try writer join catch interrupted exception e throw new io exception waiting writer thread interrupted hash code return super hash code block write streams create streams boolean create bytes per chunk checksum size throws io exception file block file get block file file meta file get meta file data node log debug enabled data node log debug write to blockfile block file size block file length data node log debug write to metafile meta file size meta file length block disk size l crc disk size l create check disk file block disk size bytes on disk crc disk size block metadata header get header size block disk size bytes per chunk bytes per chunk checksum size block disk size block disk size block file length crc disk size meta file length throw new io exception corrupted block file output stream block out null file output stream crc out null try block out new file output stream new random access file block file rw get fd crc out new file output stream new random access file meta file rw get fd create block out get channel position block disk size crc out get channel position crc disk size return new block write streams block out crc out catch io exception e io utils close stream block out io utils close stream crc out throw e string string return super string n bytes acked bytes acked n bytes on disk bytes
693	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaInPipelineInterface.java	unrelated	package org apache hadoop hdfs server datanode this defines replica pipeline written replica in pipeline interface extends replica set number bytes received set num bytes bytes received get number bytes acked get bytes acked set number bytes acked set bytes acked bytes acked store checksum last chunk along data length set last checksum and data len data length byte last checksum gets last chunk checksum length block corresponding checksum chunk checksum get last checksum and data len create output streams writing replica one block file one crc file block write streams create streams boolean create bytes per chunk checksum size throws io exception
694	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaNotFoundException.java	unrelated	package org apache hadoop hdfs server datanode exception indicating data node replica matches target block replica not found exception extends io exception serial version uid l string non rbw replica cannot recover non rbw replica string unfinalized replica cannot append unfinalized replica string unfinalized and nonrbw replica cannot recover append close replica finalized rbw string non existent replica cannot append non existent replica string unexpected gs replica cannot append replica unexpeted generation stamp replica not found exception super replica not found exception extended block b super replica found b replica not found exception string msg super msg
695	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicasMap.java	pooling	package org apache hadoop hdfs server datanode maintains replicas map replicas map object using synchronized object mutex map block pool id another map block id replica info map string map long replica info map new hash map string map long replica info replicas map object mutex mutex null throw new hadoop illegal argument exception object synchronize cannot null mutex mutex string get block pool list synchronized mutex return map key set array new string map key set size check block pool string bpid bpid null throw new illegal argument exception block pool id null check block block b b null throw new illegal argument exception block null get meta information replica matches block id generation stamp replica info get string bpid block block check block pool bpid check block block replica info replica info get bpid block get block id replica info null block get generation stamp replica info get generation stamp return replica info return null get meta information replica matches block id replica info get string bpid block id check block pool bpid synchronized mutex map long replica info map get bpid return null get block id null add replica meta information map replica info add string bpid replica info replica info check block pool bpid check block replica info synchronized mutex map long replica info map get bpid null add entry block pool exist already new hash map long replica info map put bpid return put replica info get block id replica info remove replica meta information map matches input block id generation stamp replica info remove string bpid block block check block pool bpid check block block synchronized mutex map long replica info map get bpid null long key long value of block get block id replica info replica info get key replica info null block get generation stamp replica info get generation stamp return remove key return null remove replica meta information map present replica info remove string bpid block id check block pool bpid synchronized mutex map long replica info map get bpid null return remove block id return null get size map given block pool size string bpid map long replica info null synchronized mutex map get bpid return null size get collection replicas given block pool this method b synchronized b it needs synchronized externally using mutex getting replicas values map iterating mutex accessed using link get mutext method collection replica info replicas string bpid map long replica info null map get bpid return null values null init block pool string bpid check block pool bpid synchronized mutex map long replica info map get bpid null add entry block pool exist already new hash map long replica info map put bpid clean up block pool string bpid check block pool bpid synchronized mutex map remove bpid give access mutex used synchronizing replicas map object get mutext return mutex
696	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaUnderRecovery.java	unrelated	package org apache hadoop hdfs server datanode this represents replicas block recovery it recovery id equal generation stamp replica bumped recovery the recovery id used handle multiple concurrent block recoveries a recovery higher recovery id preempts recoveries lower id replica under recovery extends replica info replica info original original replica needs recovered recovery id recovery id also generation stamp replica bumped recovery replica under recovery replica info replica recovery id super replica get block id replica get num bytes replica get generation stamp replica get volume replica get dir replica get state replica state finalized replica get state replica state rbw replica get state replica state rwr throw new illegal argument exception cannot recover replica replica original replica recovery id recovery id copy constructor replica under recovery replica under recovery super original get original replica recovery id get recovery id get recovery id get recovery id return recovery id set recovery id set recovery id recovery id recovery id recovery id recovery id recovery id else throw new illegal argument exception the new rcovery id recovery id must greater current one recovery id get original replica recovery replica info get original replica return original get original replica state replica state get orignal replica state return original get state boolean unlinked return original unlinked set unlinked original set unlinked replica state get state return replica state rur get visible length return original get visible length get bytes on disk return original get bytes on disk set block id block id super set block id block id original set block id block id set generation stamp gs super set generation stamp gs original set generation stamp gs set num bytes num bytes super set num bytes num bytes original set num bytes num bytes set dir file dir super set dir dir original set dir dir set volume fs volume vol super set volume vol original set volume vol boolean equals object return super equals hash code return super hash code string string return super string n recovery id recovery id n original original replica recovery info create info return new replica recovery info original get block id original get bytes on disk original get generation stamp get orignal replica state
697	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\ReplicaWaitingToBeRecovered.java	unrelated	package org apache hadoop hdfs server datanode this represents replica waiting recovered after datanode restart replica rbw directory loaded replica waiting recovered a replica waiting recovered provision read participates pipeline recovery it become outdated client continues write recovered result lease recovery replica waiting to be recovered extends replica info boolean unlinked copy write done block constructor replica waiting to be recovered block id len gen stamp fs volume vol file dir super block id len gen stamp vol dir constructor replica waiting to be recovered block block fs volume vol file dir super block vol dir copy constructor replica waiting to be recovered replica waiting to be recovered super unlinked unlinked replica state get state return replica state rwr boolean unlinked return unlinked set unlinked unlinked true get visible length return bytes visible get bytes on disk return get num bytes boolean equals object return super equals hash code return super hash code string string return super string n unlinked unlinked
698	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\RoundRobinVolumesPolicy.java	unrelated	package org apache hadoop hdfs server datanode round robin volumes policy implements block volume choosing policy cur volume synchronized fs volume choose volume list fs volume volumes block size throws io exception volumes size throw new disk out of space exception no available volumes since volumes could removed failure make sure bounds cur volume volumes size cur volume start volume cur volume max available true fs volume volume volumes get cur volume cur volume cur volume volumes size available volume size volume get available available volume size block size return volume available volume size max available max available available volume size cur volume start volume throw new disk out of space exception insufficient space additional block volume available space max available bytes free configured block size block size
699	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\SecureDataNodeStarter.java	authenticate	package org apache hadoop hdfs server datanode utility start datanode secure cluster first obtaining privileged resources main startup handing datanode secure data node starter implements daemon stash necessary resources needed datanode operation secure env secure resources server socket streaming socket select channel connector listener secure resources server socket streaming socket select channel connector listener streaming socket streaming socket listener listener server socket get streaming socket return streaming socket select channel connector get listener return listener string args secure resources resources init daemon context context throws exception system err println initializing secure datanode resources we start secure datanode kerberos secured cluster configuration conf new configuration skip ugi method log conf get hadoop security authentication equals kerberos throw new runtime exception cannot start secure datanode unsecure cluster stash command line arguments regular datanode args context get arguments obtain secure port data streaming datanode inet socket address soc addr data node get streaming addr conf socket write timeout conf get int dfs config keys dfs datanode socket write timeout key hdfs constants write timeout server socket ss socket write timeout server socket channel open socket new server socket ss bind soc addr check got port need ss get local port soc addr get port throw new runtime exception unable bind specified streaming port secure context needed soc addr get port got ss get local port obtain secure listener web server select channel connector listener select channel connector http server create default channel connector inet socket address info soc addr data node get info addr conf listener set host info soc addr get host name listener set port info soc addr get port open listener order bind port root listener open listener get port info soc addr get port throw new runtime exception unable bind specified info port secure context needed soc addr get port got ss get local port system err println successfully obtained privileged resources streaming port ss http listener port listener get connection ss get local port listener get port throw new runtime exception cannot start secure datanode unprivileged ports resources new secure resources ss listener start throws exception system err println starting regular datanode initialization data node secure main args resources
700	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\UpgradeManagerDatanode.java	unrelated	package org apache hadoop hdfs server datanode upgrade manager data nodes distributed upgrades data node performed separate thread the upgrade starts data node receives start upgrade command namenode at point manager finds respective upgrade object starts daemon order perform upgrade defined object upgrade manager datanode extends upgrade manager data node data node null daemon upgrade daemon null string bpid null upgrade manager datanode data node data node string bpid super data node data node bpid bpid hdfs constants node type get type return hdfs constants node type data node synchronized initialize upgrade namespace info ns info throws io exception super initialize upgrade return distr upgrade needed data node log info n distributed upgrade data node data node get machine name version get upgrade version current lv fs constants layout version initialized upgrade object datanode cur uo upgrade object datanode current upgrades first cur uo set datanode data node bpid upgrade state cur uo pre upgrade action ns info upgrade state true data node start upgrade start distributed upgrade instantiates distributed upgrade objects synchronized boolean start upgrade throws io exception upgrade state upgrade already progress assert current upgrades null upgrade manager datanode current upgrades null upgrade object datanode cur uo upgrade object datanode current upgrades first cur uo start upgrade return true broadcast command null broadcast command get version get upgrade version stop broadcasting cluster moved start upgrade next version broadcast command null else upgrade finished data node cluster still running reply broadcast command assert current upgrades null upgrade manager datanode current upgrades null assert upgrade daemon null upgrade manager datanode upgrade daemon null datanode protocol nn data node get bp namenode bpid nn process upgrade command broadcast command return true current upgrades null current upgrades get distributed upgrades current upgrades null data node log info n distributed upgrade data node version get upgrade version current lv fs constants layout version cannot started the upgrade object defined return false upgrade state true upgrade object datanode cur uo upgrade object datanode current upgrades first cur uo set datanode data node bpid cur uo start upgrade upgrade daemon new daemon cur uo upgrade daemon start data node log info n distributed upgrade data node data node get machine name version get upgrade version current lv fs constants layout version started return true synchronized process upgrade command upgrade command command throws io exception assert command get action upgrade command uc action start upgrade only start upgrade action processed time upgrade version command get version start distributed upgrade start upgrade upgrade started return throw new io exception distributed upgrade data node data node get machine name version get upgrade version current lv fs constants layout version cannot started the upgrade object defined synchronized complete upgrade throws io exception assert current upgrades null upgrade manager datanode current upgrades null upgrade object datanode cur uo upgrade object datanode current upgrades first broadcast command cur uo complete upgrade upgrade state false current upgrades null upgrade daemon null data node log info n distributed upgrade data node data node get machine name version get upgrade version current lv fs constants layout
701	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\UpgradeObjectDatanode.java	pooling	package org apache hadoop hdfs server datanode base data node upgrade objects data node upgrades run separate threads upgrade object datanode extends upgrade object implements runnable data node data node null string bpid null hdfs constants node type get type return hdfs constants node type data node protected data node get datanode return data node protected datanode protocol get namenode throws io exception return data node get bp namenode bpid set datanode data node data node string bpid data node data node bpid bpid specifies upgrade performed upgrade throws io exception specifies upgrade started the default implementation checks whether data node missed upgrade throws exception this leads data node shutdown data nodes usually start distributed upgrade name node replies heartbeat start upgrade command sometimes though e g data node missed upgrade wants catchup rest cluster necessary initiate upgrade directly data node since name node might ever start an method return true and upgrade start data ndoe registration sending first heartbeat object talk name node version necessary false wait name node starts upgrade boolean pre upgrade action namespace info ns info throws io exception ns upgrade version ns info get distributed upgrade version ns upgrade version get version return false name node perform upgrade missed upgrade report problem name node throw exception string error msg n data node missed distributed upgrade shutdown n get description name node version ns info get layout version data node log fatal error msg string bpid ns info get block pool id datanode protocol nn data node get bp namenode bpid try nn error report data node get dn registration for bp bpid datanode protocol notify error msg catch socket timeout exception e namenode busy data node log info problem connecting server data node get name node addr ns info get block pool id throw new io exception error msg run assert data node null upgrade object datanode data node null data node run try upgrade catch exception e data node log error exception upgrade e break report results get upgrade status data node log info n distributed upgrade data node version get version current lv fs constants layout version cannot completed complete upgrade calling manager method try upgrade manager datanode upgrade manager data node get upgrade manager datanode bpid upgrade manager null upgrade manager complete upgrade catch io exception e data node log error exception complete upgrade e complete upgrade return status complete command broadcasting data nodes finish upgrade different times the data node needs confirm name node upgrade complete nodes still upgrading upgrade command complete upgrade throws io exception return new upgrade command upgrade command uc action report status get version short
702	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\metrics\DataNodeMetrics.java	heartbeat	package org apache hadoop hdfs server datanode metrics this maintaining various data node statistics publishing metrics interfaces this also registers jmx m bean rpc p this number metrics variables publicly accessible variables objects methods update values example p link blocks read inc data node metrics metrics registry registry new metrics registry datanode string name data node metrics string name string session id name name registry tag session id session id data node metrics create configuration conf string dn name string session id conf get dfs config keys dfs metrics session id key metrics system ms default metrics system instance jvm metrics create data node session id ms string name data node activity dn name empty undefined data node name dfs util get random next int dn name replace return ms register name null new data node metrics name session id string name return name add heartbeat latency heartbeats add latency add block report latency block reports add latency incr blocks replicated delta blocks replicated incr delta incr blocks written blocks written incr incr blocks removed delta blocks removed incr delta incr bytes written delta bytes written incr delta incr block verification failures block verification failures incr incr blocks verified blocks verified incr add read block op latency read block op add latency add write block op latency write block op add latency add replace block op latency replace block op add latency add copy block op latency copy block op add latency add block checksum op latency block checksum op add latency incr bytes read delta bytes read incr delta incr blocks read blocks read incr shutdown default metrics system shutdown incr writes from client boolean local local writes from local client writes from remote client incr incr reads from client boolean local local reads from local client reads from remote client incr incr volume failures volume failures incr
703	hdfs\src\java\org\apache\hadoop\hdfs\server\datanode\metrics\FSDatasetMBean.java	pooling	package org apache hadoop hdfs server datanode metrics this interface defines methods get status fs dataset data node it also used publishing via jmx hence follow jmx naming convention note used metrics dynamic m bean base implement fs dataset m bean stable published p data node runtime statistic info report another m bean fs dataset m bean returns total space bytes used block pool get block pool used string bpid throws io exception returns total space bytes used dfs datanode get dfs used throws io exception returns total capacity bytes storage used unused get capacity throws io exception returns amount free storage space bytes get remaining throws io exception returns storage id underlying storage string get storage info returns number failed volumes datanode get num failed volumes
704	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\BackupImage.java	pooling	package org apache hadoop hdfs server namenode extension fs image backup node this handles setup journaling spool backup namenode backup image extends fs image names journal spool directory spool file string storage jspool dir jspool string storage jspool file nn storage name node file edits new get name backup input stream loading edits memory edit log backup input stream backup input stream is journal spooling progress volatile j spool state js state enum j spool state off inprogress wait backup image super storage set disable pre upgradable layout check true js state j spool state off analyze backup storage directories consistency br recover incomplete checkpoints required br read version fstime files exist br do load image edits recover create read collection uri image dirs collection uri edits dirs throws io exception storage set storage directories image dirs edits dirs storage set checkpoint time l iterator storage directory storage dir iterator next storage directory sd next storage state cur state try cur state sd analyze storage hdfs constants startup option regular storage sd locked opened switch cur state case non existent fail configured storage dirs inaccessible throw new inconsistent fs state exception sd get root checkpoint directory exist accessible case not formatted backup node directories may unformatted initially log info storage directory sd get root formatted log info formatting sd clear directory create empty current break case normal break default recovery possible sd recover cur state cur state storage state not formatted read verify consistency directories storage read properties sd catch io exception ioe sd unlock throw ioe reset storage directories p unlock storage rename code current code code lastcheckpoint tmp code recreate empty code current code synchronized reset throws io exception reset name space tree fs directory fs dir get fs namesystem dir fs dir reset unlock close rename storage directories storage unlock all recover unsuccessful checkpoint necessary recover create read storage get image directories storage get edits directories rename recreate iterator storage directory storage dir iterator next storage directory sd next rename current lastcheckpoint tmp storage move current sd load checkpoint local files memory state empty br set new checkpoint time received name node br move code lastcheckpoint tmp code code previous checkpoint code load checkpoint checkpoint signature sig throws io exception load current image journal memory already edit log open edit log open fs directory fs dir get fs namesystem dir fs dir empty iterator storage directory image storage dir iterator name node dir type image iterator storage directory edits storage dir iterator name node dir type edits image next edits next throw new io exception could locate checkpoint directories storage directory sd name image next storage directory sd edits edits next get fs directory root lock write lock try load image root dir lock load fs image nn storage get storage file sd name name node file image finally get fs directory root lock write unlock load fs edits sd edits set storage fields storage set storage info sig storage set image digest sig image digest storage set checkpoint time sig checkpoint time save meta data fsimage files create
705	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\BackupNode.java	unrelated	package org apache hadoop hdfs server namenode backup node p backup node play two roles ol li link namenode role checkpoint node periodically creates checkpoints downloads image edits active node merges uploads new image back active li li link namenode role backup node keeps namespace sync active node periodically creates checkpoints simply saving namespace image local disk li ol backup node extends name node string bn address name key dfs config keys dfs namenode backup address key string bn address default dfs config keys dfs namenode backup address default string bn http address name key dfs config keys dfs namenode backup http address key string bn http address default dfs config keys dfs namenode backup http address default string bn service rpc address key dfs config keys dfs namenode backup service rpc address key name node proxy namenode protocol namenode name node rpc address string nn rpc address name node http address string nn http address checkpoint manager checkpointer checkpoint manager cluster id backup node belongs string cluster id block pool id peer namenode backup node string block pool id backup node configuration conf namenode role role throws io exception super conf role common name node methods implementation backup node protected inet socket address get rpc server address configuration conf throws io exception string addr conf get bn address name key bn address default return net utils create socket addr addr protected inet socket address get service rpc server address configuration conf throws io exception string addr conf get bn service rpc address key addr null addr empty return null return net utils create socket addr addr protected set rpc server address configuration conf conf set bn address name key get host port string rpc address protected set rpc service server address configuration conf conf set bn service rpc address key get host port string service rpc address protected inet socket address get http server address configuration conf assert rpc address null rpc address calculated first string addr conf get bn http address name key bn http address default return net utils create socket addr addr protected set http server address configuration conf conf set bn http address name key get host port string get http address protected load namesystem configuration conf throws io exception backup image bn image new backup image namesystem new fs namesystem conf bn image bn image recover create read fs namesystem get namespace dirs conf fs namesystem get namespace edits dirs conf protected initialize configuration conf throws io exception trash disabled backup name node turned back ever becomes active conf set long common configuration keys fs trash interval key common configuration keys fs trash interval default namespace info ns info handshake conf super initialize conf backup node never lease recovery therefore lease hard limit never expire namesystem lease manager set lease period fs constants lease softlimit period long max value cluster id ns info get cluster id block pool id ns info get block pool id register active name node register with ns info checkpoint daemon start rpc server started run checkpoint daemon conf stop checkpoint manager
706	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\CancelDelegationTokenServlet.java	authenticate	package org apache hadoop hdfs server namenode cancel delegation tokens http use hftp cancel delegation token servlet extends dfs servlet log log log factory get log cancel delegation token servlet string path spec cancel delegation token string token token protected get http servlet request req http servlet response resp throws servlet exception io exception user group information ugi servlet context context get servlet context configuration conf name node http server get conf from context context try ugi get ugi req conf catch io exception ioe log info request token received authentication req get remote addr ioe resp send error http servlet response sc forbidden unable identify authenticate user return name node nn name node http server get name node from context context string token string req get parameter token token string null resp send error http servlet response sc multiple choices token renew specified token delegation token identifier token new token delegation token identifier token decode from url string token string try ugi as new privileged exception action void void run throws exception nn cancel delegation token token return null catch exception e log info exception cancelling token re throwing e resp send error http servlet response sc internal server error e get message
707	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\Checkpointer.java	unrelated	package org apache hadoop hdfs server namenode the checkpointer responsible supporting periodic checkpoints hdfs metadata the checkpointer daemon periodically wakes determined schedule specified configuration triggers periodic checkpoint goes back sleep the start checkpoint triggered one two factors time size edits file checkpointer extends daemon log log log factory get log checkpointer get name backup node backup node volatile boolean run checkpoint period seconds checkpoint size size mb current edit log string info bind address backup image get fs image return backup image backup node get fs image namenode protocol get namenode return backup node namenode create connection primary namenode checkpointer configuration conf backup node bn node throws io exception backup node bn node try initialize conf catch io exception e shutdown throw e initialize checkpoint initialize configuration conf throws io exception create connection namenode run true initialize scheduling parameters configuration checkpoint period conf get long dfs config keys dfs namenode checkpoint period key dfs config keys dfs namenode checkpoint period default checkpoint size conf get long dfs config keys dfs namenode checkpoint size key dfs config keys dfs namenode checkpoint size default pull exact http address posting url avoid ip aliasing issues string full info addr conf get dfs namenode backup http address key dfs namenode backup http address default info bind address full info addr substring full info addr index of log info checkpoint period checkpoint period secs checkpoint period min log info log size trigger checkpoint size bytes checkpoint size kb shut checkpointer shutdown run false backup node stop the main work loop run check size edit log every minutes period m sec minutes checkpoint period period m sec period m sec checkpoint period period m sec last checkpoint time backup node checkpoint at startup last checkpoint time run try boolean checkpoint false last checkpoint time period m sec checkpoint true else size get journal size size checkpoint size checkpoint true checkpoint checkpoint last checkpoint time catch io exception e log error exception checkpoint e catch throwable e log error throwable exception checkpoint e shutdown break try thread sleep period m sec catch interrupted exception ie nothing get journal size throws io exception if backup node loaded get edits size local file active backup node role namenode role backup get fs image get edit log open return backup node journal size go active node size return get namenode journal size backup node get registration download code fsimage code code edits code files remote name node download checkpoint checkpoint signature sig throws io exception retrieve image file string fileid getimage collection file list get fs image get storage get files name node file image name node dir type image file files list array new file list size assert files length no checkpoint targets string nn http addr backup node nn http address transfer fs image get file client nn http addr fileid files false log info downloaded file files get name size files length bytes retrieve edits file fileid getedit list get fs image get storage get files name node file edits name node dir type edits files list array new
708	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\CheckpointSignature.java	unrelated	package org apache hadoop hdfs server namenode a unique signature intended identify checkpoint transactions checkpoint signature extends storage info implements writable comparable checkpoint signature string field separator edits time l checkpoint time l md hash image digest null string blockpool id checkpoint signature checkpoint signature fs image fs image super fs image get storage blockpool id fs image get block pool id edits time fs image get edit log get fs edit time checkpoint time fs image get storage get checkpoint time image digest fs image get storage get image digest checkpoint time fs image get storage get checkpoint time checkpoint signature string str string fields str split field separator assert fields length must fields checkpoint signature layout version integer value of fields namespace id integer value of fields c time long value of fields edits time long value of fields checkpoint time long value of fields image digest new md hash fields cluster id fields blockpool id fields get md image digest md hash get image digest return image digest get cluster id checkpoint signature string get cluster id return cluster id get block pool id checkpoint signature string get blockpool id return blockpool id set block pool id checkpoint signature set blockpool id string blockpool id blockpool id blockpool id string string return string value of layout version field separator string value of namespace id field separator string value of c time field separator string value of edits time field separator string value of checkpoint time field separator image digest string field separator cluster id field separator blockpool id validate storage info fs image si throws io exception layout version si get layout version namespace id si get namespace id c time si get storage c time checkpoint time si get storage get checkpoint time image digest equals si get storage image digest cluster id equals si get cluster id blockpool id equals si get block pool id checkpoint time change image saved compare throw new io exception inconsistent checkpoint fields n lv layout version namespace id namespace id c time c time checkpoint time checkpoint time image digest image digest cluster id cluster id blockpool id blockpool id n expecting respectively si get layout version si get namespace id si get storage c time si get storage get checkpoint time si get storage image digest si get cluster id si get block pool id comparable compare to checkpoint signature return layout version layout version layout version layout version namespace id namespace id namespace id namespace id c time c time c time c time edits time edits time edits time edits time checkpoint time checkpoint time checkpoint time checkpoint time cluster id compare to cluster id cluster id compare to cluster id blockpool id compare to blockpool id blockpool id compare to blockpool id image digest compare to image digest boolean equals object instanceof checkpoint signature return false return compare to checkpoint signature hash code return layout version namespace id c time edits time checkpoint time image digest hash code cluster id hash code blockpool id hash code writable
709	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ClusterJspHelper.java	pooling	package org apache hadoop hdfs server namenode this generates data needed displayed cluster web console cluster jsp helper log log log factory get log cluster jsp helper string overall status overall status string dead dead string jmx qry jmx qry hadoop service name node name name node info jsp helper function generates cluster health report when encountering exception getting namenode status exception listed page corresponding stack trace cluster status generate cluster health report cluster status cs new cluster status configuration conf new configuration list inet socket address isas null try isas dfs util get nn service rpc addresses conf catch exception e could build cluster status cs set error e return cs process namenode add cluster status inet socket address isa isas namenode mx bean helper nn helper null try nn helper new namenode mx bean helper isa conf string mbean props query mbean nn helper http address conf namenode status nn nn helper get namenode status mbean props cs clusterid empty cs clusterid equals set clusterid cs clusterid nn helper get cluster id mbean props cs add namenode status nn catch exception e track exceptions encountered connecting namenodes cs add exception isa get host name e continue return cs helper function generates decommissioning report connect namenode http via jmx json servlet collect data nodes status decommission status generate decommissioning report string clusterid configuration conf new configuration list inet socket address isas null try isas dfs util get nn service rpc addresses conf catch exception e catch exception encountered connecting namenodes decommission status info new decommission status clusterid e return info outer map key datanode inner map key namenode value decom status datanode corresponding namenode map string map string string status map new hash map string map string string map exceptions encountered connecting namenode key namenode value exception map string exception decommission exceptions new hash map string exception list string unreported namenode new array list string inet socket address isa isas namenode mx bean helper nn helper null try nn helper new namenode mx bean helper isa conf string mbean props query mbean nn helper http address conf clusterid equals clusterid nn helper get cluster id mbean props nn helper get decom node info for report status map mbean props catch exception e catch exceptions encountered connecting namenodes string nn host isa get host name decommission exceptions put nn host e unreported namenode add nn host continue update unknown status status map unreported namenode get decommission node cluster state status map return new decommission status status map clusterid get datanode http port conf decommission exceptions based state datanode namenode marks overall state datanode across namenodes one following ol li link decommission states decommissioned li li link decommission states decommission inprogress li li link decommission states partially decommissioned li li link decommission states unknown li ol map whose key datanode value inner map key namenode value decommission state get decommission node cluster state map string map string string status map status map null status map empty return for datanodes iterator entry string map string string status map entry set iterator next map entry
710	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ContentSummaryServlet.java	unrelated	package org apache hadoop hdfs server namenode servlets file checksum content summary servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request http servlet response response throws servlet exception io exception configuration conf configuration get servlet context get attribute jsp helper current conf user group information ugi get ugi request conf try ugi as new privileged exception action void void run throws exception string path request get path info print writer response get writer xml outputter xml new xml outputter utf xml declaration try get content summary client protocol nnproxy create name node proxy content summary cs nnproxy get content summary path write xml xml start tag content summary get name cs null xml attribute length cs get length xml attribute file count cs get file count xml attribute directory count cs get directory count xml attribute quota cs get quota xml attribute space consumed cs get space consumed xml attribute space quota cs get space quota xml end tag catch io exception ioe write xml ioe path xml xml end document return null catch interrupted exception e throw new io exception e
711	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\DfsServlet.java	unrelated	package org apache hadoop hdfs server namenode a base servlets dfs dfs servlet extends http servlet for java io serializable serial version uid l log log log factory get log dfs servlet get canonical name write object xml format protected write xml exception except string path xml outputter doc throws io exception doc start tag remote exception get simple name doc attribute path path except instanceof remote exception doc attribute remote exception except get class name else doc attribute except get class get name string msg except get localized message msg index of n msg msg substring doc attribute message msg substring msg index of trim doc end tag create link name node proxy current link servlet context protected client protocol create name node proxy throws io exception servlet context context get servlet context running name node use directly rather via rpc name node nn name node http server get name node from context context nn null return nn inet socket address nn addr name node http server get name node address from context context configuration conf new hdfs configuration name node http server get conf from context context return dfs util create namenode nn addr conf create uri redirecting request datanode protected uri create redirect uri string servletpath user group information ugi datanode id host http servlet request request name node nn throws io exception uri syntax exception string hostname host instanceof datanode info datanode info host get host name host get host string scheme request get scheme port https equals scheme integer get servlet context get attribute datanode https port host get info port string filename request get path info string builder params new string builder params append filename params append filename user group information security enabled string token string ugi get tokens iterator next encode to url string params append jsp helper get delegation token url param token string else params append ugi params append ugi get short user name add namenode address url params string nn addr name node get host port string nn get name node address params append jsp helper get url param jsp helper namenode address nn addr return new uri scheme null hostname port servletpath params string null get filename request protected string get filename http servlet request request http servlet response response throws io exception string filename request get parameter filename filename null filename length throw new io exception invalid filename return filename protected user group information get ugi http servlet request request configuration conf throws io exception return jsp helper get ugi get servlet context request conf
712	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogBackupInputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log input stream used updates hdfs meta data state backup node org apache hadoop hdfs server protocol namenode registration byte edit log backup input stream extends edit log input stream string address sender address byte buffer input stream inner data input stream a byte array input stream lets modify underlying byte array byte buffer input stream extends byte array input stream byte buffer input stream super new byte byte get data return super buf set data byte new bytes super buf new bytes super count new bytes null new bytes length super mark reset number bytes read stream far length return count edit log backup input stream string name throws io exception address name inner new byte buffer input stream new data input stream inner string get name return address journal type get type return journal type backup available throws io exception return available read throws io exception return read read byte b len throws io exception return read b len close throws io exception close length throws io exception file size size buffers return inner length data input stream get data input stream return set bytes byte new bytes throws io exception inner set data new bytes reset clear throws io exception set bytes null
713	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogBackupOutputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log output stream streams edits backup node org apache hadoop hdfs server protocol namenode registration byte edit log backup output stream extends edit log output stream default buffer size namenode protocol backup node rpc proxy backup node namenode registration bn registration backup node registration namenode registration nn registration active node registration edits double buffer buf data output buffer serialized output sent backup node edit log backup output stream namenode registration bn reg backup node namenode registration nn reg active name node throws io exception super bn registration bn reg nn registration nn reg inet socket address bn address net utils create socket addr bn registration get address storage log info edit log backup output stream connects bn address try backup node namenode protocol rpc get proxy namenode protocol namenode protocol version id bn address new hdfs configuration catch io exception e storage log error error connecting bn address e throw e buf new edits double buffer default buffer size new data output buffer default buffer size string get name return bn registration get address journal type get type return journal type backup write fs edit log op op throws io exception buf write op op write raw byte bytes offset length throws io exception throw new io exception not supported there persistent storage just clear buffers create throws io exception assert buf flushed previous data flushed yet buf new edits double buffer default buffer size close throws io exception close called pending transactions flushed synced size buf count buffered bytes size throw new io exception backup edit stream size records still flushed cannot closed rpc stop proxy backup node stop rpc threads buf close buf null set ready to flush throws io exception buf set ready to flush protected flush and sync throws io exception xxx code work trunk redone hdfs simpler buf flush to size send namenode protocol ja journal there persistent storage therefore length p length used check large enough start checkpoint this criteria used backup streams length throws io exception return send ja throws io exception try length get length write fs edit log op codes op invalid get op code backup node journal nn registration ja length get data finally reset get backup node registration namenode registration get registration return bn registration verify backup node alive boolean alive try send namenode protocol ja is alive catch io exception ei storage log info bn registration get role bn registration get address alive ei return false return true
714	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogFileInputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log input stream reads edits local file edit log file input stream extends edit log input stream file file file input stream f stream edit log file input stream file name throws io exception file name f stream new file input stream name string get name return file get path journal type get type return journal type file available throws io exception return f stream available read throws io exception return f stream read read byte b len throws io exception return f stream read b len close throws io exception f stream close length throws io exception file size size buffers return file length
715	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogFileOutputStream.java	unrelated	package org apache hadoop hdfs server namenode an implementation link edit log output stream stores edits local file edit log file output stream extends edit log output stream edits file header size bytes integer size byte size file file file output stream fp file stream storing edit logs file channel fc channel file stream sync edits double buffer buf byte buffer fill byte buffer allocate direct preallocation mb fill position fill capacity fill put fs edit log op codes op invalid get op code creates output buffers file object file name store edit log size flush buffer edit log file output stream file name size throws io exception super file name buf new edits double buffer size random access file rp new random access file name rw fp new file output stream rp get fd open append fc rp get channel fc position fc size string get name return file get path journal type get type return journal type file inherit doc write fs edit log op op throws io exception buf write op op inherit doc write raw byte bytes offset length throws io exception buf write raw bytes offset length create empty edits logs file create throws io exception fc truncate fc position buf get current buf write int fs constants layout version set ready to flush flush close throws io exception try close called pending transactions flushed synced already closed skip buf null buf close buf null remove last invalid marker transaction log fc null fc open fc truncate fc position fc close fc null fp null fp close fp null finally io utils cleanup fs namesystem log fc fp buf null fc null fp null all data written stream far flushed new data still written stream flushing performed set ready to flush throws io exception buf get current buf write fs edit log op codes op invalid get op code insert eof marker buf set ready to flush flush ready buffer persistent store current buffer flushed accumulates new log records ready buffer flushed synced protected flush and sync throws io exception preallocate preallocate file necessary buf flush to fp fc force false metadata updates needed preallocation fc position fc position skip back end file marker boolean force sync return buf force sync return size current edit log including buffered data length throws io exception file size header size size buffers return fc size edits file header size bytes buf count buffered bytes allocate big chunk data preallocate throws io exception position fc position position fc size fs namesystem log debug enabled fs namesystem log debug preallocating edit log current size fc size fill position written fc write fill position fs namesystem log debug enabled fs namesystem log debug edit log size fc size written written bytes offset position operations like op jspool start op checkpoint time written edits file boolean operation supported byte op return op fs edit log op codes op jspool start get op code returns file associated stream file get file return file set file channel for testing file channel fc fc fc file channel
716	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogInputStream.java	unrelated	package org apache hadoop hdfs server namenode a generic support reading edits log data persistent storage it stream bytes storage exactly written link edit log output stream edit log input stream extends input stream implements journal stream inherit doc available throws io exception inherit doc read throws io exception inherit doc read byte b len throws io exception inherit doc close throws io exception return size current edits log length throws io exception return data input stream based edit stream data input stream get data input stream return new data input stream new buffered input stream
717	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditLogOutputStream.java	unrelated	package org apache hadoop hdfs server namenode a generic support journaling edits logs persistent storage edit log output stream implements journal stream statistics counters num sync number sync disk total time sync total time sync edit log output stream throws io exception num sync total time sync write edits log operation stream write fs edit log op op throws io exception write raw data edit log this data already transaction id checksum etc included it use within backup node replicating edits name node write raw byte bytes offset length throws io exception create initialize underlying persistent edits log storage create throws io exception inherit doc close throws io exception all data written stream far flushed new data still written stream flushing performed set ready to flush throws io exception flush sync data ready flush link set ready to flush underlying persistent store protected flush and sync throws io exception flush data persistent store collect sync metrics flush throws io exception num sync start flush and sync end total time sync end start return size current edits log length used check large enough start checkpoint length throws io exception implement policy automatically sync buffered edits log the buffered edits flushed buffer becomes full certain period time elapsed boolean force sync return false boolean operation supported byte op return true return total time spent link flush and sync get total sync time return total time sync return number calls link flush and sync get num sync return num sync string string return get name
718	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\EditsDoubleBuffer.java	unrelated	package org apache hadoop hdfs server namenode a buffer edits new edits written first buffer second available flushed each time buffer flushed two internal buffers swapped this allows edits progress concurrently flushes without allocating new buffers time edits double buffer data output buffer buf current current buffer writing data output buffer buf ready buffer ready flushing init buffer size writer writer edits double buffer default buffer size init buffer size default buffer size buf current new data output buffer init buffer size buf ready new data output buffer init buffer size writer new fs edit log op writer buf current write op fs edit log op op throws io exception writer write op op write raw byte bytes offset length throws io exception buf current write bytes offset length close throws io exception preconditions check not null buf current preconditions check not null buf ready buf size buf current size buf size throw new io exception fs edit stream buf size bytes still flushed cannot closed io utils cleanup null buf current buf ready buf current buf ready null set ready to flush assert flushed previous data flushed yet data output buffer tmp buf ready buf ready buf current buf current tmp writer new fs edit log op writer buf current writes content ready buffer given output stream resets does swap buffers flush to output stream throws io exception buf ready write to write data file buf ready reset erase data buffer boolean force sync return buf ready size init buffer size data output buffer get current buf return buf current boolean flushed return buf ready size count buffered bytes return buf ready size buf current size
719	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FileChecksumServlets.java	unrelated	package org apache hadoop hdfs server namenode servlets file checksum file checksum servlets redirect file checksum queries appropriate datanode redirect servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request http servlet response response throws servlet exception io exception servlet context context get servlet context configuration conf name node http server get conf from context context user group information ugi get ugi request conf name node namenode name node http server get name node from context context datanode id datanode namenode jsp helper get random datanode namenode try uri uri create redirect uri get file checksum ugi datanode request namenode response send redirect uri url string catch uri syntax exception e throw new servlet exception e response get writer println e string catch io exception e response send error e get message get file checksum get servlet extends dfs servlet for java io serializable serial version uid l inherit doc get http servlet request request http servlet response response throws servlet exception io exception print writer response get writer string filename get filename request response xml outputter xml new xml outputter utf xml declaration servlet context context get servlet context data node datanode data node context get attribute datanode configuration conf new hdfs configuration datanode get conf socket timeout conf get int dfs config keys dfs client socket timeout key hdfs constants read timeout socket factory socket factory net utils get socket factory conf client protocol try dfs client dfs datanode jsp helper get dfs client request datanode conf get ugi request conf client protocol nnproxy dfs get namenode md md crc file checksum checksum dfs client get file checksum filename nnproxy socket factory socket timeout md md crc file checksum write xml checksum catch io exception ioe write xml ioe filename xml catch interrupted exception e write xml e filename xml xml end document
720	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FileDataServlet.java	unrelated	package org apache hadoop hdfs server namenode redirect queries hosted filesystem appropriate datanode file data servlet extends dfs servlet for java io serializable serial version uid l create redirection uri protected uri create uri string parent hdfs file status user group information ugi client protocol nnproxy http servlet request request string dt throws io exception uri syntax exception string scheme request get scheme located blocks blks nnproxy get block locations get full path new path parent uri get path datanode id host pick src datanode blks string hostname host instanceof datanode info hostname datanode info host get host name else hostname host get host string dt param dt null dt param jsp helper get delegation token url param dt add namenode address url params name node nn name node http server get name node from context get servlet context string addr name node get host port string nn get name node address string addr param jsp helper get url param jsp helper namenode address addr return new uri scheme null hostname https equals scheme integer get servlet context get attribute datanode https port host get info port stream file get full name parent ugi ugi get short user name dt param addr param null select datanode service request currently looks first five blocks file selecting datanode randomly represented datanode id pick src datanode located blocks blks hdfs file status throws io exception get len blks get located blocks size pick random datanode name node nn name node http server get name node from context get servlet context return namenode jsp helper get random datanode nn return jsp helper best node blks service get request described request code get http nn port data path http get http servlet request request http servlet response response throws io exception configuration conf name node http server get conf from context get servlet context user group information ugi get ugi request conf try ugi as new privileged exception action void void run throws io exception client protocol nn create name node proxy string path request get path info null request get path info string delegation token request get parameter jsp helper delegation parameter name hdfs file status info nn get file info path info null info dir try response send redirect create uri path info ugi nn request delegation token url string catch uri syntax exception e response get writer println e string else info null response send error file found path else response send error path directory return null catch io exception e response send error e get message catch interrupted exception e response send error e get message
721	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FsckServlet.java	unrelated	package org apache hadoop hdfs server namenode this used namesystem web server fsck namenode fsck servlet extends dfs servlet java io serializable serial version uid l handle fsck request get http servlet request request http servlet response response throws io exception map string string pmap request get parameter map print writer response get writer inet address remote address inet address get by name request get remote addr servlet context context get servlet context configuration conf name node http server get conf from context context user group information ugi get ugi request conf try ugi as new privileged exception action object object run throws exception name node nn name node http server get name node from context context fs namesystem namesystem nn get namesystem total datanodes namesystem get number of datanodes datanode report type live short min replication namesystem get min replication new namenode fsck conf nn namenode jsp helper get network topology nn pmap total datanodes min replication remote address fsck return null catch interrupted exception e response send error e get message
722	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSClusterStats.java	unrelated	package org apache hadoop hdfs server namenode this used retrieving load related statistics cluster fs cluster stats indication total load cluster writes currently occuring cluster get total load
723	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSDirectory.java	unrelated	package org apache hadoop hdfs server namenode fs directory stores filesystem directory state it handles writing loading values disk logging changes go it keeps filename blockset mapping always current logged disk fs directory implements closeable i node directory with quota root dir fs image fs image volatile boolean ready false unknown disk space max component length max dir items ls limit max list limit lock protect directory block map reentrant read write lock dir lock condition cond utility methods acquire release read lock write lock read lock dir lock read lock lock read unlock dir lock read lock unlock write lock dir lock write lock lock write unlock dir lock write lock unlock boolean write lock return dir lock write locked by current thread boolean read lock return dir lock get read hold count caches frequently used file names used link i node reuse byte objects reduce heap usage name cache byte array name cache access existing dfs name directory fs directory fs namesystem ns configuration conf throws io exception new fs image conf ns conf fs directory fs image fs image fs namesystem ns configuration conf dir lock new reentrant read write lock true fair cond dir lock write lock new condition fs image set fs namesystem ns root dir new i node directory with quota i node directory root name ns create fs owner permissions new fs permission short integer max value unknown disk space fs image fs image configured limit conf get int dfs config keys dfs list limit dfs config keys dfs list limit default ls limit configured limit configured limit dfs config keys dfs list limit default filesystem limits max component length conf get int dfs config keys dfs namenode max component length key dfs config keys dfs namenode max component length default max dir items conf get int dfs config keys dfs namenode max directory items key dfs config keys dfs namenode max directory items default threshold conf get int dfs config keys dfs namenode name cache threshold key dfs config keys dfs namenode name cache threshold default name node log info caching file names occuring threshold times name cache new name cache byte array threshold fs namesystem get fs namesystem return fs image get fs namesystem block manager get block manager return get fs namesystem get block manager load fs image collection uri data dirs collection uri edits dirs startup option start opt throws io exception format starting requested start opt startup option format fs image get storage set storage directories data dirs edits dirs fs image get storage format fs image get storage determine cluster id reuse current id start opt startup option regular try fs image recover transition read data dirs edits dirs start opt fs image save namespace true fs edit log edit log fs image get edit log assert edit log null edit log must initialized fs image set checkpoint directories null null catch io exception e fs image close throw e write lock try set ready true name cache initialized cond signal all finally write unlock exposed unit tests protected
724	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLog.java	unrelated	package org apache hadoop hdfs server namenode fs edit log maintains log namespace modifications fs edit log implements nn storage listener string no journal streams warning warning file system changes persistent no journal streams log log log factory get log fs edit log volatile size output flush buffer array list edit log output stream edit streams null monotonically increasing counter represents transaction ids txid stores last synced transaction id synctxid time printing statistics log file last print time sync currently running volatile boolean sync running automatic sync scheduled volatile boolean auto sync scheduled false statistics counters num transactions number transactions num transactions batched in sync total time transactions total time transactions name node metrics metrics nn storage storage thread local checksum local checksum new thread local checksum protected checksum initial value return new pure java crc get thread local checksum checksum get checksum return local checksum get transaction id txid transaction id value txid value stores current transaction id thread thread local transaction id transaction id new thread local transaction id protected synchronized transaction id initial value return new transaction id long max value fs edit log nn storage storage sync running false storage storage storage register listener metrics name node get name node metrics last print time file get edit file storage directory sd return storage get edit file sd file get edit new file storage directory sd return storage get edit new file sd get num edits dirs return storage get num storage dirs name node dir type edits synchronized get num edit streams return edit streams null edit streams size return currently active edit streams this used unit tests array list edit log output stream get edit streams return edit streams boolean open return get num edit streams create empty edit log files initialize output stream logging synchronized open throws io exception num transactions total time transactions num transactions batched in sync edit streams null edit streams new array list edit log output stream array list storage directory al null iterator storage directory storage dir iterator name node dir type edits next storage directory sd next file e file get edit file sd try add new edit log stream e file catch io exception e log warn unable open edit log file e file remove directory list storage directories al null al new array list storage directory al add sd al null storage report errors on directories al if error every storage dir one removed list storage directories storage get num storage dirs name node dir type edits throw new io exception failed initialize edits log storage directory synchronized add new edit log stream file e file throws io exception edit log output stream e stream new edit log file output stream e file size output flush buffer edit streams add e stream synchronized create edit log file file name throws io exception wait for sync to finish edit log output stream e stream new edit log file output stream name size output flush buffer e stream create e stream close shutdown file store synchronized close wait for sync
725	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogLoader.java	unrelated	package org apache hadoop hdfs server namenode fs edit log loader fs namesystem fs namesys fs edit log loader fs namesystem fs namesys fs namesys fs namesys load edit log apply changes memory structure this apply edits writing disk along load fs edits edit log input stream edits throws io exception start time num edits load fs edits edits true fs image log info edits file edits get name size edits length edits num edits loaded start time seconds return num edits read header fsedit log read log version data input stream throws io exception log version read log file version could missing mark if edits log greater g available method return negative numbers avoid call available boolean available true try log version read byte catch eof exception e available false available reset log version read int log version fs constants layout version future version throw new io exception unexpected version file system log file log version current version fs constants layout version assert log version storage last upgradable layout version unsupported version log version return log version load fs edits edit log input stream edits boolean close on exit throws io exception buffered input stream bin new buffered input stream edits data input stream new data input stream bin num edits log version try log version read log version checksum checksum null layout version supports feature edits cheskum log version checksum fs edit log get checksum new data input stream new checked input stream bin checksum num edits load edit records log version checksum false finally close on exit close log version fs constants layout version version num edits save image asap return num edits load edit records log version data input stream checksum checksum boolean close on exit throws io exception fs directory fs dir fs namesys dir num edits num op add num op close num op delete num op rename old num op set repl num op mk dir num op set perm num op set owner num op set gen stamp num op times num op rename num op concat delete num op symlink num op get delegation token num op renew delegation token num op cancel delegation token num op update master key num op reassign lease num op other fs namesys write lock fs dir write lock keep track file offsets last several opcodes this handy manually recovering corrupted edits files position tracking input stream tracker new position tracking input stream new data input stream tracker recent opcode offsets new arrays fill recent opcode offsets try try fs edit log op reader reader new fs edit log op reader log version checksum fs edit log op op op reader read op null recent opcode offsets num edits recent opcode offsets length tracker get pos num edits switch op op code case op add case op close add close op add close op add close op op versions support per file replication get name replication short replication fs namesys adjust replication add close op replication block size add close op block size block info blocks new
726	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogOp.java	unrelated	package org apache hadoop hdfs server namenode helper reading ops input stream all ops derive fs edit log op instantiated reader read op fs edit log op fs edit log op codes op code thread local enum map fs edit log op codes fs edit log op op instances new thread local enum map fs edit log op codes fs edit log op protected enum map fs edit log op codes fs edit log op initial value enum map fs edit log op codes fs edit log op instances new enum map fs edit log op codes fs edit log op fs edit log op codes instances put op add new add op instances put op close new close op instances put op set replication new set replication op instances put op concat delete new concat delete op instances put op rename old new rename old op instances put op delete new delete op instances put op mkdir new mkdir op instances put op set genstamp new set genstamp op instances put op datanode add new datanode add op instances put op datanode remove new datanode remove op instances put op set permissions new set permissions op instances put op set owner new set owner op instances put op set ns quota new set ns quota op instances put op clear ns quota new clear ns quota op instances put op set quota new set quota op instances put op times new times op instances put op symlink new symlink op instances put op rename new rename op instances put op reassign lease new reassign lease op instances put op get delegation token new get delegation token op instances put op renew delegation token new renew delegation token op instances put op cancel delegation token new cancel delegation token op instances put op update master key new update master key op instances put op checkpoint time new checkpoint time op instances put op jspool start new j spool start op return instances constructor edit log op edit log ops cannot constructed directly reader read op fs edit log op fs edit log op codes op code op code op code read fields data input stream log version throws io exception write fields data output stream throws io exception add close op extends fs edit log op length string path short replication mtime atime block size block blocks permission status permissions string client name string client machine datanode descriptor data node descriptors unused add close op fs edit log op codes op code super op code assert op code op add op code op close t extends add close op t set path string path path path return t t extends add close op t set replication short replication replication replication return t t extends add close op t set modification time mtime mtime mtime return t t extends add close op t set access time atime atime atime return t t extends add close op t set block size block size block size block size return t t extends add close op
727	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSEditLogOpCodes.java	unrelated	package org apache hadoop hdfs server namenode op codes edits file enum fs edit log op codes last op code file op invalid byte op add byte op rename old byte deprecated operation op delete byte op mkdir byte op set replication byte op set permissions byte op set owner byte op close byte op set genstamp byte op set ns quota byte obsolete op clear ns quota byte obsolete op times byte set atime mtime op set quota byte op rename byte filecontext rename op concat delete byte concat files op symlink byte op get delegation token byte op renew delegation token byte op cancel delegation token byte op update master key byte op reassign lease byte must namenode protocol ja jspool start op jspool start byte must namenode protocol ja checkpoint time op checkpoint time byte byte op code constructor fs edit log op codes byte op code op code op code return byte value enum byte get op code return op code map byte fs edit log op codes byte to enum new hash map byte fs edit log op codes initialize byte enum map fs edit log op codes op code values byte to enum put op code get op code op code converts byte fs edit log op codes enum value fs edit log op codes byte byte op code return byte to enum get op code
728	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImage.java	pooling	package org apache hadoop hdfs server namenode fs image handles checkpointing logging namespace edits fs image implements nn storage listener closeable protected log log log factory get log fs image get name simple date format date form new simple date format yyyy mm dd hh mm ss checkpoint states enum checkpoint states start rolled edits upload start upload done protected fs namesystem namesystem null protected fs edit log edit log null boolean upgrade finalized false protected md hash new image digest null protected nn storage storage null ur is importing image checkpoint in default case ur is represent directories collection uri checkpoint dirs collection uri checkpoint edits dirs configuration conf can fs image rolled volatile protected checkpoint states ckpt state fs image checkpoint states start fs image fs namesystem null constructor fs image configuration conf throws io exception conf conf todo many constructors mess conf get boolean dfs config keys dfs namenode name dir restore key dfs config keys dfs namenode name dir restore default name node log info set fs image restore failed storage storage set restore failed storage true set checkpoint directories fs image get checkpoint dirs conf null fs image get checkpoint edits dirs conf null fs image fs namesystem ns conf new configuration storage new nn storage conf ns null storage set upgrade manager ns upgrade manager storage register listener edit log new fs edit log storage set fs namesystem ns fs image collection uri fs dirs collection uri fs edits dirs throws io exception storage set storage directories fs dirs fs edits dirs fs image storage info storage info string bpid storage new nn storage storage info bpid represents image image edit file fs image uri image dir throws io exception array list uri dirs new array list uri array list uri edits dirs new array list uri dirs add image dir edits dirs add image dir storage set storage directories dirs edits dirs protected fs namesystem get fs namesystem return namesystem set fs namesystem fs namesystem ns namesystem ns ns null storage set upgrade manager ns upgrade manager set checkpoint directories collection uri dirs collection uri edits dirs checkpoint dirs dirs checkpoint edits dirs edits dirs analyze storage directories recover previous transitions required perform fs state transition necessary depending namespace info read storage info boolean recover transition read collection uri data dirs collection uri edits dirs startup option start opt throws io exception assert start opt startup option format name node formatting performed reading image none data dirs exist data dirs size edits dirs size start opt startup option import throw new io exception all specified directories accessible exist start opt startup option import checkpoint dirs null checkpoint dirs empty throw new io exception cannot import image checkpoint dfs namenode checkpoint dir set start opt startup option import checkpoint edits dirs null checkpoint edits dirs empty throw new io exception cannot import image checkpoint dfs namenode checkpoint dir set storage set storage directories data dirs edits dirs for data directory calculate state check whether consistent transitioning map storage directory storage state data dir states new hash
729	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageCompression.java	unrelated	package org apache hadoop hdfs server namenode simple container handles support compressed fsimage files fs image compression codec use save load image null image compressed compression codec image codec create noop compression e uncompressed fs image compression create compression using particular codec fs image compression compression codec codec image codec codec create noop compression e uncompressed fs image compression create noop compression return new fs image compression create compression instance based user configuration given configuration object fs image compression create compression configuration conf throws io exception boolean compress image conf get boolean dfs config keys dfs image compress key dfs config keys dfs image compress default compress image return create noop compression string codec class name conf get dfs config keys dfs image compression codec key dfs config keys dfs image compression codec default return create compression conf codec class name create compression instance using codec specified code codec class name code fs image compression create compression configuration conf string codec class name throws io exception compression codec factory factory new compression codec factory conf compression codec codec factory get codec by class name codec class name codec null throw new io exception not supported codec codec class name return new fs image compression codec create compression instance based header read input stream underlying io fails fs image compression read compression header configuration conf data input stream dis throws io exception boolean compressed dis read boolean compressed return create noop compression else string codec class name text read string dis return create compression conf codec class name unwrap compressed input stream wrapping decompressor based codec if instance represents compression simply adds buffering input stream error occurs data input stream unwrap input stream input stream throws io exception image codec null return new data input stream image codec create input stream else return new data input stream new buffered input stream write header given stream indicates chosen compression codec return stream wrapped codec if codec specified simply adds buffering stream returned stream always buffered unbuffered compression enabled instantiated data output stream write header and wrap stream output stream os throws io exception data output stream dos new data output stream os dos write boolean image codec null image codec null string codec class name image codec get class get canonical name text write string dos codec class name return new data output stream image codec create output stream os else use buffered output stream return new data output stream new buffered output stream os string string image codec null return codec image codec get class get canonical name else return compression
730	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageFormat.java	unrelated	package org apache hadoop hdfs server namenode contains inner reading writing disk format fs images fs image format log log fs image log static fs image format a one shot responsible loading image the load function called getter methods may used retrieve information image loaded loading successful loader configuration conf namesystem loader working fs namesystem namesystem set true file loaded using loader boolean loaded false the image version loaded file img version the namespace id loaded file img namespace id the md sum loaded file md hash img digest loader configuration conf fs namesystem namesystem conf conf namesystem namesystem return version number image loaded get loaded image version check loaded return img version return md checksum image loaded md hash get loaded image md check loaded return img digest return namespace id image loaded get loaded namespace id check loaded return img namespace id throw illegal state exception load yet called check loaded loaded throw new illegal state exception image yet loaded throw illegal state exception load already called check not loaded loaded throw new illegal state exception image already loaded load file cur file throws io exception check not loaded assert cur file null cur file null start time load bits message digest digester md hash get digester digest input stream fin new digest input stream new file input stream cur file digester data input stream new data input stream fin try note remove checks version earlier storage last upgradable layout version since never get older images todo need change format image file contain version namespace fields read image version first appeared version img version read int read namespace id first appeared version img namespace id read int read number files num files read num files read last generation stamp img version genstamp read long namesystem set generation stamp genstamp read compression related info fs image compression compression layout version supports feature fsimage compression img version compression fs image compression read compression header conf else compression fs image compression create noop compression compression unwrap input stream fin log info loading image file cur file using compression load inodes log info number files num files layout version supports feature fsimage name optimization img version load local name i nodes num files else load full name i nodes num files load datanode info load datanodes load files under construction load files under construction load secret manager state make sure read end file eof read assert eof should reached end image file cur file finally close img digest new md hash digester digest loaded true log info image file size cur file length loaded start time seconds update root node attributes update root attr i node root ns quota root get ns quota ds quota root get ds quota fs directory fs dir namesystem dir ns quota ds quota fs dir root dir set quota ns quota ds quota fs dir root dir set modification time root get modification time fs dir root dir set permission status root get permission status load fsimage files assuming local names stored load local name i nodes num
731	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSImageSerialization.java	unrelated	package org apache hadoop hdfs server namenode static utility functions serializing various pieces data correct format fs image file some members currently benefit offline image viewer located outside package these members made package protected oiv refactored fs image serialization static fs image serialization in order reduce allocation reuse objects however methods thread safe since image saving multithreaded need keep objects thread local thread local tl data tl data new thread local tl data protected tl data initial value return new tl data simple container threadlocal data tl data deprecated utf u str new deprecated utf fs permission file perm new fs permission short helper function reads i node under construction input stream i node file under construction read i node under construction data input stream throws io exception byte name read bytes short block replication read short modification time read long preferred block size read long num blocks read int block info blocks new block info num blocks block blk new block num blocks blk read fields blocks new block info blk block replication last block under construction num blocks blk read fields blocks new block info under construction blk block replication block uc state under construction null permission status perm permission status read string client name read string string client machine read string these locations used num locs read int datanode descriptor locations new datanode descriptor num locs num locs locations new datanode descriptor locations read fields return new i node file under construction name block replication modification time preferred block size blocks perm client name client machine null helper function writes i node under construction input stream write i node under construction data output stream i node file under construction cons string path throws io exception write string path write short cons get replication write long cons get modification time write long cons get preferred block size nr blocks cons get blocks length write int nr blocks nr blocks cons get blocks write cons get permission status write write string cons get client name write string cons get client machine write int store locations last block save one inode attributes image save i node image i node node data output stream throws io exception byte name node get local name bytes write short name length write name fs permission file perm tl data get file perm node directory write short replication write long node get modification time write long access time write long preferred block size write int blocks write long node get ns quota write long node get ds quota file perm short node get fs permission short permission status write node get user name node get group name file perm else node link write short replication write long modification time write long access time write long preferred block size write int blocks text write string i node symlink node get link value file perm short node get fs permission short permission status write node get user name node get group name file perm else i node file file i node i node file node write short file i
732	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSInodeInfo.java	unrelated	package org apache hadoop hdfs server namenode this used used pluggable block placement policy expose characteristics inode fs inode info representation inode string get full path name
733	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSNamesystem.java	authenticate	package org apache hadoop hdfs server namenode fs namesystem actual bookkeeping work data node it tracks several important tables valid fsname blocklist kept disk logged set valid blocks inverted block machinelist kept memory rebuilt dynamically reports machine blocklist inverted lru cache updated heartbeat machines fs namesystem implements fs constants fs namesystem m bean fs cluster stats name node mx bean log log log factory get log fs namesystem thread local string builder audit buffer new thread local string builder protected string builder initial value return new string builder log audit event user group information ugi inet address addr string cmd string src string dst hdfs file status stat string builder sb audit buffer get sb set length sb append ugi append ugi append sb append ip append addr append sb append cmd append cmd append sb append src append src append sb append dst append dst append null stat sb append perm null else sb append perm sb append stat get owner append sb append stat get group append sb append stat get permission audit log info sb logger audit events noting successful fs namesystem operations emits fs namesystem audit info each event causes set tab separated code key value code pairs written following properties code ugi lt ugi rpc gt ip lt remote ip gt cmd lt command gt src lt src path gt dst lt dst path optional gt perm lt permissions optional gt code log audit log log factory get log fs namesystem get name audit default max corrupt fileblocks returned block deletion increment boolean permission enabled user group information fs owner string supergroup permission status default permission fs namesystem metrics counter variables capacity total l capacity used l capacity remaining l block pool used l total load boolean block token enabled block token secret manager block token secret manager block key update interval block token lifetime scan interval configurable delegation token remover scan interval time unit milliseconds convert time unit hours delegation token secret manager dt secret manager stores correct file name hierarchy fs directory dir block manager block manager block pool id used namenode string block pool id stores subset datanode map containing nodes considered alive the heartbeat monitor periodically checks dated entries removes list array list datanode descriptor heartbeats new array list datanode descriptor lease manager lease manager new lease manager threaded object checks see getting heartbeats clients daemon hbthread null heartbeat monitor thread daemon lmthread null lease monitor thread daemon smmthread null safe mode monitor thread daemon nnrmthread null namenode resource monitor thread volatile boolean resources available false volatile boolean fs running true system start heartbeat recheck interval often namenode checks expired datanodes heartbeat recheck interval resource recheck interval often namenode checks disk space availability resource recheck interval the actual resource checker instance name node resource checker nn resource checker fs server defaults server defaults allow appending hdfs files boolean support appends true replace datanode on failure dtp replace datanode on failure replace datanode on failure default volatile safe mode info safe mode safe mode information max fs objects maximum number fs objects the global
734	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\FSPermissionChecker.java	unrelated	package org apache hadoop hdfs server namenode perform permission checking link fs namesystem fs permission checker log log log factory get log user group information user group information ugi string user set string groups new hash set string boolean super fs permission checker string fs owner string supergroup throws access control exception try ugi user group information get current user catch io exception e throw new access control exception e groups add all arrays list ugi get group names user ugi get short user name super user equals fs owner groups contains supergroup check callers group contains required values boolean contains group string group return groups contains group verify caller required permission this result exception caller allowed access resource check superuser privilege user group information owner string supergroup throws access control exception fs permission checker checker new fs permission checker owner get short user name supergroup checker super throw new access control exception access denied user checker user superuser privilege required check whether current user permissions access path traverse always checked parent path means parent directory path ancestor path means last closest existing ancestor directory path note parent path exists parent path ancestor path for example suppose path foo bar baz no matter baz file directory parent path foo bar if bar exists ancestor path also foo bar if bar exist foo exists ancestor path foo further foo bar exist ancestor path access required path sub directories if path directory effect check permission string path i node directory root boolean check owner fs action ancestor access fs action parent access fs action access fs action sub access throws access control exception unresolved link exception log debug enabled log debug access check check owner check owner ancestor access ancestor access parent access parent access access access sub access sub access check parent access null file exists check sb resolve symlinks check performed link target i node inodes root get existing path i nodes path true ancestor index inodes length ancestor index inodes ancestor index null ancestor index check traverse inodes ancestor index parent access null parent access implies fs action write inodes inodes length null check sticky bit inodes inodes length inodes inodes length ancestor access null inodes length check inodes ancestor index ancestor access parent access null inodes length check inodes inodes length parent access access null check inodes inodes length access sub access null check sub access inodes inodes length sub access check owner check owner inodes inodes length check owner i node inode throws access control exception inode null user equals inode get user name return throw new access control exception permission denied check traverse i node inodes last throws access control exception j j last j check inodes j fs action execute check sub access i node inode fs action access throws access control exception inode null inode directory return stack i node directory directories new stack i node directory directories push i node directory inode directories empty i node directory directories pop check access i node child get children child directory directories push i node directory child check
735	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\GetDelegationTokenServlet.java	unrelated	package org apache hadoop hdfs server namenode serve delegation tokens http use hftp get delegation token servlet extends dfs servlet log log log factory get log get delegation token servlet string path spec get delegation token string renewer renewer protected get http servlet request req http servlet response resp throws servlet exception io exception user group information ugi servlet context context get servlet context configuration conf name node http server get conf from context context try ugi get ugi req conf catch io exception ioe log info request token received authentication req get remote addr ioe resp send error http servlet response sc forbidden unable identify authenticate user return log info sending token ugi get user name req get remote addr name node nn name node http server get name node from context context string renewer req get parameter renewer string renewer final renewer null req get user principal get name renewer data output stream dos null try dos new data output stream resp get output stream data output stream dos final dos as block ugi as new privileged exception action void void run throws exception string name node get address conf get address get host address name node get address conf get port token delegation token identifier token nn get delegation token new text renewer final token null throw new exception get token token set service new text credentials ts new credentials ts add token new text ugi get short user name token ts write dos final return null catch exception e log info exception sending token re throwing e resp send error http servlet response sc internal server error finally dos null dos close
736	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\GetImageServlet.java	unrelated	package org apache hadoop hdfs server namenode this used namesystem jetty retrieve file typically used secondary name node retrieve image edit file periodic checkpointing get image servlet extends http servlet serial version uid l log log log factory get log get image servlet get http servlet request request http servlet response response throws servlet exception io exception map string string pmap request get parameter map try servlet context context get servlet context fs image nn image name node http server get fs image from context context transfer fs image ff new transfer fs image pmap request response configuration conf configuration get servlet context get attribute jsp helper current conf user group information security enabled valid requestor request get remote user conf response send error http servlet response sc forbidden only namenode secondary namenode may access servlet log warn received non nn snn request image edits request get remote host return user group information get current user as new privileged exception action void void run throws exception ff get image response set header transfer fs image content length string value of nn image get storage get fs image name length send fs image transfer fs image get file server response get output stream nn image get storage get fs image name get throttler conf else ff get edit response set header transfer fs image content length string value of nn image get storage get fs edit name length send edits transfer fs image get file server response get output stream nn image get storage get fs edit name get throttler conf else ff put image issue http get request download new fsimage nn image validate checkpoint upload ff get token nn image new image digest ff get new checksum md hash download image digest relogin if necessary as new privileged exception action md hash md hash run throws exception return transfer fs image get file client ff get info server getimage nn image get storage get fs image name checkpoint true nn image new image digest equals download image digest throw new io exception the downloaded image corrupt expecting checksum nn image new image digest received checksum download image digest nn image checkpoint upload done return null we may lost ticket since last time tried open http connection log case user group information relogin if necessary throws io exception this method called nn therefore safe use key values return user group information login user from keytab and return ugi security util get server principal conf get dfs config keys dfs namenode krb https user name key name node get address conf get host name conf get dfs config keys dfs namenode keytab file key catch exception ie string err msg get image failed string utils stringify exception ie response send error http servlet response sc gone err msg throw new io exception err msg finally response get output stream close construct throttler conf data transfer throttler get throttler configuration conf transfer bandwidth conf get long dfs config keys dfs image transfer rate key dfs config keys dfs image transfer rate default data transfer
737	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INode.java	unrelated	package org apache hadoop hdfs server namenode we keep memory representation file block hierarchy this base i node containing common fields file directory inodes i node implements comparable byte fs inode info the inode name java utf encoding the name hdfs file status keep encoding encoding changed implicitly get file info list status client protocol changed the decoding client side change accordingly protected byte name protected i node directory parent protected modification time protected access time simple wrapper two counters ns count namespace consumed ds count diskspace consumed dir counts ns count ds count returns namespace count get ns count return ns count returns diskspace count get ds count return ds count only updated update permission status other codes modify permission enum permission status format mode group mode offset mode length user group offset group length offset length bit length mask permission status format offset length offset offset length length mask l length offset retrieve record return record mask offset combine bits record return record mask bits offset protected i node name null parent null modification time access time i node permission status permissions time atime name null parent null modification time time set access time atime set permission status permissions protected i node string name permission status permissions permissions l l set local name name copy constructor i node i node set local name get local name parent get parent set permission status get permission status set modification time get modification time set access time get access time check whether root inode boolean root return name length set link permission status protected set permission status permission status ps set user ps get user name set group ps get group name set permission ps get permission get link permission status protected permission status get permission status return new permission status get user name get group name get fs permission synchronized update permission status permission status format f n permission f combine n permission get user name string get user name n permission status format user retrieve permission return serial number manager instance get user n set user protected set user string user n serial number manager instance get user serial number user update permission status permission status format user n get group name string get group name n permission status format group retrieve permission return serial number manager instance get group n set group protected set group string group n serial number manager instance get group serial number group update permission status permission status format group n get link fs permission fs permission get fs permission return new fs permission short permission status format mode retrieve permission protected short get fs permission short return short permission status format mode retrieve permission set link fs permission link i node protected set permission fs permission permission update permission status permission status format mode permission short check whether directory boolean directory collect blocks children i node count return number files sub tree also clears references since i node deleted collect subtree blocks and clear list block v compute link content summary content summary compute content
738	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeDirectory.java	unrelated	package org apache hadoop hdfs server namenode directory i node i node directory extends i node protected default files per directory string root name list i node children i node directory string name permission status permissions super name permissions children null i node directory permission status permissions time super permissions time children null constructor i node directory byte local name permission status permissions time permissions time name local name copy constructor i node directory i node directory super children get children check whether directory boolean directory return true i node remove child i node node assert children null low collections binary search children node name low return children remove low else return null replace child name new child new child replace child i node new child children null throw new illegal argument exception the directory empty low collections binary search children new child name low old child exists replace new child children set low new child else throw new illegal argument exception no child exists replaced i node get child string name return get child i node dfs util bytes name i node get child i node byte name children null return null low collections binary search children name low return children get low return null return i node last component components null last component exist i node get node byte components boolean resolve link throws unresolved link exception i node inode new i node get existing path i nodes components inode resolve link return inode this external i node get node string path boolean resolve link throws unresolved link exception return get node get path components path resolve link retrieve existing i nodes path if existing big enough store path components existing non existing existing i nodes stored starting root i node existing existing big enough store path components last existing non existing i nodes stored existing existing length refers i node component an unresolved path exception always thrown intermediate path component refers symbolic link if path component refers symbolic link unresolved path exception thrown resolve link true p example br given path c c c c c exists resulting following path components c c c p code get existing path i nodes c c code fill array c br code get existing path i nodes c c c code fill array null p code get existing path i nodes c c code fill array c c br code get existing path i nodes c c c code fill array c null p code get existing path i nodes c c code fill array root i node c c null br code get existing path i nodes c c c code fill array root i node c c null thrown path refers symbolic link get existing path i nodes byte components i node existing boolean resolve link throws unresolved link exception assert compare bytes name components incorrect name get local name expected dfs util bytes string components i node cur node count index existing length components length index index count components length cur node null boolean last comp count components length
739	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeDirectoryWithQuota.java	unrelated	package org apache hadoop hdfs server namenode directory i node quota restriction i node directory with quota extends i node directory ns quota name space quota ns count ds quota disk space quota diskspace convert existing directory inode one given quota i node directory with quota ns quota ds quota i node directory throws quota exceeded exception super i node dir counts counts new i node dir counts space consumed in tree counts ns count counts get ns count diskspace counts get ds count set quota ns quota ds quota constructor quota verification i node directory with quota permission status permissions modification time ns quota ds quota super permissions modification time ns quota ns quota ds quota ds quota ns count constructor quota verification i node directory with quota string name permission status permissions ns quota ds quota super name permissions ns quota ns quota ds quota ds quota ns count get directory namespace quota get ns quota return ns quota get directory diskspace quota get ds quota return ds quota set directory quota set quota new ns quota new ds quota ns quota new ns quota ds quota new ds quota dir counts space consumed in tree dir counts counts counts ns count ns count counts ds count diskspace return counts get number names subtree rooted directory num items in tree return ns count diskspace consumed return diskspace update size tree update num items in tree ns delta ds delta ns count ns delta diskspace ds delta update size tree unprotected update num items in tree ns delta ds delta ns count ns count ns delta diskspace diskspace ds delta sets namespace diskspace take directory rooted i node this used carefully it check quota violations set space consumed namespace diskspace ns count namespace diskspace diskspace verify namespace count disk space satisfies quota restriction verify quota ns delta ds delta throws quota exceeded exception new count ns count ns delta new diskspace diskspace ds delta ns delta ds delta ns quota ns quota new count throw new ns quota exceeded exception ns quota new count ds quota ds quota new diskspace throw new ds quota exceeded exception ds quota new diskspace
740	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeFile.java	unrelated	package org apache hadoop hdfs server namenode i node closed file i node file extends i node fs permission umask fs permission create immutable short number bits block size short blockbits header mask bit representation format bits replication bits preferred block size headermask xffff l blockbits protected header protected block info blocks null i node file permission status permissions nr blocks short replication modification time atime preferred block size permissions new block info nr blocks replication modification time atime preferred block size protected i node file blocks null header protected i node file permission status permissions block info blklist short replication modification time atime preferred block size super permissions modification time atime set replication replication set preferred block size preferred block size blocks blklist set link fs permission link i node file since file link fs action execute action ignored protected set permission fs permission permission super set permission permission apply u mask umask boolean directory return false get block replication file short get replication return short header headermask blockbits set replication short replication replication throw new illegal argument exception unexpected value replication header replication blockbits header headermask get preferred block size file get preferred block size return header headermask set preferred block size preferred blk size preferred blk size preferred blk size headermask throw new illegal argument exception unexpected value block size header header headermask preferred blk size headermask get file blocks block info get blocks return blocks append array blocks blocks append blocks i node file inodes total added blocks size blocks length block info newlist new block info size total added blocks system arraycopy blocks newlist size i node file inodes system arraycopy blocks newlist size blocks length size blocks length block info bi newlist bi set i node blocks newlist add block block list add block block info newblock blocks null blocks new block info blocks newblock else size blocks length block info newlist new block info size system arraycopy blocks newlist size newlist size newblock blocks newlist set file block set block idx block info blk blocks idx blk collect subtree blocks and clear list block v parent null blocks null v null block info blk blocks v add blk blk set i node null blocks null return inherit doc compute content summary summary summary compute file size true summary summary diskspace consumed return summary compute file size may may block info under construction compute file size boolean includes block info under construction blocks null blocks length return last blocks length check last block block info under construction bytes blocks last instanceof block info under construction includes block info under construction blocks last get num bytes last bytes blocks get num bytes return bytes dir counts space consumed in tree dir counts counts counts ns count counts ds count diskspace consumed return counts diskspace consumed return diskspace consumed blocks diskspace consumed block blk arr size blk arr null return block blk blk arr blk null size blk get num bytes if last block written use preffered block size rather actual block size blk arr length blk arr
741	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeFileUnderConstruction.java	unrelated	package org apache hadoop hdfs server namenode i node file written i node file under construction extends i node file string client name lease holder string client machine datanode descriptor client node client cluster node i node file under construction permission status permissions short replication preferred block size mod time string client name string client machine datanode descriptor client node super permissions apply u mask umask replication mod time mod time preferred block size client name client name client machine client machine client node client node i node file under construction byte name short block replication modification time preferred block size block info blocks permission status perm string client name string client machine datanode descriptor client node super perm blocks block replication modification time modification time preferred block size set local name name client name client name client machine client machine client node client node string get client name return client name set client name string client name client name client name string get client machine return client machine datanode descriptor get client node return client node is inode constructed boolean under construction return true converts i node file under construction i node file use modification time access time i node file convert to inode file i node file obj new i node file get permission status get blocks get replication get modification time get modification time get preferred block size return obj remove block block list this block last one list remove last block block oldblock throws io exception blocks null throw new io exception trying delete non existant block oldblock size blocks length blocks size equals oldblock throw new io exception trying delete non last block oldblock copy new list block info newlist new block info size system arraycopy blocks newlist size blocks newlist convert last block file construction block set locations block info under construction set last block block info last block datanode descriptor targets throws io exception blocks null blocks length throw new io exception trying update non existant block file empty block info under construction uc block last block convert to block under construction block uc state under construction targets uc block set i node set block num blocks uc block return uc block
742	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\INodeSymlink.java	unrelated	package org apache hadoop hdfs server namenode an i node representing symbolic link i node symlink extends i node byte symlink the target uri i node symlink string value mod time atime permission status permissions super permissions mod time atime assert value null set link value value set modification time force mod time set access time atime boolean link return true set link value string value symlink dfs util bytes value string get link value return dfs util bytes string symlink byte get symlink return symlink dir counts space consumed in tree dir counts counts counts ns count return counts collect subtree blocks and clear list block v return compute content summary summary summary increment file count return summary boolean directory return false
743	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\JournalStream.java	unrelated	package org apache hadoop hdfs server namenode a generic journal input output streams journal stream type underlying persistent storage type stream based upon ul li link journal type file streams edits local file see link fs edit log edit log file output stream link fs edit log edit log file input stream li li link journal type backup streams edits backup node see link edit log backup output stream link edit log backup input stream li ul enum journal type file backup boolean of type journal type return null get stream name string get name get type stream determines underlying persistent storage type journal type get type
744	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\LeaseExpiredException.java	unrelated	package org apache hadoop hdfs server namenode the lease used create file expired lease expired exception extends io exception serial version uid l lease expired exception string msg super msg
745	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\LeaseManager.java	unrelated	package org apache hadoop hdfs server namenode lease manager lease housekeeping writing files this also provides useful methods lease recovery lease recovery algorithm namenode retrieves lease information for file f lease consider last block b f get datanodes contains b assign one datanodes primary datanode p p obtains new generation stamp namenode p gets block info datanode p computes minimum block length p updates datanodes valid generation stamp new generation stamp minimum block length p acknowledges namenode update results namenode updates block info namenode removes f lease removes lease files removed namenode commit changes edit log lease manager log log log factory get log lease manager fs namesystem fsnamesystem soft limit fs constants lease softlimit period hard limit fs constants lease hardlimit period used handling lock leases mapping lease holder lease sorted map string lease leases new tree map string lease set lease sorted set lease sorted leases new tree set lease map path names leases it protected sorted leases lock the map stores pathnames lexicographical order sorted map string lease sorted leases by path new tree map string lease lease manager fs namesystem fsnamesystem fsnamesystem fsnamesystem lease get lease string holder return leases get holder sorted set lease get sorted leases return sorted leases lease get lease by path string src return sorted leases by path get src synchronized count lease return sorted leases size synchronized count path count lease lease sorted leases count lease get paths size return count adds adds lease specified file synchronized lease add lease string holder string src lease lease get lease holder lease null lease new lease holder leases put holder lease sorted leases add lease else renew lease lease sorted leases by path put src lease lease paths add src return lease remove specified lease src synchronized remove lease lease lease string src sorted leases by path remove src lease remove path src log error src found lease paths lease paths lease path leases remove lease holder sorted leases remove lease log error lease found sorted leases remove lease specified holder src synchronized remove lease string holder string src lease lease get lease holder lease null remove lease lease src reassign lease file src new holder synchronized lease reassign lease lease lease string src string new holder assert new holder null new lease holder null lease null remove lease lease src return add lease new holder src finds pathname specified pending file synchronized string find path i node file under construction pending file throws io exception lease lease get lease pending file get client name lease null string src lease find path pending file src null return src throw new io exception pending file pending file found lease lease renew lease held given client synchronized renew lease string holder renew lease get lease holder synchronized renew lease lease lease lease null sorted leases remove lease lease renew sorted leases add lease a lease governs locks held single client for client corresponding lease whose timestamp updated client periodically checks if client dies allows lease expire corresponding locks released lease implements comparable lease string holder last
746	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\ListPathsServlet.java	unrelated	package org apache hadoop hdfs server namenode obtain meta information filesystem list paths servlet extends dfs servlet for java io serializable serial version uid l thread local simple date format df new thread local simple date format protected simple date format initial value return hftp file system get date format write node output node information includes path modification permission owner group for files also includes size replication block size write info path fullpath hdfs file status xml outputter doc throws io exception simple date format ldf df get doc start tag dir directory file doc attribute path fullpath uri get path doc attribute modified ldf format new date get modification time doc attribute accesstime ldf format new date get access time dir doc attribute size string value of get len doc attribute replication string value of get replication doc attribute blocksize string value of get block size doc attribute permission dir get permission doc attribute owner get owner doc attribute group get group doc end tag build map query setting values defaults protected map string string build root http servlet request request xml outputter doc string path request get path info null request get path info string exclude request get parameter exclude null request get parameter exclude string filter request get parameter filter null request get parameter filter boolean recur request get parameter recursive null yes equals request get parameter recursive map string string root new hash map string string root put path path root put recursive recur yes root put filter filter root put exclude exclude root put time df get format new date root put version version info get version return root service get request described request code get http nn port list paths path option option http where option default recursive quot quot filter quot quot exclude quot crc quot response a flat list files directories following format code listing path recursive yes filter time yyyy mm dd hh mm ss utc version directory path modified yyyy mm dd hh mm ss file path modified yyyy mm dd t hh mm ss z accesstime yyyy mm dd t hh mm ss z blocksize replication size listing get http servlet request request http servlet response response throws servlet exception io exception print writer response get writer xml outputter doc new xml outputter utf map string string root build root request doc string path root get path try boolean recur yes equals root get recursive pattern filter pattern compile root get filter pattern exclude pattern compile root get exclude configuration conf configuration get servlet context get attribute jsp helper current conf get ugi request conf as new privileged exception action void void run throws io exception client protocol nn create name node proxy doc declaration doc start tag listing map entry string string root entry set doc attribute get key get value hdfs file status base nn get file info path base null base dir write info base get full path new path path base doc stack string pathstack new stack string pathstack push path pathstack empty string p pathstack pop
747	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameCache.java	unrelated	package org apache hadoop hdfs server namenode caches frequently used names facilitate reuse example byte representation file name link i node this used initially adding file names cache tracks number times name used transient map it promotes name used code use threshold cache one names added link initialized called finish initialization the transient map use count tracked discarded cache ready use p this must synchronized externally name cache k class tracking use count name use count count k value internal value name use count k value count value value increment count get return count log log log factory get log name cache get name indicates initialization progress boolean initialized false names used code use threshold added cache use threshold times cache look successful lookups cached names hash map k k cache new hash map k k names number occurrences tracked initialization map k use count transient map new hash map k use count constructor cache name cache use threshold use threshold use threshold add given name cache track use count exist if name already exists internal value returned k put k name k internal cache get name internal null lookups return internal track usage count initialization initialized use count use count transient map get name use count null use count increment use count get use threshold promote name return use count value use count new use count name transient map put name use count return null lookup count lookup name returned cached object get lookup count return lookups size cache size return cache size mark name cache initialized the use count longer tracked transient map used initializing cache discarded save heap space initialized log info initialized size entries lookups lookups initialized true transient map clear transient map null promote frequently used name cache promote k name transient map remove name cache put name name lookups use threshold
748	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNode.java	heartbeat	package org apache hadoop hdfs server namenode name node serves directory namespace manager inode table hadoop dfs there single name node running dfs deployment well except second backup failover name node using federated name nodes the name node controls two critical tables filename blocksequence namespace block machinelist inodes the first table stored disk precious the second table rebuilt every time name node comes name node refers well name node server the fs namesystem actually performs filesystem management the majority name node concerned exposing ipc http server outside world plus configuration management name node implements link org apache hadoop hdfs protocol client protocol allows clients ask dfs services link org apache hadoop hdfs protocol client protocol designed direct use authors dfs client code end users instead use link org apache hadoop fs file system name node also implements link org apache hadoop hdfs server protocol datanode protocol used data nodes actually store dfs data blocks these methods invoked repeatedly automatically data nodes dfs deployment name node also implements link org apache hadoop hdfs server protocol namenode protocol used secondary namenodes rebalancing processes get partial name node state example partial blocks map etc name node implements namenode protocols fs constants hdfs configuration init hdfs federation configuration two types parameters ol li parameter common name services cluster li li parameters specific name service this keys suffixed nameservice id configuration for example dfs namenode rpc address nameservice li ol following nameservice specific keys string nameservice specific keys dfs namenode rpc address key dfs namenode name dir key dfs namenode edits dir key dfs namenode checkpoint dir key dfs namenode checkpoint edits dir key dfs namenode service rpc address key dfs namenode http address key dfs namenode https address key dfs namenode keytab file key dfs namenode secondary http address key dfs secondary namenode keytab file key dfs namenode backup address key dfs namenode backup http address key dfs namenode backup service rpc address key get protocol version string protocol client version throws io exception protocol equals client protocol get name return client protocol version id else protocol equals datanode protocol get name return datanode protocol version id else protocol equals namenode protocol get name return namenode protocol version id else protocol equals refresh authorization policy protocol get name return refresh authorization policy protocol version id else protocol equals refresh user mappings protocol get name return refresh user mappings protocol version id else protocol equals get user mappings protocol get name return get user mappings protocol version id else throw new io exception unknown protocol name node protocol protocol signature get protocol signature string protocol client version client methods hash throws io exception return protocol signature get protocol signature protocol client version client methods hash default port log log log factory get log name node get name log state change log log factory get log org apache hadoop hdfs state change protected fs namesystem namesystem protected namenode role role rpc server package protected use tests server server rpc server hdfs services communication backup node datanodes services connecting server configured clients go name node server protected server service
749	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NamenodeFsck.java	pooling	package org apache hadoop hdfs server namenode this provides rudimentary checking dfs volumes errors sub optimal conditions p the tool scans files directories starting indicated root path the following abnormal conditions detected handled p ul li files blocks completely missing datanodes br in case tool perform one following actions ul li none link fixing none li li move corrupted files lost found directory dfs link fixing move remaining data blocks saved block chains representing longest consecutive series valid blocks li li delete corrupted files link fixing delete li ul li li detect files replicated replicated blocks li ul additionally tool collects detailed overall dfs statistics optionally print detailed statistics block locations replication factors file namenode fsck log log log factory get log name node get name return marking fsck status string corrupt status corrupt string healthy status healthy string nonexistent status exist string failure status failed don attempt fixing fixing none move corrupted files lost found fixing move delete corrupted files fixing delete name node namenode network topology networktopology total datanodes short min replication inet address remote address string lost found null boolean lf inited false boolean lf inited ok false boolean show files false boolean show open files false boolean show blocks false boolean show locations false boolean show racks false boolean show corrupt file blocks false fixing fixing none string path we return back n files corrupt list files returned ordered block id allow continuation support pass last block previous call string start block after null configuration conf print writer filesystem checker namenode fsck configuration conf name node namenode network topology networktopology map string string pmap print writer total datanodes short min replication inet address remote address conf conf namenode namenode networktopology networktopology total datanodes total datanodes min replication min replication remote address remote address iterator string pmap key set iterator next string key next key equals path path pmap get path else key equals move fixing fixing move else key equals delete fixing fixing delete else key equals files show files true else key equals blocks show blocks true else key equals locations show locations true else key equals racks show racks true else key equals openforwrite show open files true else key equals listcorruptfileblocks show corrupt file blocks true else key equals startblockafter start block after pmap get startblockafter check files dfs starting indicated path fsck start time system current time millis try string msg fsck started user group information get current user remote address path path new date log info msg println msg namenode get namesystem log fsck event path remote address hdfs file status file namenode get file info path file null show corrupt file blocks list corrupt file blocks return result res new result conf check path file res println res println number data nodes total datanodes println number racks networktopology get num of racks println fsck ended new date system current time millis start time milliseconds df sck client scans healthy corrupt check status file system return appropriate code changing output might break testcases also note must last line report res healthy print n
750	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeHttpServer.java	unrelated	package org apache hadoop hdfs server namenode encapsulates http server started name node name node http server http server http server configuration conf name node nn log log name node log inet socket address http address inet socket address bind address string namenode address attribute key name node address string fsimage attribute key name system image protected string namenode attribute key name node name node http server configuration conf name node nn inet socket address bind address conf conf nn nn bind address bind address string get default server principal throws io exception return security util get server principal conf get dfs config keys dfs namenode user name key nn get name node address get host name start throws io exception string info host bind address get host name user group information security enabled string https user security util get server principal conf get dfs config keys dfs namenode krb https user name key info host https user null log warn dfs config keys dfs namenode krb https user name key defined config starting http server get default server principal kerberized ssl may function correctly else kerberized ssl servers must run host principal log info logging https user start http server security util login conf dfs config keys dfs namenode keytab file key dfs config keys dfs namenode krb https user name key info host user group information ugi user group information get login user try http server ugi as new privileged exception action http server http server run throws io exception interrupted exception info port bind address get port http server new http server hdfs info host info port info port conf new access control list conf get dfs config keys dfs admin boolean cert ssl conf get boolean dfs https enable false boolean use krb user group information security enabled cert ssl use krb boolean need client auth conf get boolean dfs config keys dfs client https need auth key dfs config keys dfs client https need auth default inet socket address sec info soc addr net utils create socket addr conf get dfs config keys dfs namenode https address key dfs config keys dfs namenode https address default configuration ssl conf new hdfs configuration false cert ssl ssl conf add resource conf get dfs https server keystore resource ssl server xml http server add ssl listener sec info soc addr ssl conf need client auth use krb assume ssl port datanodes inet socket address datanode ssl port net utils create socket addr conf get dfs datanode https address info host http server set attribute datanode https port datanode ssl port get port http server set attribute namenode attribute key nn http server set attribute namenode address attribute key nn get name node address http server set attribute fsimage attribute key nn get fs image http server set attribute jsp helper current conf conf setup servlets http server http server start the web server port ephemeral ensure correct info info port http server get port http address new inet socket address info host info port log info nn get role
751	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NamenodeJspHelper.java	pooling	package org apache hadoop hdfs server namenode namenode jsp helper string get safe mode text fs namesystem fsn fsn in safe mode return return safe mode on em fsn get safe mode tip em br returns security mode cluster namenode string get security mode text user group information security enabled return security em on em br else return security em off em br string get inode limit text fs namesystem fsn inodes fsn dir total inodes blocks fsn get blocks total maxobjects fsn get max objects memory mx bean mem management factory get memory mx bean memory usage heap mem get heap memory usage total memory heap get used max memory heap get max commited memory heap get committed memory usage non heap mem get non heap memory usage total non heap non heap get used max non heap non heap get max commited non heap non heap get committed used total memory commited memory used non heap total non heap commited non heap string str inodes files directories blocks blocks inodes blocks total maxobjects pct inodes blocks maxobjects str maxobjects pct str br str heap memory used string utils byte desc total memory used commited heap memory string utils byte desc commited memory max heap memory string utils byte desc max memory br str non heap memory used string utils byte desc total non heap used non heap commited non heap memory string utils byte desc commited non heap max non heap memory string utils byte desc max non heap br return str string get upgrade status text fs namesystem fsn string status text try upgrade status report status fsn distributed upgrade progress upgrade action get status status text status null there upgrades progress status get status text false catch io exception e status text upgrade status unknown return status text return table containing version information string get version table fs namesystem fsn return div id dfstable table n tr td id col started td td fsn get start time td tr n n tr td id col version td td version info get version version info get revision n tr td id col compiled td td version info get date version info get user version info get branch n tr td id col upgrades td td get upgrade status text fsn n tr td id col cluster id td td fsn get cluster id td tr n n tr td id col block pool id td td fsn get block pool id td tr n n table div generate warning text corrupt files string get corrupt files warning fs namesystem fsn missing blocks fsn get missing blocks count missing blocks string builder result new string builder warning typically displayed red result append br warning href corrupt files jsp title list corrupt files n result append b warning there missing blocks missing blocks please check logs run fsck order identify missing blocks b result append result append br div small see hadoop faq common causes potential solutions result append br br n return result string return health jsp row num col
752	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeMXBean.java	pooling	package org apache hadoop hdfs server namenode this jmx management namenode information name node mx bean gets version hadoop string get version gets used space data nodes get used gets total non used raw bytes get free gets total raw bytes including non dfs used space get total gets safemode status string get safemode checks upgrade finalized boolean upgrade finalized gets total used space data nodes non dfs purposes storing temporary files local file system get non dfs used space gets total used space data nodes percentage total capacity get percent used gets total remaining space data nodes percentage total capacity get percent remaining get total space used block pools namenode get block pool used space get total space used block pool percentage total capacity get percent block pool used gets total numbers blocks cluster get total blocks gets total number files cluster get total files gets total number missing blocks cluster get number of missing blocks gets number threads get threads gets live node information cluster string get live nodes gets dead node information cluster string get dead nodes gets decommissioning node information cluster string get decom nodes gets cluster id string get cluster id gets block pool id string get block pool id
753	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeResourceChecker.java	unrelated	package org apache hadoop hdfs server namenode name node resource checker provides method code available disk space code return true name node disk space available volumes configured checked volumes containing file system name edits dirs added default arbitrary extra volumes may configured well name node resource checker log log log factory get log name node resource checker get name space bytes reserved per volume du reserved configuration conf map string df volumes create name node resource checker check name dirs edits dirs set code conf code name node resource checker configuration conf throws io exception conf conf volumes new hash map string df du reserved conf get long dfs config keys dfs namenode du reserved key dfs config keys dfs namenode du reserved default collection uri extra checked volumes util collection as ur is conf get trimmed string collection dfs config keys dfs namenode checked volumes key add dirs to check fs namesystem get namespace dirs conf add dirs to check fs namesystem get namespace edits dirs conf add dirs to check extra checked volumes add passed directories list volumes check the directories whose volumes checked available space add dirs to check collection uri directories to check throws io exception uri directory uri directories to check file dir new file directory uri get path dir exists throw new io exception missing directory dir get absolute path df df new df dir conf volumes put df get filesystem df return true disk space available configured volumes volumes false otherwise boolean available disk space throws io exception return get volumes low on space size return set directories low space collection string get volumes low on space throws io exception log debug enabled log debug going check following volumes disk space volumes collection string low volumes new array list string df volume volumes values available space volume get available string file system volume get filesystem log debug enabled log debug space available volume file system available space available space du reserved log warn space available volume file system available space configured reserved amount du reserved low volumes add volume get filesystem return low volumes
754	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NNStorage.java	pooling	package org apache hadoop hdfs server namenode nn storage responsible management storage directories used name node nn storage extends storage implements closeable log log log factory get log nn storage get name string message digest property image md digest the filenames used storing images enum name node file image fsimage time fstime edits edits image new fsimage ckpt edits new edits new string file name null name node file string name file name name string get name return file name implementation storage dir type specific namenode storage a storage directory could type image stores fsimage type edits stores edits type image and edits stores fsimage edits enum name node dir type implements storage dir type undefined image edits image and edits storage dir type get storage dir type return boolean of type storage dir type type image and edits type image type edits return true return type interface implemented make use storage directories they notified storage directory causing errors becoming available formatted this allows implementors take specific action storage directory occurs nn storage listener an error occurred storage directory error occurred storage directory sd throws io exception a storage directory formatted format occurred storage directory sd throws io exception a storage directory available use directory available storage directory sd throws io exception list nn storage listener listeners upgrade manager upgrade manager null protected md hash image digest null protected string blockpool id id block pool flag controls try restore failed storages boolean restore failed storage false object restoration lock new object boolean disable pre upgradable layout check false checkpoint time l the age image list failed thus removed storages protected list storage directory removed storage dirs new copy on write array list storage directory construct nn storage nn storage configuration conf super node type name node storage dirs new copy on write array list storage directory listeners new copy on write array list nn storage listener construct nn storage nn storage storage info storage info string bpid super node type name node storage info storage dirs new copy on write array list storage directory listeners new copy on write array list nn storage listener blockpool id bpid boolean pre upgradable layout storage directory sd throws io exception disable pre upgradable layout check return false file old image dir new file sd get root image old image dir exists return false check layout version inside image file file old f new file old image dir fsimage random access file old file new random access file old f rws try old file seek old version old file read int old version last pre upgrade layout version return false finally old file close return true close throws io exception listeners clear unlock all storage dirs clear set flag whether attempt made restore failed storage directories next available oppurtuinity set restore failed storage boolean val log warn set restore failed storage val restore failed storage val boolean get restore failed storage return restore failed storage see removed storages writable returned service if save namespace set method called save namespace attempt restore removed storage directory alive copy
755	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\NotReplicatedYetException.java	unrelated	package org apache hadoop hdfs server namenode the file finished written enough datanodes yet not replicated yet exception extends io exception serial version uid l not replicated yet exception string msg super msg
756	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\RenewDelegationTokenServlet.java	unrelated	package org apache hadoop hdfs server namenode renew delegation tokens http use hftp renew delegation token servlet extends dfs servlet log log log factory get log renew delegation token servlet string path spec renew delegation token string token token protected get http servlet request req http servlet response resp throws servlet exception io exception user group information ugi servlet context context get servlet context configuration conf name node http server get conf from context context try ugi get ugi req conf catch io exception ioe log info request token received authentication req get remote addr ioe resp send error http servlet response sc forbidden unable identify authenticate user return name node nn name node http server get name node from context context string token string req get parameter token token string null resp send error http servlet response sc multiple choices token renew specified token delegation token identifier token new token delegation token identifier token decode from url string token string try result ugi as new privileged exception action long long run throws exception return nn renew delegation token token print stream os new print stream resp get output stream os println result os close catch exception e transfer exception http string exception class e get class get name string exception msg e get localized message string str exception exception class exception msg log info exception renewing token re throwing str exception e resp send error http servlet response sc internal server error str exception
757	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SafeModeException.java	unrelated	package org apache hadoop hdfs server namenode this exception thrown name node safe mode client cannot modified namespace safe mode safe mode exception extends io exception serial version uid l safe mode exception safe mode exception string text fs namesystem safe mode info mode super text name node safe mode n mode get turn off tip
758	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SecondaryNameNode.java	pooling	package org apache hadoop hdfs server namenode the secondary name node helper primary name node the secondary responsible supporting periodic checkpoints hdfs metadata the current design allows one secondary name node per hd fs cluster the secondary name node daemon periodically wakes determined schedule specified configuration triggers periodic checkpoint goes back sleep the secondary name node uses client protocol talk primary name node secondary name node implements runnable hdfs configuration init log log log factory get log secondary name node get name starttime system current time millis volatile last checkpoint time string fs name checkpoint storage checkpoint image namenode protocol namenode configuration conf inet socket address name node addr volatile boolean run http server info server info port image port string info bind address fs namesystem namesystem collection uri checkpoint dirs collection uri checkpoint edits dirs checkpoint period seconds checkpoint size size bytes current edit log inherit doc string string return get class get simple name status n name node address name node addr n start time new date starttime n last checkpoint time last checkpoint time new date last checkpoint time n checkpoint period checkpoint period seconds n checkpoint size string utils byte desc checkpoint size checkpoint size bytes n checkpoint dirs checkpoint dirs n checkpoint edits dirs checkpoint edits dirs fs image get fs image return checkpoint image create connection primary namenode secondary name node configuration conf throws io exception try name node initialize generic keys conf initialize conf catch io exception e shutdown log fatal failed start secondary namenode e throw e inet socket address get http address configuration conf return net utils create socket addr conf get dfs namenode secondary http address key dfs namenode secondary http address default initialize secondary name node initialize configuration conf throws io exception inet socket address info soc addr get http address conf info bind address info soc addr get host name user group information set configuration conf user group information security enabled security util login conf dfs secondary namenode keytab file key dfs secondary namenode user name key info bind address initiate java vm metrics jvm metrics create secondary name node conf get dfs metrics session id key default metrics system instance create connection namenode run true name node addr name node get service address conf true conf conf namenode namenode protocol rpc wait for proxy namenode protocol namenode protocol version id name node addr conf initialize checkpoint directories fs name get info server checkpoint dirs fs image get checkpoint dirs conf tmp hadoop dfs namesecondary checkpoint edits dirs fs image get checkpoint edits dirs conf tmp hadoop dfs namesecondary checkpoint image new checkpoint storage conf checkpoint image recover create checkpoint dirs checkpoint edits dirs initialize scheduling parameters configuration checkpoint period conf get long dfs namenode checkpoint period key dfs namenode checkpoint period default checkpoint size conf get long dfs namenode checkpoint size key dfs namenode checkpoint size default initialize webserver uploading files kerberized ssl servers must run host principal user group information http ugi user group information login user from keytab and return ugi security util get server principal conf
759	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\SerialNumberManager.java	unrelated	package org apache hadoop hdfs server namenode manage name serial number maps users groups serial number manager this instance link serial number manager serial number manager instance new serial number manager serial number map string usermap new serial number map string serial number map string groupmap new serial number map string serial number manager get user serial number string u return usermap get u get group serial number string g return groupmap get g string get user n return usermap get n string get group n return groupmap get n get user serial number null get group serial number null serial number map t max next serial number return max map t integer new hash map t integer map integer t new hash map integer t synchronized get t integer sn get sn null sn next serial number put sn put sn return sn synchronized t get contains key throw new illegal state exception contains key return get inherit doc string string return max max n n
760	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\StreamFile.java	unrelated	package org apache hadoop hdfs server namenode stream file extends dfs servlet java io serializable serial version uid l string content length content length return dfs client use make given http request protected dfs client get dfs client http servlet request request throws io exception interrupted exception configuration conf configuration get servlet context get attribute jsp helper current conf user group information ugi get ugi request conf servlet context context get servlet context data node datanode data node context get attribute datanode return datanode jsp helper get dfs client request datanode conf ugi get http servlet request request http servlet response response throws servlet exception io exception string path request get path info null request get path info string filename jsp helper validate path path filename null response set content type text plain print writer response get writer print invalid input return enumeration string req ranges request get headers range req ranges null req ranges more elements req ranges null dfs client dfs try dfs get dfs client request catch interrupted exception e response send error e get message return dfs input stream null output stream null try dfs open filename response get output stream file len get file length req ranges null list inclusive byte range ranges inclusive byte range satisfiable ranges req ranges file len stream file send partial data response file len ranges else no ranges send entire file response set header content disposition attachment filename filename response set content type application octet stream response set header content length file len stream file copy from offset l file len close null close null dfs close dfs null catch io exception ioe log debug enabled log debug response committed response committed ioe throw ioe finally io utils cleanup log io utils cleanup log io utils cleanup log dfs send partial content response given range if satisfiable ranges multiple ranges requested unsupported respond range satisfiable send partial data fs input stream output stream http servlet response response content length list inclusive byte range ranges throws io exception ranges null ranges size response set content length response set status http servlet response sc requested range not satisfiable response set header content range inclusive byte range header range string content length else inclusive byte range single satisfiable range ranges get single length single satisfiable range get size content length response set status http servlet response sc partial content response set header content range single satisfiable range header range string content length copy from offset single satisfiable range get first content length single length copy count bytes given offset one stream another copy from offset fs input stream output stream offset count throws io exception seek offset io utils copy bytes count false
761	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\TransferFsImage.java	unrelated	package org apache hadoop hdfs server namenode this provides fetching specified file name node transfer fs image implements fs constants string content length content length boolean get image boolean get edit boolean put image remoteport string machine name checkpoint signature token md hash new checksum null file downloader url parameters transfer fs image map string string pmap http servlet request request http servlet response response throws io exception get image get edit put image false remoteport machine name null token null iterator string pmap key set iterator next string key next key equals getimage get image true else key equals getedit get edit true else key equals putimage put image true else key equals port remoteport new integer pmap get port value else key equals machine machine name pmap get machine else key equals token token new checkpoint signature pmap get token else key equals new checksum new checksum new md hash pmap get new checksum num gets get image get edit num gets num gets put image throw new io exception illegal parameters transfer fs image boolean get edit return get edit boolean get image return get image boolean put image return put image checkpoint signature get token return token get md digest new image md hash get new checksum return new checksum string get info server throws io exception machine name null remoteport throw new io exception machine name port undefined return machine name remoteport a server side method respond getfile http request copies contents local file output stream get file server output stream outstream file localfile data transfer throttler throttler throws io exception byte buf new byte buffer size file input stream infile null try infile new file input stream localfile error simulator get error simulation localfile get absolute path contains secondary throw exception secondary sends image throw new io exception if exception caught name node fs image truncated error simulator get error simulation localfile get absolute path contains fsimage test sending image shorter localfile len localfile length buf new byte math min len buffer size this read half image rest image sent wire infile read buf num num num infile read buf num break outstream write buf num throttler null throttler throttle num finally infile null infile close client side method fetch file server copies response url list local files md hash get file client string fs name string id file local path boolean get checksum throws io exception byte buf new byte buffer size string proto user group information security enabled https http string builder str new string builder proto fs name getimage str append id open connection remote server url url new url str string avoid krb bug cross realm hosts security util fetch service ticket url url connection connection url open connection advertised size string content length connection get header field content length content length null advertised size long parse long content length else throw new io exception content length header provided namenode trying fetch str received input stream stream connection get input stream message digest digester null get checksum digester md hash get digester
762	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UnsupportedActionException.java	unrelated	package org apache hadoop hdfs server namenode this exception thrown operation supported unsupported action exception extends io exception java io serializable serial version uid l unsupported action exception string action super action action supported
763	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UpgradeManagerNamenode.java	unrelated	package org apache hadoop hdfs server namenode upgrade manager name nodes distributed upgrades name node starts safe mode conditions met name node exit at point name node enters manual safe mode remain upgrade completed after name nodes processes upgrade commands data nodes updates status upgrade manager namenode extends upgrade manager hdfs constants node type get type return hdfs constants node type name node fs namesystem namesystem upgrade manager namenode fs namesystem namesystem namesystem namesystem start distributed upgrade instantiates distributed upgrade objects synchronized boolean start upgrade throws io exception upgrade state initialize upgrade upgrade state return false write new upgrade state disk namesystem get fs image get storage write all assert current upgrades null current upgrades null broadcast command current upgrades first start upgrade name node log info n distributed upgrade name node version get upgrade version current lv fs constants layout version started return true synchronized upgrade command process upgrade command upgrade command command throws io exception name node log debug enabled name node log debug n distributed upgrade name node version get upgrade version current lv fs constants layout version processing upgrade command command get action status get upgrade status current upgrades null name node log info ignoring upgrade command command get action version command get version no distributed upgrades currently running name node return null upgrade object namenode cur uo upgrade object namenode current upgrades first command get version cur uo get version throw new incorrect version exception command get version upgrade command cur uo get version upgrade command reply cur uo process upgrade command command cur uo get upgrade status return reply current upgrade done cur uo complete upgrade name node log info n distributed upgrade name node version cur uo get version current lv fs constants layout version complete proceede next one current upgrades remove cur uo current upgrades empty upgrades done complete upgrade else start next upgrade cur uo upgrade object namenode current upgrades first broadcast command cur uo start upgrade return reply synchronized complete upgrade throws io exception set write new upgrade state disk set upgrade state false fs constants layout version namesystem get fs image get storage write all current upgrades null broadcast command null namesystem leave safe mode false synchronized upgrade status report distributed upgrade progress upgrade action action throws io exception boolean finalized false current upgrades null upgrades progress fs image fsimage namesystem get fs image finalized fsimage upgrade finalized finalized upgrade finalized return null nothing report return new upgrade status report fsimage get storage get layout version short finalized upgrade object namenode cur uo upgrade object namenode current upgrades first boolean details false switch action case get status break case detailed status details true break case force proceed cur uo force proceed return cur uo get upgrade status report details
764	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\UpgradeObjectNamenode.java	unrelated	package org apache hadoop hdfs server namenode base name node upgrade objects data node upgrades run separate threads upgrade object namenode extends upgrade object process upgrade command rpc one generic command upgrade related inter component communications the actual command recognition execution handled the reply sent back also upgrade command upgrade command process upgrade command upgrade command command throws io exception hdfs constants node type get type return hdfs constants node type name node upgrade command start upgrade throws io exception broadcast data nodes must start upgrade return new upgrade command upgrade command uc action start upgrade get version short force proceed throws io exception nothing default name node log info force proceed defined upgrade get description
765	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\metrics\FSNamesystemMBean.java	unrelated	package org apache hadoop hdfs server namenode metrics this interface defines methods get status fs namesystem name node it also used publishing via jmx hence follow jmx naming convention note used metrics dynamic m bean base implement name node state m bean stable published p name node runtime activity statistic info reported fs namesystem m bean the state file system safemode operational string get fs state number allocated blocks system get blocks total total storage capacity get capacity total free unused storage capacity get capacity remaining used storage capacity get capacity used total number files directories get files total blocks pending replicated get pending replication blocks blocks replicated get under replicated blocks blocks scheduled replication get scheduled replication blocks total load fs namesystem get total load number live data nodes get num live data nodes number dead data nodes get num dead data nodes
766	hdfs\src\java\org\apache\hadoop\hdfs\server\namenode\metrics\NameNodeMetrics.java	unrelated	package org apache hadoop hdfs server namenode metrics this maintaining various name node activity statistics publishing metrics interfaces name node metrics metrics registry registry new metrics registry namenode mutable counter long files deleted mutable counter long transactions batched in sync name node metrics string process name string session id registry tag process name process name tag session id session id name node metrics create configuration conf namenode role r string session id conf get dfs config keys dfs metrics session id key string process name r string metrics system ms default metrics system instance jvm metrics create process name session id ms return ms register new name node metrics process name session id shutdown default metrics system shutdown incr get block locations get block locations incr incr files created files created incr incr create file ops create file ops incr incr files appended files appended incr incr add block ops add block ops incr incr get additional datanode ops get additional datanode ops incr incr files renamed files renamed incr incr files deleted delta files deleted incr delta incr delete file ops delete file ops incr incr get listing ops get listing ops incr incr files in get listing ops delta files in get listing ops incr delta incr file info ops file info ops incr incr create symlink ops create symlink ops incr incr get link target ops get link target ops incr add transaction latency transactions add latency incr transactions batched in sync transactions batched in sync incr add sync elapsed syncs add elapsed set fs image load time elapsed fs image load time set elapsed add block report latency block report add latency set safe mode time elapsed safe mode time set elapsed
767	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlockCommand.java	pooling	package org apache hadoop hdfs server protocol a block command instruction datanode regarding blocks control it tells data node either invalidate set indicated blocks copy set indicated blocks another data node block command extends datanode command string pool id block blocks datanode info targets block command create block command transferring blocks another datanode block command action string pool id list block target pair blocktargetlist super action pool id pool id blocks new block blocktargetlist size targets new datanode info blocks length blocks length block target pair p blocktargetlist get blocks p block targets p targets datanode info empty target create block command given action block command action string pool id block blocks super action pool id pool id blocks blocks targets empty target string get block pool id return pool id block get blocks return blocks datanode info get targets return targets writable register ctor writable factories set factory block command new writable factory writable new instance return new block command write data output throws io exception super write text write string pool id write int blocks length blocks length blocks write write int targets length targets length write int targets length j j targets length j targets j write read fields data input throws io exception super read fields pool id text read string blocks new block read int blocks length blocks new block blocks read fields targets new datanode info read int targets length targets new datanode info read int j j targets length j targets j new datanode info targets j read fields
768	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlockRecoveryCommand.java	unrelated	package org apache hadoop hdfs server protocol block recovery command instruction data node recover specified blocks the data node receives command treats primary data node recover process block recovery identified recovery id also new generation stamp block recovery succeeds block recovery command extends datanode command collection recovering block recovering blocks this block locations recovered new generation stamp block successful recovery the new generation stamp block also plays role recovery id recovering block extends located block new generation stamp create empty recovering block recovering block super new generation stamp l create recovering block recovering block extended block b datanode info locs new gs super b locs false start offset unknown new generation stamp new gs return new generation stamp block also plays role recovery id get new generation stamp return new generation stamp writable register ctor writable factories set factory recovering block new writable factory writable new instance return new recovering block write data output throws io exception super write write long new generation stamp read fields data input throws io exception super read fields new generation stamp read long create empty block recovery command block recovery command create block recovery command specified capacity recovering blocks block recovery command capacity super datanode protocol dna recoverblock recovering blocks new array list recovering block capacity return list recovering blocks collection recovering block get recovering blocks return recovering blocks add recovering block command add recovering block block recovering blocks add block writable register ctor writable factories set factory block recovery command new writable factory writable new instance return new block recovery command write data output throws io exception super write write int recovering blocks size recovering block block recovering blocks block write read fields data input throws io exception super read fields num blocks read int recovering blocks new array list recovering block num blocks num blocks recovering block b new recovering block b read fields add b
769	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\BlocksWithLocations.java	unrelated	package org apache hadoop hdfs server protocol a implement array block locations it provide efficient customized serialization deserialization methods stead using default array de serialization provided rpc blocks with locations implements writable a keep track block locations block with locations implements writable block block string datanode i ds default constructor block with locations block new block datanode i ds null constructor block with locations block b string datanodes block b datanode i ds datanodes get block block get block return block get block locations string get datanodes return datanode i ds deserialization method read fields data input throws io exception block read fields len writable utils read v int variable length integer datanode i ds new string len len datanode i ds text read string serialization method write data output throws io exception block write writable utils write v int datanode i ds length variable length string id datanode i ds text write string id block with locations blocks default constructor blocks with locations constructor one parameter blocks with locations block with locations blocks blocks blocks getter block with locations get blocks return blocks serialization method write data output throws io exception writable utils write v int blocks length blocks length blocks write deserialization method read fields data input throws io exception len writable utils read v int blocks new block with locations len len blocks new block with locations blocks read fields
770	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\CheckpointCommand.java	unrelated	package org apache hadoop hdfs server protocol checkpoint command p returned backup node name node reply link namenode protocol start checkpoint namenode registration request br contains ul li link checkpoint signature identifying particular checkpoint li li indicator whether backup image discarded starting checkpoint li li indicator whether image transfered back name node upon completion checkpoint li ul checkpoint command extends namenode command checkpoint signature c sig boolean image obsolete boolean need to return image checkpoint command null false false checkpoint command checkpoint signature sig boolean img obsolete boolean need to return img super namenode protocol act checkpoint c sig sig image obsolete img obsolete need to return image need to return img checkpoint signature used ensure nodes talking checkpoint checkpoint signature get signature return c sig indicates whether current backup image obsolete therefore need discarded boolean image obsolete return image obsolete indicates whether new checkpoint image needs transfered back name node checkpoint done boolean need to return image return need to return image writable writable factories set factory checkpoint command new writable factory writable new instance return new checkpoint command write data output throws io exception super write c sig write write boolean image obsolete write boolean need to return image read fields data input throws io exception super read fields c sig new checkpoint signature c sig read fields image obsolete read boolean need to return image read boolean
771	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeCommand.java	pooling	package org apache hadoop hdfs server protocol base data node command issued name node notify data nodes done declare subclasses avro denormalized representation datanode command register datanode command finalize block command upgrade command block recovery command key update command datanode command extends server command register extends datanode command register super datanode protocol dna register read fields data input write data output finalize extends datanode command string block pool id finalize super datanode protocol dna finalize finalize string bpid super datanode protocol dna finalize block pool id bpid string get block pool id return block pool id read fields data input throws io exception block pool id writable utils read string write data output throws io exception writable utils write string block pool id register ctor writable factories set factory register new writable factory writable new instance return new register writable factories set factory finalize new writable factory writable new instance return new finalize datanode command register new register datanode command super datanode command action super action
772	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeProtocol.java	heartbeat	package org apache hadoop hdfs server protocol protocol dfs datanode uses communicate name node it used upload current load information block reports the way name node communicate data node returning values functions server principal dfs config keys dfs namenode user name key client principal dfs config keys dfs datanode user name key datanode protocol extends versioned protocol add block pool id block version id l error code notify disk error still valid volumes dn invalid block fatal disk error valid volumes left dn determines actions data node perform receiving datanode command dna unknown unknown action dna transfer transfer blocks another datanode dna invalidate invalidate blocks dna shutdown shutdown node dna register register dna finalize finalize previous upgrade dna recoverblock request block recovery dna accesskeyupdate update access key register datanode new storage id datanode one registration id communication datanode registration register datanode datanode registration registration throws io exception send heartbeat tells name node data node still alive well includes status info it also gives name node chance return array datanode command objects a datanode command tells data node invalidate local block copy data nodes etc datanode command send heartbeat datanode registration registration capacity dfs used remaining block pool used xmits in progress xceiver count failed volumes throws io exception block report tells name node locally stored blocks the name node returns array blocks become obsolete deleted this function meant upload locally stored blocks it invoked upon startup infrequently afterwards each block represented longs this done instead block reduce memory used block reports datanode command block report datanode registration registration string pool id blocks throws io exception block received allows data node tell name node recently received block data hint pereferred replica deleted excessive blocks for example whenever client code writes new block another data node copies block data node call block received block received datanode registration registration string pool id block blocks string del hints throws io exception error report tells name node something gone awry useful debugging error report datanode registration registration error code string msg throws io exception namespace info version request throws io exception this general way send command name node distributed upgrade process the generosity variety upgrade commands unpredictable the reply name node also received form upgrade command upgrade command process upgrade command upgrade command comm throws io exception link org apache hadoop hdfs protocol client protocol report bad blocks located block report bad blocks located block blocks throws io exception commit block synchronization lease recovery commit block synchronization extended block block newgenerationstamp newlength boolean close file boolean deleteblock datanode id newtargets throws io exception
773	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DatanodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol datanode registration contains information name node needs identify verify data node contacts name node this information sent data node communication request datanode registration extends datanode id implements writable node registration register ctor writable factories set factory datanode registration new writable factory writable new instance return new datanode registration storage info storage info exported block keys exported keys default constructor datanode registration create datanode registration datanode registration string node name super node name storage info new storage info exported keys new exported block keys set storage info storage info storage storage info new storage info storage get version return storage info get layout version string get registration id return storage get registration id storage info string get address return get name string string return get class get simple name name storage id storage id info port info port ipc port ipc port storage info storage info writable inherit doc write data output throws io exception super write todo move datanode id hadoop committed write short ipc port storage info write exported keys write inherit doc read fields data input throws io exception super read fields todo move datanode id hadoop committed ipc port read short x ffff storage info read fields exported keys read fields boolean equals object return super equals hash code return super hash code
774	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\DisallowedDatanodeException.java	unrelated	package org apache hadoop hdfs server protocol this exception thrown datanode tries register communicate namenode appear list included nodes specifically excluded disallowed datanode exception extends io exception java io serializable serial version uid l disallowed datanode exception datanode id node id super datanode denied communication namenode node id get name
775	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\InterDatanodeProtocol.java	pooling	package org apache hadoop hdfs server protocol an inter datanode protocol updating generation stamp server principal dfs config keys dfs datanode user name key client principal dfs config keys dfs datanode user name key inter datanode protocol extends versioned protocol log log log factory get log inter datanode protocol add block pool id block version id l initialize replica recovery null data node replica replica recovery info init replica recovery recovering block r block throws io exception update replica new generation stamp length extended block update replica under recovery extended block old block recovery id new length throws io exception
776	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\KeyUpdateCommand.java	unrelated	package org apache hadoop hdfs server protocol key update command extends datanode command exported block keys keys key update command new exported block keys key update command exported block keys keys super datanode protocol dna accesskeyupdate keys keys exported block keys get exported keys return keys writable register ctor writable factories set factory key update command new writable factory writable new instance return new key update command write data output throws io exception super write keys write read fields data input throws io exception super read fields keys read fields
777	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeCommand.java	unrelated	package org apache hadoop hdfs server protocol base name node command issued name node notify name nodes done namenode command extends server command writable factories set factory namenode command new writable factory writable new instance return new namenode command namenode command super namenode command action super action
778	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeProtocol.java	unrelated	package org apache hadoop hdfs server protocol protocol secondary name node uses communicate name node it used get part name node state server principal dfs config keys dfs namenode user name key client principal dfs config keys dfs namenode user name key namenode protocol extends versioned protocol compared previous version following changes introduced only latest change reflected the log historical changes retrieved svn added one parameter roll fs image changed definition checkpoint signature version id l error codes passed error report notify fatal journal action codes see journal byte ja is alive check whether journal alive byte ja journal journal byte ja jspool start fs edit log op codes op jspool start byte ja checkpoint time fs edit log op codes op checkpoint time act unknown unknown action act shutdown shutdown node act checkpoint checkpoint get list blocks belonging code datanode code whose total size equals code size code datanode exist blocks with locations get blocks datanode info datanode size throws io exception get current block keys exported block keys get block keys throws io exception get size current edit log bytes see link org apache hadoop hdfs server namenode secondary name node get edit log size throws io exception closes current edit log opens new one the call fails file system safe mode see link org apache hadoop hdfs server namenode secondary name node checkpoint signature roll edit log throws io exception rolls fs image log it removes old fs image copies new image fs image removes old edits renames edits new edits the call fails four files missing see link org apache hadoop hdfs server namenode secondary name node roll fs image checkpoint signature sig throws io exception request name node version storage information name node namespace info version request throws io exception report active name node error occurred subordinate node depending error code active node may decide unregister reporting node error report namenode registration registration error code string msg throws io exception register subordinate name node like backup node node registered namenode registration register namenode registration registration throws io exception a request active name node start checkpoint the name node decide whether admit reject the name node also decides done backup node image checkpoint namenode command start checkpoint namenode registration registration throws io exception a request active name node finalize previously started checkpoint end checkpoint namenode registration registration checkpoint signature sig throws io exception get size active name node journal edit log bytes journal size namenode registration registration throws io exception journal edit records this message sent active name node backup node via code edit log backup output stream order synchronize meta data changes backup namespace image journal namenode registration registration j action length byte records throws io exception
779	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeProtocols.java	unrelated	package org apache hadoop hdfs server protocol the full set rpc methods implemented namenode namenode protocols extends client protocol datanode protocol namenode protocol refresh authorization policy protocol refresh user mappings protocol get user mappings protocol
780	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamenodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol information sent subordinate name node active name node registration process namenode registration extends storage info implements node registration string rpc address rpc address node string http address http address node namenode role role node role checkpoint time l age image namenode registration super namenode registration string address string http address storage info storage info namenode role role checkpoint time super rpc address address http address http address set storage info storage info role role checkpoint time checkpoint time string get address return rpc address string get registration id return storage get registration id get version return super get layout version string string return get class get simple name rpc address role get role get name node role namenode role get role return role boolean role namenode role return role equals get age image get checkpoint time return checkpoint time writable writable factories set factory namenode registration new writable factory writable new instance return new namenode registration write data output throws io exception text write string rpc address text write string http address text write string role name super write write long checkpoint time read fields data input throws io exception rpc address text read string http address text read string role namenode role value of text read string super read fields checkpoint time read long
781	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NamespaceInfo.java	pooling	package org apache hadoop hdfs server protocol namespace info returned name node reply data node handshake namespace info extends storage info string build version distributed upgrade version string block pool id id block pool namespace info super build version null namespace info ns id string cluster id string bp id c t du version super fs constants layout version ns id cluster id c t block pool id bp id build version storage get build version distributed upgrade version du version string get build version return build version get distributed upgrade version return distributed upgrade version string get block pool id return block pool id writable register ctor writable factories set factory namespace info new writable factory writable new instance return new namespace info write data output throws io exception deprecated utf write string get build version super write write int get distributed upgrade version writable utils write string block pool id read fields data input throws io exception build version deprecated utf read string super read fields distributed upgrade version read int block pool id writable utils read string string string return super string bpid block pool id
782	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\NodeRegistration.java	unrelated	package org apache hadoop hdfs server protocol generic specifying information need sent name node registration process node registration get address server node string get address get registration id server node string get registration id get layout version server node get version string string
783	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\ReplicaRecoveryInfo.java	unrelated	package org apache hadoop hdfs server protocol replica recovery information replica recovery info extends block replica state original state replica recovery info replica recovery info block id disk len gs replica state r state set block id disk len gs original state r state replica state get original replica state return original state boolean equals object return super equals hash code return super hash code writable register ctor writable factories set factory replica recovery info new writable factory writable new instance return new replica recovery info read fields data input throws io exception super read fields original state replica state read write data output throws io exception super write original state write
784	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\ServerCommand.java	unrelated	package org apache hadoop hdfs server protocol base server command issued name node notify servers done commands defined actions defined respective protocols server command implements writable action unknown server command constructor creates command action server command create command specified action actions protocol specific server command action action action get server command action get action return action writable write data output throws io exception write int action read fields data input throws io exception action read int
785	hdfs\src\java\org\apache\hadoop\hdfs\server\protocol\UpgradeCommand.java	unrelated	package org apache hadoop hdfs server protocol this generic distributed upgrade command during upgrade cluster components send upgrade commands order obtain share information it supposed upgrade defines specific upgrade command deriving the upgrade command contains version upgrade verified receiving side current status upgrade upgrade command extends datanode command uc action unknown datanode protocol dna unknown uc action report status report upgrade status uc action start upgrade start upgrade version short upgrade status upgrade command super uc action unknown version upgrade status upgrade command action version short status super action version version upgrade status status get version return version short get current status return upgrade status writable register ctor writable factories set factory upgrade command new writable factory writable new instance return new upgrade command write data output throws io exception super write write int version write short upgrade status read fields data input throws io exception super read fields version read int upgrade status read short
786	hdfs\src\java\org\apache\hadoop\hdfs\tools\DelegationTokenFetcher.java	unrelated	package org apache hadoop hdfs tools fetch delegation token current namenode store specified file delegation token fetcher log log log factory get log delegation token fetcher string webservice webservice string renewer renewer string cancel cancel string renew renew string print print enable kerberos sockets system set property https cipher suites tls krb with des ede cbc sha print usage print stream err throws io exception err println fetchdt retrieves delegation tokens name node err println err println fetchdt opts token file err println options err println webservice url url contact nn err println renewer name name delegation token renewer err println cancel cancel delegation token err println renew renew delegation token delegation token must fetched using renewer name option err println print print delegation token err println generic options parser print generic command usage err system exit collection token read tokens path file configuration conf throws io exception credentials creds credentials read token storage file file conf return creds get all tokens command line main string args throws exception configuration conf new hdfs configuration options fetcher options new options fetcher options add option webservice true https url reach name node fetcher options add option renewer true name delegation token renewer fetcher options add option cancel false cancel token fetcher options add option renew false renew token fetcher options add option print false print token generic options parser parser new generic options parser conf fetcher options args command line cmd parser get command line get options string web url cmd option webservice cmd get option value webservice null string renewer cmd option renewer cmd get option value renewer null boolean cancel cmd option cancel boolean renew cmd option renew boolean print cmd option print string remaining parser get remaining args check option validity cancel renew cancel print renew print cancel renew print system err println error only specify cancel renew print print usage system err remaining length remaining char at system err println error must specify exacltly one token file print usage system err default using local file system file system local file system get local conf path token file new path local get working directory remaining login current user user group information get current user as new privileged exception action object object run throws exception print delegation token identifier id new delegation token secret manager null create identifier token token read tokens token file conf data input stream new data input stream new byte array input stream token get identifier id read fields system println token id token get service return null web url null renew result token token read tokens token file conf result renew delegation token web url token delegation token identifier token system println renewed token via web url token get service new date result else cancel token token read tokens token file conf cancel delegation token web url token delegation token identifier token system println cancelled token via web url token get service else credentials creds get d tfrom remote web url renewer creds write token storage file token file conf token token creds get all tokens system
787	hdfs\src\java\org\apache\hadoop\hdfs\tools\DFSAdmin.java	pooling	package org apache hadoop hdfs tools this provides dfs administrative access dfs admin extends fs shell hdfs configuration init an execution file system command dfs admin command extends command distributed file system dfs constructor dfs admin command file system fs super fs get conf fs instanceof distributed file system throw new illegal argument exception file system fs get uri distributed file system dfs distributed file system fs a supports command clear quota clear quota command extends dfs admin command string name clr quota string usage name dirname dirname string description usage clear quota directory dir name n for directory attempt clear quota an error reported n directory exist file n user administrator n it fault directory quota constructor clear quota command string args pos file system fs super fs command format c new command format integer max value list string parameters c parse args pos args parameters array new string parameters size check command clr quota command boolean matches string cmd return name equals cmd string get command name return name run path path throws io exception dfs set quota path fs constants quota reset fs constants quota dont set a supports command set quota set quota command extends dfs admin command string name set quota string usage name quota dirname dirname string description set quota quota dirname dirname set quota quota directory dir name n the directory quota integer puts hard limit n ton number names directory tree n for directory attempt set quota an error reported n n positive integer n user administrator n directory exist file n quota quota set constructor set quota command string args pos file system fs super fs command format c new command format integer max value list string parameters c parse args pos quota long parse long parameters remove args parameters array new string parameters size check command set quota command boolean matches string cmd return name equals cmd string get command name return name run path path throws io exception dfs set quota path quota fs constants quota dont set a supports command clear space quota clear space quota command extends dfs admin command string name clr space quota string usage name dirname dirname string description usage clear disk space quota directory dir name n for directory attempt clear quota an error reported n directory exist file n user administrator n it fault directory quota constructor clear space quota command string args pos file system fs super fs command format c new command format integer max value list string parameters c parse args pos args parameters array new string parameters size check command clr quota command boolean matches string cmd return name equals cmd string get command name return name run path path throws io exception dfs set quota path fs constants quota dont set fs constants quota reset a supports command set quota set space quota command extends dfs admin command string name set space quota string usage name quota dirname dirname string description usage set disk space quota quota directory dir name n the space quota integer puts hard limit n
788	hdfs\src\java\org\apache\hadoop\hdfs\tools\DFSck.java	unrelated	package org apache hadoop hdfs tools this provides rudimentary checking dfs volumes errors sub optimal conditions p the tool scans files directories starting indicated root path the following abnormal conditions detected handled p ul li files blocks completely missing datanodes br in case tool perform one following actions ul li none link org apache hadoop hdfs server namenode namenode fsck fixing none li li move corrupted files lost found directory dfs link org apache hadoop hdfs server namenode namenode fsck fixing move remaining data blocks saved block chains representing longest consecutive series valid blocks li li delete corrupted files link org apache hadoop hdfs server namenode namenode fsck fixing delete li ul li li detect files replicated replicated blocks li ul additionally tool collects detailed overall dfs statistics optionally print detailed statistics block locations replication factors file the tool also provides option filter open files scan df sck extends configured implements tool hdfs configuration init user group information ugi print stream filesystem checker df sck configuration conf throws io exception conf system df sck configuration conf print stream throws io exception super conf ugi user group information get current user print fsck usage information print usage system err println usage df sck path list corruptfileblocks move delete openforwrite files blocks locations racks system err println path tstart checking path system err println move tmove corrupted files lost found system err println delete tdelete corrupted files system err println files tprint files checked system err println openforwrite tprint files opened write system err println list corruptfileblocks tprint list missing blocks files belong system err println blocks tprint block report system err println locations tprint locations every block system err println racks tprint network topology data node locations system err println by default fsck ignores files opened write use openforwrite report files they usually tagged corrupt healthy depending block allocation status tool runner print generic command usage system err run string args throws io exception args length print usage return try return user group information get current user as new privileged exception action integer integer run throws exception return work args catch interrupted exception e throw new io exception e to get list need call iteratively server says left integer list corrupt file blocks string dir string base url throws io exception err code num corrupt string last block null string corrupt line corrupt files string more corrupt line corrupt files boolean done false done string buffer url new string buffer base url last block null url append startblockafter append last block url path new url url string security util fetch service ticket path url connection connection path open connection input stream stream connection get input stream buffered reader input new buffered reader new input stream reader stream utf try string line null line input read line null line ends with corrupt line line ends with more corrupt line line ends with namenode fsck nonexistent status done true break line empty line starts with fsck started line starts with the filesystem path continue num corrupt num corrupt println the list corrupt files path dir
789	hdfs\src\java\org\apache\hadoop\hdfs\tools\GetConf.java	unrelated	package org apache hadoop hdfs tools tool getting configuration information configuration file adding options ul li if adding simple option get value corresponding key configuration use regular link get conf command handler see link get conf command exclude file example li li if adding option return value key add subclass link get conf command handler set link get conf command see link get conf command namenode example ul get conf extends configured implements tool string description hdfs getconf utility getting configuration information config file n enum command namenode namenodes new name nodes command handler gets list namenodes cluster secondary secondary name nodes new secondary name nodes command handler gets list secondary namenodes cluster backup backup nodes new backup nodes command handler gets list backup nodes cluster include file file new command handler dfs config keys dfs hosts gets file path defines datanodes join cluster exclude file exclude file new command handler dfs config keys dfs hosts exclude gets exclude file path defines datanodes need decommissioned nnrpcaddresses nn rpc addresses new nn rpc addresses command handler gets namenode rpc addresses string cmd command handler handler string description command string cmd command handler handler string description cmd cmd handler handler description description string get name return cmd string get description return description command handler get handler string name command cmd values cmd get name equals ignore case name return cmd handler return null string usage hdfs configuration init initialize usage based command values string builder usage new string builder description usage append nhadoop getconf n command cmd command values usage append cmd get name cmd get description n usage usage string handler return value key corresponding link command command handler string key configuration key lookup command handler null command handler string key key key work get conf tool try return work internal tool catch exception e tool print error e get message return method overridden sub specific behavior work internal get conf tool throws exception string value tool get conf get key value null tool print out value return tool print error configuration key missing return handler link command namenode name nodes command handler extends command handler work internal get conf tool throws io exception tool print list dfs util get nn service rpc addresses tool get conf return handler link command backup backup nodes command handler extends command handler work internal get conf tool throws io exception tool print list dfs util get backup node addresses tool get conf return handler link command secondary secondary name nodes command handler extends command handler work internal get conf tool throws io exception tool print list dfs util get secondary name node addresses tool get conf return handler link command nnrpcaddresses if rpc addresses defined configuration return otherwise return empty nn rpc addresses command handler extends command handler work internal get conf tool throws io exception configuration config tool get conf list inet socket address rpclist dfs util get nn service rpc addresses config rpclist null inet socket address rpc rpclist tool print out rpc get host name rpc get port return tool print error did
790	hdfs\src\java\org\apache\hadoop\hdfs\tools\GetGroups.java	unrelated	package org apache hadoop hdfs tools hdfs implementation tool getting groups given user belongs get groups extends get groups base hdfs configuration init get groups configuration conf super conf get groups configuration conf print stream super conf protected inet socket address get protocol address configuration conf throws io exception return name node get address conf main string argv throws exception res tool runner run new get groups new hdfs configuration argv system exit res
791	hdfs\src\java\org\apache\hadoop\hdfs\tools\HDFSConcat.java	unrelated	package org apache hadoop hdfs tools hdfs concat string def uri hdfs localhost main string args throws io exception args length system err println usage hdfs concat target srcs system exit configuration conf new configuration string uri conf get fs default name def uri path path new path uri distributed file system dfs distributed file system file system get path uri conf path srcs new path args length args length srcs new path args dfs concat new path args srcs
792	hdfs\src\java\org\apache\hadoop\hdfs\tools\JMXGet.java	unrelated	package org apache hadoop hdfs tools tool get data name node data node using m beans currently following m beans available hadoop domain hadoop service name node name fs namesystem state hadoop service name node name name node activity dynamic hadoop service name node name rpc activity for port dynamic hadoop service data node name rpc activity for port dynamic hadoop name service data node fs dataset state undefined storage id hadoop service data node name data node activity undefined storage id dynamic implementation note logging sent system err since command line tool jmx get string format n array list object name hadoop object names m bean server connection mbsc string service name node port server localhost string local vm url null jmx get set service string service service service set port string port port port set server string server server server set local vm url string url local vm url url print attributes values print all values throws exception err list available keys object val null object name oname hadoop object names err jmx name oname get canonical key property list string m bean info mbinfo mbsc get m bean info oname m bean attribute info mbinfos mbinfo get attributes m bean attribute info mb mbinfos val mbsc get attribute oname mb get name system format format mb get name val null val string get single value key string get value string key throws exception object val null object name oname hadoop object names try val mbsc get attribute oname key catch attribute not found exception anfe go next continue catch reflection exception get cause instanceof no such method exception continue err info key key val val get class val break return val null val string initializes m bean server init throws exception err init server server port port service service local vm url local vm url string url null build connection url local vm url null use jstat snap vmpid grep sun management jmx connector server address get url url local vm url err url local pid local vm url url else port empty server empty using server port url service jmx rmi jndi rmi server port jmxrmi else url stays null create rmi connector client connect rmi connector server url null assume local vm example testing mbsc management factory get platform m bean server else jmx service url url new jmx service url url err create rmi connector connect rmi connector server url jmx connector jmxc jmx connector factory connect url null get m bean server connection err n get m bean server connection mbsc jmxc get m bean server connection get domains m bean server err n domains string domains mbsc get domains arrays sort domains string domain domains err domain domain get m bean server default domain err n m bean server default domain mbsc get default domain get m bean count err n m bean count mbsc get m bean count query m bean names specific domain hadoop service object name query new object name hadoop service service hadoop object names new array list object name
793	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\BinaryEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer binary edits visitor implements binary edits visitor binary edits visitor extends edits visitor data output stream create processor writes given file reads using given tokenizer binary edits visitor string filename tokenizer tokenizer throws io exception filename tokenizer false create processor writes given file reads using given tokenizer may also print screen binary edits visitor string filename tokenizer tokenizer boolean print to screen throws io exception super tokenizer new data output stream new file output stream filename start visitor initialization start throws io exception nothing binary format finish visitor finish throws io exception close finish visitor indicate error finish abnormally throws io exception system err println error processing edit log file exiting close close output stream prevent writing close throws io exception close visit enclosing element element elements visit enclosing element tokenizer token value throws io exception nothing binary format end eclosing element leave enclosing element throws io exception nothing binary format visit token tokenizer token visit tokenizer token value throws io exception value binary return value
794	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\BinaryTokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer reads tokens binary file binary tokenizer implements tokenizer data input stream binary tokenizer constructor binary tokenizer string filename throws file not found exception new data input stream new file input stream filename binary tokenizer constructor binary tokenizer data input stream throws io exception token read token throws io exception binary return
795	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsElement.java	unrelated	package org apache hadoop hdfs tools offline edits viewer structural elements edit log may encountered within file edits visitor able process elements enum edits element edits edits version record opcode data elements data part edit log records length op set genstamp generation stamp op add op close path replication mtime atime blocksize numblocks block block id block num bytes block generation stamp permission status fs permissions client name client machine op rename old source destination timestamp op set owner username groupname op set quota ns quota ds quota op rename rename options op concat delete concat target concat source op get delegation token t version t owner t renewer t real user t issue date t max date t sequence number t master key id t expiry time op update master key key id key expiry date key length key blob checksum
796	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsLoader.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an edits loader read hadoop edit log file walk structure using supplied edits visitor each implementation edits loader designed rapidly process edits log file as minor changes made one layout version another acceptable tweak one implementation read next however layout version changes enough would make processor slow difficult read another processor created this allows processor quickly read edits log without getting bogged dealing significant differences layout versions edits loader loads edits file load edits throws io exception can processor handle specified version edit log file boolean load version version factory obtaining version edits log loader read particular edits log format loader factory java support methods interfaces necessitates factory create edits log loader point one might need add later edits loader get loader edits visitor v return new edits loader current v
797	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsLoaderCurrent.java	unrelated	package org apache hadoop hdfs tools offline edits viewer edits loader current processes hadoop edit logs files walks using provided edits visitor calling visitor element enumerated edits loader current implements edits loader supported versions edits visitor v edits version constructor edits loader current edits visitor visitor v visitor checks edits loader load given version edits boolean load version version v supported versions v version return true return false visit op invalid visit op invalid throws io exception nothing op code data visit op add visit op add throws io exception visit op add op close fs edit log op codes op add visit op close visit op close throws io exception visit op add op close fs edit log op codes op close visit op add op close almost visit op add op close fs edit log op codes edits op code throws io exception int token op add length v visit int edits element length happens edits properly ended op code padded end zeros op add zero without check would treat zeros empty op add op add length value throw new io exception op code edits op code zero length corrupted edits v visit string utf edits element path v visit string utf edits element replication v visit string utf edits element mtime v visit string utf edits element atime v visit string utf edits element blocksize read blocks int token num blocks token v visit int edits element numblocks num blocks token value v visit enclosing element edits element block v visit long edits element block id v visit long edits element block num bytes v visit long edits element block generation stamp v leave enclosing element permission status v visit enclosing element edits element permission status v visit string text edits element username v visit string text edits element groupname v visit short edits element fs permissions v leave enclosing element edits op code fs edit log op codes op add v visit string utf edits element client name v visit string utf edits element client machine visit op rename old visit op rename old throws io exception v visit int edits element length v visit string utf edits element source v visit string utf edits element destination v visit string utf edits element timestamp visit op delete visit op delete throws io exception v visit int edits element length v visit string utf edits element path v visit string utf edits element timestamp visit op mkdir visit op mkdir throws io exception v visit int edits element length v visit string utf edits element path v visit string utf edits element timestamp v visit string utf edits element atime permission status v visit enclosing element edits element permission status v visit string text edits element username v visit string text edits element groupname v visit short edits element fs permissions v leave enclosing element visit op set replication visit op set replication throws io exception v visit string utf edits element path v visit string utf edits element replication visit op set permissions visit op set permissions throws
798	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an implementation edits visitor traverse structure hadoop edits log respond structures within file edits visitor tokenizer tokenizer edits visitor tokenizer tokenizer tokenizer tokenizer begin visiting edits log structure opportunity perform initialization necessary implementing visitor start throws io exception finish visiting edits log structure opportunity perform clean necessary implementing visitor finish throws io exception finish visiting edits log structure error occurred processing opportunity perform clean necessary implementing visitor finish abnormally throws io exception visit non enclosing element edits log specified value tokenizer token visit tokenizer token value throws io exception convenience shortcut method parse specific token type byte token visit byte edits element e throws io exception return byte token visit tokenizer read new byte token e convenience shortcut method parse specific token type short token visit short edits element e throws io exception return short token visit tokenizer read new short token e convenience shortcut method parse specific token type int token visit int edits element e throws io exception return int token visit tokenizer read new int token e convenience shortcut method parse specific token type v int token visit v int edits element e throws io exception return v int token visit tokenizer read new v int token e convenience shortcut method parse specific token type long token visit long edits element e throws io exception return long token visit tokenizer read new long token e convenience shortcut method parse specific token type v long token visit v long edits element e throws io exception return v long token visit tokenizer read new v long token e convenience shortcut method parse specific token type string utf token visit string utf edits element e throws io exception return string utf token visit tokenizer read new string utf token e convenience shortcut method parse specific token type string text token visit string text edits element e throws io exception return string text token visit tokenizer read new string text token e convenience shortcut method parse specific token type blob token visit blob edits element e length throws io exception return blob token visit tokenizer read new blob token e length convenience shortcut method parse specific token type bytes writable token visit bytes writable edits element e throws io exception return bytes writable token visit tokenizer read new bytes writable token e convenience shortcut method parse specific token type empty token visit empty edits element e throws io exception return empty token visit tokenizer read new empty token e begin visiting element encloses another element beginning list blocks comprise file visit enclosing element tokenizer token value throws io exception convenience shortcut method virutally always uses empty token visit enclosing element edits element e throws io exception visit enclosing element tokenizer read new empty token e leave current enclosing element called instance end processing blocks compromise file leave enclosing element throws io exception
799	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\EditsVisitorFactory.java	unrelated	package org apache hadoop hdfs tools offline edits viewer edits visitor factory different implementations edits visitor edits visitor factory factory function creates edits visitor object edits visitor get edits visitor string filename string processor tokenizer tokenizer boolean print to screen throws io exception processor lower case equals xml return new xml edits visitor filename tokenizer print to screen else processor lower case equals stats return new statistics edits visitor filename tokenizer print to screen else processor lower case equals binary return new binary edits visitor filename tokenizer print to screen else throw new io exception unknown proccesor processor valid processors xml binary stats
800	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\OfflineEditsViewer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer this implements offline edits viewer tool used view edit logs offline edits viewer extends configured implements tool edits loader edits loader string default processor xml set edits loader set edits loader edits loader edits loader edits loader edits loader process edit log file go edits visitor visitor throws io exception set edits loader edits loader loader factory get loader visitor edits loader load edits print help print help string summary usage bin hdfs oev options input file output file n offline edits viewer n parse hadoop edits log file input file save results n output file n required command line arguments n input file arg edits file process xml case n insensitive extension means xml format n filename means binary format n output file arg name output file if specified n file exists overwritten n format file determined n p option n n optional command line arguments n p processor arg select type processor apply n image file currently supported n processors binary native binary format n hadoop uses xml default xml n format stats prints statistics n edits file n help display usage information exit n v verbose more verbose output prints input n output filenames processors write n file also output screen on large n image files dramatically increase n processing time default false n system println summary system println tool runner print generic command usage system build command line options descriptions options build options options options new options build output file arguments required add option method specify option builder required option builder args option builder long opt output filename options add option option builder create option builder required option builder args option builder long opt input filename options add option option builder create options add option p processor true options add option v verbose false options add option help false return options main entry point tool runner see tool runner docs run string argv throws exception exit code options options build options argv length print help return command line parser parser new posix parser command line cmd try cmd parser parse options argv catch parse exception e system println error parsing command line options e get message print help return cmd option print help exit print help return boolean print to screen false string input filename arg cmd get option value string output filename arg cmd get option value string processor cmd get option value p processor null processor default processor cmd option v print output screen print to screen true system println input input filename arg system println output output filename arg try go edits visitor factory get edits visitor output filename arg processor tokenizer factory get tokenizer input filename arg print to screen catch eof exception e system err println input file ended unexpectedly exiting catch io exception e system err println encountered exception exiting e get message return exit code main runs offline edits viewer using tool runner main string argv throws exception res tool runner run new offline edits viewer argv system exit res
801	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\StatisticsEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer statistics edits visitor implements text version edits visitor aggregates counts op codes processed statistics edits visitor extends edits visitor boolean print to screen false boolean ok to write false file writer fw map fs edit log op codes long op code count new hash map fs edit log op codes long create processor writes file named statistics edits visitor string filename tokenizer tokenizer throws io exception filename tokenizer false create processor writes file named may may also output screen specified statistics edits visitor string filename tokenizer tokenizer boolean print to screen throws io exception super tokenizer print to screen print to screen fw new file writer filename ok to write true start visitor initialization start throws io exception nothing non javadoc finish throws io exception write get statistics string close non javadoc finish abnormally throws io exception close close output stream prevent writing close throws io exception fw close ok to write false visit enclosing element element elements visit enclosing element tokenizer token value throws io exception nothing end eclosing element leave enclosing element throws io exception nothing visit token calculate statistics tokenizer token visit tokenizer token value throws io exception count op codes value get edits element edits element opcode value instanceof tokenizer byte token increment op code count fs edit log op codes byte tokenizer byte token value value else throw new io exception token edits element opcode type tokenizer byte token value get class return value write parameter output file possibly screen protected write string write throws io exception ok to write throw new io exception file open writing print to screen system print write try fw write write catch io exception e ok to write false throw e increment op code counter increment op code count fs edit log op codes op code op code count contains key op code op code count put op code l long new value op code count get op code op code count put op code new value get statistics map fs edit log op codes long get statistics return op code count get statistics format suitable printing string get statistics string string buffer sb new string buffer fs edit log op codes op code fs edit log op codes values sb append string format n op code op code get op code op code count get op code return sb string
802	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\TextEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer text edits visitor implements text version edits visitor text edits visitor extends edits visitor boolean print to screen false boolean ok to write false file writer fw create processor writes file named text edits visitor string filename tokenizer tokenizer throws io exception filename tokenizer false create processor writes file named may may also output screen specified text edits visitor string filename tokenizer tokenizer boolean print to screen throws io exception super tokenizer print to screen print to screen fw new file writer filename ok to write true non javadoc finish throws io exception close non javadoc finish abnormally throws io exception close close output stream prevent writing close throws io exception fw close ok to write false write parameter output file possibly screen protected write string write throws io exception ok to write throw new io exception file open writing print to screen system print write try fw write write catch io exception e ok to write false throw e
803	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\Tokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer hides details different input formats tokenizer abstract token derive tokens needed types token edits element e constructor token edits element e e e edits element accessor edits element get edits element return e creates token string string throws io exception creates token binary stream binary data input stream throws io exception converts token string string writes token value binary format binary data output stream throws io exception byte token extends token byte value byte token edits element e super e string string throws io exception value byte value of binary data input stream throws io exception value read byte string string return byte string value binary data output stream throws io exception write byte value short token extends token short value short token edits element e super e string string throws io exception value short parse short binary data input stream throws io exception value read short string string return short string value binary data output stream throws io exception write short value int token extends token value int token edits element e super e string string throws io exception value integer parse int binary data input stream throws io exception value read int string string return integer string value binary data output stream throws io exception write int value v int token extends token value v int token edits element e super e string string throws io exception value integer parse int binary data input stream throws io exception value writable utils read v int string string return integer string value binary data output stream throws io exception writable utils write v int value long token extends token value long token edits element e super e string string throws io exception value long parse long binary data input stream throws io exception value read long string string return long string value binary data output stream throws io exception write long value v long token extends token value v long token edits element e super e string string throws io exception value long parse long binary data input stream throws io exception value writable utils read v long string string return long string value binary data output stream throws io exception writable utils write v long value string utf token extends token string value string utf token edits element e super e string string throws io exception value binary data input stream throws io exception value deprecated utf read string string string return value binary data output stream throws io exception deprecated utf write string value string text token extends token string value string text token edits element e super e string string throws io exception value binary data input stream throws io exception value text read string string string return value binary data output stream throws io exception text write string value blob token extends token byte value null blob token edits element e length super e value length null new byte length string string throws io exception value base decode base binary data input stream throws io exception read fully
804	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\TokenizerFactory.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer factory different implementations tokenizer tokenizer factory factory function creates tokenizer object input format set based filename xml xml otherwise binary tokenizer get tokenizer string filename throws io exception filename lower case ends with xml return new xml tokenizer filename else return new binary tokenizer filename
805	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\XmlEditsVisitor.java	unrelated	package org apache hadoop hdfs tools offline edits viewer an xml edits visitor walks edit log structure writes equivalent xml document contains edit log components xml edits visitor extends text edits visitor linked list edits element tag q new linked list edits element depth counter depth counter new depth counter create processor writes file named may may also output screen specified xml edits visitor string filename tokenizer tokenizer throws io exception super filename tokenizer false create processor writes file named may may also output screen specified xml edits visitor string filename tokenizer tokenizer boolean print to screen throws io exception super filename tokenizer print to screen start visitor initialization start throws io exception write xml version n finish visitor finish throws io exception super finish finish error finish abnormally throws io exception write n error processing edit log file exiting n super finish abnormally visit token tokenizer token visit tokenizer token value throws io exception write tag value get edits element string value string return value visit enclosing element element cntains elements visit enclosing element tokenizer token value throws io exception print indents write value get edits element string n tag q push value get edits element depth counter inc level leave enclosing element leave enclosing element throws io exception depth counter dec level tag q size throw new io exception tried exit non existent enclosing element edit log file edits element element tag q pop print indents write element string n write xml tag write tag string tag string value throws io exception print indents value length write tag value tag n else write tag n prepared values print indents likely use string indents prints leading spaces based depth level print indents throws io exception try write indents depth counter get level catch index out of bounds exception e unlikely needed slow depth counter get level write
806	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineEditsViewer\XmlTokenizer.java	unrelated	package org apache hadoop hdfs tools offline edits viewer tokenizer reads tokens xml file xml tokenizer implements tokenizer file input stream null xml stream reader xml tokenizer constructor xml tokenizer string filename throws io exception xml input factory f xml input factory new instance file input stream null try new file input stream filename f create xml stream reader catch xml stream exception e null close throw new io exception cannot create xml stream e catch file not found exception e null close throw new io exception cannot open input file filename e get next element value checks element name wanted name string get next elements value string wanted name throws io exception boolean got start element false try event type get event type true switch event type case xml stream constants characters got start element xml returns n instead empty zero length elements like x x return get text trim break case xml stream constants end document throw new io exception end xml looking element wanted name break case xml stream constants start element got start element throw new io exception start element get name event expecting characters event wanted name else get name string equals wanted name got start element true else throw new io exception unexpected element name get name expecting wanted name break case xml stream constants comment case xml stream constants end element case xml stream constants space case xml stream constants start document xml need break never appear edits xml case xml stream constants attribute case xml stream constants cdata case xml stream constants dtd case xml stream constants entity declaration case xml stream constants entity reference case xml stream constants namespace case xml stream constants notation declaration case xml stream constants processing instruction default throw new io exception unsupported event type event type see xml stream constants next break event type next catch xml stream exception e throw new io exception error reading xml stream e throw new io exception error reading xml stream never reach line likely xml elements loking token read token throws io exception string get next elements value get edits element string return
807	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\DelimitedImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer a delimited image visitor generates text representation fsimage element separated delimiter all elements common inodes inodes construction included when processing fsimage layout version element access time output file column value value included individual block information file currently included the default delimiter tab unlikely value included inode path text metadata the delimiter value via constructor delimited image visitor extends text writer image visitor string default delimiter linked list image element elem q new linked list image element file size elements fsimage interested tracking collection image element elements to track values elements elements to track abstract map image element string elements new hash map image element string string delimiter elements to track new array list image element this collection determines elements tracked order output collections add all elements to track image element inode path image element replication image element modification time image element access time image element block size image element num blocks image element num bytes image element ns quota image element ds quota image element permission string image element user name image element group name delimited image visitor string filename throws io exception filename false delimited image visitor string output file boolean print to screen throws io exception output file print to screen default delimiter delimited image visitor string output file boolean print to screen string delimiter throws io exception super output file print to screen delimiter delimiter reset reset values elements tracking order handle next file reset elements clear image element e elements to track elements put e null file size leave enclosing element throws io exception image element elem elem q pop if done inode write results start elem image element inode elem image element inode under construction write line write n reset iterate elements tracking value recorded write write line throws io exception iterator image element elements to track iterator next image element e next string v null e image element num bytes v string value of file size else v elements get e v null write v next write delimiter visit image element element string value throws io exception explicitly label root path element image element inode path value equals value special case file size sum num bytes block element image element num bytes file size long value of value elements contains key element element image element num bytes elements put element value visit enclosing element image element element throws io exception elem q push element visit enclosing element image element element image element key string value throws io exception special case num blocks attribute blocks element key image element num blocks elements contains key image element num blocks elements put key value elem q push element start throws io exception nothing
808	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\DepthCounter.java	unrelated	package org apache hadoop hdfs tools offline image viewer utility tracking descent structure visitor image visitor edits visitor etc depth counter depth inc level depth dec level depth depth get level return depth
809	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\FileDistributionVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer file size distribution visitor description this tool analyzing file sizes namespace image in order run tool one define range integers tt max size tt specifying tt max size tt tt step tt the range integers divided segments size tt step tt tt sub sub sub n sub max size tt visitor calculates many files system fall segment tt sub sub sub sub tt note files larger tt max size tt always fall last segment input ul li tt filename tt specifies location image file li li tt max size tt determines range tt max size tt files sizes considered visitor li li tt step tt range divided segments size step li ul output the output file formatted tab separated two column table size num files where size represents start segment num files number files form image size falls segment file distribution visitor extends text writer image visitor linked list image element elem s new linked list image element max size default x l tb interval default x mb distribution max size step total files total directories total blocks total space max file size file context current boolean inode false file directory information file context string path file size num blocks replication file distribution visitor string filename max size step throws io exception super filename false max size max size max size default max size step step interval default step num intervals max size step num intervals integer max value throw new io exception too many distribution intervals num intervals distribution new num intervals total files total directories total blocks total space max file size start throws io exception finish throws io exception write distribution output file write size num files n distribution length write step distribution n system println total files total files system println total directories total directories system println total blocks total blocks system println total space total space system println max file size max file size super finish leave enclosing element throws io exception image element elem elem s pop elem image element inode elem image element inode under construction return inode false current num blocks total directories return total files total blocks current num blocks total space current file size current replication max file size current file size max file size current file size high current file size max size high distribution length else high math ceil current file size step distribution high total files system println files processed total files current current path visit image element element string value throws io exception inode switch element case inode path current path value equals value break case replication current replication integer value of value break case num bytes current file size long value of value break default break visit enclosing element image element element throws io exception elem s push element element image element inode element image element inode under construction current new file context inode true visit enclosing element image element element image element key string value throws io exception elem s push element element image element inode element
810	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageLoader.java	unrelated	package org apache hadoop hdfs tools offline image viewer an image loader accept data input stream hadoop fs image file walk structure using supplied image visitor each implementation image loader designed rapidly process image file as minor changes made one layout version another acceptable tweak one implementation read next however layout version changes enough would make processor slow difficult read another processor created this allows processor quickly read image without getting bogged dealing significant differences layout versions image loader load image data input stream image visitor v boolean enumerate blocks throws io exception can processor handle specified version fs image file boolean load version version factory obtaining version image loader read particular image format loader factory java support methods interfaces necessitates factory find image loader capable interpreting specified layout version number if none return null image loader get loader version easy add image processors written image loader loaders new image loader current image loader loaders load version version return return null
811	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageLoaderCurrent.java	unrelated	package org apache hadoop hdfs tools offline image viewer image loader current processes hadoop fs image files walks using provided image visitor calling visitor element enumerated the difference v v utilization stickybit therefore viewer reader either format versions fsimage layout changes image version namepsace id num files generation stamp i nodes count num files i node path string replication short modification time date access time added block size num blocks blocks count num blocks block block id num bytes generation stamp namespace quota diskspace quota added permissions username string groupname string octal perms short string modified symlink string added num i nodes under construction i nodes under construction count num i nodes under construction i node under construction path bytes replication short modification time date preferred block size num blocks blocks block block id num bytes generation stamp permissions username string groupname string octal perms short string client name string client machine string num locations datanode descriptors count num locations loaded memory short still file enum current delegation key id num delegation keys delegation keys count num delegation keys delegation key length vint delegation key bytes delegation token sequence number num delegation tokens delegation tokens count num delegation tokens delegation token identifier owner string renewer string real user string issue date vlong max date vlong sequence number vint master key id vint expiry time image loader current implements image loader protected date format date format new simple date format yyyy mm dd hh mm versions image version non javadoc boolean load version version v versions v version return true return false non javadoc load image data input stream image visitor v boolean skip blocks throws io exception try v start v visit enclosing element image element fs image image version read int load version image version throw new io exception cannot process fslayout version image version v visit image element image version image version v visit image element namespace id read int num inodes read long v visit image element generation stamp read long layout version supports feature fsimage compression image version boolean compressed read boolean v visit image element is compressed image version compressed string codec class name text read string v visit image element compress codec codec class name compression codec factory codec fac new compression codec factory new configuration compression codec codec codec fac get codec by class name codec class name codec null throw new io exception image compression codec supported codec class name new data input stream codec create input stream process i nodes v num inodes skip blocks process i nodes uc v skip blocks layout version supports feature delegation token image version process delegation tokens v v leave enclosing element fs image v finish catch io exception e tell visitor clean throw exception v finish abnormally throw e process delegation token related section fsimage process delegation tokens data input stream image visitor v throws io exception v visit image element current delegation key id read int num d keys read int v visit enclosing element image element delegation keys image element num delegation keys num
812	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\ImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer an implementation image visitor traverse structure hadoop fsimage respond structures within file image visitor structural elements fs image may encountered within file image visitors able handle processing elements enum image element fs image image version namespace id is compressed compress codec layout version num inodes generation stamp inodes inode inode path replication modification time access time block size num blocks blocks block block id num bytes ns quota ds quota permissions symlink num inodes under construction inodes under construction inode under construction preferred block size client name client machine user name group name permission string current delegation key id num delegation keys delegation keys delegation key delegation token sequence number num delegation tokens delegation tokens delegation token identifier delegation token expiry time begin visiting fsimage structure opportunity perform initialization necessary implementing visitor start throws io exception finish visiting fsimage structure opportunity perform clean necessary implementing visitor finish throws io exception finish visiting fsimage structure error occurred processing opportunity perform clean necessary implementing visitor finish abnormally throws io exception visit non enclosing element fsimage specified value visit image element element string value throws io exception convenience methods automatically convert numeric value types strings visit image element element value throws io exception visit element integer string value visit image element element value throws io exception visit element long string value begin visiting element encloses another element beginning list blocks comprise file visit enclosing element image element element throws io exception begin visiting element encloses another element beginning list blocks comprise file also provide additional key value element number items within element visit enclosing element image element element image element key string value throws io exception convenience methods automatically convert value types strings visit enclosing element image element element image element key value throws io exception visit enclosing element element key integer string value visit enclosing element image element element image element key value throws io exception visit enclosing element element key long string value leave current enclosing element called instance end processing blocks compromise file leave enclosing element throws io exception
813	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\IndentedImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer indented image visitor walks fs image displays structure using indenting organize sections within image file indented image visitor extends text writer image visitor indented image visitor string filename throws io exception super filename indented image visitor string filename boolean print to screen throws io exception super filename print to screen depth counter dc new depth counter track leading spacing start throws io exception finish throws io exception super finish finish abnormally throws io exception system println image processing finished abnormally ending super finish abnormally leave enclosing element throws io exception dc dec level visit image element element string value throws io exception print indents write element value n visit enclosing element image element element throws io exception print indents write element n dc inc level print element along associated key value pair brackets visit enclosing element image element element image element key string value throws io exception print indents write element key value n dc inc level print appropriate number spaces current level fs images potentially millions lines caching significantly speed output string indents print indents throws io exception try write indents dc get level catch index out of bounds exception e there reason fsimage would need deeper indent dc get level write
814	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\LsImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer ls image visitor displays blocks namespace format similar output ls lsr entries marked directories permissions listed replication username groupname along size modification date full path note a significant difference output lsr command image visitor cannot sort file entries listed order stored within fsimage file therefore output cannot directly compared output lsr command ls image visitor extends text writer image visitor linked list image element elem q new linked list image element num blocks string perms replication string username string group filesize string mod time string path string link target boolean inode false string builder sb new string builder formatter formatter new formatter sb ls image visitor string filename throws io exception super filename ls image visitor string filename boolean print to screen throws io exception super filename print to screen start new line output reset values new line num blocks perms username group path link target filesize replication inode true all values gathered print console ls style format width repl width user width group width size width mod string ls str width repl width user width group width size width mod print line throws io exception sb append num blocks sb append perms link target length path path link target formatter format ls str replication replication username group filesize mod time path sb append n write sb string sb set length clear builder inode false start throws io exception finish throws io exception super finish finish abnormally throws io exception system println input ended unexpectedly super finish abnormally leave enclosing element throws io exception image element elem elem q pop elem image element inode print line maintain state location within image tree record values needed display inode ls style format visit image element element string value throws io exception inode switch element case inode path value equals path else path value break case permission string perms value break case replication replication integer value of value break case user name username value break case group name group value break case num bytes filesize long value of value break case modification time mod time value break case symlink link target value break default this ok we looking values break visit enclosing element image element element throws io exception elem q push element element image element inode new line visit enclosing element image element element image element key string value throws io exception elem q push element element image element inode new line else element image element blocks num blocks integer value of value
815	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\NameDistributionVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer file name distribution visitor p it analyzes file names fsimage prints following information li number unique file names li li number file names corresponding number range files use names li li heap saved file name objects reused li name distribution visitor extends text writer image visitor hash map string integer counts new hash map string integer name distribution visitor string filename boolean print to screen throws io exception super filename print to screen finish throws io exception bytearray overhead write total unique file names counts size columns frequency file occurrence savings heap total files using name number file names stats highbound integer min value entry string integer entry counts entry set highbound math max highbound entry get value stats length entry get value stats stats bytearray overhead entry get key length entry get value stats entry get value stats break lowbound totalsavings stat stats lowbound stat totalsavings stat string range lowbound highbound lowbound lowbound highbound write n stat names used stat files range times heap savings stat bytes highbound stat write n n total saved heap totalsavings bytes n super finish visit image element element string value throws io exception element image element inode path string filename value substring value last index of counts contains key filename counts put filename counts get filename else counts put filename leave enclosing element throws io exception start throws io exception visit enclosing element image element element throws io exception visit enclosing element image element element image element key string value throws io exception
816	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\OfflineImageViewer.java	unrelated	package org apache hadoop hdfs tools offline image viewer offline image viewer dump contents hadoop image file xml console main entry point utility either via command line programatically offline image viewer string usage usage bin hdfs oiv options inputfile outputfile n offline image viewer n view hadoop fsimage inputfile using specified processor n saving results outputfile n n the oiv utility attempt parse correctly formed image files n abort fail mal formed image files n n the tool works offline require running cluster n order process image file n n the following image processors available n ls the default image processor generates lsr style listing n files namespace fields n order note order correctly determine file sizes n formatter cannot skip blocks n skip blocks option n indented this processor enumerates elements n fsimage file using levels indentation delineate n sections within file n delimited generate text file elements common n inodes inodes construction separated n delimiter the default delimiter u though may n changed via delimiter argument this processor also overrides n skip blocks option reason ls processor n xml this processor creates xml document elements n fsimage enumerated suitable analysis xml n tools n file distribution this processor analyzes file size n distribution image n max size specifies range max size file sizes n analyzed gb default n step defines granularity distribution mb default n name distribution this processor analyzes file names n image prints total number file names frequently file names reused n n required command line arguments n input file arg fs image file process n output file arg name output file if specified n file exists overwritten n n optional command line arguments n p processor arg select type processor apply n image file ls xml delimited indented file distribution n help display usage information exit n print to screen for processors write file also n output screen on large image files n dramatically increase processing time n skip blocks skip inodes blocks information may n significantly decrease output n default false n delimiter arg delimiting use delimited processor n boolean skip blocks string input file image visitor processor offline image viewer string input file image visitor processor boolean skip blocks input file input file processor processor skip blocks skip blocks process image file go throws io exception data input stream null try new data input stream new buffered input stream new file input stream new file input file image version file find image version image loader fsip image loader loader factory get loader image version file fsip null throw new io exception no image processor read version image version file available fsip load image processor skip blocks finally null close check fsimage datainputstream version number the datainput stream returned point passed method effect datainputstream read pointer find image version data input stream throws io exception mark arbitrary amount resetting immediately version read int reset return version build command line options descriptions options build options options options new options build output file arguments required add option method specify option builder required option builder args option builder long
817	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\TextWriterImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer text writer image processor mixes ability image visitor implementations easily write output text file implementing sure call super methods constructors finish finish abnormally methods order underlying file may opened closed correctly note add newlines text written file enabled screen this implementing responsibility text writer image visitor extends image visitor boolean print to screen false boolean ok to write false file writer fw create processor writes file named text writer image visitor string filename throws io exception filename false create processor writes file named may may also output screen specified text writer image visitor string filename boolean print to screen throws io exception super print to screen print to screen fw new file writer filename ok to write true non javadoc finish throws io exception close non javadoc finish abnormally throws io exception close close output stream prevent writing close throws io exception fw close ok to write false write parameter output file possibly screen protected write string write throws io exception ok to write throw new io exception file open writing print to screen system print write try fw write write catch io exception e ok to write false throw e
818	hdfs\src\java\org\apache\hadoop\hdfs\tools\offlineImageViewer\XmlImageVisitor.java	unrelated	package org apache hadoop hdfs tools offline image viewer an xml image visitor walks fsimage structure writes equivalent xml document contains fsimage components xml image visitor extends text writer image visitor linked list image element tag q new linked list image element xml image visitor string filename throws io exception super filename false xml image visitor string filename boolean print to screen throws io exception super filename print to screen finish throws io exception super finish finish abnormally throws io exception write n error processing image file exiting n super finish abnormally leave enclosing element throws io exception tag q size throw new io exception tried exit non existent enclosing element fs image file image element element tag q pop write element string n start throws io exception write xml version n visit image element element string value throws io exception write tag element string value visit enclosing element image element element throws io exception write element string n tag q push element visit enclosing element image element element image element key string value throws io exception write element string key value n tag q push element write tag string tag string value throws io exception write tag value tag n
819	hdfs\src\java\org\apache\hadoop\hdfs\util\ByteArray.java	unrelated	package org apache hadoop hdfs util wrapper byte use byte key hash map byte array hash cache hash code byte bytes byte array byte bytes bytes bytes byte get bytes return bytes hash code hash hash arrays hash code bytes return hash boolean equals object instanceof byte array return false return arrays equals bytes byte array bytes
820	hdfs\src\java\org\apache\hadoop\hdfs\util\ByteBufferOutputStream.java	unrelated	package org apache hadoop hdfs util output stream writes link byte buffer byte buffer output stream extends output stream byte buffer buf byte buffer output stream byte buffer buf buf buf write b throws io exception buf put byte b write byte b len throws io exception buf put b len
821	hdfs\src\java\org\apache\hadoop\hdfs\util\DataTransferThrottler.java	unrelated	package org apache hadoop hdfs util throttle data transfers this thread safe it shared multiple threads the parameter bandwidth per sec specifies total bandwidth shared threads data transfer throttler period period bw imposed period extension max period bw accumulates bytes per period total number bytes sent period cur period start current period starting time cur reserve remaining bytes sent period bytes already used constructor data transfer throttler bandwidth per sec bandwidth per sec default throttling period ms constructor period data transfer throttler period bandwidth per sec cur period start system current time millis period period cur reserve bytes per period bandwidth per sec period period extension period synchronized get bandwidth return bytes per period period sets throttle bandwidth this takes affect latest end current period synchronized set bandwidth bytes per second bytes per second throw new illegal argument exception bytes per second bytes per period bytes per second period given num of bytes sent received since last time throttle called make current thread sleep i o rate fast compared given bandwidth number bytes sent received since last time throttle called synchronized throttle num of bytes num of bytes return cur reserve num of bytes bytes already used num of bytes cur reserve system current time millis cur period end cur period start period cur period end wait next period cur reserve increased try wait cur period end catch interrupted exception ignored else cur period start period extension cur period start cur period end cur reserve bytes per period else discard prev period throttler might used time cur period start cur reserve bytes per period bytes already used bytes already used num of bytes
822	hdfs\src\java\org\apache\hadoop\hdfs\util\ExactSizeInputStream.java	unrelated	package org apache hadoop hdfs util an input stream implementations reads input stream expects exact number bytes any attempts read past specified number bytes return end stream reached if end underlying stream reached prior specified number bytes eof exception thrown exact size input stream extends filter input stream remaining construct input stream read num bytes bytes if eof occurs underlying stream num bytes bytes read eof exception thrown exact size input stream input stream num bytes super preconditions check argument num bytes negative expected bytes num bytes remaining num bytes available throws io exception return math min super available remaining read throws io exception eof reached limit remaining return result super read result remaining else remaining underlying stream reached eof read expected number bytes throw new eof exception premature eof expected remaining bytes return result read byte b len throws io exception remaining return len math min len remaining result super read b len result remaining result else remaining underlying stream reached eof read expected number bytes throw new eof exception premature eof expected remaining bytes return result skip n throws io exception result super skip math min n remaining result remaining result else remaining underlying stream reached eof read expected number bytes throw new eof exception premature eof expected remaining bytes return result boolean mark supported return false mark readlimit throw new unsupported operation exception
823	hdfs\src\java\org\apache\hadoop\hdfs\util\GSet.java	unrelated	package org apache hadoop hdfs util a link g set set supports link get object operation the link get object operation uses key lookup element null element supported g set k e extends k extends iterable e size does set contain element corresponding given key otherwise return false boolean contains k key return stored element equal given key this operation similar link java util map get object otherwise return null e get k key add replace element if element exist add set otherwise replace existing element note operation similar link java util map put object object different link java util set add object replace existing element otherwise return null e put e element remove element corresponding given key this operation similar link java util map remove object otherwise return null e remove k key
824	hdfs\src\java\org\apache\hadoop\hdfs\util\GSetByHashMap.java	unrelated	package org apache hadoop hdfs util a link g set implementation link hash map g set by hash map k e extends k implements g set k e hash map k e g set by hash map initial capacity load factor new hash map k e initial capacity load factor size return size boolean contains k k return contains key k e get k k return get k e put e element element null throw new unsupported operation exception null element supported return put element element e remove k k return remove k iterator e iterator return values iterator
825	hdfs\src\java\org\apache\hadoop\hdfs\util\LightWeightGSet.java	unrelated	package org apache hadoop hdfs util a low memory footprint link g set implementation uses array storing elements linked lists collision resolution no rehash performed therefore internal array never resized this support null element this thread safe subclass k implementing link linked element light weight g set k e extends k implements g set k e elements link light weight g set linked element set next element set next linked element next get next element linked element get next log log log factory get log g set max array length prevent overflow problem min array length an internal array entries rows hash table the size must power two linked element entries a mask computing array index hash value element hash mask the size set entry array size modification version fail fast volatile modification light weight g set recommended length actual actual array length recommended length log info recommended recommended length actual actual entries new linked element actual hash mask entries length compute actual length actual array length recommended recommended max array length return max array length else recommended min array length return min array length else integer highest one bit recommended return recommended size return size get index k key return key hash code hash mask e convert linked element e e r e e return r e get k key validate key key null throw new null pointer exception key null find element index get index key linked element e entries index e null e e get next e equals key return convert e element found return null boolean contains k key return get key null e put e element validate element element null throw new null pointer exception null element supported element instanceof linked element throw new hadoop illegal argument exception element instanceof linked element element get class element get class linked element e linked element element find index index get index element remove already exists e existing remove index element insert element head linked list modification size e set next entries index entries index e return existing remove element corresponding key given key hash code index otherwise return null e remove index k key entries index null return null else entries index equals key remove head linked list modification size linked element e entries index entries index e get next e set next null return convert e else head null key equal head search element linked element prev entries index linked element curr prev get next curr null curr equals key found element remove modification size prev set next curr get next curr set next null return convert curr else prev curr curr curr get next element found return null e remove k key validate key key null throw new null pointer exception key null return remove get index key key iterator e iterator return new set iterator string string string builder b new string builder get class get simple name b append size append size append string format x hash mask append modification append modification append entries length append entries length append return b string print detailed information
826	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\CombinerJobCreator.java	unrelated	package org apache hadoop mapreduce combiner job creator job create job string args throws exception configuration conf new configuration num reduces string indir null string outdir null boolean mapoutput compressed false boolean output compressed false args length try r equals args num reduces integer parse int args else indir equals args indir args else outdir equals args outdir args else mapoutput compressed equals args mapoutput compressed boolean value of args boolean value else output compressed equals args output compressed boolean value of args boolean value catch number format exception except system println error integer expected instead args return null catch array index out of bounds exception except system println error required parameter missing args return null conf set boolean mr job config map output compress mapoutput compressed conf set boolean file output format compress output compressed job job new job conf job set job name gridmix combiner job keys words strings job set output key class text values counts ints job set output value class int writable job set mapper class token counter mapper job set combiner class int sum reducer job set reducer class int sum reducer job set num reduce tasks num reduces indir null file input format set input paths job indir outdir null file output format set output path job new path outdir return job
827	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\GenericMRLoadJobCreator.java	unrelated	package org apache hadoop mapreduce generic mr load job creator extends generic mr load generator job create job string argv boolean mapoutput compressed boolean output compressed throws exception job job new job job set jar by class generic mr load generator job set mapper class sample mapper job set reducer class sample reducer parse args argv job return null null file output format get output path job no output dir no writes job set output format class null output format configuration conf job get configuration file input format get input paths job length no input dir generate random data system err println no input path ignoring input format conf random job else null conf get class indirect input format null specified indirect input format build src list job client j client new job client conf path sysdir j client get system dir random r new random path indir input file new path sysdir integer string r next int integer max value files conf set indirect input file indir input file string sequence file writer writer sequence file create writer sysdir get file system conf conf indir input file long writable text sequence file compression type none try path p file input format get input paths job file system fs p get file system conf stack path pathstack new stack path pathstack push p pathstack empty file status stat fs list status pathstack pop stat directory stat get path get name starts with pathstack push stat get path else writer sync writer append new long writable stat get len new text stat get path uri string finally writer close conf set boolean mr job config map output compress mapoutput compressed conf set boolean file output format compress output compressed return job
828	mapreduce\src\benchmarks\gridmix2\src\java\org\apache\hadoop\mapreduce\GridMixRunner.java	unrelated	package org apache hadoop mapreduce grid mix runner num of large jobs per class num of medium jobs per class num of small jobs per class num of reducers for small job num of reducers for medium job num of reducers for large job string grid mix data gridmix data string varcompseq grid mix data web simulation block compressed string fixcompseq grid mix data monster query block compressed string varinfltext grid mix data sort uncompressed string gridmixconfig gridmix config xml configuration config init config file system fs init fs job control gridmix num of jobs enum size small small name part part part default input subset num of small jobs per class defuault num jobs num of reducers for small job default num reducers medium medium name part part part default input subset num of medium jobs per class defuault num jobs num of reducers for medium job default num reducers large large name default input subset num of large jobs per class defuault num jobs num of reducers for large job default num reducers string str string path num jobs num reducers size string str string path num jobs num reducers str str path path num jobs num jobs num reducers num reducers string default path string base return base path default num jobs return num jobs default num reducers return num reducers string string return str enum grid mix job streamsort stream sort add job num reducers boolean mapoutput compressed boolean output compressed size size job control gridmix string prop string format stream sort jobs input files size string indir get input dirs for prop size default path varinfltext string outdir add ts suffix perf stream dir size string buffer sb new string buffer sb append input append indir append sb append output append outdir append sb append mapper cat sb append reducer cat sb append num reduce tasks append num reducers string args sb string split clear dir outdir try configuration conf stream job create job args conf set boolean file output format compress output compressed conf set boolean mr job config map output compress mapoutput compressed job job new job conf gridmix streaming sorter size controlled job cjob new controlled job job null gridmix add job cjob catch exception ex ex print stack trace javasort java sort add job num reducers boolean mapoutput compressed boolean output compressed size size job control gridmix string prop string format java sort jobs input files size string indir get input dirs for prop size default path varinfltext string outdir add ts suffix perf sort dir size clear dir outdir try configuration conf new configuration conf set boolean file output format compress output compressed conf set boolean mr job config map output compress mapoutput compressed job job new job conf job set jar by class sort job set job name gridmix java sorter size job set mapper class mapper job set reducer class reducer job set num reduce tasks num reducers job set input format class key value text input format job set output format class text output format job set output key
829	mapreduce\src\contrib\block_forensics\src\java\org\apache\hadoop\blockforensics\BlockSearch.java	unrelated	package org apache hadoop blockforensics block search mapred job designed search input appearances strings the syntax bin hadoop jar jar location hdfs input path hdfs output dir comma delimited list block ids all arguments required this tool designed used search one block ids log files used general text search assuming search strings contain tokens it assumes one search appear per line block search extends configured implements tool map extends mapper long writable text text text text block id text new text text val text new text list string block ids null protected setup context context throws io exception interrupted exception configuration conf context get configuration string tokenizer st new string tokenizer conf get block ids block ids new linked list string st more tokens string block id st next token block ids add block id map long writable key text value context context throws io exception interrupted exception block ids null system err println error no block ids specified else string val str value string string block id block ids val str index of block id block id text set block id val text set val str context write block id text val text break assume one block id appears per line reduce extends reducer text text text text text val new text reduce text key iterator text values context context throws io exception interrupted exception values next context write key values next run string args throws exception args length system println block search logs dir comma delimited list blocks tool runner print generic command usage system return configuration conf get conf conf set block ids args job job new job conf job set combiner class reduce job set jar by class block search job set job name block search job set mapper class map job set output key class text job set output value class text job set reducer class reduce file input format set input paths job new path args file output format set output path job new path args return job wait for completion true main string args throws exception res tool runner run new configuration new block search args system exit res
830	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\AbstractQueue.java	scheduler	package org apache hadoop mapred parent hierarchy queues all queues extend p even though queue extend categories queues define p container queue composite queues job queue leaf level queues p typically container queue consists job queue all scheduling context data container queue cummulative children p job queue consists actual job list e running job waiting job etc p this done make sure job related data one place queues higher level typically cummulative data organization children level abstract queue log log log factory get log abstract queue protected queue scheduling context qsc protected abstract queue parent protected abstract queue abstract queue parent queue scheduling context qsc set parent parent set queue scheduling context qsc incase root value would null parent null parent add child this involves updating q c structure p first update queue scheduling context level updated update queue scheduling context children p children consider parent capacity totalclustercapacity calculations accordingly update map cluster capacity reduce cluster capacity qsc update context map cluster capacity reduce cluster capacity queue scheduling context get queue scheduling context return qsc set link queue scheduling context link abstract queue passed context set queue scheduling context queue scheduling context qsc qsc qsc string get name return qsc get queue name protected abstract queue get parent return parent protected set parent abstract queue queue parent queue get list link job queue link abstract queue hierarchy rooted link abstract queue p the list returned depth first order children link abstract queue hierarchy already ordered list abstract queue get descendent job queues get list link container queue link abstract queue hierarchy rooted link abstract queue excluding queue p the list returned depth first order children link abstract queue hierarchy already ordered hierarchy list abstract queue get descendant container queues sorts levels current level sort comparator queue comparator returns list immediate children null case leaf list abstract queue get children adds children current level there support adding children leaf level node add child abstract queue queue distribute unconfigured capacity among queues distribute un configured capacity string string return get name string n get queue scheduling context string comparator compare link abstract queue natural order corresponding queue names abstract queue comparator implements comparator abstract queue compare abstract queue abstract queue return get name compare to get name returns true object abstract queue name boolean equals object null return false instanceof abstract queue return false abstract queue queue abstract queue return queue get name equals get name hash code return get name hash code copy configuration enclosed via link queue scheduling context destination queue source queue recursively p this method assumes total hierarchy passed queues link abstract queue get children queue well source queue sorted according comparator link abstract queue comparator validate and copy queue contexts abstract queue source queue throws io exception do validation copying queue scheduling context source context source queue get queue scheduling context qsc supports priorities source context supports priorities throw new io exception changing priorities yet supported attempt made change priority queue get name first update children queues recursively list abstract queue dest children get children dest children null iterator
831	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\CapacitySchedulerConf.java	scheduler	package org apache hadoop mapred class providing access capacity scheduler configuration default values queue configuration capacity scheduler configuration includes settings link job initialization poller default values queue configuration these read file link capacity scheduler conf scheduler conf file classpath the main queue configuration defined file link queue manager queue conf file name classpath p this also provides ap is get set configuration queues capacity scheduler conf log log log factory get log capacity scheduler conf string capacity property capacity string supports priority property supports priority string maximum initialized jobs per user property maximum initialized jobs per user string minimum user limit percent property minimum user limit percent default file name capacity scheduler configuration read string scheduler conf file capacity scheduler xml default ulimit minimum boolean default support priority string queue conf property name prefix mapred capacity scheduler queue map string properties queue properties new hash map string properties if link job conf mapred task maxpmem property set link job conf disabled memory limit configuration used calculate job physical memory requirements percentage job virtual memory requirements set via link job conf set max virtual memory for task this property thus provides default value physical memory job explicitly specify physical memory requirements p it defaults link job conf disabled memory limit explicitly set valid value scheduler consider physical memory scheduling even virtual memory based scheduling enabled string default percentage of pmem in vmem property mapred capacity scheduler task default pmem percentage vmem configuration provides upper limit maximum physical memory specified job the job configuration link job conf mapred task maxpmem property definition less value if job rejected scheduler if set link job conf disabled memory limit scheduler consider physical memory scheduling even virtual memory based scheduling enabled string upper limit on task pmem property mapred capacity scheduler task limit maxpmem a maximum capacity defines limit beyond sub queue cannot use capacity parent queue string max capacity property maximum capacity the constant defines default initialization thread polling interval denoted milliseconds initialization thread polling interval the constant defines maximum number worker threads spawned job initialization max initialization worker threads configuration rm conf default max jobs per users to initialize create new capacity scheduler conf this method reads default configuration file mentioned link scheduler conf file must present classpath application capacity scheduler conf rm conf new configuration false get cs conf add resource scheduler conf file initialize defaults create new capacity scheduler conf reading specified configuration file capacity scheduler configuration capacity scheduler conf path config file rm conf new configuration false get cs conf add resource config file initialize defaults method used initialize default values queue list used capacity scheduler initialize defaults default ulimit minimum get cs conf get int mapred capacity scheduler default minimum user limit percent default support priority get cs conf get boolean mapred capacity scheduler default supports priority false default max jobs per users to initialize get cs conf get int mapred capacity scheduler default maximum initialized jobs per user set properties string queue name properties properties queue properties put queue name properties get percentage cluster specified queue this method defaults configured default
832	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\CapacityTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler implements requirements hadoop provides hod less way share large clusters this scheduler provides following features support queues job submitted queue queues assigned fraction capacity grid capacity sense certain capacity resources disposal all jobs submitted queues org access capacity org free resources allocated queue beyond capacity queues optionally support job priorities disabled default within queue jobs higher priority access queue resources jobs lower priority however job running preempted higher priority job in order prevent one users monopolizing resources queue enforces limit percentage resources allocated user given time competition capacity task scheduler extends task scheduler quick way get qsc object given queue name map string queue scheduling context queue info map new hash map string queue scheduling context root level queue it cluster capacity disposal queues declared users would children queue cs would handle root abstract queue root null this captures scheduling information want display log scheduling display info string queue name capacity task scheduler scheduler scheduling display info string queue name capacity task scheduler scheduler queue name queue name scheduler scheduler string string note call update context objects performance reasons this means data print may slightly stale this data updated whenever assign tasks called if happen data gets stale if see often may need detect situation call update context objects call time return scheduler get display info queue name encapsulates result task lookup task lookup result enum look up status task found no task found task failing memory requirement constant task lookup result objects should accessed directly task lookup result no task lookup result new task lookup result null task lookup result look up status no task found task lookup result mem failed lookup result new task lookup result null task lookup result look up status task failing memory requirement look up status look up status task task call constructor directly use factory methods task lookup result task look up status u status task look up status u status task lookup result get task found result task log debug enabled log debug returning task return new task lookup result look up status task found task lookup result get no task found result return no task lookup result task lookup result get mem failed result return mem failed lookup result task get task return task look up status get look up status return look up status this handles scheduling algorithms the algos map reduce tasks there may slight variations later case make base derived map reduce task scheduling mgr task scheduler object protected capacity task scheduler scheduler protected task type type null task obtain new task task tracker status task tracker job in progress job throws io exception get cluster capacity task scheduling context get tsc queue scheduling context qsc to check job speculative task particular tracker boolean speculative task job in progress job task tracker status tts comparator sort queues for maps need sort queue scheduling context map tsc for reducers use reduce tsc so need separate comparators queue comparator implements comparator abstract queue task scheduling context get tsc queue scheduling context qsi compare
833	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\ContainerQueue.java	scheduler	package org apache hadoop mapred composite queue hierarchy container queue extends abstract queue list immediate children container queue duplicate childrens allowed list abstract queue children container queue abstract queue parent queue scheduling context qsc super parent qsc update current contexts update children contexts update map cluster capacity reduce cluster capacity super update map cluster capacity reduce cluster capacity update children context set normalized capacity values children update children update children context abstract queue queue children normalized map cluster capacity qsc get map tsc get capacity normalized reduce cluster capacity qsc get reduce tsc get capacity update children context normalize map cluster capacity reduce cluster capacity current queue update normalized map cluster capacity normalized reduce cluster capacity update current task scheduling context information at parent level information cumulative children tsc values typically job queue tsc would change first parental level values would stale unless call update happens incase new heartbeat this behaviour shuold fine assign task first update sort whole hierarchy qsc get map tsc update queue get queue scheduling context get map tsc qsc get reduce tsc update queue get queue scheduling context get reduce tsc sort comparator queue comparator sort immediate children collections sort children queue comparator recursive sort children abstract queue child children child sort queue comparator returns sorted order leaf level queues list abstract queue get descendent job queues list abstract queue new array list abstract queue abstract queue child children add all child get descendent job queues return list abstract queue get descendant container queues list abstract queue new array list abstract queue abstract queue child get children child get children null child get children size add child add all child get descendant container queues return used test list abstract queue get children return children add child abstract queue queue children null children new array list abstract queue children contains queue log warn the queue queue get name already exists hence ignoring current value return children add queue distribute un configured capacity list abstract queue un configured queues new array list abstract queue total capacity abstract queue q children q qsc get capacity percent add un configured queue un configured queues add q else if capacity set add total capacity log info capacity percent queue q get name q qsc get capacity percent total capacity q qsc get capacity percent as already know current capacity percent queue make children distribute unconfigured capacity q distribute un configured capacity un configured queues empty log info total capacity distributed among others total capacity we list queues level unconfigured total capacity capacity remaining divide equally among un configured queues capacity share total capacity un configured queues size we dont check total capacity already loading abstract queue q un configured queues q qsc get max capacity percent q qsc get max capacity percent capacity share throw new illegal state exception capacity share capacity share unconfigured queue q get name greater maximum capacity percentage q qsc get max capacity percent q qsc set capacity percent capacity share log info capacity share un configured queue q get name capacity share q capacity make children also
834	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobInitializationPoller.java	scheduler	package org apache hadoop mapred this asynchronously initializes jobs submitted map reduce cluster running link capacity task scheduler p the comprises main poller thread set worker threads together initialize jobs the poller thread periodically looks jobs submitted scheduler selects set initialized it passes worker threads initializing each worker thread configured look jobs submitted fixed set queues it initializes jobs round robin manner selecting first job order queue ready initialized p p an initialized job occupies memory resources job tracker hence poller limits number jobs initialized given time configured limit the limit specified per user per queue p p however since job needs initialized scheduler select tasks run tries keep backlog jobs initialized scheduler need wait let empty slots go waste the core logic poller pick right jobs good potential run next scheduler to picks jobs submitted across users across queues account guaranteed capacities user limits it also always initializes high priority jobs whenever need initialized even means going limit initialized jobs p job initialization poller extends thread log log log factory get log job initialization poller get name the poller picks jobs across users initialize based user limits suppose user limit queue means atmost users jobs run together however order account jobs user might complete faster others initializes jobs additional number users backlog this variable defines additional number users whose jobs considered initializing max additional users to init job queues manager job queue manager sleep interval pool size a worker thread initializes jobs one queues assigned jobs initialized round robin fashion one queue time job initialization thread extends thread job in progress initializing job volatile boolean start initing atomic integer current job count new atomic integer number jobs initialize the hash map maintains relationship queue jobs initialize per queue hash map string tree map job scheduling info job in progress jobs per queue job initialization thread start initing true jobs per queue new hash map string tree map job scheduling info job in progress run start initing initialize jobs try start initing thread sleep sleep interval else break catch throwable the key method initializes jobs queues this method package allow test cases call synchronously controlled manner initialize jobs jobs initialize current job count get set string queues jobs per queue key set string queue queues job in progress job get first job in queue queue job null continue log info initializing job job get job id abstract queue job get profile get queue name for user job get profile get user start initing set initializing job job ttm init job job set initializing job null else break this method returns first job queue removes queue name job in progress get first job in queue string queue tree map job scheduling info job in progress jobs list jobs per queue get queue synchronized jobs list jobs list empty return null iterator job in progress job iterator jobs list values iterator job in progress job job iterator next job iterator remove current job count get and decrement return job test method check thread currently initialising job synchronized job in progress get initializing job return
835	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobQueue.java	scheduler	package org apache hadoop mapred job queue extends abstract queue log log log factory get log job queue job queue abstract queue parent queue scheduling context qsc super parent qsc qsc supports priorities use default priority aware comparator comparator job queue job in progress listener fifo job queue comparator else comparator starttime job comparator waiting jobs new tree map job scheduling info job in progress comparator running jobs new tree map job scheduling info job in progress comparator if queue supports priorities jobs must sorted priorities start times technically insertion time if queue support priorities jobs sorted based start time comparator job scheduling info starttime job comparator starttime job comparator new comparator job scheduling info comparator jobs queues support priorities compare job scheduling info job scheduling info job started earlier wins get start time get start time return else return get start time get start time get job id compare to get job id this involves updating q c structure update map cluster capacity reduce cluster capacity super update map cluster capacity reduce cluster capacity job in progress j get running jobs update stats on running job qsc j update stats on running job queue scheduling context q c job in progress j j get status get run state job status running return task scheduling context map tsi q c get map tsc task scheduling context reduce tsi q c get reduce tsc num maps running for this job j running maps num reduces running for this job j running reduces task data view map scheduler task data view get task data view task type map task data view reduce scheduler task data view get task data view task type reduce num running map slots num maps running for this job map scheduler get slots per task j num running reduce slots num reduces running for this job reduce scheduler get slots per task j num map slots for this job map scheduler get slots occupied j num reduce slots for this job reduce scheduler get slots occupied j num reserved map slots for this job map scheduler get num reserved task trackers j map scheduler get slots per task j num reserved reduce slots for this job reduce scheduler get num reserved task trackers j reduce scheduler get slots per task j j set scheduling info get job queue sched info num maps running for this job num running map slots num reserved map slots for this job num reduces running for this job num running reduce slots num reserved reduce slots for this job map tsi set num running tasks map tsi get num running tasks num maps running for this job reduce tsi set num running tasks reduce tsi get num running tasks num reduces running for this job map tsi set num slots occupied map tsi get num slots occupied num map slots for this job reduce tsi set num slots occupied reduce tsi get num slots occupied num reduce slots for this job integer map tsi get num slots occupied by user get j get profile get
836	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\JobQueuesManager.java	scheduler	package org apache hadoop mapred a link job in progress listener maintains jobs managed one queues job queues manager extends job in progress listener maintain hashmap queue names queue info map string job queue job queues new hash map string job queue log log log factory get log job queues manager job queues manager add given queue map queue name job queues add queue job queue queue job queues put queue get name queue job added job in progress job throws io exception log info job job get job id submitted queue job get profile get queue name add job right queue job queue qi get job queue job get profile get queue name null qi job submitted queue aware log warn invalid queue job get profile get queue name specified job job get profile get job id ignoring job return let scheduler know qi job added job note job removed job completes e job upated job removed job in progress job job updated job change event event job in progress job event get job in progress job queue qi get job queue job get profile get queue name qi job updated event comparator job scheduling info get comparator string queue return get job queue queue comparator job queue get job queue job in progress jip return get job queue jip get profile get queue name job queue get job queue string name return job queues get name set string get job queue names return job queues key set
837	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\MemoryMatcher.java	scheduler	package org apache hadoop mapred memory matcher log log log factory get log memory matcher mem size for map slot on jt job conf disabled memory limit mem size for reduce slot on jt job conf disabled memory limit limit max mem for map tasks job conf disabled memory limit limit max mem for reduce tasks job conf disabled memory limit memory matcher find memory already used running tasks residing given task tracker null memory cannot computed reason synchronized long get mem reserved for tasks task tracker status task tracker task type task type vmem task status task task tracker get task reports following task states one slot still occupied hence memory task accounted used memory task get run state task status state running task get run state task status state unassigned task task cleanup phase get memory allotted task based number slots vmem task get is map task type task type map mem size per map slot get mem size for map slot vmem mem size per map slot task get num slots else task get is map task type task type reduce mem size per reduce slot get mem size for reduce slot vmem mem size per reduce slot task get num slots vmem vmem return long value of vmem check tt enough memory run task specified job boolean matches memory requirements job in progress job task type task type task tracker status task tracker log debug enabled log debug matching memory requirements job get job id string scheduling task tracker tracker name scheduling based on mem enabled log debug enabled log debug scheduling based job memory requirements disabled ignoring value set job return true long mem used on tt get mem reserved for tasks task tracker task type total mem usable on tt mem for this task task type task type map mem for this task job get memory for map task total mem usable on tt get mem size for map slot task tracker get max map slots else task type task type reduce mem for this task job get memory for reduce task total mem usable on tt get mem size for reduce slot task tracker get max reduce slots free mem on tt total mem usable on tt mem used on tt value mem for this task free mem on tt log debug enabled log debug mem for this task mem for this task free mem on tt free mem on tt a task type task job get job id string cannot scheduled tt task tracker tracker name return false log debug enabled log debug mem for this task mem for this task free mem on tt free mem on tt a task type string task job get job id string matches memory requirements tt task tracker tracker name return true boolean scheduling based on mem enabled get limit max mem for map slot job conf disabled memory limit get limit max mem for reduce slot job conf disabled memory limit get mem size for map slot job conf disabled memory limit get mem size for
838	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\QueueHierarchyBuilder.java	scheduler	package org apache hadoop mapred hierarchy builder capacity scheduler queue hierarchy builder log log log factory get log queue hierarchy builder queue hierarchy builder create new link abstract queue hierarchy set new queue properties passed link capacity scheduler conf abstract queue create hierarchy list job queue info root children capacity scheduler conf sched conf log debug enabled log debug root queues defined job queue info q root children log debug q get queue name create root abstract queue new root abstract queue create root abstract queue create complete hierarchy rooted new root abstract queue create hierarchy new root abstract queue root children sched conf distribute un configured capacities new root abstract queue distribute un configured capacity return new root abstract queue string total capacity overflown msg the cumulative capacity queues level overflown f recursively create complete abstract queues hierarchy parent root hierarchy children immediate children parent may turn parent child queues any job queue info children used create job queue abstract queues hierarchy every abstract queue used create container queue p while creating hierarchy make sure level total capacity children level cross properties set the new queue properties set key names obtained expanding queue names reflect whole hierarchy create hierarchy abstract queue parent list job queue info children capacity scheduler conf sched config check children childrens children null children empty total capacity f job queue info qs children check child children list job queue info child queues qs get children child queues null child queues size generate new container queue recursively create hierarchy abstract queue cq new container queue parent load context qs get properties qs get queue name sched config update total capacity total capacity cq qsc get capacity percent log info created container queue qs get queue name added child parent get name create child hiearchy create hierarchy cq child queues sched config else job queue create job queue abstract queue jq new job queue parent load context qs get properties qs get queue name sched config total capacity jq qsc get capacity percent log info created job queue qs get queue name added child parent get name check total capacity level total children cross total capacity string builder child queue names new string builder job queue info child children child queue names append child get queue name append throw new illegal argument exception string format total capacity overflown msg child queue names string substring child queue names string length float value of total capacity create new link queue scheduling context given props also set properties passed scheduler configuration object queue scheduling context load context properties props string queue name capacity scheduler conf sched conf sched conf set properties queue name props capacity sched conf get capacity queue name stretch capacity sched conf get max capacity queue name capacity log info no capacity specified queue queue name ul min sched conf get minimum user limit percent queue name create qsc add hashmap queue scheduling context qsi new queue scheduling context queue name capacity stretch capacity ul min qsi set supports priorities sched conf priority supported queue name return qsi create
839	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\QueueSchedulingContext.java	scheduler	package org apache hadoop mapred keeping track scheduling information queues p we need maintain scheduling information relevant queue name capacity etc along information specific kind task map reduce num running tasks pending tasks etc p this scheduling information used decide allocate tasks redistribute capacity etc p a queue scheduling context qsc object represents scheduling information queue queue scheduling context name queue string queue name get maximum capacity queue running map tasks cluster map capacity get maximum capacity queue running reduce tasks cluster reduce capacity capacity set config mapred capacity scheduler queue queue name capacity percentage number slots cluster available jobs queue capacity percent max capacity stretch set config mapred capacity scheduler queue queue name maximum capacity maximum capacity stretch defines limit beyond sub queue cannot use capacity parent queue max capacity percent handle user limits need know many users jobs queue map string integer num jobs by user new hash map string integer min value user limit users ul min whether queue supports priorities default false boolean supports priorities false no waiting jobs num of waiting jobs state map capacity prev map capacity state reduce capacity prev reduce capacity we keep task scheduling info object kind task support task scheduling context map tsc task scheduling context reduce tsc queue scheduling context string queue name capacity percent max capacity percent ul min set queue name queue name set capacity percent capacity percent set max capacity percent max capacity percent set ul min ul min set map tsc new task scheduling context set reduce tsc new task scheduling context return information queue string string we print queue information first followed info map reduce tasks job info string buffer sb new string buffer sb append queue configuration n sb append capacity percentage sb append get capacity percent sb append n sb append string format user limit n get ul min sb append string format priority supported n supports priorities yes no sb append n sb append map tasks n sb append get map tsc string sb append n sb append reduce tasks n sb append get reduce tsc string sb append n sb append job info n sb append string format number waiting jobs n get num of waiting jobs sb append string format number users submitted jobs n get num jobs by user size return sb string string get queue name return queue name set queue name string queue name queue name queue name get map capacity return map capacity set map capacity map capacity map capacity map capacity get reduce capacity return reduce capacity set reduce capacity reduce capacity reduce capacity reduce capacity get capacity percent return capacity percent set capacity percent capacity percent capacity percent capacity percent map string integer get num jobs by user return num jobs by user set num jobs by user map string integer num jobs by user num jobs by user num jobs by user get ul min return ul min set ul min ul min ul min ul min task scheduling context get map tsc return map tsc set map tsc task scheduling context map tsc
840	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\TaskDataView.java	scheduler	package org apache hadoop mapred task view job returns running pending information job job in progress factory method provides map reduce data view based type task data view get running tasks job in progress job get pending tasks job in progress job get slots per task job in progress job task scheduling context get tsi queue scheduling context qsi get num reserved task trackers job in progress job get slots occupied job in progress job return get num reserved task trackers job get running tasks job get slots per task job check given job sufficient reserved tasktrackers pending tasks else code false code boolean sufficient reserved task trackers job in progress job return get num reserved task trackers job get pending tasks job task data view map task data view task data view reduce task data view task data view get task data view task type type type task type map map task data view null map task data view new map task data view return map task data view else type task type reduce reduce task data view null reduce task data view new reduce task data view return reduce task data view return null the data view map tasks map task data view extends task data view map task data view get running tasks job in progress job return job running maps get pending tasks job in progress job return job pending maps get slots per task job in progress job return job get num slots per map task scheduling context get tsi queue scheduling context qsi return qsi get map tsc get num reserved task trackers job in progress job return job get num reserved task trackers for maps the data view reduce tasks reduce task data view extends task data view reduce task data view get running tasks job in progress job return job running reduces get pending tasks job in progress job return job pending reduces get slots per task job in progress job return job get num slots per reduce task scheduling context get tsi queue scheduling context qsi return qsi get reduce tsc get num reserved task trackers job in progress job return job get num reserved task trackers for reduces
841	mapreduce\src\contrib\capacity-scheduler\src\java\org\apache\hadoop\mapred\TaskSchedulingContext.java	scheduler	package org apache hadoop mapred keeping track scheduling information queues p maintain information specific kind task map reduce num running tasks pending tasks etc p this scheduling information used decide allocate tasks redistribute capacity etc p a task scheduling context tsi object represents scheduling information particular kind task map reduce p task scheduling context actual capacity depends many slots available cluster given time capacity number running tasks num running tasks number slots occupied running tasks num slots occupied actual capacity stretch depends many slots available cluster given time max capacity user need keep track number slots occupied running tasks map string integer num slots occupied by user new hash map string integer reset variables associated tasks reset task vars set num running tasks set num slots occupied string get num slots occupied by user key set get num slots occupied by user put integer value of returns capacity queue slots get capacity return capacity mutator method capacity set capacity capacity capacity capacity return information tasks string string occupied slots as percent get capacity get num slots occupied get capacity string buffer sb new string buffer sb append capacity get capacity slots n get max capacity sb append maximum capacity get max capacity slots n sb append string format used capacity f capacity n integer value of get num slots occupied float value of occupied slots as percent sb append string format running tasks n integer value of get num running tasks info active users get num slots occupied sb append active users n map entry string integer entry get num slots occupied by user entry set entry get value null entry get value value user tasks running continue sb append user entry get key num slots occupied by this user entry get value value p num slots occupied by this user get num slots occupied sb append string format f used capacity n long value of num slots occupied by this user float value of p return sb string get num running tasks return num running tasks set num running tasks num running tasks num running tasks num running tasks get num slots occupied return num slots occupied set num slots occupied num slots occupied num slots occupied num slots occupied map string integer get num slots occupied by user return num slots occupied by user set num slots occupied by user map string integer num slots occupied by user num slots occupied by user num slots occupied by user get max capacity return max capacity set max capacity max capacity max capacity max capacity update task scheduling context tc num slots occupied tc num slots occupied num running tasks tc num running tasks max task limit tc max task limit update no of slots occupied by user tc num slots occupied by user update no of slots occupied by user map string integer nou iterator map entry string integer nou entry set iterator next map entry string integer entry next string key entry get key integer current val num slots occupied by user get key current val null num slots occupied
842	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleDataJoinMapper.java	unrelated	package org apache hadoop contrib utils join this subclass data join mapper base used demonstrate functionality inner join data sources tab separated text files based first column sample data join mapper extends data join mapper base protected text generate input tag string input file tag row input file name data source return new text input file protected text generate group key tagged map output record first column input tab separated files becomes key perform join string line text record get data string string group key string tokens line split group key tokens return new text group key protected tagged map output generate tagged map output object value tagged map output retv new sample tagged map output text value retv set tag new text input tag return retv
843	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleDataJoinReducer.java	unrelated	package org apache hadoop contrib utils join this subclass data join reducer base used demonstrate functionality inner join data sources tab separated text files based first column sample data join reducer extends data join reducer base list source tags value per source protected tagged map output combine object tags object values eliminate rows didnot match one two tables inner join tags length return null string joined str tags length joined str strip first column key joined string line text tagged map output values get data string string tokens line split joined str tokens tagged map output retv new sample tagged map output new text joined str retv set tag text tags return retv
844	mapreduce\src\contrib\data_join\src\examples\org\apache\hadoop\contrib\utils\join\SampleTaggedMapOutput.java	unrelated	package org apache hadoop contrib utils join this subclass tagged map output used demonstrate functionality inner join data sources tab separated text files based first column sample tagged map output extends tagged map output text data sample tagged map output data new text sample tagged map output text data data data writable get data return data write data output throws io exception tag write data write read fields data input throws io exception tag read fields data read fields
845	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop contrib utils join this provides implementation resetable iterator the implementation based array list array list backed iterator implements resetable iterator iterator iter array list object data array list backed iterator new array list object array list backed iterator array list object data data data iter data iterator add object item data add item boolean next return iter next object next return iter next remove reset iter data iterator close throws io exception iter null data null
846	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinJob.java	unrelated	package org apache hadoop contrib utils join this implements main function creating map reduce job join data different sources to create sucn job user must implement mapper extends data join mapper base reducer extends data join reducer base data join job class get class by name string name class retv null try class loader loader thread current thread get context class loader retv class name name true loader catch exception e throw new runtime exception e return retv job conf create data join job string args throws io exception string input dir args string output dir args class input format sequence file input format args compare to ignore case text system println using sequence file input format args else system println using text input format args input format text input format num of reducers integer parse int args class mapper get class by name args class reducer get class by name args class mapoutput value class get class by name args class output format text output format class output value class text args compare to ignore case text system println using sequence file output format args output format sequence file output format output value class get class by name args else system println using text output format args max num of values per group string job name args length max num of values per group long parse long args args length job name args configuration defaults new configuration job conf job new job conf defaults data join job job set job name data join job job name file system fs file system get defaults fs delete new path output dir true file input format set input paths job input dir job set input format input format job set mapper class mapper file output format set output path job new path output dir job set output format output format sequence file output format set output compression type job sequence file compression type block job set map output key class text job set map output value class mapoutput value class job set output key class text job set output value class output value class job set reducer class reducer job set num map tasks job set num reduce tasks num of reducers job set long datajoin max num of values per group max num of values per group return job submit run map reduce job boolean run job job conf job throws io exception job client jc new job client job boolean sucess true running job running null try running jc submit job job job id job id running get id system println job job id submitted running complete system println job job id still running try thread sleep catch interrupted exception e running jc get job job id sucess running successful finally sucess running null running kill job jc close return sucess main string args boolean success args length args length system println usage data join job inputdirs outputdir map input file format numof parts mapper reducer map output value output value max num of values per group description of job system exit
847	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinMapperBase.java	unrelated	package org apache hadoop contrib utils join this serves base mapper data join job this expects subclasses implement methods following functionalities compute source tag input values compute map output value object compute map output key object the source tag used reducer determine source table sql terminology value comes computing map output value object amounts performing projecting filtering work sql statement select clauses computing map output key amounts choosing join key this provides appropriate plugin points user defined subclasses implement appropriate logic data join mapper base extends job base protected string input file null protected job conf job null protected text input tag null protected reporter reporter null configure job conf job super configure job job job input file job get mr job config map input file input tag generate input tag input file determine source tag based input file name protected text generate input tag string input file generate tagged map output value the user code also perform projection filtering if decides discard input record certain conditions met simply return null protected tagged map output generate tagged map output object value generate map output key the user code compute key programmatically selecting values fields in sense general joining capabilities sql protected text generate group key tagged map output record map object key object value output collector output reporter reporter throws io exception reporter null reporter reporter add long value total count tagged map output record generate tagged map output value record null add long value discarded count return text group key generate group key record group key null add long value null group key count return output collect group key record add long value collected count close throws io exception reporter null reporter set status super get report reduce object arg iterator arg output collector arg reporter arg throws io exception todo auto generated method stub
848	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\DataJoinReducerBase.java	unrelated	package org apache hadoop contrib utils join this serves base reducer data join job the reduce function first group values according input tags compute cross product groups for tuple cross product calls following method expected implemented subclass protected tagged map output combine object tags object values the method expected produce one output value array records different sources the user code also perform filtering it return null decides records meet certain conditions data join reducer base extends job base protected reporter reporter null max num of values per group protected largest num of values protected num of values protected collected protected job conf job close throws io exception reporter null reporter set status super get report configure job conf job super configure job job job max num of values per group job get long datajoin max num of values per group the subclass provide different implementation resetable iterator this necessary number values reduce call high the default provided uses array list backed iterator protected resetable iterator create resetable iterator return new array list backed iterator this function groups values key sub groups based secondary key input tag sorted map object resetable iterator regroup object key iterator arg reporter reporter throws io exception num of values sorted map object resetable iterator retv new tree map object resetable iterator tagged map output record null arg next num of values num of values reporter set status key key string num of values num of values num of values max num of values per group continue record tagged map output arg next clone job text tag record get tag resetable iterator data retv get tag data null data create resetable iterator retv put tag data data add record num of values largest num of values largest num of values num of values log info key key string largest num of values largest num of values return retv reduce object key iterator values output collector output reporter reporter throws io exception reporter null reporter reporter sorted map object resetable iterator groups regroup key values reporter object tags groups key set array resetable iterator group values new resetable iterator tags length tags length group values groups get tags join and collect tags group values key output reporter add long value group count tags length group values close the subclass overwrite method perform additional filtering processing logic value collected protected collect object key tagged map output record output collector output reporter reporter throws io exception collected add long value collected count record null output collect key record get data reporter set status key key string collected collected add long value actually collected count join list value lists collect results list input tags list value lists corresponding one input source join and collect object tags resetable iterator values object key output collector output reporter reporter throws io exception values length return object partial list new object values length join and collect tags values partial list key output reporter perform actual join recursively list input tags list value lists corresponding one input source indicating next value list joined list values one value
849	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\JobBase.java	unrelated	package org apache hadoop contrib utils join a common base implementing statics collecting mechanisms commonly used typical map reduce job job base implements mapper reducer log log log factory get log datajoin job sorted map object long counters null sorted map object double counters null set given counter given value counter name value counter protected set long value object name value counters put name new long value set given counter given value counter name value counter protected set double value object name value counters put name new double value counter name protected long get long value object name return counters get name counter name protected double get double value object name return counters get name increment given counter given incremental value if counter exist one created value counter name incremental value protected long add long value object name inc long val counters get name long retv null val null retv new long inc else retv new long val value inc counters put name retv return retv increment given counter given incremental value if counter exist one created value counter name incremental value protected double add double value object name inc double val counters get name double retv null val null retv new double inc else retv new double val value inc counters put name retv return retv log counters protected report log info get report log counters protected string get report string buffer sb new string buffer iterator iter counters entry set iterator iter next entry e entry iter next sb append e get key string append append e get value append n iter counters entry set iterator iter next entry e entry iter next sb append e get key string append append e get value append n return sb string initializes new instance link job conf configuration configure job conf job counters new tree map object long counters new tree map object double
850	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\ResetableIterator.java	unrelated	package org apache hadoop contrib utils join this defines iterator help reducer group input source tags once values grouped reducer receive cross product values different groups resetable iterator extends iterator reset add object item close throws io exception
851	mapreduce\src\contrib\data_join\src\java\org\apache\hadoop\contrib\utils\join\TaggedMapOutput.java	unrelated	package org apache hadoop contrib utils join this serves base values flow mappers reducers data join job typically job mappers compute source tag input record based attributes based file name input file this tag used reducers group values given key according source tags tagged map output implements writable protected text tag tagged map output tag new text text get tag return tag set tag text tag tag tag writable get data tagged map output clone job conf job return tagged map output writable utils clone job
852	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\AllocationStore.java	unrelated	package org apache hadoop mapred abstract implementing persistent store allocation information allocation store map string budget queue queue cache new hash map string budget queue initializes configuration init configuration conf loads allocations persistent store load saves allocations persistent store save gets current remaining budget associated queue get budget string queue budget f budget queue budget queue queue cache get queue budget queue null budget budget queue budget return budget gets current spending rate associated queue deducted budget get spending string queue spending budget queue budget queue queue cache get queue budget queue null spending budget queue spending return spending adds budget queue synchronized add budget string queue budget budget queue budget queue queue cache get queue budget queue null return budget queue add budget budget adds new queue synchronized add queue string queue queue cache put queue new budget queue queue f f gets queue info string get queue info string queue budget queue budget queue queue cache get queue budget queue null return return budget float string budget queue budget budget n spending float string budget queue spending spending n used integer string budget queue used used n pending budget queue pending pending n remove queue synchronized remove queue string queue queue cache remove queue sets spending rate queue deducted budget synchronized set spending string queue spending budget queue budget queue queue cache get queue budget queue null return budget queue spending spending sets queue usage accounting synchronized set usage string queue used pending budget queue budget queue queue cache get queue budget queue null return budget queue used used budget queue pending pending gets queue status budget spending usage collection budget queue get queues return queue cache values
853	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\BudgetQueue.java	unrelated	package org apache hadoop mapred class hold accounting info queue remaining budget spending rate whether queue usage budget queue string name volatile budget volatile spending volatile used volatile pending deduct budget budget queue string name budget spending name name budget budget spending spending used pending thread safe addition budget synchronized add budget new budget budget new budget
854	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\DynamicPriorityScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler provides following features the purpose scheduler allow users increase decrease queue priorities continuosly meet requirements current workloads the scheduler aware current demand makes expensive boost priority peak usage times thus users move workload low usage times rewarded discounts priorities boosted within limited quota all users given quota budget deducted periodically configurable accounting intervals how much budget deducted determined per user spending rate may modified time directly user the cluster slots share allocated particular user computed users spending rate sum spending rates accounting period this scheduler designed meta scheduler top existing map reduce schedulers responsible enforcing shares computed dynamic scheduler cluster dynamic priority scheduler extends task scheduler this periodically checks spending rates queues updates queue capacity shares budgets allocations extends timer task implements queue allocator map string queue allocation allocation new hash map string queue allocation configuration conf hash map string string queue info new hash map string string total spending set string info queues queue manager queue manager allocation store store allocations configuration conf queue manager queue manager conf conf queue manager queue manager info queues queue manager get leaf queue names store reflection utils new instance conf get class priority scheduler options dynamic scheduler store file allocation store allocation store conf store init conf store load add budget string queue budget store add budget queue budget add queue string queue store add queue queue synchronized get price return total spending remove queue string queue store remove queue queue queue manager set scheduler info queue set spending string queue spending store set spending queue spending string get info string queue return store get queue info queue string get queue infos string info price float string total spending price n budget queue queue store get queues info queue name queue name queue info get queue name queue n return info synchronized update allocation string queue list total spending f budget queue queue store get queues info queues contains queue name queue new queues new queue info queues size string info queue info queues new queues queue manager get queue info queue queue new queue new queue new queue set name queue name new queues new queue queue manager set queues new queues queue info new queue info new queue info queue name null queue manager set scheduler info queue name new queue info info queues queue manager get leaf queue names queue list equals queue list queue list queue name what published price spending per slot queue spending queue budget queue used queue pending total spending queue spending conf set priority scheduler options mapred queue names queue list set shares calculates shares proportion spending rates sets appropriate configuration parameter schedulers read synchronized set shares map string queue allocation shares new hash map string queue allocation budget queue queue store get queues spending queue spending queue budget queue spending queue used queue used queue pending spending f else queue add budget queue spending queue used queue share f total spending f queue share spending total spending queue info put queue name budget float string queue
855	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\DynamicPriorityServlet.java	scheduler	package org apache hadoop mapred servlet controlling queue allocations installed job tracker url scheduler link dynamic priority scheduler use operations supported br price br time br info queue query requires user admin privilege br infos requires admin privilege br add budget budget add queue queue change requires admin privilege br set spending spending set queue queue change requires user admin privilege br add queue queue add requires admin privilege br remove queue queue remove requires admin privilege br dynamic priority servlet extends http servlet dynamic priority scheduler scheduler job tracker job tracker priority authorization auth init throws servlet exception super init servlet context servlet context get servlet context scheduler dynamic priority scheduler servlet context get attribute scheduler job tracker job tracker scheduler task tracker manager auth new priority authorization auth init scheduler conf protected post http servlet request req http servlet response resp throws servlet exception io exception get req resp same handler get post check admin role string query throws io exception role priority authorization admin throw new io exception access denied query check user role http servlet request request string queue string query throws io exception role priority authorization admin return role priority authorization user request get parameter user equals queue return throw new io exception access denied query get http servlet request request http servlet response response throws servlet exception io exception string query request get query string role auth authorize query request get header authorization request get parameter user request get parameter timestamp string queue request get parameter queue string info admin request get parameter add queue null check admin role query queue request get parameter add queue scheduler allocations add queue queue info scheduler allocations get info queue admin request get parameter remove queue null check admin role query queue request get parameter remove queue scheduler allocations remove queue queue info scheduler allocations get info queue admin request get parameter add budget null check admin role query budget float parse float request get parameter add budget scheduler allocations add budget queue budget info scheduler allocations get info queue user request get parameter set spending null check user role request queue query spending float parse float request get parameter set spending scheduler allocations set spending queue spending info scheduler allocations get info queue user request get parameter info null queue request get parameter info check user role request queue query info scheduler allocations get queue info queue admin request get parameter infos null check admin role query info scheduler allocations get queue infos request get parameter price null info float string scheduler allocations get price info price info price n request get parameter time null info start long string priority authorization start time start n info time long string system current time millis time n info null info response set content type text xml print writer new print writer response get output stream string hostname string utils simple hostname job tracker get job tracker machine print queue info printf host host n hostname printf info print queue info n close
856	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\FileAllocationStore.java	scheduler	package org apache hadoop mapred implements persistent storage queue budget spending information file file allocation store extends allocation store log log log factory get log file allocation store string file name boolean loaded false inherit doc init configuration conf file name conf get priority scheduler options dynamic scheduler budget file etc hadoop budget inherit doc save print writer null try new print writer new buffered writer new file writer file name budget queue queue get queues printf f f n queue name queue budget queue spending catch exception e log error error writing file file name e finally close close closeable closeable closeable null try closeable close catch exception ce log error error closing file file name ce inherit doc load loaded return buffered reader null try new buffered reader new file reader file name string line read line line null string name value line split name value length continue queue cache put name value new budget queue name value float parse float name value float parse float name value line read line loaded true catch exception e log error error reading file file name e finally close
857	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\PriorityAuthorization.java	scheduler	package org apache hadoop mapred this implements symmetric key hmac sha signature based authorization users admins priority authorization user admin no access hash map string user acl acl new hash map string user acl last successful reload start time system current time millis string acl file log log log factory get log priority authorization boolean debug log debug enabled string hmac sha algorithm hmac sha initializes authorization configuration init configuration conf acl file conf get mapred priority scheduler acl file etc hadoop acl adapted aws query authentication cookbook computes rfc compliant hmac signature the data signed the signing key the base encoded rfc compliant hmac signature java security signature exception signature generation fails string hmac string data string key throws java security signature exception string result try get hmac sha key raw key bytes secret key spec signing key new secret key spec key get bytes hmac sha algorithm get hmac sha mac instance initialize signing key mac mac mac get instance hmac sha algorithm mac init signing key compute hmac input data bytes byte raw hmac mac final data get bytes base encode hmac result new string base encode base raw hmac catch exception e throw new signature exception failed generate hmac e e return result user acl string user string role string key replay detection last timestamp start time user acl string user string role string key user user role role key key reload acl buffered reader null try new buffered reader new file reader acl file string line read line line null string name value line split name value length continue acl put name value new user acl name value name value name value debug log debug loading line line read line catch exception e log error e try close catch exception e log error e load acl time system current time millis try file file new file acl file last modified file last modified last modified last successful reload reload acl last successful reload time catch exception e log error failed reload acl file e boolean replay string timestamp string signature user acl user acl signature time long parse long timestamp debug log debug signaturetime long string signature time log debug lasttime long string user acl last timestamp signature time user acl last timestamp return true user acl last timestamp signature time return false returns authorized role user checks whether signature obtained user made key stored local acl also checks replay attacks admin user no access authorize string data string signature string user string timestamp try signature url decoder decode signature utf catch exception e log error authorization exception e return no access debug log debug data sig signature user user time timestamp try load acl user acl user acl acl get user user acl null return no access string signature test hmac data user acl key debug log debug signature test signature test log debug signature signature signature test equals signature replay timestamp signature user acl return user acl role equals admin admin user catch exception e log error athorization exception e return no access
858	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\PriorityScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler provides following features allows continuous enforcement user controlled dynamic queue shares preempts tasks exceeding queue shares instantaneously new jobs arrive work conserving tracks queue usage charge jobs pending running authorizes queue submissions based symmetric key hmac sha signatures priority scheduler extends queue task scheduler init thread extends thread job in progress job init thread job in progress job job job run task tracker manager init job job job listener extends job in progress listener job added job in progress job new init thread job start synchronized priority scheduler string queue authorize job queue equals job kill return job queue add job queue jobs jobs queue jobs get queue jobs null jobs new queue jobs queue queue jobs put queue jobs jobs jobs add job debug log debug add job job get profile get job id job removed job in progress job synchronized priority scheduler job queue remove job string queue get queue job queue jobs get queue jobs remove job job updated job change event event comparator task in progress task comparator new comparator task in progress compare task in progress task in progress res get progress get progress res else res get progress get progress res get exec start time get exec start time res else res get exec start time get exec start time return res comparator kill queue queue comparator new comparator kill queue compare kill queue kill queue start time start time return start time start time return return queue jobs string name linked list job in progress jobs new linked list job in progress queue jobs string name name name queue quota quota map used reduce used map pending reduce pending mappers reducers string name queue quota string name name name queue allocator allocator log log log factory get log priority scheduler boolean map true boolean reduce false boolean fill true boolean no fill false job listener job listener new job listener boolean debug log debug enabled boolean sort tasks true last kill kill interval priority authorization auth new priority authorization linked list job in progress job queue new linked list job in progress hash map string queue jobs queue jobs new hash map string queue jobs start throws io exception task tracker manager add job in progress listener job listener sort tasks conf get boolean mapred priority scheduler sort tasks true kill interval conf get long mapred priority scheduler kill interval auth init conf terminate throws io exception set allocator queue allocator allocator allocator allocator boolean assign map red task job in progress job task tracker status task tracker num trackers list task assigned tasks map string queue quota queue quota boolean fill boolean map throws io exception string queue get queue job queue quota quota queue quota get queue quota null log error queue queue configured properly return false quota quota fill return false task null map job obtain new local map task task tracker num trackers task tracker manager get number of unique hosts null debug log debug assigned local task job job get profile
859	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\PrioritySchedulerOptions.java	scheduler	package org apache hadoop mapred configuration options used priority schedulers one place ease referencing code priority scheduler options value string dynamic scheduler budget file mapred dynamic scheduler budget file value string dynamic scheduler store mapred dynamic scheduler store value string mapred queue names mapred queue names value string dynamic scheduler scheduler mapred dynamic scheduler scheduler value string dynamic scheduler alloc interval mapred dynamic scheduler alloc interval
860	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueAllocation.java	scheduler	package org apache hadoop mapred class hold queue share info communicated scheduler queue share manager queue allocation string name share queue allocation string name share name name share share gets queue share get share return share gets queue name string get name return name
861	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueAllocator.java	scheduler	package org apache hadoop mapred this intended allowing schedulers communicate queue share management implementation schedulers periodically poll obtain latest queue allocations queue allocator used schedulers obtain queue allocations periodically map string queue allocation get allocation used schedulers push queue usage info accounting purposes set usage string queue used pending
862	mapreduce\src\contrib\dynamic-scheduler\src\java\org\apache\hadoop\mapred\QueueTaskScheduler.java	scheduler	package org apache hadoop mapred this allows scheduler retrieve periodic queue allocation info queue share manager queue task scheduler extends task scheduler sets queue share manager scheduler set allocator queue allocator allocator
863	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\Activator.java	unrelated	package org apache hadoop eclipse the activator controls plug life cycle activator extends abstract ui plugin the plug id string plugin id org apache hadoop eclipse the shared unique instance singleton activator plugin constructor activator synchronized activator plugin null not singleton throw new runtime exception activator plugin id singleton plugin start bundle context context throws exception super start context stop bundle context context throws exception server registry get instance dispose plugin null super stop context returns shared unique instance singleton activator get default return plugin
864	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\ErrorMessageDialog.java	unrelated	package org apache hadoop eclipse error dialog helper error message dialog display string title string message display get default sync exec new runnable run message dialog open error display get default get active shell title message display exception e display an exception occured exception description n e get localized message
865	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\HadoopPerspectiveFactory.java	unrelated	package org apache hadoop eclipse creates links new map reduce based wizards views map reduce perspective hadoop perspective factory implements i perspective factory create initial layout i page layout layout layout add new wizard shortcut org apache hadoop eclipse new driver wizard layout add new wizard shortcut org apache hadoop eclipse new mapper wizard layout add new wizard shortcut org apache hadoop eclipse new reducer wizard i folder layout left layout create folder org apache hadoop eclipse perspective left i page layout left f layout get editor area left add view org eclipse ui navigator project explorer i folder layout bottom layout create folder org apache hadoop eclipse perspective bottom i page layout bottom f layout get editor area bottom add view i page layout id problem view bottom add view i page layout id task list bottom add view java ui id javadoc view bottom add view org apache hadoop eclipse view servers bottom add placeholder java ui id source view bottom add placeholder i page layout id progress view bottom add placeholder i console constants id console view bottom add placeholder i page layout id bookmarks i folder layout right layout create folder org apache hadoop eclipse perspective right i page layout right f layout get editor area right add view i page layout id outline right add view org eclipse ui cheatsheets views cheat sheet view right add view layout id cheat sheet layout add action set i debug ui constants launch action set layout add action set java ui id action set layout add action set java ui id coding action set layout add action set java ui id element creation action set layout add action set i page layout id navigate action set layout add action set java ui id search action set layout add new wizard shortcut org eclipse jdt ui wizards new package creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new class creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new interface creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new enum creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new annotation creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new source folder creation wizard layout add new wizard shortcut org eclipse jdt ui wizards new snippet file creation wizard layout add new wizard shortcut org eclipse ui wizards new folder layout add new wizard shortcut org eclipse ui wizards new file layout add new wizard shortcut org eclipse ui editors wizards untitled text file wizard cheat sheet viewer factory create cheat sheet view set input org apache hadoop eclipse cheatsheet
866	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\ImageLibrary.java	pooling	package org apache hadoop eclipse icons manager image library bundle bundle activator get default get bundle singleton instance volatile image library instance null i shared images shared images platform ui get workbench get shared images where resources icons images available bundle string resource dir resources public access image descriptors image descriptor get string name return get instance get image descriptor by name name public access images image get image string name return get instance get image by name name singleton access image library get instance instance null synchronized image library instance null instance new image library return instance map registered resources image descriptor image map string image descriptor desc map new hash map string image descriptor map string image image map new hash map string image image library constructor put image definitions image library servers view new image server view location entry elephant x png new image server view job entry job gif new image server view action location new location new x png new image server view action location edit location edit x png new shared image server view action delete i shared images img tool delete dfs browser new image dfs browser root entry files gif new image dfs browser location entry elephant x png new shared image dfs browser folder entry i shared images img obj folder new shared image dfs browser file entry i shared images img obj file dfs files editor new shared image dfs file editor i shared images img obj file actions new image dfs browser action mkdir new folder png new image dfs browser action download download png new image dfs browser action upload files upload png new image dfs browser action upload dir upload png new shared image dfs browser action delete i shared images img tool delete new image dfs browser action refresh refresh png wizards new image wizard mapper new mapwiz png new image wizard reducer new reducewiz png new image wizard driver new driverwiz png new image wizard mapreduce project new projwiz png accessor images image descriptor get image descriptor by name string name return desc map get name accessor images image get image by name string name return image map get name access platform shared images image descriptor get shared by name string name return shared images get image descriptor name load register new image if image resource exist fails load default error resource supplied boolean new image string name string filename image descriptor id boolean success try url file url file locator find bundle new path resource dir filename null id image descriptor create from url file locator file url file url success true catch exception e e print stack trace id image descriptor get missing image descriptor id get shared by name i shared images img objs error tsk success false desc map put name id image map put name id create image true return success register image workspace shared image pool if image resource exist fails load default error resource supplied boolean new shared image string name string shared name boolean success true image descriptor id
867	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\MapReduceNature.java	unrelated	package org apache hadoop eclipse class configure deconfigure eclipse project map reduce project nature map reduce nature implements i project nature string id org apache hadoop eclipse nature i project project logger log logger get logger map reduce nature get name configures eclipse project map reduce project adding hadoop libraries project classpath configure throws core exception string path project get persistent property new qualified name activator plugin id hadoop runtime path file dir new file path array list file core jars new array list file dir list files new file filter boolean accept file pathname string file name pathname get name get hadoop core jar without touching test examples older version hadoop use word core eyhung file name index of hadoop file name ends with jar file name index of test file name index of examples core jars add pathname return false care returns file dir new file path file separator char lib dir exists dir directory dir list files new file filter boolean accept file pathname pathname directory pathname get name ends with jar core jars add pathname return false care returns add hadoop libraries onto classpath i java project java project java core create get project bundle bundle activator get default get bundle try i classpath entry current cp java project get raw classpath i classpath entry new cp new i classpath entry current cp length core jars size system arraycopy current cp new cp current cp length iterator file core jars iterator count next core jar names length file f file next url url file locator file url file locator find bundle new path lib core jar names null url url f uri url log finer hadoop library url get path url get path new cp new cp length count java core new library entry new path url get path null null count java project set raw classpath new cp new null progress monitor catch exception e log log level severe io exception generated get class get canonical name e deconfigure project map reduce status currently unimplemented deconfigure throws core exception todo auto generated method stub returns project project nature applies i project get project return project sets project nature applies used instantiating project nature runtime set project i project project project project
868	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewDriverWizard.java	unrelated	package org apache hadoop eclipse wizard creating new driver runs map reduce job new driver wizard extends new element wizard implements i new wizard i runnable with progress new driver wizard page page run i progress monitor monitor try page create type monitor catch core exception e todo auto generated catch block e print stack trace catch interrupted exception e todo auto generated catch block e print stack trace new driver wizard set window title new map reduce driver init i workbench workbench i structured selection selection super init workbench selection page new new driver wizard page add page page page set selection selection performs actions appropriate response user pressed finish button refuse finishing permitted boolean perform finish super perform finish get created element null select and reveal page get modified resource open resource i file page get modified resource return true else return false protected finish page i progress monitor monitor throws interrupted exception core exception run monitor i java element get created element return page get created type get primary element
869	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewDriverWizardPage.java	unrelated	package org apache hadoop eclipse pre fills new map reduce driver template new driver wizard page extends new type wizard page button create map method text reducer text text mapper text boolean show container selector new driver wizard page true new driver wizard page boolean show container selector super true map reduce driver show container selector show container selector set title map reduce driver set description create new map reduce driver set image descriptor image library get wizard driver new set selection i structured selection selection init container page get initial java element selection init type page get initial java element selection creates new type using entered field values create type i progress monitor monitor throws core exception interrupted exception super create type monitor protected create type members i type new type imports manager imports i progress monitor monitor throws core exception super create type members new type imports monitor todo jz move code runnable get container get shell get display sync exec new runnable run string method main string args n job client client new job client method job conf conf new job conf new type get fully qualified name n n method todo specify output types nconf set output key class text nconf set output value class int writable n n method todo specify input output directories files nconf set input path new path src nconf set output path new path n n mapper text get text length method conf set mapper class mapper text get text n n else method todo specify mapper nconf set mapper class org apache hadoop mapred lib identity mapper n n reducer text get text length method conf set reducer class reducer text get text n n else method todo specify reducer nconf set reducer class org apache hadoop mapred lib identity reducer n n method client set conf conf n method try n job client run job conf n catch exception e n te print stack trace n n method n try new type create method method null false monitor catch java model exception e todo auto generated catch block e print stack trace create control composite parent super create control parent initialize dialog units parent composite composite new composite parent swt none grid layout layout new grid layout layout num columns composite set layout layout create container controls composite create package controls composite create separator composite create type name controls composite create super class controls composite create super interfaces controls composite create separator composite create mapper controls composite create reducer controls composite show container selector set package fragment root null false set super class java lang object false set super interfaces new array list false set control composite set focus handle field changed container set super class org apache hadoop mapred map reduce base true set super interfaces arrays list new string org apache hadoop mapred mapper true protected handle field changed string field name super handle field changed field name validate validate show container selector update status new i status f container status f package status f type name status f
870	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewMapperWizard.java	unrelated	package org apache hadoop eclipse wizard creating new mapper runs map portion map reduce job the pre filled template new mapper wizard extends new element wizard implements i new wizard i runnable with progress page page new mapper wizard set window title new mapper run i progress monitor monitor try page create type monitor catch core exception e todo auto generated catch block e print stack trace catch interrupted exception e todo auto generated catch block e print stack trace init i workbench workbench i structured selection selection super init workbench selection page new page add page page page set selection selection page extends new type wizard page button create map method page super true mapper set title mapper set description create new mapper implementation set image descriptor image library get wizard mapper new set selection i structured selection selection init container page get initial java element selection init type page get initial java element selection create type i progress monitor monitor throws core exception interrupted exception super create type monitor protected create type members i type new type imports manager imports i progress monitor monitor throws core exception super create type members new type imports monitor new type create method map writable comparable key writable values output collector output reporter reporter throws io exception n n n null false monitor create control composite parent super create control parent initialize dialog units parent composite composite new composite parent swt none grid layout layout new grid layout layout num columns composite set layout layout create container controls composite create package controls composite create separator composite create type name controls composite create super class controls composite create super interfaces controls composite create separator composite set control composite set super class org apache hadoop mapred map reduce base true set super interfaces arrays list new string org apache hadoop mapred mapper true set focus validate protected handle field changed string field name super handle field changed field name validate validate update status new i status f container status f package status f type name status f super class status f super interfaces status boolean perform finish super perform finish get created element null open resource i file page get modified resource select and reveal page get modified resource return true else return false protected finish page i progress monitor monitor throws interrupted exception core exception run monitor i java element get created element return page get created type get primary element
871	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewMapReduceProjectWizard.java	unrelated	package org apache hadoop eclipse wizard creating new map reduce project new map reduce project wizard extends wizard implements i workbench wizard i executable extension logger log logger get logger new map reduce project wizard get name hadoop first page first page new java project wizard page java page new driver wizard page new driver page i configuration element config new map reduce project wizard set window title new map reduce project wizard init i workbench workbench i structured selection selection boolean finish return first page page complete java page page complete first page generate driver get selection new driver page page complete i wizard page get next page i wizard page page page first page first page generate driver get selection return new driver page generate mapper checked second page new driver page else i wizard page answer super get next page page answer new driver page return null dont flip new driver page unless generate driver checked else answer java page return answer else return answer i wizard page get previous page i wizard page page page new driver page return first page new driver page appears second page else return super get previous page page hadoop first page extends wizard new project creation page implements selection listener hadoop first page super new hadoop project set image descriptor image library get wizard mapreduce project new link open preferences button workspace hadoop button project hadoop text location button browse string path string current path button generate driver create control composite parent super create control parent set title map reduce project set description create map reduce project group group new group composite get control swt none group set layout data new grid data grid data fill horizontal group set text hadoop map reduce library installation path grid layout layout new grid layout true layout margin left convert horizontal dl us to pixels i dialog constants horizontal margin layout margin right convert horizontal dl us to pixels i dialog constants horizontal margin layout margin top convert horizontal dl us to pixels i dialog constants vertical margin layout margin bottom convert horizontal dl us to pixels i dialog constants vertical margin group set layout layout workspace hadoop new button group swt radio grid data new grid data grid data beginning grid data beginning false false horizontal span workspace hadoop set layout data workspace hadoop set text use default workbench hadoop library location workspace hadoop set selection true update hadoop dir label from preferences open preferences new link group swt none open preferences set text configure hadoop install directory open preferences set layout data new grid data grid data end grid data center false false open preferences add selection listener project hadoop new button group swt radio project hadoop set layout data new grid data grid data beginning grid data center false false project hadoop set text specify hadoop library location location new text group swt single swt border location set text new grid data grid data end grid data center true false horizontal span width hint grab excess horizontal space true location set layout
872	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\NewReducerWizard.java	unrelated	package org apache hadoop eclipse wizard creating new reducer runs reduce portion map reduce job the pre filled template new reducer wizard extends new element wizard implements i new wizard i runnable with progress page page new reducer wizard set window title new reducer run i progress monitor monitor try page create type monitor catch core exception e todo auto generated catch block e print stack trace catch interrupted exception e todo auto generated catch block e print stack trace init i workbench workbench i structured selection selection super init workbench selection page new page add page page page set selection selection page extends new type wizard page page super true reducer set title reducer set description create new reducer implementation set image descriptor image library get wizard reducer new set selection i structured selection selection init container page get initial java element selection init type page get initial java element selection create type i progress monitor monitor throws core exception interrupted exception super create type monitor protected create type members i type new type imports manager imports i progress monitor monitor throws core exception super create type members new type imports monitor new type create method reduce writable comparable key iterator values output collector output reporter reporter throws io exception n n replace key type real type key n key type key key type key n n twhile values next n replace value type real type value n value type value value type values next n n process value n n n null false monitor create control composite parent super create control parent initialize dialog units parent composite composite new composite parent swt none grid layout layout new grid layout layout num columns composite set layout layout create container controls composite create package controls composite create separator composite create type name controls composite create super class controls composite create super interfaces controls composite create separator composite set control composite set super class org apache hadoop mapred map reduce base true set super interfaces arrays list new string org apache hadoop mapred reducer true set focus validate protected handle field changed string field name super handle field changed field name validate validate update status new i status f container status f package status f type name status f super class status f super interfaces status boolean perform finish super perform finish get created element null select and reveal page get modified resource open resource i file page get modified resource return true else return false protected finish page i progress monitor monitor throws interrupted exception core exception run monitor i java element get created element return page get created type null null page get created type get primary element
873	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\PropertyTester.java	unrelated	package org apache hadoop eclipse class help debugging properties property tester extends org eclipse core expressions property tester logger log logger get logger property tester get name property tester boolean test object receiver string property object args object expected value log fine test property property receiver get class return true todo jz support test deployable module hadoop nature etc
874	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\DFSActionImpl.java	unrelated	package org apache hadoop eclipse actions actual implementation dfs actions dfs action impl implements i object action delegate i selection selection i workbench part target part set active part i action action i workbench part target part target part target part run i action action ignore non structured selections selection instanceof i structured selection return operate dfs asynchronously prevent blocking main ui i structured selection ss i structured selection selection string action id action get action definition id display get default async exec new runnable run try switch dfs actions get by id action id case delete delete ss break case open open ss break case mkdir mkdir ss break case upload files upload files to dfs ss break case upload dir upload directory to dfs ss break case refresh refresh ss break case download download from dfs ss break case reconnect reconnect ss break case disconnect disconnect ss break default system err printf unhandled dfs action action id break catch exception e e print stack trace message dialog open error display get default get active shell dfs action error an error occurred performing dfs operation e get message create new sub folder existing directory mkdir i structured selection selection list dfs folder folders filter selection dfs folder selection folders size dfs folder folder folders get input dialog dialog new input dialog display get current get active shell create subfolder enter name subfolder null dialog open input dialog ok folder mkdir dialog get value implement import action upload files current machine hdfs upload files to dfs i structured selection selection throws invocation target exception interrupted exception ask user files upload file dialog dialog new file dialog display get current get active shell swt open swt multi dialog set text select local files upload dialog open list file files new array list file string fname dialog get file names files add new file dialog get filter path file separator fname todo enable upload command selection exactly one folder list dfs folder folders filter selection dfs folder selection folders size upload to dfs folders get files implement import action upload directory current machine hdfs upload directory to dfs i structured selection selection throws invocation target exception interrupted exception ask user local directory upload directory dialog dialog new directory dialog display get current get active shell swt open swt multi dialog set text select local file directory upload string dir name dialog open file dir new file dir name list file files new array list file files add dir todo enable upload command selection exactly one folder list dfs folder folders filter selection dfs folder selection folders size upload to dfs folders get files upload to dfs dfs folder folder list file files throws invocation target exception interrupted exception platform ui get workbench get progress service busy cursor while new i runnable with progress run i progress monitor monitor throws invocation target exception work file file files work compute upload work file monitor begin task uploading files distributed file system work file file files try folder upload monitor file catch io exception ioe ioe print
875	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\EditLocationAction.java	unrelated	package org apache hadoop eclipse actions editing server properties action edit location action extends action server view server view edit location action server view server view server view server view set text edit hadoop location set image descriptor image library get server view action location edit run hadoop server server server view get selected server server null return wizard dialog dialog new wizard dialog null new wizard hadoop location wizard page new hadoop location wizard server add pages super add pages set window title edit hadoop location add page page boolean perform finish page perform finish return true dialog create dialog set block on open true dialog open super run
876	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\NewLocationAction.java	unrelated	package org apache hadoop eclipse actions action corresponding creating new map reduce server new location action extends action new location action set text new hadoop location set image descriptor image library get server view action location new run wizard dialog dialog new wizard dialog null new wizard hadoop location wizard page new hadoop location wizard add pages super add pages set window title new hadoop location add page page boolean perform finish page perform finish return true dialog create dialog set block on open true dialog open super run
877	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\OpenNewMRClassWizardAction.java	unrelated	package org apache hadoop eclipse actions action open new map reduce class open new mr class wizard action extends action implements i cheat sheet action logger log logger get logger open new mr class wizard action get name run string params i cheat sheet manager manager params null params length i workbench workbench platform ui get workbench i new wizard wizard get wizard params wizard init workbench new structured selection wizard dialog dialog new wizard dialog platform ui get workbench get active workbench window get shell wizard dialog create dialog open wizard succeed notify result dialog get return code window ok i new wizard get wizard string type name type name equals mapper return new new mapper wizard else type name equals reducer return new new reducer wizard else type name equals driver return new new driver wizard else log severe invalid wizard requested return null
878	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\actions\OpenNewMRProjectAction.java	unrelated	package org apache hadoop eclipse actions action open new map reduce project open new mr project action extends action run i workbench workbench platform ui get workbench shell shell workbench get active workbench window get shell new map reduce project wizard wizard new new map reduce project wizard wizard init workbench new structured selection wizard dialog dialog new wizard dialog shell wizard dialog create dialog open wizard succeed notify result dialog get return code window ok
879	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\ActionProvider.java	unrelated	package org apache hadoop eclipse dfs allows user delete refresh items dfs tree action provider extends common action provider i common action extension site site action provider init i common action extension site site action provider site null system err printf multiple init n get class get canonical name return super init site action provider site site fill action bars i action bars action bars action bars set global action handler action factory delete get id new dfs action dfs actions delete action bars set global action handler action factory refresh get id new dfs action dfs actions refresh site null return site get structured viewer get selection instanceof i structured selection i structured selection site get structured viewer get selection size i structured selection site get structured viewer get selection get first element instanceof dfs file action bars set global action handler i common action constants open new dfs action dfs actions open action bars update action bars fill context menu i menu manager menu actions multiple selections menu append to group i common menu constants group edit new dfs action dfs actions delete menu append to group i common menu constants group open new dfs action dfs actions refresh menu append to group i common menu constants group new new dfs action dfs actions download site null return i selection isel site get structured viewer get selection isel instanceof i structured selection return actions single selections i structured selection issel i structured selection isel issel size return object element issel get first element element instanceof dfs file menu append to group i common menu constants group open new dfs action dfs actions open else element instanceof dfs folder menu append to group i common menu constants group new new dfs action dfs actions mkdir menu append to group i common menu constants group new new dfs action dfs actions upload files menu append to group i common menu constants group new new dfs action dfs actions upload dir else element instanceof dfs location menu append to group i common menu constants group open new dfs action dfs actions reconnect else element instanceof dfs locations root menu append to group i common menu constants group open new dfs action dfs actions disconnect representation action dfs entry browser dfs action extends action string id string title dfs actions action dfs action string id string title id id title title dfs action dfs actions action id action id title action title string get text return title image descriptor get image descriptor return image library get get action definition id string get action definition id return id run dfs action impl action new dfs action impl action set active part platform ui get workbench get active workbench window get active page get active part action selection changed site get structured viewer get selection action run boolean enabled return true
880	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSActions.java	unrelated	package org apache hadoop eclipse dfs enum dfs actions delete delete refresh refresh download download dfs open view mkdir create new directory upload files upload files dfs upload dir upload directory dfs reconnect reconnect disconnect disconnect string title string id string prefix dfs browser action dfs actions get by id string def def starts with prefix return null return value of def substring prefix length upper case dfs actions string title title title id prefix name lower case
881	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSContent.java	unrelated	package org apache hadoop eclipse dfs interface define content entities dfs browser dfs content boolean children dfs content get children refresh
882	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSContentProvider.java	unrelated	package org apache hadoop eclipse dfs handles viewing dfs locations p the content handled provider tree tt br dfs locations root br hadoop server br dfs folder br dfs file br dfs folder br br hadoop server tt the code block blocking operations need done asynchronously freeze ui dfs content provider implements i tree content provider i label provider the viewer displays tree content viewer viewer structured viewer sviewer map hadoop server dfs content root folders new hash map hadoop server dfs content constructor load resources icons dfs content provider dfs locations root locations root new dfs locations root i tree content provider implementation object get children object parent parent instanceof dfs content return null dfs content content dfs content parent return content get children object test object parent element parent element instanceof dfs locations root return server registry get instance get servers array else parent element instanceof hadoop server hadoop server location hadoop server parent element object root root folders get location root null return new object root return new object connecting dfs else parent element instanceof dfs folder dfs folder folder dfs folder parent element return folder get children return new object unknown dfs content object get parent object element element instanceof dfs path return dfs path element get parent else element instanceof hadoop server return locations root return null boolean children object element element instanceof dfs content dfs content content dfs content element return content children return false i structure content provider implementation object get elements object input element return new object locations root return server registry get instance get servers array i label provider implementation image get image object element element instanceof dfs locations root return image library get image dfs browser root entry else element instanceof dfs location return image library get image dfs browser location entry else element instanceof dfs folder return image library get image dfs browser folder entry else element instanceof dfs file return image library get image dfs browser file entry return null string get text object element element instanceof dfs file return dfs file element detailed string return element string i base label provider implementation add listener i label provider listener listener remove listener i label provider listener listener boolean label property object element string property return false i content provider implementation dispose input changed viewer viewer object old input object new input viewer viewer viewer null viewer instanceof structured viewer sviewer structured viewer viewer else sviewer null miscellaneous ask viewer content refresh refresh display nothing update viewer null return display get default async exec new runnable run dfs content provider viewer refresh ask viewer refresh single element refresh dfs content content sviewer null display get default async exec new runnable run dfs content provider sviewer refresh content else refresh viewer get viewer return viewer
883	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSFile.java	unrelated	package org apache hadoop eclipse dfs file handling methods dfs dfs file extends dfs path implements dfs content protected length protected short replication constructor upload file distributed file system dfs file dfs path parent path path file file i progress monitor monitor super parent path upload monitor file dfs file dfs path parent path path super parent path try file status fs get dfs get file status path length fs get len replication fs get replication catch io exception e e print stack trace download view contents file input stream open throws io exception return get dfs open path download file local file system this creates download status monitor download to local file file file throws invocation target exception interrupted exception platform ui get workbench get progress service busy cursor while new i runnable with progress run i progress monitor monitor throws invocation target exception dfs file download to local file monitor file download to local directory i progress monitor monitor file dir file dfs path new file get path string file destination new file dir dfs path get name destination exists boolean answer message dialog open question null overwrite existing local file the file attempting download dfs get path already exists local directory destination n overwrite existing file answer return try download to local file monitor destination catch exception e e print stack trace message dialog open warning null download local file system downloading file path local directory dir failed n e provides detailed file tt lt filename gt lt size gt r lt replication gt tt string detailed string string units b kb mb gb tb unit length unit units length unit return string format f r super string units unit replication string string return path string download dfs file local file use given monitor report status operation download to local file i progress monitor monitor file file throws invocation target exception task size monitor set task name download file path buffered output stream ostream null data input stream istream null try istream get dfs open path ostream new buffered output stream new file output stream file bytes byte buffer new byte task size bytes istream read buffer monitor canceled return ostream write buffer bytes monitor worked catch exception e throw new invocation target exception e finally clean opened resources istream null try istream close catch io exception e e print stack trace nothing try ostream close catch io exception e e print stack trace nothing upload local file file distributed file system upload i progress monitor monitor file file task size monitor set task name upload file path buffered input stream istream null data output stream ostream null try istream new buffered input stream new file input stream file ostream get dfs create path bytes byte buffer new byte task size bytes istream read buffer monitor canceled return ostream write buffer bytes monitor worked catch exception e error message dialog display string format unable uploade file file path e get localized message finally try istream null istream close catch io exception e e print stack trace nothing try ostream
884	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSFolder.java	unrelated	package org apache hadoop eclipse dfs local representation folder dfs the constructor creates empty representation folder spawn thread fill dfs folder extends dfs path implements dfs content logger log logger get logger dfs folder get name dfs content children protected dfs folder dfs content provider provider hadoop server location throws io exception super provider location dfs folder dfs path parent path path super parent path protected load dfs folder children throws io exception list dfs path list new array list dfs path file status status get dfs list status get path status dir list add new dfs folder status get path else list add new dfs file status get path children list array new dfs content list size upload given file directory dfs folder upload i progress monitor monitor file file throws io exception file directory path file path new path path file get name get dfs mkdirs file path dfs folder new folder new dfs folder file path monitor worked file child file list files monitor canceled return new folder upload monitor child else file file path file path new path path file get name dfs file new file new dfs file file path file monitor else xxx know file download to local directory i progress monitor monitor file dir dir exists dir mkdirs dir directory message dialog open error null download local file system invalid directory location dir return file dfs path new file get path string file destination new file dir dfs path get name destination exists destination mkdir message dialog open error null download local directory unable create directory destination get absolute path return download dfs path children object child obj get children child obj instanceof dfs path dfs path child obj download to local directory monitor destination monitor worked compute download work work dfs content child get children child instanceof dfs path work dfs path child compute download work return work create new sub directory directory mkdir string folder name try get dfs mkdirs new path path folder name catch io exception ioe ioe print stack trace refresh implementation dfs content boolean children children null return true else return children length dfs content get children children null new job connecting dfs location protected i status run i progress monitor monitor try load dfs folder children return status ok status catch io exception ioe children new dfs content new dfs message error ioe get localized message return status cancel status finally under circumstances update ui provider refresh dfs folder schedule return new dfs content new dfs message listing folder content return children refresh children null refresh string string return string format super string get children length
885	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSLocation.java	unrelated	package org apache hadoop eclipse dfs dfs content representation hdfs location dfs location implements dfs content dfs content provider provider hadoop server location dfs content root folder null dfs location dfs content provider provider hadoop server server provider provider location server string string return location get location name implementation dfs content dfs content get children root folder null dfs folder constructor might block contacts name node work asynchronously potentially freeze ui new job connecting dfs location protected i status run i progress monitor monitor try root folder new dfs folder provider location return status ok status catch io exception ioe root folder new dfs message error ioe get localized message return status cancel status finally under circumstances update ui provider refresh dfs location schedule return new dfs content new dfs message connecting dfs string return new dfs content root folder boolean children return true refresh root folder null provider refresh actions refresh location using new connection reconnect refresh
886	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSLocationsRoot.java	unrelated	package org apache hadoop eclipse dfs representation root element containing dfs servers this content registers observer hadoop servers update servers updated dfs locations root implements dfs content i hadoop server listener dfs content provider provider map hadoop server dfs location map new hash map hadoop server dfs location register listeners track dfs locations updates dfs locations root dfs content provider provider provider provider server registry get instance add listener refresh implementation i hadoop server listener synchronized server changed hadoop server location type switch type case server registry server state changed provider refresh map get location break case server registry server added dfs location dfs loc new dfs location provider location map put location dfs loc provider refresh break case server registry server removed map remove location provider refresh break recompute map hadoop locations synchronized reload locations map clear hadoop server location server registry get instance get servers map put location new dfs location provider location string string return dfs locations implementation dfs content synchronized dfs content get children return map values array new dfs content map size boolean children return map size refresh reload locations provider refresh actions disconnect thread close thread new thread run try system printf closing opened file systems n file system close all system printf file systems closed n catch io exception ioe ioe print stack trace wait seconds connections closed close thread start try close thread join catch interrupted exception ie ignore
887	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSMessage.java	unrelated	package org apache hadoop eclipse dfs dfs content displays message dfs message implements dfs content string message dfs message string message message message string string return message implementation dfs content dfs content get children return null boolean children return false refresh nothing
888	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\dfs\DFSPath.java	unrelated	package org apache hadoop eclipse dfs dfs path handling dfs dfs path implements dfs content protected dfs content provider provider protected hadoop server location distributed file system dfs null protected path path protected dfs path parent for debugging purpose logger log logger get logger dfs path get name create path representation given location given viewer dfs path dfs content provider provider hadoop server location throws io exception provider provider location location path new path parent null create sub path representation given parent path protected dfs path dfs path parent path path provider parent provider location parent location dfs parent dfs parent parent path path protected dispose free dfs connection string string path equals return location get conf prop conf prop fs default uri else return path get name does recursive delete remote directory tree node delete try get dfs delete path true catch io exception e e print stack trace message dialog open warning null delete file unable delete file path n e dfs path get parent return parent refresh refresh ui element content refresh provider refresh copy dfs path given local directory download to local directory i progress monitor monitor file dir path get path return path gets connection dfs distributed file system get dfs throws io exception dfs null file system fs location get dfs fs instanceof distributed file system error message dialog display dfs browser the dfs browser cannot browse anything else distributed file system throw new io exception dfs browser expects distributed file system dfs distributed file system fs return dfs compute download work
889	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\HadoopApplicationLaunchShortcut.java	unrelated	package org apache hadoop eclipse launch add shortcut run hadoop run menu hadoop application launch shortcut extends java application launch shortcut logger log logger get logger hadoop application launch shortcut get name action delegate delegate new run on hadoop action delegate hadoop application launch shortcut protected i launch configuration find launch configuration i type type i launch configuration type config type find existing create launch configuration standard way i launch configuration conf super find launch configuration type config type conf null conf super create configuration type i launch configuration working copy conf wc try tune default launch configuration setup run time classpath manually conf wc conf get working copy conf wc set attribute i java launch configuration constants attr default classpath false list string path new array list string i resource resource type get resource i java project project i java project resource get project get nature java core nature id i runtime classpath entry cp entry java runtime new default project classpath entry project path add cp entry get memento conf wc set attribute i java launch configuration constants attr classpath path catch core exception e e print stack trace fixme error dialog return null update selected configuration specific hadoop location target i resource resource type get resource resource instanceof i file return null run on hadoop wizard wizard new run on hadoop wizard i file resource conf wc wizard dialog dialog new wizard dialog display get default get active shell wizard dialog create dialog set block on open true dialog open wizard dialog ok return null try only save configuration different conf wc contents equal conf conf wc save catch core exception e e print stack trace fixme error dialog return null return conf wc was used run run on hadoop wizard inside provide progress monitor dialog extends wizard dialog dialog shell parent shell i wizard new wizard super parent shell new wizard create super create run on hadoop wizard get wizard set progress monitor get progress monitor
890	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\LocalMapReduceLaunchTabGroup.java	unrelated	package org apache hadoop eclipse launch handler local map reduce job launches todo jz may needed almost always deploy remote server locally locally may able exec scripts without going java local map reduce launch tab group extends abstract launch configuration tab group local map reduce launch tab group todo auto generated constructor stub create tabs i launch configuration dialog dialog string mode set tabs new i launch configuration tab new map reduce launch tab new java arguments tab new java jre tab new java classpath tab new common tab map reduce launch tab extends abstract launch configuration tab text combiner class text reducer class text mapper class boolean save return true boolean valid i launch configuration launch config todo proper types return true create control composite parent composite panel new composite parent swt none grid layout layout new grid layout false panel set layout layout label mapper label new label panel swt none mapper label set text mapper mapper class new text panel swt single swt border create row parent panel mapper class label reducer label new label panel swt none reducer label set text reducer reducer class new text panel swt single swt border create row parent panel reducer class label combiner label new label panel swt none combiner label set text combiner combiner class new text panel swt single swt border create row parent panel combiner class panel pack set control panel create row composite parent composite panel text text text set layout data new grid data grid data fill horizontal button button new button panel swt border button set text browse button add listener swt selection new listener handle event event arg try ast ast ast new ast selection dialog dialog java ui create type dialog parent get shell new progress monitor dialog parent get shell search engine create workspace scope i java element search constants consider classes false dialog set message select mapper type implementing dialog set block on open true dialog set title select mapper type dialog open dialog get return code window ok dialog get result length i type type i type dialog get result text set text type get fully qualified name set dirty true catch java model exception e todo auto generated catch block e print stack trace string get name return hadoop initialize from i launch configuration configuration try mapper class set text configuration get attribute org apache hadoop eclipse launch mapper reducer class set text configuration get attribute org apache hadoop eclipse launch reducer combiner class set text configuration get attribute org apache hadoop eclipse launch combiner catch core exception e todo auto generated catch block e print stack trace set error message e get message perform apply i launch configuration working copy configuration configuration set attribute org apache hadoop eclipse launch mapper mapper class get text configuration set attribute org apache hadoop eclipse launch reducer reducer class get text configuration set attribute org apache hadoop eclipse launch combiner combiner class get text set defaults i launch configuration working copy configuration
891	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\MutexRule.java	unrelated	package org apache hadoop eclipse launch mutex rule implements i scheduling rule string id mutex rule string id id id boolean contains i scheduling rule rule return rule instanceof mutex rule mutex rule rule id equals id boolean conflicting i scheduling rule rule return rule instanceof mutex rule mutex rule rule id equals id
892	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\launch\StartHadoopLaunchTabGroup.java	unrelated	package org apache hadoop eclipse launch create tab group dialog window starting hadoop job start hadoop launch tab group extends abstract launch configuration tab group start hadoop launch tab group todo auto generated constructor stub todo jz consider appropriate tabs case create tabs i launch configuration dialog dialog string mode set tabs new i launch configuration tab new java arguments tab new java jre tab new java classpath tab new common tab
893	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\MapReducePreferencePage.java	unrelated	package org apache hadoop eclipse preferences this represents preference page contributed preferences dialog by sub classing tt field editor preference page tt use field support built j face allows us create page small knows save restore apply p this page used modify preferences they stored preference store belongs main plug that way preferences accessed directly via preference store map reduce preference page extends field editor preference page implements i workbench preference page map reduce preference page super grid set preference store activator get default get preference store set title hadoop map reduce tools set description hadoop map reduce preferences creates field editors field editors abstractions common gui blocks needed manipulate various types preferences each field editor knows save restore create field editors add field new directory field editor preference constants p path hadoop installation directory get field editor parent init i workbench workbench
894	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\PreferenceConstants.java	unrelated	package org apache hadoop eclipse preferences constant definitions plug preferences preference constants string p path path preference string p boolean boolean preference string p choice choice preference string p string preference
895	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\preferences\PreferenceInitializer.java	unrelated	package org apache hadoop eclipse preferences class used initialize default preference values preference initializer extends abstract preference initializer initialize default preferences
896	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\ConfProp.java	unrelated	package org apache hadoop eclipse server enum conf prop property name hadoop location name pi location name true location name new hadoop location property name master host name job tracker pi job tracker host true jobtracker host localhost property name dfs master host name name node pi name node host true namenode host localhost property name installation directory master node pi install dir true install dir dir hadoop version user name use hadoop operations pi user name true user name system get property user name property name socks proxy activation pi socks proxy enable true socks proxy enable property name socks proxy host pi socks proxy host true socks proxy host host property name socks proxy port pi socks proxy port true socks proxy port tcp port number name node pi name node port true namenode port tcp port number job tracker pi job tracker port true jobtracker port are map reduce distributed fs masters hosted machine pi colocate masters true masters colocate yes property name naming job tracker uri this property related link pi master host name job tracker uri false mapreduce jobtracker address localhost property name naming default file system uri fs default uri false fs default name hdfs localhost property name default socket factory socket factory default false hadoop rpc socket factory default org apache hadoop net standard socket factory property name socks server uri socks server false hadoop socks server host map property name conf prop map string conf prop map synchronized register property string name conf prop prop conf prop map null conf prop map new hash map string conf prop conf prop map put name prop conf prop get by name string prop name return map get prop name string name string def val conf prop boolean internal string name string def val internal name eclipse plug name name name def val def val conf prop register property name string get configuration conf return conf get name set configuration conf string value assert value null conf set name value
897	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopJob.java	unrelated	package org apache hadoop eclipse server representation map reduce running job given location hadoop job enum representation job state enum job state prepare job status prep running job status running failed job status failed succeeded job status succeeded state job state state state state job state int state state job status prep return prepare else state job status running return running else state job status failed return failed else state job status succeeded return succeeded else return null location job runs hadoop server location unique identifier job job id job id status representation running job this actually contains reference job client its methods might block running job running last polled status job status status last polled counters counters counters job configuration job conf job conf null boolean completed false boolean successful false boolean killed false total maps total reduces completed maps completed reduces map progress reduce progress constructor hadoop job representation hadoop job hadoop server location job id id running job running job status status location location job id id running running load job file update status try locate load job conf file job get details job number maps reduces load job file try string job file get job file file system fs location get dfs file tmp file create temp file get job id string xml file util copy fs new path job file tmp false location get configuration job conf new job conf tmp string total maps job conf get num map tasks total reduces job conf get num reduce tasks catch io exception ioe ioe print stack trace hash code prime result result prime result job id null job id hash code result prime result location null location hash code return result boolean equals object obj obj return true obj null return false obj instanceof hadoop job return false hadoop job hadoop job obj job id null job id null return false else job id equals job id return false location null location null return false else location equals location return false return true get running status job see link job status job state get state completed successful return job state succeeded else return job state failed else return job state running return job state int status get run state job id get job id return job id hadoop server get location return location boolean completed return completed string get job name return running get job name string get job file return running get job file return tracking url job string get tracking url return running get tracking url returns representation job status string get status string buffer new string buffer append maps completed maps total maps append map progress append reduces completed reduces total reduces append reduce progress return string update job status according given job status update job status status status status try counters running get counters completed running complete successful running successful map progress running map progress reduce progress running reduce progress running get task completion events event catch io exception ioe ioe print stack trace completed maps total maps map progress completed reduces total reduces
898	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopPathPage.java	unrelated	package org apache hadoop eclipse server hadoop path page implements i editor part i editor input get editor input todo auto generated method stub return null i editor site get editor site todo auto generated method stub return null init i editor site site i editor input input throws part init exception todo auto generated method stub add property listener i property listener listener todo auto generated method stub create part control composite parent todo auto generated method stub dispose todo auto generated method stub i workbench part site get site todo auto generated method stub return null string get title todo auto generated method stub return null image get title image todo auto generated method stub return null string get title tool tip todo auto generated method stub return null remove property listener i property listener listener todo auto generated method stub set focus todo auto generated method stub object get adapter class adapter todo auto generated method stub return null save i progress monitor monitor todo auto generated method stub save as todo auto generated method stub boolean dirty todo auto generated method stub return false boolean save as allowed todo auto generated method stub return false boolean save on close needed todo auto generated method stub return false
899	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\HadoopServer.java	unrelated	package org apache hadoop eclipse server representation hadoop location meaning master node name node job tracker p this create ssh connection anymore tunneling must setup outside eclipse using putty tt ssh d lt port gt lt host gt tt p em todo em li disable updater location becomes unreachable fails tool li stop updater location disposal removal hadoop server frequency location status observations expressed delay ms observation todo add preference parameter protected status observation delay location status updater extends job job client client null setup updater location status updater super map reduce location status updater set system true protected i status run i progress monitor monitor client null try client hadoop server get job client catch io exception ioe client null return new status status error activator plugin id cannot connect map reduce location hadoop server get location name ioe try set known existing job i ds want fresh info set job id missing job ids new hash set job id running jobs key set job status jstatus client jobs to complete job status status jstatus job id job id status get job id missing job ids remove job id hadoop job job synchronized hadoop server running jobs job running jobs get job id job null unknown job create entry running job running client get job job id job new hadoop job hadoop server job id running status new job job update hadoop job fresh infos update job job status ask explicitly fresh info job i ds job id job id missing job ids hadoop job job running jobs get job id job completed update job job null catch io exception ioe client null return new status status error activator plugin id cannot retrieve running jobs location hadoop server get location name ioe schedule next observation schedule status observation delay return status ok status stores make new job available new job hadoop job data running jobs put data get job id data display get default async exec new runnable run fire job added data updates status job update job hadoop job job job status status job update status display get default async exec new runnable run fire job changed job logger log logger get logger hadoop server get name hadoop configuration location also contains specific parameters plug these parameters prefix eclipse plug configuration conf jobs listeners set i job listener job listeners new hash set i job listener jobs running location the keys map job i ds transient map job id hadoop job running jobs collections synchronized map new tree map job id hadoop job status updater location location status updater status updater state status transient transient string state creates new default hadoop location hadoop server conf new configuration add plugin config default properties creates location file hadoop server file file throws parser configuration exception sax exception io exception conf new configuration add plugin config default properties load from xml file create new hadoop location copying already existing one hadoop server hadoop server existing load existing add job listener i job listener job listeners add dispose todo close dfs connections list elements
900	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\IJobListener.java	unrelated	package org apache hadoop eclipse server interface updating adding jobs map reduce server view i job listener job changed hadoop job job job added hadoop job job job removed hadoop job job publish start jar module jar publish done jar module jar
901	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\server\JarModule.java	unrelated	package org apache hadoop eclipse server methods interacting jar file containing mapper reducer driver map reduce job jar module implements i runnable with progress logger log logger get logger jar module get name i resource resource file jar file jar module i resource resource resource resource string get name return resource get project get name resource get name creates jar file containing given resource java main associated resources run i progress monitor monitor log fine build jar jar package data jarrer new jar package data jarrer set export java files true jarrer set export class files true jarrer set export output folders true jarrer set overwrite true try i java project project i java project resource get project get nature java core nature id check case letting method get called object element resource get adapter i java element i type type i compilation unit element find primary type jarrer set manifest main class type create temporary jar file name file base dir activator get default get state location file string prefix string format resource get project get name resource get name file jar file file create temp file prefix jar base dir jarrer set jar location new path jar file get absolute path jarrer set elements resource get project members i resource file i jar export runnable runnable jarrer create jar export runnable display get default get active shell runnable run monitor jar file jar file catch exception e e print stack trace throw new runtime exception e allow retrieval resulting jar file file get jar file return jar file static way create jar package given resource showing progress bar file create jar package i resource resource jar module jar module new jar module resource try platform ui get workbench get progress service run false true jar module catch exception e e print stack trace return null file jar file jar module get jar file jar file null error message dialog display run hadoop unable create locate jar file job return null return jar file
902	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\HadoopLocationWizard.java	unrelated	package org apache hadoop eclipse servers wizard editing settings hadoop location the wizard contains tabs general tunneling advanced it edits parameters location member either new location copy existing registered location hadoop location wizard extends wizard page image circle the location effectively edited wizard this location copy new one hadoop server location the original location edited wizard null create new instance hadoop server original new hadoop location wizard hadoop location wizard super hadoop server new hadoop location null original null location new hadoop server location set location name constructor edit parameters existing hadoop server hadoop location wizard hadoop server server super create new hadoop location edit hadoop location null original server location new hadoop server server performs actions appropriate response user pressed finish button refuse finishing permitted hadoop server perform finish try original null new location display get default sync exec new runnable run server registry get instance add server hadoop location wizard location return location else update location string original name original get location name original load location display get default sync exec new runnable run server registry get instance update server original name hadoop location wizard location return original catch exception e e print stack trace set message invalid server location values i message provider error return null validates current hadoop location settings look hadoop installation directory test location set message not implemented yet i message provider warning location complete finish button available host name specified boolean page complete string loc name location get conf prop conf prop pi location name loc name null loc name length loc name contains set message bad location name location name contain character prohibited file name warning return false string master location get conf prop conf prop pi job tracker host master null master length set message bad master host name master host name refers machine runs job tracker warning return false string job tracker location get conf prop conf prop job tracker uri string strs job tracker split boolean ok strs length ok try port integer parse int strs ok port port catch number format exception nfe ok false ok set message the job tracker information conf prop job tracker uri name invalid this usually looks like host port warning return false string fs default uri location get conf prop conf prop fs default uri try uri uri new uri fs default uri catch uri syntax exception e set message the default file system uri invalid this usually looks like hdfs host port file dir warning set message define location hadoop infrastructure running map reduce applications return true create wizard create control composite parent set title define hadoop location set description define location hadoop infrastructure running map reduce applications composite panel new composite parent swt fill grid layout glayout new grid layout false panel set layout glayout tab mediator mediator new tab mediator panel grid data gdata new grid data grid data fill both gdata horizontal span mediator folder set layout data gdata set control panel mediator folder button btn new button panel swt none btn set text load file btn set enabled false
903	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\HadoopServerSelectionListContentProvider.java	unrelated	package org apache hadoop eclipse servers provider enables selection predefined hadoop server hadoop server selection list content provider implements i content provider i table label provider i structured content provider dispose input changed viewer viewer object old input object new input image get column image object element column index return null string get column text object element column index element instanceof hadoop server hadoop server location hadoop server element column index return location get location name else column index return location get master host name return element string add listener i label provider listener listener boolean label property object element string property return false remove listener i label provider listener listener object get elements object input element return server registry get instance get servers array
904	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\IHadoopServerListener.java	unrelated	package org apache hadoop eclipse servers interface monitoring server changes i hadoop server listener server changed hadoop server location type
905	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\RunOnHadoopWizard.java	unrelated	package org apache hadoop eclipse servers wizard publishing job hadoop server run on hadoop wizard extends wizard main wizard page main page hadoop location wizard create new page the file resource containing main run hadoop location i file resource the launch configuration update i launch configuration working copy conf i progress monitor progress monitor run on hadoop wizard i file resource i launch configuration working copy conf resource resource conf conf set force previous and next buttons true set needs progress monitor true set window title run hadoop this wizard contains pages li first one lets user choose already existing location li second one allows user create new location case already exist add pages add page main page new main wizard page add page create new page new hadoop location wizard performs actions appropriate response user pressed finish button refuse finishing permitted boolean perform finish create new location get existing one hadoop server location null main page create new get selection location create new page perform finish else main page table get selection length location hadoop server main page table get selection get data location null return false get base directory plug storing configurations ja rs file base dir activator get default get state location file package job jar file jar file jar module create jar package resource jar file null error message dialog display run hadoop unable create locate jar file job return false generate temporary hadoop configuration directory add classpath launch configuration file conf dir try conf dir file create temp file hadoop conf base dir conf dir delete conf dir mkdirs conf dir directory error message dialog display run hadoop cannot create temporary directory conf dir return false catch io exception ioe ioe print stack trace return false prepare hadoop configuration job conf conf new job conf location get configuration conf set jar jar file get absolute path write disk file try file conf file file create temp file core site xml conf dir file conf file new file conf dir core site xml file output stream fos new file output stream conf file conf write xml fos fos close catch io exception ioe ioe print stack trace return false setup launch path list string path try path conf get attribute i java launch configuration constants attr classpath new array list i path conf i path new path conf dir get absolute path i runtime classpath entry cp entry java runtime new archive runtime classpath entry conf i path path add cp entry get memento conf set attribute i java launch configuration constants attr classpath path conf set attribute i java launch configuration constants attr program arguments main page arguments text get text catch core exception e e print stack trace return false location run resource resource progress monitor return true refresh buttons get container update buttons allows finish existing server selected new server location defined boolean finish main page null return main page finish return false this main page wizard it allows user either choose already existing location indicate wants create new location main wizard page extends wizard
906	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\servers\ServerRegistry.java	unrelated	package org apache hadoop eclipse servers register hadoop locations each location corresponds hadoop link configuration stored xml file workspace plug configuration directory p tt lt workspace dir gt metadata plugins org apache hadoop eclipse locations xml tt server registry server registry instance new server registry server added server removed server state changed file base dir activator get default get state location file file save dir new file base dir locations server registry save dir exists save dir directory save dir delete save dir exists save dir mkdirs load map string hadoop server servers set i hadoop server listener listeners new hash set i hadoop server listener server registry get instance return instance synchronized collection hadoop server get servers return collections unmodifiable collection servers values load available locations workspace configuration directory synchronized load map string hadoop server map new tree map string hadoop server file file save dir list files try hadoop server server new hadoop server file map put server get location name server catch exception exn system err println exn servers map synchronized store try file dir file create temp file locations new base dir dir delete dir mkdirs hadoop server server servers values server store settings to file new file dir server get location name xml filename filter xml filter new filename filter boolean accept file dir string name string lower name lower case return lower ends with xml file backup new file base dir locations backup backup exists file file backup list files xml filter file delete throw new io exception unable delete backup location file file backup delete throw new io exception unable delete backup location directory backup save dir rename to backup dir rename to save dir catch io exception ioe ioe print stack trace message dialog open error null saving configuration hadoop locations failed ioe string dispose hadoop server server get servers server dispose synchronized hadoop server get server string location return servers get location hadoop server map listeners add listener i hadoop server listener synchronized listeners listeners add remove listener i hadoop server listener synchronized listeners listeners remove fire listeners hadoop server location kind synchronized listeners i hadoop server listener listener listeners listener server changed location kind synchronized remove server hadoop server server servers remove server get location name store fire listeners server server removed synchronized add server hadoop server server servers put server get location name server store fire listeners server server added update one hadoop location synchronized update server string original name hadoop server server update map location name changed server get location name equals original name servers remove original name servers put server get location name server store fire listeners server server state changed
907	mapreduce\src\contrib\eclipse-plugin\src\java\org\apache\hadoop\eclipse\view\servers\ServerView.java	unrelated	package org apache hadoop eclipse view servers map reduce locations view displays available hadoop locations jobs running finished locations server view extends view part implements i tree content provider i table label provider i job listener i hadoop server listener deletion action delete hadoop location kill running job remove finished job entry delete action extends action delete action set text delete set image descriptor image library get server view action delete run i selection selection get view site get selection provider get selection selection null selection instanceof i structured selection object sel item i structured selection selection get first element sel item instanceof hadoop server hadoop server location hadoop server sel item message dialog open confirm display get default get active shell confirm delete hadoop location do really want remove hadoop location location get location name server registry get instance remove server location else sel item instanceof hadoop job kill job hadoop job job hadoop job sel item job completed job already finished remove entry job get location purge job job else job running kill job message dialog open confirm display get default get active shell confirm kill running job do really want kill running job job get job id job kill this object root content content provider object content root new object i action delete action new delete action i action edit server action new edit location action i action new location action new new location action tree viewer viewer server view init i view site site throws part init exception super init site dispose server registry get instance remove listener creates columns view create part control composite parent tree main new tree parent swt single swt full selection swt h scroll swt v scroll main set header visible true main set lines visible false main set layout data new grid data grid data fill both tree column server col new tree column main swt single server col set text location server col set width server col set resizable true tree column location col new tree column main swt single location col set text master node location col set width location col set resizable true tree column state col new tree column main swt single state col set text state state col set width state col set resizable true tree column status col new tree column main swt single status col set text status status col set width status col set resizable true viewer new tree viewer main viewer set content provider viewer set label provider viewer set input content root care get view site set selection provider viewer get view site get action bars set global action handler action factory delete get id delete action get view site get action bars get tool bar manager add edit server action get view site get action bars get tool bar manager add new location action create actions create context menu actions create actions add item action new action add run add item add item action set image descriptor image library get server view location new delete item action new action delete run
908	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\AllocationConfigurationException.java	pooling	package org apache hadoop mapred thrown allocation file link pool manager malformed allocation configuration exception extends exception serial version uid l allocation configuration exception string message super message
909	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\CapBasedLoadManager.java	scheduler	package org apache hadoop mapred a link load manager use link fair scheduler allocates tasks evenly across nodes per node maximum using default load management algorithm hadoop cap based load manager extends load manager max diff f set conf configuration conf super set conf conf max diff conf get float mapred fairscheduler load max diff f determine many tasks given type want run task tracker this cap chosen based many tasks type outstanding total cluster used capacity tasks spread uniformly across nodes rather clumped whichever machines sent heartbeats earliest get cap total runnable tasks local max tasks total slots load max diff total runnable tasks total slots return math ceil local max tasks math min load boolean assign map task tracker status tracker total runnable maps total map slots return tracker count map tasks get cap total runnable maps tracker get max map slots total map slots boolean assign reduce task tracker status tracker total runnable reduces total reduce slots return tracker count reduce tasks get cap total runnable reduces tracker get max reduce slots total reduce slots boolean launch task task tracker status tracker job in progress job task type type return true
910	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\DefaultTaskSelector.java	scheduler	package org apache hadoop mapred a link task selector implementation wraps around default link job in progress obtain new map task task tracker status link job in progress obtain new reduce task task tracker status methods link job in progress using default hadoop locality speculative threshold algorithms default task selector extends task selector needed speculative maps job in progress job count time system current time millis task in progress tip job maps tip running tip be speculated time count return count needed speculative reduces job in progress job count time system current time millis avg progress job get status reduce progress task in progress tip job reduces tip running tip be speculated time count return count task obtain new map task task tracker status task tracker job in progress job locality level throws io exception cluster status cluster status task tracker manager get cluster status num task trackers cluster status get task trackers return job obtain new map task task tracker num task trackers task tracker manager get number of unique hosts locality level task obtain new reduce task task tracker status task tracker job in progress job throws io exception cluster status cluster status task tracker manager get cluster status num task trackers cluster status get task trackers return job obtain new reduce task task tracker num task trackers task tracker manager get number of unique hosts
911	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairScheduler.java	pooling	package org apache hadoop mapred a link task scheduler implements fair sharing fair scheduler extends task scheduler log log log factory get log org apache hadoop mapred fair scheduler how often fair shares calculated protected update interval how often dump scheduler state event log protected dump interval how often tasks preempted must longer couple heartbeats give task kill commands chance act protected preemption interval used iterate map reduce task types task type map and reduce new task type task type map task type reduce maximum locality delay auto computing locality delays max autocomputed locality delay protected pool manager pool mgr protected load manager load mgr protected task selector task selector protected weight adjuster weight adjuster can null weight adjuster protected map job in progress job info infos per job scheduling variables new hash map job in progress job info protected last update time time last updated infos protected last preemption update time time last updated preemption vars protected boolean initialized are initialized protected volatile boolean running are running protected boolean assign multiple simultaneously assign map reduce protected map assign cap max maps launch per heartbeat protected reduce assign cap max reduces launch per heartbeat protected node locality delay time wait node locality protected rack locality delay time wait rack locality protected boolean auto compute locality delay false compute locality delay heartbeat interval protected boolean size based weight give larger weights larger jobs protected boolean wait for maps before launching reduces true protected boolean preemption enabled protected boolean log preemption only log tasks killed clock clock job listener job listener job initializer job initializer boolean mock mode used unit tests disables background updates scheduler event log fair scheduler event log event log protected last dump time time last dumped state log protected last heartbeat time time last ran assign tasks last preempt check time time last ran preempt tasks if necessary a holding per job scheduler variables these always contain values variables last update used along time delta update map reduce deficits new update job info boolean runnable false can job run given user pool limits does job need initialized volatile boolean needs initializing true job schedulable map schedulable job schedulable reduce schedulable variables used delay scheduling locality level last map locality level locality level last map launched time waited for local map time waiting local map since last map boolean skipped at last heartbeat was job skipped previous assign tasks used update time waited for local map job info job schedulable map sched job schedulable reduce sched map schedulable map sched reduce schedulable reduce sched last map locality level locality level node fair scheduler new clock false constructor used tests change clock disable updates protected fair scheduler clock clock boolean mock mode clock clock mock mode mock mode job listener new job listener start try configuration conf get conf create scheduling log initialize enabled event log new fair scheduler event log boolean log enabled conf get boolean mapred fairscheduler eventlog enabled false mock mode log enabled string hostname localhost task tracker manager instanceof job tracker hostname job tracker task tracker manager get
912	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairSchedulerEventLog.java	scheduler	package org apache hadoop mapred event log used fair scheduler machine readable debug info this uses log j rolling file appender write log uses custom tab separated event format form pre date event type param param pre various event types used fair scheduler the purpose logging format enable tools parse history log easily read internal scheduler variables rather trying make log human readable the fair scheduler also logs human readable messages job tracker main log constructing creates disabled log it must initialized using link fair scheduler event log init configuration string begin writing file fair scheduler event log log log log factory get log org apache hadoop mapred fair scheduler event log set true logging disabled due error boolean log disabled true log directory set mapred fairscheduler eventlog location conf file defaults hadoop log dir fairscheduler string log dir active log file log dir hadoop user fairscheduler host log older files also stored log file date date format yyyy mm dd string log file log j appender used write log file daily rolling file appender appender boolean init configuration conf string jobtracker hostname try log dir conf get mapred fairscheduler eventlog location new file system get property hadoop log dir get absolute path file separator fairscheduler path log dir path new path log dir file system fs log dir path get file system conf fs exists log dir path fs mkdirs log dir path throw new io exception mkdirs failed create log dir path string string username system get property user name log file string format shadoop fairscheduler log log dir file separator username jobtracker hostname log disabled false pattern layout layout new pattern layout iso n appender new daily rolling file appender layout log file yyyy mm dd appender activate options log info initialized fair scheduler event log logging log file catch io exception e log error failed initialize fair scheduler event log disabling e log disabled true return log disabled log event writing line log file form pre date event type param param pre synchronized log string event type object params try log disabled return string buffer buffer new string buffer buffer append event type object param params buffer append buffer append param string message buffer string logger logger logger get logger get class appender append new logging event logger level info message null catch exception e log error failed append fair scheduler event log e log disabled true flush close log shutdown try appender null appender close catch exception e log disabled true boolean enabled return log disabled
913	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FairSchedulerServlet.java	scheduler	package org apache hadoop mapred servlet displaying fair scheduler information installed job tracker url scheduler link fair scheduler use the main features viewing job task count fair share admin controls change job priorities pools ui there also advanced view debugging turned going job tracker url scheduler advanced fair scheduler servlet extends http servlet serial version uid l date format date format new simple date format mmm dd hh mm fair scheduler scheduler job tracker job tracker last id used generate unique element i ds init throws servlet exception super init servlet context servlet context get servlet context scheduler fair scheduler servlet context get attribute scheduler job tracker job tracker scheduler task tracker manager protected post http servlet request req http servlet response resp throws servlet exception io exception get req resp same handler get post get http servlet request request http servlet response response throws servlet exception io exception if request set param handle redirect regular view page user resubmit data hit refresh boolean advanced view request get parameter advanced null jsp util actions allowed job tracker conf request get parameter set pool null collection job in progress running jobs get inited jobs pool manager pool mgr null synchronized scheduler pool mgr scheduler get pool manager string pool request get parameter set pool string job id request get parameter jobid job in progress job running jobs job get profile get job id string equals job id synchronized scheduler pool mgr set pool job pool scheduler update break response send redirect scheduler advanced view advanced return jsp util actions allowed job tracker conf request get parameter set priority null collection job in progress running jobs get inited jobs job priority priority job priority value of request get parameter set priority string job id request get parameter jobid job in progress job running jobs job get profile get job id string equals job id job set priority priority scheduler update break response send redirect scheduler advanced view advanced return print normal response response set content type text html because client may read arbitrarily slow hold locks servlet outputs want write buffer know block byte array output stream baos new byte array output stream print writer new print writer baos string hostname string utils simple hostname job tracker get job tracker machine print html head printf title fair scheduler administration title n hostname print link rel stylesheet type text css href hadoop css n print head body n printf href jobtracker jsp fair scheduler administration n hostname show pools advanced view show jobs advanced view print body html n close flush buffer real servlet output output stream servlet out response get output stream baos write to servlet out servlet out close print view pools given output writer show pools print writer boolean advanced view synchronized scheduler boolean warn inverted false pool manager pool manager scheduler get pool manager print pools n print table border cellpadding cellspacing n print tr th rowspan pool th th rowspan running jobs th th colspan map tasks th th colspan reduce tasks th th rowspan scheduling mode th tr n
914	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\FifoJobComparator.java	scheduler	package org apache hadoop mapred order link job in progress objects priority submit time default scheduler hadoop fifo job comparator implements comparator job in progress compare job in progress j job in progress j res j get priority compare to j get priority res j get start time j get start time res else res j get start time j get start time res if tie break job id get deterministic order res j get job id compare to j get job id return res
915	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\JobSchedulable.java	scheduler	package org apache hadoop mapred job schedulable extends schedulable fair scheduler scheduler job in progress job task type task type demand job schedulable fair scheduler scheduler job in progress job task type task type scheduler scheduler job job task type task type init metrics task type get task type return task type string get name return job get job id string job in progress get job return job update demand demand runnable for reduces make sure enough maps done reduces launch task type task type reduce job schedule reduces return add demand task in progress tip either attempts running case demands slot n attempts running case demands n slots may potentially demand one slot needs speculated task in progress tips task type task type map job get tasks task type map job get tasks task type reduce boolean speculation enabled task type task type map job speculative maps job speculative reduces time scheduler get clock get time task in progress tip tips tip complete tip running count active tasks speculative task want launch demand tip get active tasks size speculation enabled tip be speculated time demand else need launch task demand boolean runnable job info info scheduler get job info job run state job get status get run state return info null info runnable run state job status running get demand return demand redistribute share job priority get priority return job get priority get running tasks job inited return return task type task type map job running maps job running reduces get start time return job start time get weight return scheduler get job weight job task type get min share return task assign task task tracker status tts current time collection job in progress visited throws io exception runnable visited add job task tracker manager ttm scheduler task tracker manager cluster status cluster status ttm get cluster status num task trackers cluster status get task trackers check load manager whether safe launch task task tracker load manager load mgr scheduler get load manager load mgr launch task tts job task type return null task type task type map locality level locality level scheduler get allowed locality level job current time scheduler get event log log allowed loc level job get job id locality level obtain new map task needs passed desired locality level return job obtain new map task tts num task trackers ttm get number of unique hosts locality level cache level cap else return job obtain new reduce task tts num task trackers ttm get number of unique hosts else return null protected string get metrics context name return jobs update metrics assert metrics null super set metric values metrics metrics update
916	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\LoadManager.java	scheduler	package org apache hadoop mapred a pluggable object manages load link task tracker telling link task scheduler launch new tasks load manager implements configurable protected configuration conf protected task tracker manager task tracker manager protected fair scheduler event log scheduling log configuration get conf return conf set conf configuration conf conf conf synchronized set task tracker manager task tracker manager task tracker manager task tracker manager task tracker manager set event log fair scheduler event log scheduling log scheduling log scheduling log lifecycle method allow load manager start work separate threads start throws io exception nothing lifecycle method allow load manager stop work terminate throws io exception nothing can given link task tracker run another map task this method may check whether specified tracker enough resources run another map task boolean assign map task tracker status tracker total runnable maps total map slots can given link task tracker run another reduce task this method may check whether specified tracker enough resources run another reduce task boolean assign reduce task tracker status tracker total runnable reduces total reduce slots can given link task tracker run another new task given job this method provided use load managers take account jobs individual resource needs placing tasks boolean launch task task tracker status tracker job in progress job task type type
917	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\LocalityLevel.java	scheduler	package org apache hadoop mapred represents level data locality job fair scheduler allowed launch tasks by default jobs allowed launch non data local tasks waited small number seconds find slot node data if job waited allowed launch rack local tasks well nodes may task input data share rack node finally wait jobs allowed launch tasks anywhere cluster this enum defines three levels node rack any allowing tasks launched node a map task level obtained job link task job in progress task task tracker status in addition locality level possible get level cap pass link job in progress obtain new map task task tracker status ensure tasks level lower launched link cache level cap method enum locality level node rack any locality level task job in progress job task map task task tracker status tracker task id tip id map task get task id get task id task in progress tip job get task in progress tip id switch job get locality level tip tracker case return locality level node case return locality level rack default return locality level any obtain job in progress cache level cap pass link job in progress obtain new map task task tracker status ensure tasks locality level lower launched cache level cap switch case node return case rack return default return integer max value
918	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\NewJobWeightBooster.java	scheduler	package org apache hadoop mapred a link weight adjuster implementation gives weight boost new jobs certain amount time default x weight boost seconds this used make shorter jobs finish faster emulating shortest job first scheduling starving jobs new job weight booster extends configured implements weight adjuster default factor default duration factor duration set conf configuration conf conf null factor conf get float mapred newjobweightbooster factor default factor duration conf get long mapred newjobweightbooster duration default duration super set conf conf adjust weight job in progress job task type task type cur weight start job get start time system current time millis start duration return cur weight factor else return cur weight
919	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\Pool.java	pooling	package org apache hadoop mapred a schedulable pool jobs pool name default pool jobs pool parameter go string default pool name default pool name string name jobs specific pool children pools jobs collection job in progress jobs new array list job in progress scheduling mode jobs inside pool fair fifo scheduling mode scheduling mode pool schedulable map schedulable pool schedulable reduce schedulable pool fair scheduler scheduler string name name name map schedulable new pool schedulable scheduler task type map reduce schedulable new pool schedulable scheduler task type reduce collection job in progress get jobs return jobs add job job in progress job jobs add job map schedulable add job job reduce schedulable add job job remove job job in progress job jobs remove job map schedulable remove job job reduce schedulable remove job job string get name return name scheduling mode get scheduling mode return scheduling mode set scheduling mode scheduling mode scheduling mode scheduling mode scheduling mode boolean default pool return pool default pool name equals name pool schedulable get map schedulable return map schedulable pool schedulable get reduce schedulable return reduce schedulable pool schedulable get schedulable task type type return type task type map map schedulable reduce schedulable update metrics map schedulable update metrics reduce schedulable update metrics
920	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolManager.java	pooling	package org apache hadoop mapred maintains list pools well scheduling parameters pool guaranteed share allocations fair scheduler config file pool manager log log log factory get log org apache hadoop mapred pool manager time wait checks allocation file alloc reload interval time wait allocation modified reloading done prevent loading file fully written alloc reload wait string explicit pool property mapred fairscheduler pool fair scheduler scheduler map reduce minimum allocations pool map string integer map allocs new hash map string integer map string integer reduce allocs new hash map string integer if set cap number map reduce tasks pool map string integer pool max maps new hash map string integer map string integer pool max reduces new hash map string integer sharing weights pool map string double pool weights new hash map string double max concurrent running jobs pool user addition users max specified use user max jobs default map string integer pool max jobs new hash map string integer map string integer user max jobs new hash map string integer user max jobs default integer max value pool max jobs default integer max value min share preemption timeout pool seconds if job pool waits without receiving guaranteed share allowed preempt jobs tasks map string long min share preemption timeouts new hash map string long default min share preemption timeout pools set explicitly default min share preemption timeout long max value preemption timeout jobs fair share seconds if job remains half fair share allowed preempt tasks fair share preemption timeout long max value scheduling mode default scheduling mode scheduling mode fair object alloc file path xml file containing allocations this either url specify classpath resource fair scheduler xml classpath used string specify absolute path mapred fairscheduler allocation file used string pool name property jobconf property use determining job pool name default mapreduce job user name map string pool pools new hash map string pool last reload attempt last time tried reload pools file last successful reload last time successfully reloaded pools boolean last reload attempt failed false pool manager fair scheduler scheduler scheduler scheduler initialize throws io exception sax exception allocation configuration exception parser configuration exception configuration conf scheduler get conf pool name property conf get mapred fairscheduler poolnameproperty job context user name alloc file conf get mapred fairscheduler allocation file alloc file null no allocation file specified jobconf use default allocation file fair scheduler xml looking classpath alloc file new configuration get resource fair scheduler xml alloc file null log error the fair scheduler allocation file fair scheduler xml found classpath config file given mapred fairscheduler allocation file reload allocs last successful reload system current time millis last reload attempt system current time millis create default pool shows web ui get pool pool default pool name get pool name creating necessary synchronized pool get pool string name pool pool pools get name pool null pool new pool scheduler name pool set scheduling mode default scheduling mode pools put name pool return pool get pool given job pool get pool job in progress job return get pool get pool name job reload allocations
921	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\PoolSchedulable.java	pooling	package org apache hadoop mapred pool schedulable extends schedulable log log log factory get log pool schedulable get name fair scheduler scheduler pool pool task type task type pool manager pool mgr list job schedulable job scheds new linked list job schedulable demand variables used preemption last time at min share last time at half fair share pool schedulable fair scheduler scheduler pool pool task type type scheduler scheduler pool pool task type type pool mgr scheduler get pool manager current time scheduler get clock get time last time at min share current time last time at half fair share current time init metrics add job job in progress job job info info scheduler get job info job job scheds add task type task type map info map schedulable info reduce schedulable remove job job in progress job iterator job schedulable job scheds iterator next job schedulable job sched next job sched get job job remove break update demand asking jobs pool update update demand demand job schedulable sched job scheds sched update demand demand sched get demand demand exceeds cap pool limit max max tasks pool mgr get max slots pool get name task type demand max tasks demand max tasks distribute pool fair share among jobs redistribute share pool get scheduling mode scheduling mode fair scheduling algorithms compute fair shares job scheds get fair share else job schedulable sched job scheds sched set fair share get demand return demand get min share return pool mgr get allocation pool get name task type get weight return pool mgr get pool weight pool get name job priority get priority return job priority normal get running tasks ans job schedulable sched job scheds ans sched get running tasks return ans get start time return task assign task task tracker status tts current time collection job in progress visited throws io exception running tasks get running tasks running tasks pool mgr get max slots pool get name task type return null scheduling mode mode pool get scheduling mode comparator schedulable comparator mode scheduling mode fifo comparator new scheduling algorithms fifo comparator else mode scheduling mode fair comparator new scheduling algorithms fair share comparator else throw new runtime exception unsupported pool scheduling mode mode collections sort job scheds comparator job schedulable sched job scheds task task sched assign task tts current time visited task null return task return null string get name return pool get name pool get pool return pool task type get task type return task type collection job schedulable get job schedulables return job scheds get last time at min share return last time at min share set last time at min share last time at min share last time at min share last time at min share get last time at half fair share return last time at half fair share set last time at half fair share last time at half fair share last time at half fair share last time at half fair share protected string get metrics context name return pools update metrics super set metric values metrics
922	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\Schedulable.java	pooling	package org apache hadoop mapred a schedulable represents entity launch tasks job pool it provides common algorithms fair sharing applied within pool across pools there currently two types schedulables job schedulables represent single job pool schedulables allocate among jobs pool separate sets schedulables used maps reduces each pool map schedulable reduce schedulable job a schedulable responsible three roles it launch tasks assign task it provides information job pool scheduler including demand maximum number tasks required number currently running tasks minimum share pools job pool weight fair sharing start time priority fifo it assigned fair share use fair scheduling schedulable also contains two methods performing scheduling computations update demand called periodically compute demand various jobs pools may expensive e g jobs must iterate tasks count failed tasks tasks speculated etc redistribute share called demands updated schedulable fair share set parent let distribute share among schedulables within e g pools want perform fair sharing among jobs schedulable fair share assigned schedulable fair share protected metrics record metrics name job pool used debugging well breaking ties scheduling order deterministically string get name task type get task type maximum number tasks required schedulable this defined number currently running tasks number unlaunched tasks tasks either yet launched need speculated get demand number tasks schedulable currently running get running tasks minimum share slots assigned schedulable get min share job pool weight fair sharing get weight job priority jobs fifo pools meaningless pool schedulables job priority get priority start time jobs fifo pools meaningless pool schedulables get start time refresh schedulable demand children update demand distribute fair share assigned schedulable among children used pools internal scheduler fair sharing redistribute share obtain task given task tracker null schedulable tasks launch moment wish launch task task tracker e g waiting task tracker local data in addition job skipped search waiting task tracker local data method expected add tt visited tt collection passed scheduler properly mark skipped heartbeat please see link fair scheduler get allowed locality level job in progress details delay scheduling waiting trackers local data considered search job assign task assign task task tracker status tts current time collection job in progress visited throws io exception assign fair share schedulable set fair share fair share fair share fair share get fair share assigned schedulable get fair share return fair share return name metrics context schedulable protected string get metrics context name set metrics context protected init metrics metrics context metrics context metrics util get context fairscheduler metrics metrics util create record metrics context get metrics context name metrics set tag name get name metrics set tag task type get task type string cleanup metrics metrics remove metrics null protected set metric values metrics record metrics metrics set metric fair share get fair share metrics set metric min share get min share metrics set metric demand get demand metrics set metric weight get weight metrics set metric running tasks get running tasks update metrics convenient string implementation debugging string string return string format demand running share f w f get name get demand get running tasks fair share get weight
923	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\SchedulingAlgorithms.java	scheduler	package org apache hadoop mapred utility containing scheduling algorithms used fair scheduler scheduling algorithms log log log factory get log scheduling algorithms get name compare schedulables order priority submission time default fifo scheduler hadoop fifo comparator implements comparator schedulable compare schedulable schedulable res get priority compare to get priority res res math signum get start time get start time res in rare case jobs submitted exact time compare name job id get deterministic ordering alternately launch tasks different jobs res get name compare to get name return res compare schedulables via weighted fair sharing in addition schedulables min share get priority whose min share met schedulables min share compared far ratio for example job a min share tasks job b min share job b scheduled next b min share a min share schedulables min share compared running tasks weight if weights equal slots given job fewest tasks otherwise jobs weight get proportionally slots fair share comparator implements comparator schedulable compare schedulable schedulable min share ratio min share ratio tasks to weight ratio tasks to weight ratio min share math min get min share get demand min share math min get min share get demand boolean needy get running tasks min share boolean needy get running tasks min share min share ratio get running tasks math max min share min share ratio get running tasks math max min share tasks to weight ratio get running tasks get weight tasks to weight ratio get running tasks get weight res needy needy res else needy needy res else needy needy res math signum min share ratio min share ratio else neither schedulable needy res math signum tasks to weight ratio tasks to weight ratio res jobs tied fairness ratio break tie submit time job name get deterministic ordering useful unit tests res math signum get start time get start time res res get name compare to get name return res number iterations binary search compute fair shares this equivalent number bits precision output iterations gives precision better slots clusters one million slots compute fair shares iterations given set schedulables number slots compute weighted fair shares the min shares demands schedulables assumed set beforehand we compute fairest possible allocation shares schedulables respects min shares demands to understand method must first define weighted fair sharing means presence minimum shares demands if minimum shares every schedulable infinite demand e could launch infinitely many tasks weighted fair sharing would achieved ratio slots assigned weight equal schedulable slots assigned minimum shares demands add two twists some schedulables may enough tasks fill share some schedulables may min share higher assigned share to deal possibilities define assignment slots fair exists ratio r schedulables s s demand r s weight assigned share s demand schedulables s s min share r s weight given share s min share all schedulables s assigned share r s weight the sum shares total slots we call r weight slots ratio converts schedulable weight number slots assigned we compute fair allocation finding suitable weight slot ratio r to use binary search given ratio r compute number slots would
924	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\SchedulingMode.java	pooling	package org apache hadoop mapred internal scheduling modes pools enum scheduling mode fair fifo
925	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\TaskSelector.java	scheduler	package org apache hadoop mapred a pluggable object selecting tasks run link job in progress given link task tracker use link task scheduler the code task selector code charge managing locality speculative execution for latter purpose must also provide counts many tasks speculative job needs launch scheduler take account calculations task selector implements configurable protected configuration conf protected task tracker manager task tracker manager configuration get conf return conf set conf configuration conf conf conf synchronized set task tracker manager task tracker manager task tracker manager task tracker manager task tracker manager lifecycle method allow task selector start work separate threads start throws io exception nothing lifecycle method allow task selector stop work terminate throws io exception nothing how many speculative map tasks given job want launch needed speculative maps job in progress job how many speculative reduce tasks given job want launch needed speculative reduces job in progress job choose map task run given job given task tracker map launched job task tracker task obtain new map task task tracker status task tracker job in progress job locality level throws io exception choose reduce task run given job given task tracker reduce launched job task tracker task obtain new reduce task task tracker status task tracker job in progress job throws io exception
926	mapreduce\src\contrib\fairscheduler\src\java\org\apache\hadoop\mapred\WeightAdjuster.java	scheduler	package org apache hadoop mapred a pluggable object altering weights jobs fair scheduler used example link new job weight booster give higher weight new jobs short jobs finish faster may implement link configurable access configuration parameters weight adjuster adjust weight job in progress job task type task type cur weight
927	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\AvgRecordFactory.java	unrelated	package org apache hadoop mapred gridmix given byte record targets emit roughly equal sized records satisfying contract avg record factory extends record factory percentage record key data string gridmix key frc gridmix key fraction string gridmix missing rec size gridmix missing rec size target bytes target records step avgrec key len acc bytes l acc records l unspilled bytes min spilled bytes avg record factory target bytes target records configuration conf target bytes target records conf avg record factory target bytes target records configuration conf min spilled bytes target bytes target bytes target records target records target bytes math max target bytes conf get int gridmix missing rec size target records tmp target bytes target records step target bytes target records tmp avgrec math min integer max value tmp key len math max tmp math min f conf get float gridmix key frc f min spilled bytes min spilled bytes boolean next gridmix key key gridmix record val throws io exception acc bytes target bytes return false reclen acc records step avgrec avgrec len math min target bytes acc bytes reclen unspilled bytes len len reclen key null unspilled bytes min spilled bytes acc records target records key set size val set size acc bytes key get size val get size unspilled bytes key get size val get size else key set size key len val set size unspilled bytes key get size acc bytes unspilled bytes unspilled bytes else unspilled bytes min spilled bytes acc records target records val set size acc bytes val get size unspilled bytes val get size else val set size unspilled bytes acc bytes unspilled bytes unspilled bytes return true get progress throws io exception return math min f acc bytes target bytes close throws io exception noop
928	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ClusterSummarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes hadoop cluster used link gridmix run statistics reported ul li total number active trackers cluster li li total number blacklisted trackers cluster li li max map task capacity cluster li li max reduce task capacity cluster li ul apart statistics link job tracker link file system addresses also recorded summary cluster summarizer implements stat listener cluster stats log log log factory get log cluster summarizer num blacklisted trackers num active trackers max map tasks max reduce tasks string job tracker info summarizer na string namenode info summarizer na update cluster stats item try num blacklisted trackers item get status get blacklisted trackers num active trackers item get status get task trackers max map tasks item get status get max map tasks max reduce tasks item get status get max reduce tasks catch exception e time system current time millis log info error processing cluster status fast date format get instance format time summarizes cluster used link gridmix run string string string builder builder new string builder builder append cluster summary builder append n job tracker append get job tracker info builder append n file system append get namenode info builder append n number blacklisted trackers append get num blacklisted trackers builder append n number active trackers append get num active trackers builder append n max map task capacity append get max map tasks builder append n max reduce task capacity append get max reduce tasks builder append n n return builder string start configuration conf job tracker info conf get jt config jt ipc address namenode info conf get common configuration keys fs default name key getters protected get num blacklisted trackers return num blacklisted trackers protected get num active trackers return num active trackers protected get max map tasks return max map tasks protected get max reduce tasks return max reduce tasks protected string get job tracker info return job tracker info protected string get namenode info return namenode info
929	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\CompressionEmulationUtil.java	pooling	package org apache hadoop mapred gridmix this utility compression related modules compression emulation util log log log factory get log compression emulation util enable compression usage grid mix runs string compression emulation enable gridmix compression emulation enable enable input data decompression string input decompression emulation enable gridmix compression emulation input decompression enable configuration property setting compression ratio map input data string gridmix map input compression ratio gridmix compression emulation map input decompression ratio configuration property setting compression ratio map output string gridmix map output compression ratio gridmix compression emulation map output compression ratio configuration property setting compression ratio reduce output string gridmix reduce output compression ratio gridmix compression emulation reduce output compression ratio default compression ratio default compression ratio f compression ratio lookup table compression lookup table new compression ratio lookup table this link mapper implementation generating random text data it uses link random text data generator generating text data output files compressed random text data mapper extends mapper null writable long writable text text random text data generator rtg protected setup context context throws io exception interrupted exception configuration conf context get configuration list size random text data generator get random text data generator list size conf word size random text data generator get random text data generator word size conf rtg new random text data generator list size word size emits random words sequence desired size note desired output size passed value parameter map map null writable key long writable value context context throws io exception interrupted exception todo control extra data written todo should key tvalue n considered measuring size can counters like bytes written used what value counters local job runner bytes value get bytes string random key rtg get random word string random value rtg get random word context write new text random key new text random value bytes random value get bytes length random key get bytes length configure link job enabling compression emulation configure job job throws io exception interrupted exception class not found exception set random text mapper job set mapper class random text data mapper job set num reduce tasks job set map output key class text job set map output value class text job set input format class gen data format job set jar by class generate data set output compression true file output format set compress output job true try file input format add input path job new path ignored catch io exception e log error error adding input path e this lookup table mapping compression ratio size word link random text data generator dictionary note table computed empirically using dictionary default length e value random text data generator default list size compression ratio lookup table map float integer map new hash map float integer min ratio f max ratio f add empirically obtained data points lookup table compression ratio lookup table map put f map put f map put f map put f map put f map put f map put f map put f map put f map put f map put f map put f map put f
930	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\DistributedCacheEmulator.java	unrelated	package org apache hadoop mapred gridmix emulation distributed cache usage gridmix br emulation distributed cache load gridmix put load task trackers affects execution time tasks localization distributed cache files task trackers br gridmix creates distributed cache files simulated jobs launching map reduce job link generate dist cache data advance e launching simulated jobs br the distributed cache file paths used original cluster mapped unique file names simulated cluster br all hdfs based distributed cache files generated gridmix distributed cache files but gridmix makes sure load incurred due localization distributed cache files original cluster also faithfully simulated gridmix emulates load due distributed cache files mapping distributed cache files different users original cluster different distributed cache files simulated cluster br the configuration properties like link mr job config cache files link mr job config cache file visibilities link mr job config cache files sizes link mr job config cache file timestamps obtained trace used decide li file size distributed cache file generated li whether distributed cache file already seen trace file li whether distributed cache file considered br br gridmix configures generated files distributed cache files simulated jobs distributed cache emulator log log log factory get log distributed cache emulator avg bytes per map l mb if least distributed cache file missing expected distributed cache dir gridmix cannot proceed emulation distributed cache load missing dist cache files error path dist cache path map simulated cluster distributed cache file paths file sizes unique distributed cache files entered map distributed cache files considered file paths visibilities timestamps map string long dist cache files new hash map string long configuration property whether gridmix emulate distributed cache usage default value true string gridmix emulate distributedcache gridmix distributed cache emulation enable whether emulate distributed cache usage boolean emulate distributed cache true whether generate distributed cache data boolean generate dist cache data false configuration conf gridmix configuration pseudo local file system local fs based distributed cache files created gridmix file system pseudo local fs null need handle deprecation map reduce internal configuration properties map reduce handle deprecation configuration add deprecation mapred cache files filesizes new string mr job config cache files sizes configuration add deprecation mapred cache files visibilities new string mr job config cache file visibilities cache directory distributed cache emulator configuration conf path io path conf conf dist cache path new path io path distributed cache conf set class fs pseudo impl pseudo local fs file system this called method distributed cache emulator br checks emulation distributed cache load needed feasible sets flags generate dist cache data emulate distributed cache appropriate values br gridmix emulate distributed cache load ol li specific gridmix job type need emulation distributed cache load or li trace coming stream instead file or li distributed cache dir distributed cache data generated gridmix local file system or li execute permission ascendant directories lt io path gt till root this emulation distributed cache load distributed cache files created lt io path distributed cache gt considered hadoop distributed cache files li creation pseudo local file system fails ol br for generation distributed cache data also disabled stream
931	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\EchoUserResolver.java	unrelated	package org apache hadoop mapred gridmix echos ugi offered echo user resolver implements user resolver log log log factory get log gridmix echo user resolver log info current user resolver echo user resolver synchronized boolean set target users uri userdesc configuration conf throws io exception return false synchronized user group information get target ugi user group information ugi return ugi inherit doc br br since link echo user resolver simply returns user name passed argument need target list users boolean needs target users list return false
932	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ExecutionSummarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes link gridmix run statistics reported ul li total number jobs input trace li li trace signature li li total number jobs processed input trace li li total number jobs submitted li li total number successful failed jobs li li total number map reduce tasks launched li li gridmix start end time li li total time gridmix run data generation simulation li li gridmix configuration e job type submission type resolver li ul execution summarizer implements stat listener job stats log log log factory get log execution summarizer fast date format util fast date format get instance num jobs in input trace total successful jobs total failed jobs total map tasks launched total reduce tasks launched total simulation time total runtime string command line args start time end time simulation start time string input trace location string input trace signature string job submission policy string resolver data statistics data stats string expected data size basic constructor initialized runtime arguments execution summarizer string args start time system current time millis flatten args store command line args org apache commons lang string utils join args default constructor execution summarizer start time system current time millis command line args summarizer na start configuration conf simulation start time system current time millis process job state job stats stats throws exception job job stats get job job successful total successful jobs else total failed jobs process job tasks job stats stats throws exception total map tasks launched stats get no of maps job job stats get job total reduce tasks launched job get num reduce tasks process job stats stats try process job run state process job state stats process tasks information process job tasks stats catch exception e log info error processing job stats get job get job id update job stats item process simulation started simulation start time process item total simulation time system current time millis get simulation start time generates signature trace file based filename modification time file length owner protected string get trace signature string input throws io exception path input path new path input file system fs input path get file system new configuration file status status fs get file status input path path q path fs make qualified status get path string trace id status get modification time q path string status get owner status get len return md hash digest trace id string finalize job factory factory string input path data size user resolver user resolver data statistics stats configuration conf throws io exception num jobs in input trace factory num jobs in trace end time system current time millis path input trace path new path input path file system fs input trace path get file system conf input trace location fs make qualified input trace path string input trace signature get trace signature input trace location job submission policy gridmix get job submission policy conf name resolver user resolver get class get name data size expected data size string utils human readable int data size else expected data size summarizer na data
933	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\FilePool.java	pooling	package org apache hadoop mapred gridmix class caching pool input data used synthetic jobs simulating read traffic file pool log log log factory get log file pool the minimum file size added pool default mi b string gridmix min file gridmix min file size the maximum size files added pool defualts ti b string gridmix max total gridmix max total scan node root path path file system fs configuration conf read write lock update lock initialize filepool path provided populate cache file pool configuration conf path input throws io exception root null conf conf path input fs path get file system conf update lock new reentrant read write lock gather collection files least large min size get input files min size collection file status files throws io exception update lock read lock lock try return root select files min size files finally update lock read lock unlock re generate cache input file status objects refresh throws io exception update lock write lock lock try root new inner desc fs fs get file status path new min file filter conf get long gridmix min file conf get long gridmix max total l l root get size throw new io exception found satisfactory file path finally update lock write lock unlock get set locations given file block location locations for file status stat start len throws io exception todo cache return fs get file block locations stat start len node protected random rand new random total size files directories current node get size return set files whose cumulative size least tt target size tt todo clearly size criterion e g refresh generated data without including running task output tolerance permission issues etc select files target size collection file status files throws io exception files current directory node leaf desc extends node size array list file status curdir leaf desc array list file status curdir size size size curdir curdir get size return size select files target size collection file status files throws io exception target size get size files add all curdir return get size selector selector new selector curdir size target size get size rand array list integer selected new array list integer ret l index selector next selected add index ret curdir get index get len ret target size integer selected files add curdir get return ret a subdirectory current node inner desc extends node size dist node subdir comparator node node comparator new comparator node compare node n node n return n get size n get size n get size n get size inner desc file system fs file status dir min file filter filter throws io exception file sum l array list file status cur files new array list file status array list file status cur dirs new array list file status file status stat fs list status dir get path stat directory cur dirs add stat else filter accept stat cur files add stat file sum stat get len array list node subdir list new array list node cur files empty subdir list add new leaf desc cur files
934	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\FileQueue.java	unrelated	package org apache hadoop mapred gridmix given link org apache hadoop mapreduce lib input combine file split circularly read input source file queue extends input stream idx curlen l input stream input byte z new byte path paths lengths startoffset configuration conf file queue combine file split split configuration conf throws io exception conf conf paths split get paths startoffset split get start offsets lengths split get lengths next source protected next source throws io exception paths length return input null input close idx idx paths length curlen lengths idx path file paths idx input compression emulation util get possibly decompressed input stream file conf startoffset idx read throws io exception tmp read z return tmp x ff z read byte b throws io exception return read b b length read byte b len throws io exception kvread kvread len curlen next source continue src read math min len kvread curlen io utils read fully input b kvread src read curlen src read kvread src read return kvread close throws io exception input close
935	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GenerateData.java	unrelated	package org apache hadoop mapred gridmix todo replace form gridmix job generate data extends gridmix job total bytes write string gridmix gen bytes gridmix gen bytes maximum size per file written string gridmix gen chunk gridmix gen bytes per file size writes output file string gridmix val bytes gendata val bytes status reporting interval megabytes string gridmix gen interval gendata interval mb blocksize generated data string gridmix gen blocksize gridmix gen blocksize replication generated data string gridmix gen replication gridmix gen replicas string job name gridmix generate input data generate data configuration conf path outdir genbytes throws io exception super conf l job name job get configuration set long gridmix gen bytes genbytes file output format set output path job outdir represents input data characteristics data statistics data size num files boolean data compressed data statistics data size num files boolean compressed data size data size num files num files data compressed compressed get data size return data size get num files return num files boolean data compressed return data compressed publish data statistics data statistics publish data statistics path input dir gen bytes configuration conf throws io exception compression emulation util compression emulation enabled conf return compression emulation util publish compressed data statistics input dir conf gen bytes else return publish plain data statistics conf input dir data statistics publish plain data statistics configuration conf path input dir throws io exception file system fs input dir get file system conf obtain input data file statuses data size file count remote iterator located file status iter fs list files input dir true path filter filter new utils output file utils output files filter iter next located file status status iter next filter accept status get path data size status get len file count publish plain data statistics log info total size input data string utils human readable int data size log info total number input data files file count return new data statistics data size file count false job call throws io exception interrupted exception class not found exception user group information ugi user group information get login user ugi as new privileged exception action job job run throws io exception class not found exception interrupted exception check compression emulation enabled compression emulation util compression emulation enabled job get configuration compression emulation util configure job else configure random bytes data generator job submit return job configure random bytes data generator job set mapper class gen data mapper job set num reduce tasks job set map output key class null writable job set map output value class bytes writable job set input format class gen data format job set output format class raw bytes output format job set jar by class generate data try file input format add input path job new path ignored catch io exception e log error error adding input path e return job protected boolean emulate compression return false gen data mapper extends mapper null writable long writable null writable bytes writable bytes writable val random r new random protected setup context context throws io exception interrupted exception val
936	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GenerateDistCacheData.java	unrelated	package org apache hadoop mapred gridmix gridmix job generates distributed cache files link generate dist cache data expects list distributed cache files generated input this list expected stored sequence file filename expected configured using code gridmix distcache file list this input file contains list distributed cache files sizes for record e file size file path input file file specific file size specific path created generate dist cache data extends gridmix job number distributed cache files created gridmix string gridmix distcache file count gridmix distcache file count total number bytes written distributed cache files gridmix e sum sizes unique distributed cache files created gridmix string gridmix distcache byte count gridmix distcache byte count the special file created used gridmix contains list unique distributed cache files created sizes string gridmix distcache file list gridmix distcache file list string job name gridmix generate distcache data generate dist cache data configuration conf throws io exception super conf l job name job call throws io exception interrupted exception class not found exception user group information ugi user group information get login user ugi as new privileged exception action job job run throws io exception class not found exception interrupted exception job set mapper class gen dc data mapper job set num reduce tasks job set map output key class null writable job set map output value class bytes writable job set input format class gen dc data format job set output format class null output format job set jar by class generate dist cache data try file input format add input path job new path ignored catch io exception e log error error adding input path e job submit return job return job protected boolean emulate compression return false gen dc data mapper extends mapper long writable bytes writable null writable bytes writable bytes writable val random r new random file system fs protected setup context context throws io exception interrupted exception val new bytes writable new byte context get configuration get int generate data gridmix val bytes fs file system get context get configuration create one distributed cache file needed file size key distributed cache file size value distributed cache file path map long writable key bytes writable value context context throws io exception interrupted exception string file name new string value get bytes value get length path path new path file name create distributed cache file permissions since distributed cache directory execute permission others ok set read permission others files directory still become distributed cache files simulated cluster fs data output stream dos file system create fs path new fs permission short bytes key get bytes bytes val get length r next bytes val get bytes val set size math min val get length bytes dos write val get bytes val get length write dist cache file dos close input format generate dist cache data input generate dist cache data special file sequence file format contains list distributed cache files generated along file sizes gen dc data format extends input format long writable bytes writable split special file contains list distributed cache file paths file
937	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Gridmix.java	pooling	package org apache hadoop mapred gridmix driver gridmix benchmark gridmix accepts timestamped stream trace job task descriptions for job trace client submit corresponding synthetic job target cluster rate original trace the intent provide benchmark configured extended closely match measured resource profile actual production loads gridmix extends configured implements tool log log log factory get log gridmix output scratch directory submitted jobs relative paths resolved path provided input absolute paths remain independent the default quot gridmix quot string gridmix out dir gridmix output directory number submitting threads client upper bound memory split data submitting threads precompute input splits submitted jobs this limits number splits held memory waiting submission also permits parallel computation split data string gridmix sub thr gridmix client submit threads the depth queue job descriptions before splits computed queue pending descriptions stored memoory this parameter limits depth queue string gridmix que dep gridmix client pending queue depth multiplier accelerate decelerate job submission as crude means sizing job trace cluster time separating two jobs multiplied factor string gridmix sub mul gridmix submit multiplier class used resolve users trace list target users cluster string gridmix usr rsv gridmix user resolve configuration property set simulated job configuration whose value set corresponding original job name this configurable gridmix user string original job name gridmix job original job name configuration property set simulated job configuration whose value set corresponding original job id this configurable gridmix user string original job id gridmix job original job id distributed cache emulator dist cache emulator submit data structures job factory factory job submitter submitter job monitor monitor statistics statistics summarizer summarizer shutdown hook shutdown sdh new shutdown gridmix string args summarizer new summarizer args gridmix summarizer new summarizer get input data directory gridmix input directory io path input path get gridmix input data path path io path return new path io path input write random bytes path lt input dir gt protected write input data genbytes path input dir throws io exception interrupted exception configuration conf get conf configure compression ratio needed compression emulation util setup data generator config conf generate data gen data new generate data conf input dir genbytes log info generating string utils human readable int genbytes test data launch gridmix job gen data fs shell shell new fs shell conf try log info changing permissions input path input dir string shell run new string chmod r input dir string catch exception e log error couldnt change file permissions e throw new io exception e log info input data generation successful write random bytes distributed cache files used simulated jobs current gridmix run files generated do part map reduce job link generate dist cache data job name protected write dist cache data configuration conf throws io exception interrupted exception file count conf get int generate dist cache data gridmix distcache file count file count generate distributed cache files gridmix job gen dist cache data new generate dist cache data conf log info generating distributed cache data size conf get long generate dist cache data gridmix distcache byte count launch gridmix job gen dist cache data launch input
938	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixJob.java	pooling	package org apache hadoop mapred gridmix synthetic job generated trace description gridmix job implements callable job delayed gridmix job name format gridmix digit sequence number string job name prefix gridmix log log log factory get log gridmix job thread local formatter name format new thread local formatter protected formatter initial value string builder sb new string builder job name prefix length sb append job name prefix return new formatter sb protected seq protected path outdir protected job job protected job story jobdesc protected user group information ugi protected submission time nanos concurrent hash map integer list input split desc cache new concurrent hash map integer list input split protected string gridmix job seq gridmix job seq protected string gridmix use queue in trace gridmix job submission use queue trace protected string gridmix default queue gridmix job submission default queue configuration key enable disable high ram feature emulation string gridmix highram emulation enable gridmix highram emulation enable configuration key enable disable task jvm options string gridmix task jvm options enable gridmix task jvm options enable pattern max heap pattern pattern compile xmx k km mg gt t set job queue job job string queue queue null job get configuration set mr job config queue name queue gridmix job configuration conf submission millis job story jobdesc path root user group information ugi seq throws io exception ugi ugi jobdesc jobdesc seq seq string builder name format get set length job name prefix length try job ugi as new privileged exception action job job run throws io exception string job id null jobdesc get job id unknown jobdesc get job id string job ret new job conf name format get format seq string ret get configuration set int gridmix job seq seq ret get configuration set gridmix original job id job id ret get configuration set gridmix original job name jobdesc get name conf get boolean gridmix use queue in trace false set job queue ret jobdesc get queue name else set job queue ret conf get gridmix default queue check job emulate compression emulate compression set compression related configs compression emulation enabled compression emulation util compression emulation enabled conf compression emulation util configure compression emulation jobdesc get job conf ret get configuration configure high ram properties enabled conf get boolean gridmix highram emulation enable true configure high ram properties jobdesc get job conf ret get configuration configure task jvm options enabled knob turned mismatch target simulation cluster original cluster such mismatch result job failures due memory issues target simulated cluster todo if configured scale original task jvm heap related options suit target simulation cluster conf get boolean gridmix task jvm options enable true configure task jvm options jobdesc get job conf ret get configuration return ret catch interrupted exception e throw new io exception e submission time nanos time unit nanoseconds convert submission millis time unit milliseconds outdir new path root seq protected configure task jvm options configuration original job conf configuration simulated job conf get heap related java opts used original job set simulated job set task task heap options configure task jvm
939	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixJobSubmissionPolicy.java	unrelated	package org apache hadoop mapred gridmix enum gridmix job submission policy replay replay job factory cluster stats create job factory job submitter submitter job story producer producer path scratch dir configuration conf count down latch start flag user resolver user resolver throws io exception return new replay job factory submitter producer scratch dir conf start flag user resolver stress stress job factory cluster stats create job factory job submitter submitter job story producer producer path scratch dir configuration conf count down latch start flag user resolver user resolver throws io exception return new stress job factory submitter producer scratch dir conf start flag user resolver serial serial job factory job stats create job factory job submitter submitter job story producer producer path scratch dir configuration conf count down latch start flag user resolver user resolver throws io exception return new serial job factory submitter producer scratch dir conf start flag user resolver string job submission policy gridmix job submission policy string name polling interval gridmix job submission policy string name polling interval name name polling interval polling interval job factory create job factory job submitter submitter job story producer producer path scratch dir configuration conf count down latch start flag user resolver user resolver throws io exception get polling interval return polling interval gridmix job submission policy get policy configuration conf gridmix job submission policy default policy string policy conf get job submission policy default policy name return value of policy upper case
940	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixKey.java	unrelated	package org apache hadoop mapred gridmix gridmix key extends gridmix record byte reduce spec byte data meta bytes byte type partition not serialized spec spec new spec gridmix key data l gridmix key byte type size seed super size seed type type setting type may change pcnt random bytes set size size get size switch type case reduce spec return super get size spec get size meta bytes case data return super get size meta bytes default throw new illegal state exception invalid type type set size size switch type case reduce spec super set size size meta bytes spec get size break case data super set size size meta bytes break default throw new illegal state exception invalid type type partition serialized get partition return partition set partition partition partition partition get reduce input records assert reduce spec get type return spec rec set reduce input records rec assert reduce spec get type orig size get size spec rec rec set size orig size get reduce output records assert reduce spec get type return spec rec set reduce output records rec assert reduce spec get type orig size get size spec rec rec set size orig size get reduce output bytes assert reduce spec get type return spec bytes set reduce output bytes b assert reduce spec get type orig size get size spec bytes b set size orig size get link resource usage metrics stored key resource usage metrics get reduce resource usage metrics assert reduce spec get type return spec metrics store link resource usage metrics key set reduce resource usage metrics resource usage metrics metrics assert reduce spec get type spec set resource usage specification metrics byte get type return type set type byte type throws io exception orig size get size switch type case reduce spec case data type type break default throw new io exception invalid type type set size orig size set spec spec spec assert reduce spec get type orig size get size spec set spec set size orig size read fields data input throws io exception super read fields set type read byte reduce spec get type spec read fields write data output throws io exception super write byte get type write byte reduce spec spec write fixed bytes return super fixed bytes reduce spec get type spec get size meta bytes compare to gridmix record gridmix key gridmix key byte get type byte get type return return super compare to note spec explicitly included changing spec may change size affect equality boolean equals object return true null get class get class gridmix key gridmix key return get type get type super equals return false hash code return super hash code get type spec implements writable rec rec bytes resource usage metrics metrics null size of resource usage metrics spec set spec rec rec bytes bytes rec rec set resource usage specification metrics sets link resource usage metrics link spec set resource usage specification resource usage metrics metrics metrics metrics metrics null size of resource usage metrics metrics size else size of resource
941	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixRecord.java	unrelated	package org apache hadoop mapred gridmix gridmix record implements writable comparable gridmix record fixed bytes size seed data input buffer dib new data input buffer data output buffer dob new data output buffer long size byte size byte literal dob get data boolean compressible false compression ratio compression emulation util default compression ratio random text data generator rtg null gridmix record l gridmix record size seed seed seed set size internal size get size return size set size size set size internal size set compressibility boolean compressible ratio compressible compressible compression ratio ratio initialize random text data generator every grid mix record note random text data generator needed grid mix record configured generate compressible text data compressible rtg compression emulation util get random text data generator ratio random text data generator default seed set size internal size size math max size try seed mask seed seed size dob reset dob write long seed catch io exception e throw new runtime exception e set seed seed seed seed marsaglia next rand x x x x x return x x generate random text data compressed if record marked compressible via link file output format compress random data text data else link gridmix record write random data output invoked write random text data output size throws io exception tmp seed write long tmp size long size byte size todo should use size what data g string random word rtg get random word byte bytes random word get bytes utf random word size bytes length random word size write bytes random word size get next random word random word rtg get random word bytes random word get bytes utf determine random word size random word size bytes length pad remaining bytes write bytes write random data output size throws io exception tmp seed write long tmp size long size byte size long size byte size tmp next rand tmp write long tmp long size byte size tmp next rand tmp write byte tmp x ff tmp byte size read fields data input throws io exception size writable utils read v int payload size writable utils get v int size size payload long size byte size seed read long payload long size byte size else arrays fill literal byte read fully literal payload dib reset literal literal length seed dib read long payload v bytes skip bytes payload v bytes payload throw new eof exception expected payload read v bytes write data output throws io exception data bytes including vint encoding writable utils write v int size payload size writable utils get v int size size payload long size byte size compressible write random text payload else write random payload else payload todo what compressible turned log bad idea write literal payload compare to gridmix record return compare seed seed math max get size fixed bytes fixed bytes min vint size return fixed bytes mask seed sd sz don use fixed bytes subclasses set intended random len sz fixed bytes sd l else sz long size byte size fixed bytes tmp sz fixed bytes mask l byte
942	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\GridmixSplit.java	unrelated	package org apache hadoop mapred gridmix gridmix split extends combine file split id n spec maps reduces input records output bytes output records max memory reduce bytes new reduce records new spec reduces id mod reduce output bytes new reduce output records new gridmix split super gridmix split combine file split cfsplit maps id input bytes input records output bytes output records reduce bytes reduce records reduce output bytes reduce output records throws io exception super cfsplit id id maps maps reduces reduce bytes length input records input records output bytes output bytes output records output records reduce bytes reduce bytes reduce records reduce records n spec reduce output bytes length reduce output bytes reduce output bytes reduce output records reduce output records get id return id get map count return maps get input records return input records get output bytes reduces return new output bytes ret new reduces reduces ret math round output bytes reduce bytes return ret get output records reduces return new output records ret new reduces reduces ret math round output records reduce records return ret get reduce bytes return reduce output bytes get reduce records return reduce output records write data output throws io exception super write writable utils write v int id writable utils write v int maps writable utils write v long input records writable utils write v long output bytes writable utils write v long output records writable utils write v long max memory writable utils write v int reduces reduces write double reduce bytes write double reduce records writable utils write v int n spec n spec writable utils write v long reduce output bytes writable utils write v long reduce output records read fields data input throws io exception super read fields id writable utils read v int maps writable utils read v int input records writable utils read v long output bytes writable utils read v long output records writable utils read v long max memory writable utils read v long reduces writable utils read v int reduce bytes length reduces reduce bytes new reduces reduce records new reduces reduces reduce bytes read double reduce records read double n spec writable utils read v int reduce output bytes length n spec reduce output bytes new n spec reduce output records new n spec n spec reduce output bytes writable utils read v long reduce output records writable utils read v long
943	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\InputStriper.java	pooling	package org apache hadoop mapred gridmix given link file pool obtain set files capable satisfying full set splits iterate source fill request input striper log log log factory get log input striper idx current start file status current list file status files new array list file status configuration conf new configuration input striper file pool input dir map bytes throws io exception input bytes input dir get input files map bytes files map bytes input bytes log warn using input bytes map bytes bytes files empty map bytes throw new io exception failed satisfy request map bytes current files empty null files get dominating proportion input bytes combine file split split for file pool input dir bytes n locs throws io exception array list path paths new array list path array list long start new array list long array list long length new array list long hash map string double sb new hash map string double paths add current get path start add current start file math min bytes current get len current start length add file block location loc input dir locations for current current start file tedium loc get length bytes string loc get hosts double j sb get null j sb put tedium else sb put j value tedium current start file bytes file switch new file current file uncompressed completely used current file compressed compression codec factory compression codecs new compression codec factory conf compression codec codec compression codecs get codec current get path current get len current start codec null current files get idx files size current start bytes array list entry string double sort new array list entry string double sb entry set collections sort sort host rank string hosts new string math min n locs sort size n locs sort size hosts sort get get key return new combine file split paths array new path long array start long array length hosts long array array list long sigh ret new sigh size ret length ret sigh get return ret comparator entry string double host rank new comparator entry string double compare entry string double entry string double b va get value vb b get value return va vb va vb
944	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\IntermediateRecordFactory.java	unrelated	package org apache hadoop mapred gridmix factory passing reduce specification last record intermediate record factory extends record factory gridmix key spec spec record factory factory partition target records boolean done false acc records l boundary passed intermediate record factory target bytes target records partition gridmix key spec spec configuration conf new avg record factory target bytes target records conf partition target records spec conf boundary passed intermediate record factory record factory factory partition target records gridmix key spec spec configuration conf spec spec factory factory partition partition target records target records boolean next gridmix key key gridmix record val throws io exception assert key null boolean rslt factory next key val acc records rslt acc records target records key set type gridmix key data else orig key get size key set type gridmix key reduce spec spec rec acc records key set spec spec val set size val get size key get size orig reset counters acc records l spec bytes l spec rec l done true else done ensure spec emitted key set type gridmix key reduce spec key set partition partition key set size val set size spec rec l key set spec spec done true return true key set partition partition return rslt get progress throws io exception return factory get progress close throws io exception factory close
945	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobCreator.java	unrelated	package org apache hadoop mapred gridmix enum job creator loadjob gridmix job create gridmix job configuration gridmix conf submission millis job story jobdesc path root user group information ugi seq throws io exception build configuration simulated job configuration conf new configuration gridmix conf dce configure dist cache files conf jobdesc get job conf return new load job conf submission millis jobdesc root ugi seq boolean emulate dist cache load return true sleepjob string hosts gridmix job create gridmix job configuration conf submission millis job story jobdesc path root user group information ugi seq throws io exception num locations conf get int sleepjob random locations num locations num locations num locations hosts null job client client new job client new job conf conf cluster status stat client get cluster status true n trackers stat get task trackers array list string host list new array list string n trackers pattern tracker pattern pattern compile tracker matcher tracker pattern matcher string tracker stat get active tracker names reset tracker find continue string name group host list add name hosts host list array new string host list size return new sleep job conf submission millis jobdesc root ugi seq num locations hosts boolean emulate dist cache load return false string gridmix job type gridmix job type string sleepjob random locations gridmix sleep fake locations create gridmix simulated job done gridmix job create gridmix job configuration conf submission millis job story jobdesc path root user group information ugi seq throws io exception job creator get policy configuration conf job creator default policy return conf get enum gridmix job type default policy distributed cache load boolean emulate dist cache load distributed cache emulator dce this method called calling method job creator except emulate dist cache load especially emulate dist cache load returns true job type set dist cache emulator distributed cache emulator e dce e
946	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobFactory.java	scheduler	package org apache hadoop mapred gridmix component reading job traces generated rumen each job trace assigned sequence number given submission time relative job preceded jobs enqueued job submitter provided construction job factory t implements gridmix component void stat listener t log log log factory get log job factory protected path scratch protected rate factor protected configuration conf protected thread r thread protected atomic integer sequence protected job submitter submitter protected count down latch start flag protected user resolver user resolver protected job creator job creator protected volatile io exception error null protected job story producer job producer protected reentrant lock lock new reentrant lock true protected num jobs in trace creating new instance start thread link org apache hadoop tools rumen zombie job producer job factory job submitter submitter input stream job trace path scratch configuration conf count down latch start flag user resolver user resolver throws io exception submitter new zombie job producer job trace null scratch conf start flag user resolver constructor permitting job story producer mocked protected job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver user resolver sequence new atomic integer scratch scratch rate factor conf get float gridmix gridmix sub mul f job producer job producer conf new configuration conf submitter submitter start flag start flag r thread create reader thread log debug enabled log debug the submission thread name r thread get name user resolver user resolver job creator job creator get policy conf job creator loadjob min task info extends task info min task info task info info super info get input bytes info get input records info get output bytes info get output records info get task memory info get resource usage metrics get input bytes return math max super get input bytes get input records return math max super get input records get output bytes return math max super get output bytes get output records return math max super get output records get task memory return math max super get task memory protected filter job story implements job story protected job story job filter job story job story job job job job conf get job conf return job get job conf string get name return job get name job id get job id return job get job id string get user return job get user get submission time return job get submission time input split get input splits return job get input splits get number maps return job get number maps get number reduces return job get number reduces task info get task info task type task type task number return job get task info task type task number task attempt info get task attempt info task type task type task number task attempt number return job get task attempt info task type task number task attempt number task attempt info get map task attempt info adjusted task number task attempt number locality return job get map task attempt info adjusted task number task attempt number locality values get outcome return
947	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobMonitor.java	unrelated	package org apache hadoop mapred gridmix component accepting submitted running jobs responsible monitoring jobs success failure once job submitted polled status complete if job complete monitor thread returns immediately queue if monitor sleep duration job monitor implements gridmix component job log log log factory get log job monitor queue job jobs monitor thread thread blocking queue job running jobs poll delay millis statistics statistics boolean graceful false boolean shutdown false job monitor statistics statistics time unit seconds statistics create job monitor sleeps specified duration polling still running job job monitor poll delay time unit unit statistics statistics thread new monitor thread running jobs new linked blocking queue job jobs new linked list job poll delay millis time unit milliseconds convert poll delay unit statistics statistics add job polling queue add job job throws interrupted exception running jobs put job add submission failed job communicated back serial todo cleaner solution problem submission failed job job log info job submission failed notification job job get job id statistics add job temporary hook recording job success protected success job job log info job get job name job get job id success temporary hook recording job failure protected failure job job log info job get job name job get job id failure if shutdown jobs completed still running jobs may extracted component list job get remaining jobs thread alive log warn internal error polling running monitor jobs synchronized jobs return new array list job jobs monitoring thread pulling running jobs component queue polled status monitor thread extends thread monitor thread super gridmix job monitor check job success failure process job job throws io exception interrupted exception job successful success job else failure job run boolean graceful boolean shutdown true try synchronized jobs graceful job monitor graceful shutdown job monitor shutdown running jobs drain to jobs shutdown conditions either shutdown requested jobs completed abort requested recently submitted jobs monitored set shutdown graceful running jobs empty synchronized jobs running jobs drain to jobs break else jobs empty break jobs empty job job synchronized jobs job jobs poll try job complete process job statistics add job continue catch io exception e e get cause instanceof closed by interrupt exception job throw interrupted exception rpc socket layer blocking may throw wrapped exception thread interrupted since lower level cleared flag reset thread current thread interrupt else log warn lost job null job get job name unknown job get job name e continue synchronized jobs jobs offer job log error lost job null job get job name unknown job get job name never happen break try time unit milliseconds sleep poll delay millis catch interrupted exception e shutdown true continue catch throwable e log warn unexpected exception e start internal monitoring thread start thread start wait monitor halt assuming shutdown abort called note since submission may sporatic hang form shutdown requested join millis throws interrupted exception thread join millis drain submitted jobs queue stop monitoring thread upstream submitter assumed dead abort synchronized jobs graceful false shutdown true thread interrupt when monitored jobs completed stop monitoring thread upstream submitter assumed dead shutdown synchronized
948	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\JobSubmitter.java	pooling	package org apache hadoop mapred gridmix component accepting deserialized job traces computing split data submitting cluster deadline each job added upstream factory must submitted cluster deadline recorded once submitted jobs must added downstream component monitoring job submitter implements gridmix component gridmix job log log log factory get log job submitter semaphore sem statistics statistics file pool input dir job monitor monitor executor service sched volatile boolean shutdown false initialize submission component downstream monitor pool files split data may read see link gridmix gridmix sub thr see link gridmix gridmix que dep synthetic jobs job submitter job monitor monitor threads queue depth file pool input dir statistics statistics sem new semaphore queue depth sched new thread pool executor threads threads l time unit milliseconds new linked blocking queue runnable input dir input dir monitor monitor statistics statistics runnable wrapping job submitted cluster submit task implements runnable gridmix job job submit task gridmix job job job job run try pre compute split information try job build splits input dir catch io exception e log warn failed submit job get job get job name job get ugi e monitor submission failed job get job return catch exception e log warn failed submit job get job get job name job get ugi e monitor submission failed job get job return sleep deadline ns delay job get delay time unit nanoseconds ns delay time unit nanoseconds sleep ns delay ns delay job get delay time unit nanoseconds try submit job monitor add job call statistics add job stats job get job job get job desc log debug submit job system current time millis job get job get job id catch io exception e log warn failed submit job get job get job name job get ugi e e get cause instanceof closed by interrupt exception throw new interrupted exception failed submit job get job get job name monitor submission failed job get job catch class not found exception e log warn failed submit job get job get job name e monitor submission failed job get job catch interrupted exception e abort execution remove splits nesc todo release thd loc gridmix job pull description job id thread current thread interrupt monitor submission failed job get job catch exception e due exception job wasnt submitted log info job job get job get job id submission failed e monitor submission failed job get job finally sem release enqueue job submitted per deadline associated add gridmix job job throws interrupted exception boolean add to queue shutdown add to queue submit task task new submit task job sem acquire try sched execute task catch rejected execution exception e sem release re scan set input files splits derived refresh file pool throws io exception input dir refresh does nothing threadpool already initialized waiting work upstream factory start continue running queued jobs submitted cluster join millis throws interrupted exception shutdown throw new illegal state exception cannot wait active submit thread sched await termination millis time unit milliseconds finish jobs pending submission accept new work shutdown complete pending tasks accept new tasks shutdown true sched
949	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\LoadJob.java	pooling	package org apache hadoop mapred gridmix synthetic job generated trace description load job extends gridmix job log log log factory get log load job load job configuration conf submission millis job story jobdesc path root user group information ugi seq throws io exception super conf submission millis jobdesc root ugi seq job call throws io exception interrupted exception class not found exception ugi as new privileged exception action job job run throws io exception class not found exception interrupted exception job set mapper class load mapper job set reducer class load reducer job set num reduce tasks jobdesc get number reduces job set map output key class gridmix key job set map output value class gridmix record job set sort comparator class gridmix key comparator job set grouping comparator class spec grouping comparator job set input format class load input format job set output format class raw bytes output format job set partitioner class draft partitioner job set jar by class load job job get configuration set boolean job used generic parser true file output format set output path job outdir job submit return job return job protected boolean emulate compression return true this progress based resource usage matcher resource usage matcher runner extends thread resource usage matcher matcher progressive progress sleep time string sleep config gridmix emulators resource usage sleep duration default sleep time ms resource usage matcher runner task input output context context resource usage metrics metrics configuration conf context get configuration set resource calculator plugin class extends resource calculator plugin clazz conf get class tt config tt resource calculator plugin null resource calculator plugin resource calculator plugin plugin resource calculator plugin get resource calculator plugin clazz conf set parameters sleep time conf get long sleep config default sleep time progress new progressive get progress return context get progress instantiate resource usage matcher matcher new resource usage matcher matcher configure conf plugin metrics progress protected match throws exception match resource usage matcher match resource usage run log info resource usage matcher thread started try progress get progress match match sleep time try thread sleep sleep time catch exception e match progress match log info resource usage emulation complete matcher exiting catch exception e log info exception running resource usage emulation matcher thread exiting e makes sure task tracker kill map reduce tasks emulating status reporter extends thread task attempt context context status reporter task attempt context context context context run log info status reporter thread started try context get progress report progress context progress sleep time try thread sleep sleep ms catch exception e log info status reporter thread exiting catch exception e log info exception running status reporter thread e load mapper extends mapper null writable gridmix record gridmix key gridmix record acc ratio array list record factory reduces new array list record factory random r new random gridmix key key new gridmix key gridmix record val new gridmix record resource usage matcher runner matcher null status reporter reporter null protected setup context ctxt throws io exception interrupted exception configuration conf ctxt get configuration load split split load split
950	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\LoadSplit.java	unrelated	package org apache hadoop mapred gridmix load split extends combine file split id n spec maps reduces input records output bytes output records max memory reduce bytes new reduce records new spec reduces id mod reduce output bytes new reduce output records new resource usage metrics map metrics resource usage metrics reduce metrics load split super load split combine file split cfsplit maps id input bytes input records output bytes output records reduce bytes reduce records reduce output bytes reduce output records resource usage metrics metrics resource usage metrics r metrics throws io exception super cfsplit id id maps maps reduces reduce bytes length input records input records output bytes output bytes output records output records reduce bytes reduce bytes reduce records reduce records n spec reduce output bytes length reduce output bytes reduce output bytes reduce output records reduce output records map metrics metrics reduce metrics r metrics get id return id get map count return maps get input records return input records get output bytes reduces return new output bytes ret new reduces reduces ret math round output bytes reduce bytes return ret get output records reduces return new output records ret new reduces reduces ret math round output records reduce records return ret get reduce bytes return reduce output bytes get reduce records return reduce output records resource usage metrics get map resource usage metrics return map metrics resource usage metrics get reduce resource usage metrics return reduce metrics write data output throws io exception super write writable utils write v int id writable utils write v int maps writable utils write v long input records writable utils write v long output bytes writable utils write v long output records writable utils write v long max memory writable utils write v int reduces reduces write double reduce bytes write double reduce records writable utils write v int n spec n spec writable utils write v long reduce output bytes writable utils write v long reduce output records map metrics write num reduce metrics reduce metrics null reduce metrics length writable utils write v int num reduce metrics num reduce metrics reduce metrics write read fields data input throws io exception super read fields id writable utils read v int maps writable utils read v int input records writable utils read v long output bytes writable utils read v long output records writable utils read v long max memory writable utils read v long reduces writable utils read v int reduce bytes length reduces reduce bytes new reduces reduce records new reduces reduces reduce bytes read double reduce records read double n spec writable utils read v int reduce output bytes length n spec reduce output bytes new n spec reduce output records new n spec n spec reduce output bytes writable utils read v long reduce output records writable utils read v long map metrics new resource usage metrics map metrics read fields num reduce metrics writable utils read v int reduce metrics new resource usage metrics num reduce metrics num reduce metrics reduce metrics new resource usage metrics
951	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Progressive.java	unrelated	package org apache hadoop mapred gridmix used track progress tasks progressive get progress
952	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\PseudoLocalFs.java	unrelated	package org apache hadoop mapred gridmix pseudo local file system generates random data file fly instead storing files disk so opening file multiple times give file content there directories file system root files root e all file ur is pseudo local file system format code pseudo lt name gt lt file size gt code name unique name lt file size gt number representing size file bytes pseudo local fs extends file system path home the creation time modification time files link pseudo local fs time system current time millis string home dir block size l mb default buffer size mb uri name uri create pseudo pseudo local fs new path home dir pseudo local fs path home super home home uri get uri return name path get home directory return home path get working directory return get home directory generates valid pseudo local file path given code file id code code file size code path generate file path string file id file size return new path file id file size creating pseudo local file nothing validating file path actual data file generated fly client tries open file reading fs data output stream create path path throws io exception try validate file name format path catch file not found exception e throw new io exception file creation failed path return null validate path provided expected format pseudo local file system based files validate file name format path path throws file not found exception path path make qualified boolean valid true file size path uri get scheme equals get uri get scheme valid false else string parts path uri get path split try file size long value of parts parts length valid file size catch number format exception e valid false valid throw new file not found exception file path exist pseudo local file system return file size fs data input stream open path path buffer size throws io exception file size validate file name format path input stream new random input stream file size buffer size return new fs data input stream fs data input stream open path path throws io exception return open path default buffer size file status get file status path path throws io exception file size validate file name format path return new file status file size false block size time path boolean exists path path try validate file name format path catch file not found exception e return false return true fs data output stream create path path fs permission permission boolean overwrite buffer size short replication block size progressable progress throws io exception return create path file status list status path path throws file not found exception io exception return new file status get file status path input stream generates specified number random bytes random input stream extends input stream implements seekable positioned readable random r new random bytes writable val null position in val current position buffer val total size total number random bytes generated cur pos current position stream code buffer size code created if code buffer size code positive number default value mb used
953	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RandomAlgorithms.java	pooling	package org apache hadoop mapred gridmix random algorithms random algorithms index mapper get pos swap b get size reset a sparse index mapping table useful want non destructively permute small fraction large array sparse index mapper implements index mapper map integer integer mapping new hash map integer integer size sparse index mapper size size size get pos integer mapped mapping get pos mapped null return pos return mapped swap b b return val a get val b get b b val a mapping remove b else mapping put b val a val b mapping remove else mapping put val b get size return size reset mapping clear a dense index mapping table useful want non destructively permute large fraction array dense index mapper implements index mapper mapping dense index mapper size mapping new size size mapping get pos pos pos mapping length throw new index out of bounds exception return mapping pos swap b b return val a get val b get b mapping val b mapping b val a get size return mapping length reset return iteratively pick random numbers pool n each number picked selector index mapper mapping n random rand constructor the pool integers n percentage selected numbers this hint internal memory optimization random number generator selector n sel pcnt random rand n throw new illegal argument exception n positive boolean sparse n sel pcnt n n mapping sparse new sparse index mapper n new dense index mapper n rand rand select next random number next switch n case return case index mapping get n return index default pos rand next int n index mapping get pos mapping swap pos n return index get remaining random number pool size get pool size return n reset selector reuse usage reset mapping reset n mapping get size selecting random integers n select n random rand n ret new n n ret return ret selector selector new selector n n rand selected new selected selector next return selected
954	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RandomTextDataGenerator.java	unrelated	package org apache hadoop mapred gridmix a random text generator the words simply sequences alphabets random text data generator log log log factory get log random text data generator configuration key random text data generator list size string gridmix datagen randomtext listsize gridmix datagenerator randomtext listsize configuration key random text data generator word size string gridmix datagen randomtext wordsize gridmix datagenerator randomtext wordsize default random text data generator list size default list size default random text data generator word size default word size default random text data generator seed default seed l a list random words string words random random constructor link random text data generator default seed random text data generator size word size size default seed word size constructor link random text data generator random text data generator size long seed word size random new random seed words new string size todo change default actual stats todo u need varied sized words size words random string utils random word size true false null random get configured random text data generator list size get random text data generator list size configuration conf return conf get int gridmix datagen randomtext listsize default list size set random text data generator list size set random text data generator list size configuration conf list size log debug enabled log debug random text data generator configured use dictionary list size words conf set int gridmix datagen randomtext listsize list size get configured random text data generator word size get random text data generator word size configuration conf return conf get int gridmix datagen randomtext wordsize default word size set random text data generator word size set random text data generator word size configuration conf word size log debug enabled log debug random text data generator configured use dictionary words length word size conf set int gridmix datagen randomtext wordsize word size returns randomly selected word list random words string get random word index random next int words length return words index this mainly testing list string get random words return arrays list words
955	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ReadRecordFactory.java	unrelated	package org apache hadoop mapred gridmix for every record consumed read key val bytes stream provided read record factory extends record factory size internal scratch buffer read internal stream string gridmix read buf size gridmix read buffer size byte buf input stream src record factory factory read record factory target bytes target records input stream src configuration conf new avg record factory target bytes target records conf src conf read record factory record factory factory input stream src configuration conf src src factory factory buf new byte conf get int gridmix read buf size boolean next gridmix key key gridmix record val throws io exception factory next key val return false len null key key get size val get size len len buf length io utils read fully src buf math min buf length len return true get progress throws io exception return factory get progress close throws io exception io utils cleanup null src factory close
956	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RecordFactory.java	unrelated	package org apache hadoop mapred gridmix interface producing records inputs outputs tasks record factory implements closeable transform given record perform operation boolean next gridmix key key gridmix record val throws io exception estimate exhausted record capacity get progress throws io exception
957	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\ReplayJobFactory.java	unrelated	package org apache hadoop mapred gridmix replay job factory extends job factory statistics cluster stats log log log factory get log replay job factory creating new instance start thread link org apache hadoop tools rumen zombie job producer replay job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver resolver throws io exception super submitter job producer scratch conf start flag resolver thread create reader thread return new replay reader thread replay job factory update statistics cluster stats item replay reader thread extends thread replay reader thread string thread name super thread name run try start flag await thread current thread interrupted return init time time unit milliseconds convert system nano time time unit nanoseconds log info start replay init time first last thread current thread interrupted try job story job get next job filtered null job return first first job get submission time current job get submission time current last log warn job job get job id order continue last current submitter add job creator create gridmix job conf init time math round rate factor current first job scratch user resolver get target ugi user group information create remote user job get user sequence get and increment catch io exception e error e return catch interrupted exception e exit thread ignore jobs remaining trace finally io utils cleanup null job producer start reader thread wait latch necessary start r thread start
958	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\RoundRobinUserResolver.java	unrelated	package org apache hadoop mapred gridmix round robin user resolver implements user resolver log log log factory get log round robin user resolver uidx list user group information users collections empty list mapping user names original cluster ug is proxy users simulated cluster hash map string user group information usercache new hash map string user group information userlist assumes one user per line each line users list file form lt username gt group br group names ignored parsed list user group information parse user list uri user uri configuration conf throws io exception null user uri return collections empty list path userloc new path user uri string text raw ugi new text file system fs userloc get file system conf array list user group information ugi list new array list user group information line reader null try new line reader fs open userloc read line raw ugi line form username group e end position user name line e raw ugi find raw ugi get length e throw new io exception missing username raw ugi e e raw ugi get length string username text decode raw ugi get bytes e user group information ugi null try ugi user group information create proxy user username user group information get login user catch io exception ioe log error error creating proxy user ioe ugi null ugi list add ugi no need parse groups even exist go next line finally null close return ugi list synchronized boolean set target users uri userloc configuration conf throws io exception uidx users parse user list userloc conf users size throw new io exception build empty users error msg userloc usercache clear return true string build empty users error msg uri userloc return empty user list allowed round robin user resolver provided user resource uri userloc resulted empty user list synchronized user group information get target ugi user group information ugi ugi proxy user user group information target ugi usercache get ugi get user name target ugi null target ugi users get uidx users size usercache put ugi get user name target ugi return target ugi inherit doc p link round robin user resolver needs map users trace provided list target users so user list needed boolean needs target users list return true
959	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SerialJobFactory.java	unrelated	package org apache hadoop mapred gridmix serial job factory extends job factory job stats log log log factory get log serial job factory condition job completed lock new condition creating new instance start thread link org apache hadoop tools rumen zombie job producer serial job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver resolver throws io exception super submitter job producer scratch conf start flag resolver thread create reader thread return new serial reader thread serial job factory serial reader thread extends thread serial reader thread string thread name super thread name serial in scenario method waits notification submitted job actually completed logic simple true wait till previousjob completed break submit new job previous job new job run try start flag await thread current thread interrupted return log info start serial system current time millis gridmix job prev job thread current thread interrupted job story job try job get next job filtered null job return log debug enabled log debug serial mode submitting job job get name prev job job creator create gridmix job conf l job scratch user resolver get target ugi user group information create remote user job get user sequence get and increment lock lock try log info submitted job prev job submitter add prev job finally lock unlock catch io exception e error e if submission current job fails try submit next job return prev job null wait till previous job submitted completed lock lock try true try job completed await catch interrupted exception ie log error error serial job factory waiting job completion ie return log debug enabled log info job job get name completed break finally lock unlock prev job null catch interrupted exception e return finally io utils cleanup null job producer serial once get notification stats collector job completion simply notify waiting thread update statistics job stats item simply notify case serial submissions we bothered submitted job completed lock lock try job completed signal all finally lock unlock start reader thread wait latch necessary start log info starting serial submission r thread start
960	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SleepJob.java	pooling	package org apache hadoop mapred gridmix sleep job extends gridmix job log log log factory get log sleep job thread local random rand new thread local random return new random string sleepjob maptask only gridmix sleep maptask boolean map tasks only fake locations string hosts selector selector interval report progress seconds string gridmix sleep interval gridmix sleep interval string gridmix sleep max map time gridmix sleep max map time string gridmix sleep max reduce time gridmix sleep max reduce time map max sleep time reduce max sleep time sleep job configuration conf submission millis job story jobdesc path root user group information ugi seq num locations string hosts throws io exception super conf submission millis jobdesc root ugi seq fake locations num locations hosts hosts selector fake locations new selector hosts length fake locations hosts length rand get null map tasks only conf get boolean sleepjob maptask only false map max sleep time conf get long gridmix sleep max map time long max value reduce max sleep time conf get long gridmix sleep max reduce time long max value protected boolean emulate compression return false job call throws io exception interrupted exception class not found exception ugi as new privileged exception action job job run throws io exception class not found exception interrupted exception job set mapper class sleep mapper job set reducer class sleep reducer job set num reduce tasks map tasks only jobdesc get number reduces job set map output key class gridmix key job set map output value class null writable job set sort comparator class gridmix key comparator job set grouping comparator class spec grouping comparator job set input format class sleep input format job set output format class null output format job set partitioner class draft partitioner job set jar by class sleep job job get configuration set boolean job used generic parser true job submit return job return job sleep mapper extends mapper long writable long writable gridmix key null writable map long writable key long writable value context context throws io exception interrupted exception context set status sleeping value get ms left system current time millis key get time unit milliseconds sleep key get cleanup context context throws io exception interrupted exception n reds context get num reduce tasks n reds sleep split split sleep split context get input split id split get id n maps split get num maps this hack pass sleep duration via gridmix key todo we need come better solution gridmix key key new gridmix key gridmix key reduce spec l id idx n reds n maps key set partition key set reduce output bytes split get reduce durations idx id n reds context write key null writable get sleep reducer extends reducer gridmix key null writable null writable null writable duration l protected setup context context throws io exception interrupted exception context next key context get current key get type gridmix key reduce spec throw new io exception missing reduce spec null writable ignored context get values gridmix key spec context get current key duration spec get reduce output bytes
961	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Statistics.java	unrelated	package org apache hadoop mapred gridmix component collecting stats required components make decisions single thread collector tries collec stats each thread poll updates certain datastructure currently cluster stats components interested datastructure need register stats collector notifies listeners statistics implements component job log log log factory get log statistics stat collector statistics new stat collector job client cluster list cluster status listeners list stat listener cluster stats cluster statlisteners new copy on write array list stat listener cluster stats list job status listeners list stat listener job stats job stat listeners new copy on write array list stat listener job stats list jobids noof maps job map integer job stats job maps new concurrent hash map integer job stats completed jobs in current interval jt polling interval volatile boolean shutdown false max job completed in interval string max jobs completed in poll interval key gridmix max jobs completed poll interval reentrant lock lock new reentrant lock condition job completed lock new condition count down latch start flag statistics configuration conf polling interval count down latch start flag throws io exception interrupted exception user group information ugi user group information get login user cluster ugi as new privileged exception action job client job client run throws io exception return new job client new job conf conf jt polling interval polling interval max job completed in interval conf get int max jobs completed in poll interval key start flag start flag add job stats job job job story jobdesc seq gridmix job get job seq id job seq log info not tracking job job get job name seq id less zero seq return maps jobdesc null throw new illegal argument exception job story available job job get job name else maps jobdesc get number maps job stats stats new job stats maps job job maps put seq stats used job monitor add completed job add job job this thread notified initially jobmonitor incase data generation ignore getting input generated statistics alive return job stats stat job maps remove gridmix job get job seq id job stat null return completed jobs in current interval check reached maximum level job completions completed jobs in current interval max job completed in interval log debug enabled log debug reached maximum limit jobs polling interval completed jobs in current interval completed jobs in current interval lock lock try job completed notify listeners stat listener job stats job stat listeners update stat job completed signal all finally lock unlock todo we types listeners if listeners increase move map kind model add cluster stats observers stat listener cluster stats listener cluster statlisteners add listener add job stats listeners stat listener job stats listener job stat listeners add listener attempt start service start statistics start stat collector extends thread stat collector super stats collector thread run try start flag await thread current thread interrupted return catch interrupted exception ie log error statistics error waiting threads get ready ie return shutdown lock lock try job completed await jt polling interval time unit milliseconds catch interrupted exception ie shutdown log error statistics interrupt waiting completion
962	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\StatListener.java	unrelated	package org apache hadoop mapred gridmix stat listener stat listener t update t item
963	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\StressJobFactory.java	unrelated	package org apache hadoop mapred gridmix stress job factory extends job factory statistics cluster stats log log log factory get log stress job factory load status load status new load status condition cond underloaded lock new condition the minimum ratio pending running map tasks aka incomplete map tasks cluster map slot capacity us consider cluster overloaded for running maps count partially namely completed map counted map tasks calculation overload maptask mapslot ratio f string conf overload maptask mapslot ratio gridmix throttle maps task slot ratio overload map task map slot ratio the minimum ratio pending running reduce tasks aka incomplete reduce tasks cluster reduce slot capacity us consider cluster overloaded for running reduces count partially namely completed reduce counted reduce tasks calculation overload reducetask reduceslot ratio f string conf overload reducetask reduceslot ratio gridmix throttle reduces task slot ratio overload reduce task reduce slot ratio the maximum share cluster mapslot capacity counted toward job incomplete map tasks overload calculation max mapslot share per job f string conf max mapslot share per job gridmix throttle maps max slot share per job max map slot share per job the maximum share cluster reduceslot capacity counted toward job incomplete reduce tasks overload calculation max reduceslot share per job f string conf max reduceslot share per job gridmix throttle reducess max slot share per job max reduce slot share per job the ratio maximum number pending running jobs number task trackers max job tracker ratio f string conf max job tracker ratio gridmix throttle jobs tracker ratio max job tracker ratio creating new instance start thread link org apache hadoop tools rumen zombie job producer stress job factory job submitter submitter job story producer job producer path scratch configuration conf count down latch start flag user resolver resolver throws io exception super submitter job producer scratch conf start flag resolver overload map task map slot ratio conf get float conf overload maptask mapslot ratio overload maptask mapslot ratio overload reduce task reduce slot ratio conf get float conf overload reducetask reduceslot ratio overload reducetask reduceslot ratio max map slot share per job conf get float conf max mapslot share per job max mapslot share per job max reduce slot share per job conf get float conf max reduceslot share per job max reduceslot share per job max job tracker ratio conf get float conf max job tracker ratio max job tracker ratio thread create reader thread return new stress reader thread stress job factory worker thread responsible reading descriptions assigning sequence numbers normalizing time stress reader thread extends thread stress reader thread string name super name stress submits job stress mode jt overloaded wait if overloaded get number slots available keep submitting jobs till total jobs sufficient load jt that submit sigma maps job slots available run try start flag await thread current thread interrupted return log info start stress system current time millis thread current thread interrupted lock lock try load status overloaded wait jt overloaded try cond underloaded await catch interrupted exception ie return load status overloaded try job story job get next job
964	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\SubmitterUserResolver.java	unrelated	package org apache hadoop mapred gridmix resolves ug is submitting user submitter user resolver implements user resolver log log log factory get log submitter user resolver user group information ugi null submitter user resolver throws io exception log info current user resolver submitter user resolver ugi user group information get login user synchronized boolean set target users uri userdesc configuration conf throws io exception return false synchronized user group information get target ugi user group information ugi return ugi inherit doc p since link submitter user resolver returns user name running gridmix need target list users boolean needs target users list return false
965	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\Summarizer.java	unrelated	package org apache hadoop mapred gridmix summarizes various aspects link gridmix run summarizer execution summarizer execution summarizer cluster summarizer cluster summarizer protected string na n a summarizer new string na summarizer string args execution summarizer new execution summarizer args cluster summarizer new cluster summarizer execution summarizer get execution summarizer return execution summarizer cluster summarizer get cluster summarizer return cluster summarizer start configuration conf execution summarizer start conf cluster summarizer start conf this finalizes summarizer finalize job factory factory string path size user resolver resolver data statistics stats configuration conf throws io exception execution summarizer finalize factory path size resolver stats conf summarizes current link gridmix run cluster used string string string builder builder new string builder builder append execution summarizer string builder append cluster summarizer string return builder string
966	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\UserResolver.java	unrelated	package org apache hadoop mapred gridmix maps users trace set valid target users test cluster user resolver configure user map given uri configuration the resolver contract define resource interpreted default typically interpret uri link org apache hadoop fs path listing target users this method called link needs target users list returns true subclass contract target users boolean set target users uri userdesc configuration conf throws io exception map given ugi another per subclass contract user group information get target ugi user group information ugi indicates whether user resolver needs list target users provided user resolver boolean needs target users list
967	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\CumulativeCpuUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p a link resource usage emulator plugin emulates cumulative cpu usage performing certain cpu intensive operations performing cpu intensive operations essentially uses cpu every link resource usage emulator plugin configured feedback module e link resource calculator plugin monitor resource usage p p link cumulative cpu usage emulator plugin emulates cpu usage steps the frequency emulation configured via link cpu emulation progress interval cpu usage values matched via emulation interval boundaries p link cumulative cpu usage emulator plugin wrapper program managing cpu usage emulation feature it internally uses emulation algorithm called core described using link cpu usage emulator core performing actual emulation multiple calls core engine use amount cpu br p link cumulative cpu usage emulator plugin provides calibration feature via link initialize configuration resource usage metrics resource calculator plugin progressive calibrate plugin core underlying hardware as result calibration every call emulation engine core roughly use total usage value emulated this makes sure underlying hardware profiled use plugin accidently overuse cpu with unit emulation target value core engine roughly calls engine resulting roughly calls feedback resource usage monitor module excessive usage feedback module discouraged might result excess cpu usage resulting real cpu emulation p cumulative cpu usage emulator plugin implements resource usage emulator plugin protected cpu usage emulator core emulator core resource calculator plugin monitor progressive progress boolean enabled true emulation interval emulation interval target cpu usage last seen progress last seen cpu usage cpu usage configuration parameters string cpu emulation progress interval gridmix emulators resource usage cpu emulation interval default emulation frequency f times this core cpu usage emulation algorithm this core engine actually performs cpu intensive operations consume amount cpu multiple calls link compute help plugin emulate desired level cpu usage this core engine calibrated using link calibrate resource calculator plugin api suit underlying hardware better it also used optimize emulation cycle cpu usage emulator core performs computation use cpu compute allows core calibrate calibrate resource calculator plugin monitor total cpu usage this core engine emulate cpu usage the responsibility perform certain math intensive operations make sure desired value cpu used default cpu usage emulator implements cpu usage emulator core number times loop performing basic unit computation num iterations random random this fool jvm make think need value stored unit computation e link compute this prevent jvm optimizing code protected return value initialized link default cpu usage emulator default values note link default cpu usage emulator calibrated see link calibrate resource calculator plugin initialized using constructor default cpu usage emulator default cpu usage emulator num iterations num iterations num iterations random new random this consume desired level cpu this api try use x percent target cumulative cpu usage currently x set compute num iterations perform unit computation perform unit computation the complete cpu emulation based multiple invocations unit computation module protected perform unit computation todo configurable users emulators able pick choose math operations run example basic add sub mul div adv sqrt sin cosin compo basic adv also define input generator for use random number generator later changed accept multiple sources random data
968	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\ResourceUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p each resource emulated corresponding implementation implements link resource usage emulator plugin p br br link resource usage emulator plugin configured using link initialize configuration resource usage metrics resource calculator plugin progressive call every link resource usage emulator plugin also configured feedback module e link resource calculator plugin monitor current resource usage link resource usage metrics decides resource usage value emulate link progressive keeps track task progress p br br for configuring grid mix load use resource usage emulator see link resource usage matcher resource usage emulator plugin initialize plugin this might involve initializing variables calibrating plugin initialize configuration conf resource usage metrics metrics resource calculator plugin monitor progressive progress emulate resource usage match usage target the plugin use given link resource calculator plugin query current resource usage emulate throws io exception interrupted exception
969	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\ResourceUsageMatcher.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p this driver managing resource usage emulators link resource usage matcher expects comma separated list link resource usage emulator plugin implementations specified using link resource usage emulation plugins configuration parameter p p note order emulators invoked order configured resource usage matcher configuration key set resource usage emulators string resource usage emulation plugins gridmix emulators resource usage plugins list resource usage emulator plugin emulation plugins new array list resource usage emulator plugin configure link resource usage matcher load configured plugins initialize configure configuration conf resource calculator plugin monitor resource usage metrics metrics progressive progress class plugins conf get classes resource usage emulation plugins resource usage emulator plugin plugins null system println no resource usage emulator plugins configured else class extends resource usage emulator plugin plugin plugins plugin null emulation plugins add reflection utils new instance plugin conf initialize emulators configured emulator plugins loaded resource usage emulator plugin emulator emulation plugins emulator initialize conf metrics monitor progress match resource usage throws exception resource usage emulator plugin emulator emulation plugins match resource usage emulator emulate
970	mapreduce\src\contrib\gridmix\src\java\org\apache\hadoop\mapred\gridmix\emulators\resourceusage\TotalHeapUsageEmulatorPlugin.java	unrelated	package org apache hadoop mapred gridmix emulators resourceusage p a link resource usage emulator plugin emulates total heap usage loading jvm heap memory adding smaller chunks data heap essentially use heap space thus forcing jvm expand heap thus resulting increase heap usage p p link total heap usage emulator plugin emulates heap usage steps the frequency emulation configured via link heap emulation progress interval heap usage values matched via emulation specific interval boundaries p link total heap usage emulator plugin wrapper program managing heap usage emulation feature it internally uses emulation algorithm called core described using link heap usage emulator core performing actual emulation multiple calls core engine use amount heap total heap usage emulator plugin implements resource usage emulator plugin configuration parameters core engine emulate heap usage protected heap usage emulator core emulator core progress bar progressive progress decides plugin emulate heap usage boolean enabled true progress boundaries interval emulation done emulation interval target heap usage emulate target heap usage in mb the frequency based task progress memory emulation code run if value set emulation happen task progress the default value parameter link default emulation progress interval string heap emulation progress interval gridmix emulators resource usage heap emulation interval default value emulation interval default emulation progress interval f prev emulation progress f the minimum buffer reserved non emulation activities string min heap free ratio gridmix emulators resource usage heap min free ratio min free heap ratio default min free heap ratio f determines unit increase per call core engine load api this expressed percentage difference expected total heap usage current usage string heap load ratio gridmix emulators resource usage heap load ratio heap load ratio default heap load ratio f one mb defines core heap usage emulation algorithm this engine expected perform certain memory intensive operations consume amount heap link load load current heap increase heap usage specified value this core engine initialized using link initialize resource calculator plugin api suit underlying hardware better heap usage emulator core performs memory intensive operations use heap load size in mb initialize core initialize resource calculator plugin monitor total heap usage in mb reset resource usage reset this core engine emulate heap usage the responsibility perform certain memory intensive operations make sure desired value heap used default heap usage emulator implements heap usage emulator core store unit loads list protected array list object heap space new array list object increase heap usage current process given amount this done creating objects size mb load size in mb size in mb create another string object size mb heap space add object new byte one mb this initialize core check core emulate desired target underlying hardware initialize resource calculator plugin monitor total heap usage in mb max physical memory in mb monitor get physical memory size one mb max physical memory in mb total heap usage in mb throw new runtime exception total heap used max physical memory in mb bytes emulator configured emulate total total heap usage in mb bytes clear references grid mix allocated special objects heap usage reduced reset heap space clear total heap
971	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\HashingDistributionPolicy.java	unrelated	package org apache hadoop contrib index example choose shard insert delete based document id hashing do not use distribution policy number shards changes hashing distribution policy implements i distribution policy num shards non javadoc init shard shards num shards shards length non javadoc choose shard for insert document id key hash code key hash code return hash code hash code num shards hash code num shards non javadoc choose shard for delete document id key hash code key hash code return hash code hash code num shards hash code num shards
972	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\IdentityLocalAnalysis.java	unrelated	package org apache hadoop contrib index example identity local analysis maps inputs directly outputs identity local analysis implements i local analysis document id document and op non javadoc map document id key document and op value output collector document id document and op output reporter reporter throws io exception output collect key value non javadoc configure job conf job non javadoc close throws io exception
973	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocInputFormat.java	unrelated	package org apache hadoop contrib index example an input format line doc plain text files line doc line doc input format extends file input format document id line doc text and op non javadoc record reader document id line doc text and op get record reader input split split job conf job reporter reporter throws io exception reporter set status split string return new line doc record reader job file split split
974	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocLocalAnalysis.java	unrelated	package org apache hadoop contrib index example convert line doc text and op document and op required i local analysis line doc local analysis implements i local analysis document id line doc text and op string docid field name id string content field name content non javadoc map document id key line doc text and op value output collector document id document and op output reporter reporter throws io exception document and op op op value get op document doc null term term null op document and op op insert op document and op op update doc new document doc add new field docid field name key get text string field store yes field index un tokenized doc add new field content field name value get text string field store no field index tokenized op document and op op delete op document and op op update term new term docid field name key get text string output collect key new document and op op doc term non javadoc configure job conf job non javadoc close throws io exception
975	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocRecordReader.java	unrelated	package org apache hadoop contrib index example a simple record reader line doc plain text files line doc each line follows document id space op space content eof op ins insert insert del delete delete u upd update update line doc record reader implements record reader document id line doc text and op char space char eol n start pos end buffered input stream byte array output stream buffer new byte array output stream provide bridge get bytes byte array output stream without creating new byte array text stuffer extends output stream text target write b throw new unsupported operation exception write byte supported write byte data offset len throws io exception target set data offset len text stuffer bridge new text stuffer constructor line doc record reader configuration job file split split throws io exception start split get start end start split get length path file split get path open file seek start split file system fs file get file system job fs data input stream file in fs open split get path input stream file in boolean skip first line false start skip first line true wait till buffered input stream skip start file in seek start new buffered input stream skip first line skip first line establish start start line doc record reader read data null eol start start pos start end end non javadoc close throws io exception close non javadoc document id create key return new document id non javadoc line doc text and op create value return new line doc text and op non javadoc get pos throws io exception return pos non javadoc get progress throws io exception start end return f else return math min f pos start end start non javadoc synchronized boolean next document id key line doc text and op value throws io exception pos end return false key document id bytes first space read into key get text space return false read operation u ins del upd insert delete update text op text new text read into op text space return false string op str op text string document and op op op op str equals op str equals ins op str equals insert op document and op op insert else op str equals op str equals del op str equals delete op document and op op delete else op str equals u op str equals upd op str equals update op document and op op update else default insert op document and op op insert value set op op op document and op op delete return true else read rest line return read into value get text eol boolean read into text text char delimiter throws io exception buffer reset bytes read read data buffer delimiter bytes read return false pos bytes read bridge target text buffer write to bridge return true read data input stream output stream char delimiter throws io exception bytes true b read b break bytes byte c byte b c eol c delimiter break c r mark byte next c byte read next c
976	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\LineDocTextAndOp.java	unrelated	package org apache hadoop contrib index example this represents operation the operation insert delete update if operation insert update new document form text specified line doc text and op implements writable document and op op op text doc constructor line doc text and op doc new text set type operation set op document and op op op op op get type operation document and op op get op return op get text represents document text get text return doc non javadoc string string return get class get name op op text doc non javadoc write data output throws io exception throw new io exception get class get name write never called non javadoc read fields data input throws io exception throw new io exception get class get name read fields never called
977	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\example\RoundRobinDistributionPolicy.java	unrelated	package org apache hadoop contrib index example choose shard insert round robin fashion choose shards delete know stored round robin distribution policy implements i distribution policy num shards rr round robin implementation non javadoc init shard shards num shards shards length rr non javadoc choose shard for insert document id key chosen rr rr rr num shards return chosen non javadoc choose shard for delete document id key represents shards return
978	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\FileSystemDirectory.java	unrelated	package org apache hadoop contrib index lucene this implements lucene directory top general file system currently support locking file system directory extends directory file system fs path directory io file buffer size constructor file system directory file system fs path directory boolean create configuration conf throws io exception fs fs directory directory io file buffer size conf get int io file buffer size create create boolean dir false try file status status fs get file status directory status null dir status directory catch io exception e file exist dir already set false dir throw new io exception directory directory create throws io exception fs exists directory fs mkdirs directory boolean dir false try file status status fs get file status directory status null dir status directory catch io exception e file exist dir already set false dir throw new io exception directory directory clear old index files file status file status fs list status directory lucene index file name filter get filter file status length fs delete file status get path true throw new io exception cannot delete index file file status get path non javadoc string list throws io exception file status file status fs list status directory lucene index file name filter get filter string result new string file status length file status length result file status get path get name return result non javadoc boolean file exists string name throws io exception return fs exists new path directory name non javadoc file modified string name throw new unsupported operation exception non javadoc touch file string name throw new unsupported operation exception non javadoc file length string name throws io exception return fs get file status new path directory name get len non javadoc delete file string name throws io exception fs delete new path directory name true throw new io exception cannot delete index file name non javadoc rename file string string throws io exception fs rename new path directory new path directory non javadoc index output create output string name throws io exception path file new path directory name fs exists file fs delete file true delete existing one applicable throw new io exception cannot overwrite index file file return new file system index output file io file buffer size non javadoc index input open input string name throws io exception return open input name io file buffer size non javadoc index input open input string name buffer size throws io exception return new file system index input new path directory name buffer size non javadoc lock make lock string name return new lock boolean obtain return true release boolean locked throw new unsupported operation exception string string return lock new path directory name non javadoc close throws io exception close file system non javadoc string string return get class get name directory file system index input extends buffered index input shared clones descriptor fs data input stream position cache get pos descriptor path file io file buffer size throws io exception fs open file io file buffer size path file path debugging descriptor descriptor length boolean open
979	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\LuceneIndexFileNameFilter.java	unrelated	package org apache hadoop contrib index lucene a wrapper convert index file name filter implements java io filename filter org apache hadoop fs path filter lucene index file name filter implements path filter lucene index file name filter singleton new lucene index file name filter get instance lucene index file name filter get filter return singleton index file name filter lucene filter lucene index file name filter lucene filter index file name filter get filter non javadoc boolean accept path path return lucene filter accept null path get name
980	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\LuceneUtil.java	unrelated	package org apache hadoop contrib index lucene this copies methods lucene segment infos since lucene util index file names name index segment file string segments segments name generation reference file name string segments gen segments gen check file segments n file boolean segments file string name return name starts with index file names segments name equals index file names segments gen check file segments gen file boolean segments gen file string name return name equals index file names segments gen get generation n current segments n file directory get current segment generation directory directory throws io exception string files directory list files null throw new io exception cannot read directory directory list returned null return get current segment generation files get generation n current segments n file list files get current segment generation string files files null return max files length string file files file starts with index file names segments file equals index file names segments gen gen generation from segments file name file gen max max gen return max parse generation segments file name return generation from segments file name string file name file name equals index file names segments return else file name starts with index file names segments return long parse long file name substring index file names segments length character max radix else throw new illegal argument exception file name file name segments file
981	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\MixedDeletionPolicy.java	unrelated	package org apache hadoop contrib index lucene for mixed directory use keep all deletion policy read directory keep init use keep only last commit deletion policy writable directory initially empty keep latest init mixed deletion policy implements index deletion policy keep all from init init list commits throws io exception keep all from init commits size commit list commits throws io exception size commits size assert size keep all from init keep init latest delete rest keep all from init size index commit point commits get delete
982	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\MixedDirectory.java	unrelated	package org apache hadoop contrib index lucene the initial version index stored read file system dir file system directory index files created newer versions written writable local fs dir lucene fs directory we use general file system directory writable dir well but use lucene fs directory currently lucene randome write file system directory supports sequential write note we may delete files read file system dir segment files uncommitted checkpoint for reason may create files writable dir already exist read dir logically overwrite ones read dir mixed directory extends directory directory read dir file system directory directory write dir lucene fs directory take advantage fact lucene fs directory file exists faster mixed directory file system read fs path read path file system write fs path write path configuration conf throws io exception try read dir new file system directory read fs read path false conf check write fs local fs write dir fs directory get directory write path string catch io exception e try close catch io exception e ignore one throw original one throw e lock factory new no lock factory debugging mixed directory directory read dir directory write dir throws io exception read dir read dir write dir write dir lock factory new no lock factory string list throws io exception string read files read dir list string write files write dir list read files null read files length return write files else write files null write files length return read files else string result new string read files length write files length system arraycopy read files result read files length system arraycopy write files result read files length write files length return result delete file string name throws io exception write dir file exists name write dir delete file name read dir file exists name read dir delete file name boolean file exists string name throws io exception return write dir file exists name read dir file exists name file length string name throws io exception write dir file exists name return write dir file length name else return read dir file length name file modified string name throws io exception write dir file exists name return write dir file modified name else return read dir file modified name rename file string string throws io exception throw new unsupported operation exception touch file string name throws io exception write dir file exists name write dir touch file name else read dir touch file name index output create output string name throws io exception return write dir create output name index input open input string name throws io exception write dir file exists name return write dir open input name else return read dir open input name index input open input string name buffer size throws io exception write dir file exists name return write dir open input name buffer size else return read dir open input name buffer size close throws io exception try read dir null read dir close finally write dir null write dir close string string return get class get name read dir write dir
983	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\RAMDirectoryUtil.java	unrelated	package org apache hadoop contrib index lucene a utility writes index ram dir data output read data input index ram dir ram directory util buffer size ram output stream buffer size write number files ram directory data output write ram files data output ram directory dir string names throws io exception write int names length names length text write string names length dir file length names write long length length avoid extra copy index input input null try input dir open input names buffer size position byte buffer new byte buffer size position length len position buffer size length buffer size length position input read bytes buffer len write buffer len position len finally input null input close read number files data input ram directory read ram files data input ram directory dir throws io exception num files read int num files string name text read string length read long length avoid extra copy index output output null try output dir create output name position byte buffer new byte buffer size position length len position buffer size length buffer size length position read fully buffer len output write bytes buffer len position len finally output null output close
984	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\lucene\ShardWriter.java	unrelated	package org apache hadoop contrib index lucene the initial version index stored perm dir index files created newer versions written temp dir local fs after successfully creating new version temp dir shard writer moves new files perm dir deletes temp dir close shard writer log log log factory get log shard writer file system fs file system local fs path perm path temp directory dir index writer writer max num segments num forms constructor shard writer file system fs shard shard string temp dir index update configuration iconf throws io exception log info construct shard writer fs fs local fs file system get local iconf get configuration perm new path shard get directory temp new path temp dir init generation shard get generation fs exists perm assert init generation fs mkdirs perm else restore generation fs perm init generation dir new mixed directory fs perm local fs fs start local output perm temp iconf get configuration analyzer null use add indexes add document writer new index writer dir false null init generation new keep only last commit deletion policy new mixed deletion policy set parameters iconf process intermediate form carrying lucene instance shard deletes inserts ram index form process intermediate form form throws io exception first delete iterator term iter form delete term iterator iter next writer delete documents iter next insert writer add indexes no optimize new directory form get directory num forms close shard writer optimize lucene instance shard closing necessary copy files created temp directory permanent directory closing close throws io exception log info closing shard writer processed num forms forms try try max num segments writer optimize max num segments log info optimized shard max num segments segments finally writer close log info closed lucene index writer move from temp to perm log info moved new index files perm finally dir close log info closed shard writer non javadoc string string return get class get name perm temp set parameters index update configuration iconf max field length iconf get index max field length max field length writer set max field length max field length writer set use compound file iconf get index use compound file max num segments iconf get index max num segments max field length log info sea max field length writer get max field length log info sea use compound file writer get use compound file log info sea max num segments max num segments case previous reduce task fails restore generation original starting point deleting segments gen file segments n files whose generations greater starting generation rest unwanted files deleted unwanted segments n files deleted restore generation file system fs path perm start gen throws io exception file status file status fs list status perm new path filter boolean accept path path return lucene util segments file path get name remove segments n files whose generation greater starting generation file status length path path file status get path start gen lucene util generation from segments file name path get name fs delete path true always remove segments gen case last failed try removed segments
985	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\main\UpdateIndex.java	unrelated	package org apache hadoop contrib index main a distributed index partitioned shards each shard corresponds lucene instance this contains main method uses map reduce job analyze documents update lucene instances parallel the main method update index requires following information updating shards input formatter this specifies format input documents analysis this defines analyzer use input the analyzer determines whether document inserted updated deleted for inserts updates analyzer also converts input document lucene document input paths this provides location updated documents e g hdfs files directories h base tables shard paths index path number shards either specify path shard specify index path shards sub directories index directory output path when update shard done message put number map tasks all information specified configuration file all first two also specified command line options check conf index config xml template configurable parameters note because parallel nature map reduce behaviour multiple inserts deletes updates document undefined update index log log log factory get log update index number format number format number format get instance number format set minimum integer digits number format set grouping used false return system current time millis print usage string cmd system err println usage java update index get name n input paths input path input path n output path output path n shards shard dir shard dir n index path index path n num shards num n num map tasks num n conf conf path n note do use shards option index path option string get index path configuration conf return conf get sea index path get num shards configuration conf return conf get int sea num shards shard create shards string index path num shards configuration conf throws io exception string parent shard normalize path index path path separator version number generation file system fs file system get conf path path new path index path fs exists path file status file status fs list status path string shard names new string file status length count file status length file status directory shard names count file status get path get name count arrays sort shard names count shard shards new shard count num shards count num shards count shards new shard version number parent shard names generation number count count num shards string shard path true shard path parent number format format number fs exists new path shard path break shards new shard version number shard path generation return shards else shard shards new shard num shards shards length shards new shard version number parent number format format generation return shards the main method main string argv argv length print usage system exit string input paths string null path output path null string shards string null string index path null num shards num map tasks configuration conf new configuration string conf path null parse command line argv length parse command line argv equals input paths input paths string argv else argv equals output path output path new path argv else argv equals shards shards string argv else argv equals index path index path argv else argv equals num shards num shards integer parse
986	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\DocumentAndOp.java	unrelated	package org apache hadoop contrib index mapred this represents indexing operation the operation insert delete update if operation insert update new document must specified if operation delete update delete term must specified document and op implements writable this represents type operation insert delete update op op insert new op insert op delete new op delete op update new op update string name op string name name name string string return name op op document doc term term constructor operation document and op constructor insert operation document and op op op document doc assert op op insert op op doc doc term null constructor delete operation document and op op op term term assert op op delete op op doc null term term constructor insert delete update operation document and op op op document doc term term op op insert assert doc null assert term null else op op delete assert doc null assert term null else assert op op update assert doc null assert term null op op doc doc term term set instance insert operation set insert document doc op op insert doc doc term null set instance delete operation set delete term term op op delete doc null term term set instance update operation set update document doc term term op op update doc doc term term get type operation op get op return op get document document get document return doc get term term get term return term non javadoc string string string builder buffer new string builder buffer append get class get name buffer append op buffer append op buffer append doc doc null buffer append doc else buffer append null buffer append term term null buffer append term else buffer append null buffer append return buffer string non javadoc write data output throws io exception throw new io exception get class get name write never called non javadoc read fields data input throws io exception throw new io exception get class get name read fields never called
987	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\DocumentID.java	unrelated	package org apache hadoop contrib index mapred the represents document id type text document id implements writable comparable text doc id constructor document id doc id new text the text document id text get text return doc id non javadoc compare to object obj obj return else return doc id compare to document id obj doc id non javadoc hash code return doc id hash code non javadoc string string return get class get name doc id non javadoc write data output throws io exception throw new io exception get class get name write never called non javadoc read fields data input throws io exception throw new io exception get class get name read fields never called
988	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IDistributionPolicy.java	unrelated	package org apache hadoop contrib index mapred a distribution policy decides given document document id one shard request sent request insert shard request sent request delete i distribution policy initialization it must called choose shard called init shard shards choose shard send insert request choose shard for insert document id key choose shard shards send delete request e g round robin distribution policy would send delete request shards represents shards choose shard for delete document id key
989	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IIndexUpdater.java	unrelated	package org apache hadoop contrib index mapred a implements index updater create map reduce job configuration run map reduce job analyze documents update lucene instances parallel i index updater create map reduce job configuration run map reduce job analyze documents update lucene instances parallel run configuration conf path input paths path output path num map tasks shard shards throws io exception
990	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\ILocalAnalysis.java	unrelated	package org apache hadoop contrib index mapred application specific local analysis the output type must document id document and op i local analysis k extends writable comparable v extends writable extends mapper k v document id document and op
991	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateCombiner.java	unrelated	package org apache hadoop contrib index mapred this combiner combines multiple intermediate forms one intermediate form more specifically input intermediate forms single document ram index single delete term an output intermediate form contains multi document ram index multiple delete terms index update combiner extends map reduce base implements reducer shard intermediate form shard intermediate form log log log factory get log index update combiner index update configuration iconf max size in bytes near max size in bytes non javadoc reduce shard key iterator intermediate form values output collector shard intermediate form output reporter reporter throws io exception string message key string intermediate form form null values next intermediate form single doc form values next form size form null form total size in bytes single doc form size single doc form total size in bytes form null form size single doc form size max size in bytes close form form message output collect key form form null form null single doc form size near max size in bytes output collect key single doc form else form null form create form message form process single doc form form null close form form message output collect key form intermediate form create form string message throws io exception log info construct form writer message intermediate form form new intermediate form form configure iconf return form close form intermediate form form string message throws io exception form close writer log info closed form writer message form form non javadoc configure job conf job iconf new index update configuration job max size in bytes iconf get max ram size in bytes near max size in bytes max size in bytes max size in bytes max non javadoc close throws io exception
992	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateConfiguration.java	unrelated	package org apache hadoop contrib index mapred this provides getters setters number parameters most parameters related index update rest existing map reduce parameters index update configuration configuration conf constructor index update configuration configuration conf conf conf get underlying configuration object configuration get configuration return conf existing map reduce properties get io file buffer size return get int io file buffer size get io sort space mb get io sort mb return conf get int mr job config io sort mb set io sort space mb set io sort mb mb conf set int mr job config io sort mb mb get map reduce temp directory string get mapred temp dir return conf get mr config temp dir properties index update get distribution policy class extends i distribution policy get distribution policy class return conf get class sea distribution policy hashing distribution policy i distribution policy set distribution policy set distribution policy class class extends i distribution policy class conf set class sea distribution policy class i distribution policy get analyzer class extends analyzer get document analyzer class return conf get class sea document analyzer standard analyzer analyzer set analyzer set document analyzer class class extends analyzer class conf set class sea document analyzer class analyzer get index input format class extends input format get index input format class return conf get class sea input format line doc input format input format set index input format set index input format class class extends input format class conf set class sea input format class input format get index updater class extends i index updater get index updater class return conf get class sea index updater index updater i index updater set index updater set index updater class class extends i index updater class conf set class sea index updater class i index updater get local analysis class extends i local analysis get local analysis class return conf get class sea local analysis line doc local analysis i local analysis set local analysis set local analysis class class extends i local analysis class conf set class sea local analysis class i local analysis get representation number shards string get index shards return conf get sea index shards set representation number shards set index shards string shards conf set sea index shards shards get max field length lucene instance get index max field length return conf get int sea max field length set max field length lucene instance set index max field length max field length conf set int sea max field length max field length get max number segments lucene instance get index max num segments return conf get int sea max num segments set max number segments lucene instance set index max num segments max num segments conf set int sea max num segments max num segments check whether use compound file format lucene instance boolean get index use compound file return conf get boolean sea use compound file false set whether use compound file format lucene instance set index use compound file boolean use compound file conf set boolean sea use compound file
993	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateMapper.java	unrelated	package org apache hadoop contrib index mapred this applies local analysis key value pair convert result docid operation pair shard intermediate form pair index update mapper k extends writable comparable v extends writable extends map reduce base implements mapper k v shard intermediate form log log log factory get log index update mapper get map output key class extends writable comparable get map output key class return shard get map output value class extends writable get map output value class return intermediate form index update configuration iconf analyzer analyzer shard shards i distribution policy distribution policy i local analysis k v local analysis document id tmp key document and op tmp value output collector document id document and op tmp collector new output collector document id document and op collect document id key document and op value throws io exception tmp key key tmp value value map key value pair shard intermediate form pair internally local analysis first applied map key value pair document id operation pair docid operation pair mapped shard intermediate form pair the intermediate form form single document ram index single delete term map k key v value output collector shard intermediate form output reporter reporter throws io exception synchronized local analysis map key value tmp collector reporter tmp key null tmp value null document and op doc tmp value intermediate form form new intermediate form form configure iconf form process doc analyzer form close writer doc get op document and op op insert chosen shard distribution policy choose shard for insert tmp key chosen shard insert one shard output collect shards chosen shard form else throw new io exception chosen shard insert must else doc get op document and op op delete chosen shard distribution policy choose shard for delete tmp key chosen shard delete one shard output collect shards chosen shard form else broadcast delete shards shards length output collect shards form else update insert to shard distribution policy choose shard for insert tmp key delete from shard distribution policy choose shard for delete tmp key insert to shard insert to shard delete from shard update one shard output collect shards insert to shard form else prepare deletion form intermediate form deletion form new intermediate form deletion form configure iconf deletion form process new document and op document and op op delete doc get term analyzer deletion form close writer delete from shard delete one shard output collect shards delete from shard deletion form else broadcast delete shards shards length output collect shards deletion form prepare insertion form intermediate form insertion form new intermediate form insertion form configure iconf insertion form process new document and op document and op op insert doc get document analyzer insertion form close writer insert one shard output collect shards insert to shard insertion form else throw new io exception chosen shard insert must non javadoc configure job conf job iconf new index update configuration job analyzer analyzer reflection utils new instance iconf get document analyzer class job local analysis i local analysis reflection utils new instance iconf get local analysis class
994	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateOutputFormat.java	unrelated	package org apache hadoop contrib index mapred the record writer output format simply puts message output path shard update done index update output format extends file output format shard text non javadoc record writer shard text get record writer file system fs job conf job string name progressable progress throws io exception path perm new path get work output path job name return new record writer shard text write shard key text value throws io exception assert index update reducer done equals value string shard name key get directory shard name shard name replace path done file new path perm index update reducer done shard name fs exists done file fs create new file done file close reporter reporter throws io exception
995	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdatePartitioner.java	unrelated	package org apache hadoop contrib index mapred this partitioner puts values key case shard partition index update partitioner implements partitioner shard intermediate form shard shards map shard integer map non javadoc get partition shard key intermediate form value num partitions partition map get key value partition num partitions return partition else return num partitions non javadoc configure job conf job shards shard get index shards new index update configuration job map new hash map shard integer shards length map put shards
996	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdater.java	unrelated	package org apache hadoop contrib index mapred an implementation index updater creates map reduce job configuration run map reduce job analyze documents update lucene instances parallel index updater implements i index updater log log log factory get log index updater index updater non javadoc run configuration conf path input paths path output path num map tasks shard shards throws io exception job conf job conf create job conf input paths output path num map tasks shards job client run job job conf job conf create job configuration conf path input paths path output path num map tasks shard shards throws io exception set starting generation shard reduce task fails new reduce task know start set shard generation conf shards iconf set sets properties conf index update configuration iconf new index update configuration conf shard set index shards iconf shards map task map output buffer uses job context io sort mb decide max buffer size max buffer size job context io sort mb here half en job context io sort mb use half memory build intermediate form index combiner iconf set io sort mb iconf get io sort mb create job configuration job conf job conf new job conf conf index updater job conf set job name get class get name system current time millis provided application file input format set input paths job conf input paths file output format set output path job conf output path job conf set num map tasks num map tasks already set shards job conf set num reduce tasks shards length job conf set input format iconf get index input format class path inputs file input format get input paths job conf string builder buffer new string builder inputs string inputs length buffer append buffer append inputs string log info mapred input dir buffer string log info mapreduce output fileoutputformat outputdir file output format get output path job conf string log info mapreduce job maps job conf get num map tasks log info mapreduce job reduces job conf get num reduce tasks log info shards length shards iconf get index shards better create input format instance log info mapred input format job conf get input format get class get name set system job conf set map output key class index update mapper get map output key class job conf set map output value class index update mapper get map output value class job conf set output key class index update reducer get output key class job conf set output value class index update reducer get output value class job conf set mapper class index update mapper job conf set partitioner class index update partitioner job conf set combiner class index update combiner job conf set reducer class index update reducer job conf set output format index update output format return job conf set shard generation configuration conf shard shards throws io exception file system fs file system get conf shards length path path new path shards get directory generation fs exists path file system directory dir null try dir new file system directory fs path false conf
997	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IndexUpdateReducer.java	unrelated	package org apache hadoop contrib index mapred this reducer applies shard changes a new version shard created end reduce it important note new version shard derived scratch by leveraging lucene update algorithm new version lucene instance share many files possible previous version index update reducer extends map reduce base implements reducer shard intermediate form shard text log log log factory get log index update reducer text done new text done get reduce output key class extends writable comparable get output key class return shard get reduce output value class extends writable get output value class return text index update configuration iconf string mapred temp dir non javadoc reduce shard key iterator intermediate form values output collector shard text output reporter reporter throws io exception log info construct shard writer key file system fs file system get iconf get configuration string temp mapred temp dir path separator shard system current time millis shard writer writer new shard writer fs key temp iconf update shard values next intermediate form form values next writer process form reporter progress close shard reporter f reporter reporter new closeable volatile boolean closed false close throws io exception spawn thread give progress heartbeats thread prog new thread run closed try f reporter set status closing thread sleep catch interrupted exception e continue catch throwable e return try prog start writer null writer close finally closed true close log info closed shard writer key writer writer output collect key done non javadoc configure job conf job iconf new index update configuration job mapred temp dir iconf get mapred temp dir mapred temp dir shard normalize path mapred temp dir non javadoc close throws io exception
998	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\IntermediateForm.java	unrelated	package org apache hadoop contrib index mapred an intermediate form one parsed lucene documents delete terms it actually uses lucene file format format intermediate form using ram dir files note if process ever called close writer called otherwise need call close writer intermediate form implements writable index update configuration iconf null collection term delete list ram directory dir index writer writer num docs constructor intermediate form throws io exception delete list new concurrent linked queue term dir new ram directory writer null num docs configure using index update configuration configure index update configuration iconf iconf iconf get ram directory intermediate form directory get directory return dir get iterator delete terms intermediate form iterator term delete term iterator return delete list iterator this method used index update mapper process document operation current intermediate form process document and op doc analyzer analyzer throws io exception doc get op document and op op delete doc get op document and op op update delete list add doc get term doc get op document and op op insert doc get op document and op op update writer null analyzer null specify analyzer add document writer create writer writer add document doc get document analyzer num docs this method used index update combiner process intermediate form current intermediate form more specifically input intermediate forms single document ram index single delete term process intermediate form form throws io exception form delete list size delete list add all form delete list form dir size in bytes writer null writer create writer writer add indexes no optimize new directory form dir num docs close lucene index writer associated intermediate form created do close ram directory in fact need close ram directory close writer throws io exception writer null writer close writer null the total size files directory ram used index writer it memory used delete list total size in bytes throws io exception size dir size in bytes writer null size writer ram size in bytes return size non javadoc string string string builder buffer new string builder buffer append get class get simple name buffer append num docs buffer append num docs buffer append num deletes buffer append delete list size delete list size buffer append iterator term iter delete term iterator iter next buffer append iter next buffer append buffer append buffer append return buffer string index writer create writer throws io exception index writer writer new index writer dir false null new keep only last commit deletion policy writer set use compound file false iconf null max field length iconf get index max field length max field length writer set max field length max field length return writer reset form throws io exception delete list clear dir size in bytes ok close ram directory dir close alternative delete files reuse ram directory dir new ram directory assert writer null num docs writable non javadoc write data output throws io exception write int delete list size term term delete list text write string term field text write string term text string files dir list ram directory util write ram
999	mapreduce\src\contrib\index\src\java\org\apache\hadoop\contrib\index\mapred\Shard.java	unrelated	package org apache hadoop contrib index mapred this represents metadata shard version version number entire index directory directory shard resides generation lucene index generation version generation reserved future use note currently version number entire index used defaults shard implements writable comparable this method copied path string normalize path string path remove slashes backslashes path path replace path path replace trim trailing slash non root path ignoring windows drive path length path ends with path path substring path length return path set index shards index update configuration conf shard shards string builder shards string new string builder shards string shards length shards string append shards string append shards string conf set index shards shards string string shard get index shards index update configuration conf string shards conf get index shards shards null array list object list collections list new string tokenizer shards shard result new shard list size list size result shard create shard from string string list get return result else return null assume str formatted correctly shard shard create shard from string string str first str index of second str index of first version long parse long str substring first string dir str substring first second gen long parse long str substring second return new shard version dir gen index shard version shards version index version number version string dir gen lucene generation constructor shard version dir null gen construct shard versio number directory generation number shard version string dir gen version version dir normalize path dir gen gen construct using shard object shard shard shard version shard version dir shard dir gen shard gen get version number entire index get version return version get directory shard resides string get directory return dir get generation lucene instance get generation return gen non javadoc string string return version dir gen writable non javadoc write data output throws io exception write long version text write string dir write long gen non javadoc read fields data input throws io exception version read long dir text read string gen read long comparable non javadoc compare to object return compare to shard compare another shard compare to shard compare version version version return else version version return compare dir result dir compare to dir result return result compare gen gen gen return else gen gen return else return non javadoc boolean equals object return true instanceof shard return false shard shard return version version dir equals dir gen gen non javadoc hash code return version dir hash code gen
1000	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\AllMapsCompletedTaskAction.java	unrelated	package org apache hadoop mapred this used notifying simulator task tracker running reduce task map tasks job done a simulator job tracker notifies simulator task tracker sending task tracker action response heartbeat represents directive start running user code reduce task we introduced extra push mechanism implement corresponding complicated pull part inter tracker protocol we use proper simulation events signaling hack heartbeat instead since job tracker emit events know recipient task tracker java object all maps completed task action extends task tracker action task attempt id reduce task proceed org apache hadoop mapreduce task attempt id task id constructs all maps completed task action object given link org apache hadoop mapreduce task attempt id link org apache hadoop mapreduce task attempt id reduce task proceed all maps completed task action org apache hadoop mapreduce task attempt id task id super action type launch task task id task id get task attempt id reduce task task attempt org apache hadoop mapreduce task attempt id get task id return task id write data output throws io exception super write task id write read fields data input throws io exception super read fields task id read fields string string return all maps completed task action task id task id
1001	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\FakeConcurrentHashMap.java	unrelated	package org apache hadoop mapred a fake concurrent hash map implementation maintains insertion order entries traversing iterator the support deterministic replay mumak meant used concurrent hash map replacement multiple threads fake concurrent hash map k v extends concurrent hash map k v map k v map fake concurrent hash map map new linked hash map k v v put if absent k key v value contains key key return put key value else return get key boolean remove object key object value contains key key return false object old value get key old value null value null old value equals value remove key return true return false v replace k key v value contains key key return put key value else return null boolean replace k key v old value v new value contains key key return false object orig value get key orig value null old value null orig value equals old value put key new value return true return false clear map clear boolean contains key object key return map contains key key boolean contains value object value return map contains value value set map entry k v entry set return map entry set v get object key return map get key boolean empty return map empty set k key set return map key set v put k key v value return map put key value put all map extends k extends v map put all v remove object key return map remove key size return map size collection v values return map values
1002	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\HeartbeatEvent.java	heartbeat	package org apache hadoop mapred this used link simulator task tracker signaling next hearbeat call job tracker due heartbeat event extends simulator event constructor link simulator task tracker event delivered time event delivered heartbeat event simulator event listener listener timestamp super listener timestamp
1003	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\JobCompleteEvent.java	unrelated	package org apache hadoop mapred link job complete event created link simulator job tracker job completed link simulator job client picks event mark job completed when jobs completed simulation terminated job complete event extends simulator event simulator engine engine job status job status job complete event simulator job client jc timestamp job status job status simulator engine engine super jc timestamp engine engine job status job status simulator engine get engine return engine job status get job status return job status protected string real to string return super real to string status job status string
1004	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\JobSubmissionEvent.java	unrelated	package org apache hadoop mapred link simulator event trigging submission job job tracker job submission event extends simulator event job story job job submission event simulator event listener listener timestamp job story job super listener timestamp job job job story get job return job protected string real to string return super real to string job id job get job id
1005	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\LoadProbingEvent.java	unrelated	package org apache hadoop mapred link load probing event created link simulator job tracker link simulator job submission policy stress link simulator job client picks event would check whether system load stressed if would submit next job load probing event extends simulator event load probing event simulator job client jc timestamp super jc timestamp
1006	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorClock.java	unrelated	package org apache hadoop mapred a clock mocked testing simulator clock extends clock current time simulator clock super current time set time current time get time return current time
1007	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorCSJobInitializationThread.java	scheduler	package org apache hadoop mapred simulator cs job initialization thread implements simulator event listener last called capacity task scheduler task scheduler job initialization poller job poller string queue sleep interval the log object send messages used debugging log log log factory get log simulator cs job initialization thread simulator cs job initialization thread task scheduler task scheduler string queue task scheduler capacity task scheduler task scheduler job poller task scheduler get initialization poller sleep interval job poller get sleep interval queue queue list simulator event accept simulator event event throws io exception simulator thread wake up event e event instanceof simulator thread wake up event e simulator thread wake up event event else throw new io exception received unexpected type event sim thrd cap sched job init job poller clean up initialized jobs list job poller select jobs to initialize job initialization thread thread job poller get threads to queue map get queue thread initialize jobs last called e get time stamp list simulator event return events collections simulator event singleton list new simulator thread wake up event last called sleep interval return return events list simulator event init throws io exception return collections simulator event singleton list new simulator thread wake up event sleep interval
1008	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEngine.java	heartbeat	package org apache hadoop mapred link simulator engine main simulator to launch simulator user either run main directly two parameters input trace file corresponding topology file use script bin mumak sh trace json topology json trace file topology file produced rumen simulator engine extends configured implements tool list simulator event empty events new array list simulator event default number milliseconds required boot entire cluster default cluster startup duration protected simulator event queue queue new simulator event queue string trace file string topology file simulator job tracker jt simulator job client jc boolean shutdown false terminate time long max value current time the hash set storing simulated threads useful job initialization capacity scheduler hash set simulator cs job initialization thread thread set the log object send messages used debugging log log log factory get log simulator engine master random seed read configuration file present it used creating sub seeds random number generators master random seed start simulated task trackers based topology time stamp simulator started link simulator task tracker started uniformly randomly spread start duration trackers sending hearbeats steady rate start task trackers cluster story cluster job conf job conf port assigned t ts incremented tt port num task trackers random random new random random seed generator get seed start task trackers master random seed start duration job conf get int mumak cluster startup duration default cluster startup duration machine node node cluster get machines job conf set mumak tasktracker host name node get name job conf set mumak tasktracker tracker name tracker node get name localhost port sub random seed random seed generator get seed task tracker num task trackers master random seed job conf set long mumak tasktracker random seed sub random seed num task trackers port simulator task tracker tt new simulator task tracker jt job conf first heartbeat random next int start duration queue add all tt init first heartbeat in start duration heartbeat interval full cluster time tt started told nd heartbeat beat rate corresponding steady state cluster cluster steady start duration jt get next heartbeat interval return cluster steady reads positive integer configuration get time property configuration conf string property name default value throws illegal argument exception possible improvement change date format human readable time conf get long property name default value time throw new illegal argument exception property name time must positive time return time creates configuration mumak simulation this kept modular mostly testing purposes standard configuration modified passing init function job conf create mumak conf job conf job conf new job conf get conf job conf set class topology node switch mapping impl static mapping dns to switch mapping job conf set fs default name file job conf set mapred job tracker localhost job conf set int mapred jobtracker job history block size job conf set int mapred jobtracker job history buffer size job conf set long mapred tasktracker expiry interval job conf set int mapred reduce copy backoff job conf set long mapred job reuse jvm num tasks job conf set user mumak job conf set mapred system dir job conf get hadoop log
1009	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEvent.java	unrelated	package org apache hadoop mapred link simulator event represents specific event mumak each link simulator event expected expiry time fired link simulator event listener handle link simulator event fired simulator event protected simulator event listener listener protected timestamp protected internal count protected simulator event simulator event listener listener timestamp listener listener timestamp timestamp get expected event expiry time get time stamp return timestamp get link simulator event listener handle link simulator event simulator event listener get listener return listener get internal counter link simulator event each link simulator event holds counter incremented every event order multiple events occur time get internal count return internal count set internal counter link simulator event set internal count count internal count count string string return get class get name real to string converts list fields values human readable format name override wanted new fields show string protected string real to string return timestamp timestamp listener listener
1010	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEventListener.java	unrelated	package org apache hadoop mapred interface entities handle events simulator event listener get initial events put event queue list simulator event init throws io exception process event generate events put event queue list simulator event accept simulator event event throws io exception
1011	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorEventQueue.java	unrelated	package org apache hadoop mapred link simulator event queue maintains priority queue events scheduled future virtual time events happen virtual time order the link simulator event queue notion current time defined time stamp last event already handled an event inserted link simulator event queue time stamp must later current time simulator event queue list simulator event empty events new array list simulator event simulator event last event null event count priority queue simulator event events new priority queue simulator event new comparator simulator event compare simulator event simulator event get time stamp get time stamp return else get time stamp get time stamp return get internal count get internal count return else get internal count get internal count return return get next earliest link simulator event handled this link simulator event smallest time stamp among link simulator event currently scheduled link simulator event queue simulator event get last event events poll return last event add single link simulator event link simulator event queue link simulator event convention collection add boolean add simulator event event last event null event get time stamp last event get time stamp throw new illegal argument exception event happens past event get class event set internal count event count return events add event adding link simulator event the container contains events added convention collection add all boolean add all collection extends simulator event events last time stamp last event null long min value last event get time stamp simulator event e events e get time stamp last time stamp throw new illegal argument exception event happens past e get class e get time stamp last time stamp e set internal count event count return events add all events get current time queue it defined time stamp last event handled get current time last event null return last event get time stamp else return get size currently scheduled events number events system major scaling factor simulator get size return events size get total number events handled simulation this indicator large particular simulation run get event count return event count
1012	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobCache.java	unrelated	package org apache hadoop mapred a link job id link job story mapping used link job client link job tracker job submission simulator job cache map job id job story submitted jobs new hash map job id job story put link job id link job story mapping put job id job id job story job submitted jobs put job id job get job identified link job id remove mapping job story get job id job id return submitted jobs remove job id check job head queue without removing mapping job story peek job id job id return submitted jobs get job id
1013	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobClient.java	scheduler	package org apache hadoop mapred class simulates job client it main functionality submit jobs simulation engine shutdown simulation engine job producer runs jobs simulator job client implements simulator event listener protected job sketch info protected num maps protected num reduces job sketch info num maps num reduces num maps num maps num reduces num reduces client protocol job tracker job story producer job story producer simulator job submission policy submission policy load prob interval start load prob interval max load probing interval load prob interval start the minimum ratio pending running map tasks aka incomplete map tasks cluster map slot capacity us consider cluster overloaded for running maps count partially namely completed map counted map tasks calculation overlaod maptask mapslot ratio f keep track flight load probing event load probing event flight lpe null we handle simulator event queue thus cannot cancel events directly instead keep identity map identity set except jdk provide identity set skip events cancelled map load probing event boolean cancelled lpe new identity hash map load probing event boolean map job id job sketch info running jobs new linked hash map job id job sketch info boolean more jobs false job story next job constructor the job tracker submit job note link simulator job client interacts job tracker link client protocol simulator job client client protocol job tracker job story producer job story producer simulator job submission policy submission policy job tracker job tracker job story producer job story producer submission policy submission policy constructor the job tracker submit job note link simulator job client interacts job tracker link client protocol simulator job client client protocol job tracker job story producer job story producer job tracker job story producer simulator job submission policy replay list simulator event init throws io exception job story job job story producer get next job submission policy simulator job submission policy replay job get submission time throw new io exception inconsistent submission time first job job get submission time job submission event event new job submission event job submission policy simulator job submission policy stress return collections simulator event singleton list event else array list simulator event ret new array list simulator event ret add event flight lpe new load probing event load probing interval ret add flight lpe return ret doing exponential back probing load probing could pretty expensive many pending jobs adjust load probing interval boolean overloaded overloaded we extend lpe interval flight lpe flight lpe null load probing interval math min load probing interval load prob interval max else load probing interval load prob interval start we try use light weight mechanism determine cluster load boolean overloaded throws io exception try cluster metrics cluster metrics job tracker get cluster metrics if jobs number task trackers assume cluster overloaded this bound memory usage simulator job tracker situations jobs small number map tasks large number reduce tasks running jobs size cluster metrics get task tracker count system printf overloaded running jobs task tracker count n boolean true string running jobs size cluster metrics get task tracker count return true incomplete map tasks
1014	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobInProgress.java	scheduler	package org apache hadoop mapred simulator job in progress extends job in progress log log log factory get log simulator job in progress job story contains information read cache job story job story task split meta info task split meta info simulator job in progress job id jobid string job submit dir job tracker jobtracker job conf default conf job story job story super default conf job setup cleanup needed set false parent cstr though default true job id jobid string url http jobtracker get job tracker machine jobtracker get info port jobdetails jsp jobid jobid jobtracker jobtracker conf job story get job conf priority conf get job priority path job dir new path job submit dir job file new path job dir job xml status new job status jobid f f f f job status prep priority conf get user conf get job name job file string url profile new job profile job story get user jobid job file string url job story get name conf get queue name start time job tracker get clock get time status set start time start time resource estimator new resource estimator num map tasks job story get number maps num reduce tasks job story get number reduces task completion events new array list task completion event num map tasks num reduce tasks map failures percent conf get max map task failures percent reduce failures percent conf get max reduce task failures percent max level jobtracker get num task cache levels cache level max level non local maps new linked list task in progress non local running maps new linked hash set task in progress running map cache new identity hash map node set task in progress non running reduces new linked list task in progress running reduces new linked hash set task in progress slow task threshold math max f conf get float mapred speculative execution slow task threshold f speculative cap conf get float mapred speculative execution speculative cap f slow node threshold conf get float mapred speculative execution slow node threshold f job story job story job history jobtracker get job history init tasks update information job story object synchronized init tasks throws io exception boolean logging enabled log debug enabled logging enabled log debug init tasks sjip starting initialization job id num map tasks job story get number maps num reduce tasks job story get number reduces log submission to job history logging enabled log debug init tasks sjip logged job history job id check task limits logging enabled log debug init tasks sjip checked task limits job id string job file default task split meta info create splits job story logging enabled log debug init tasks sjip created splits job job id number splits task split meta info length create map tasks job file task split meta info num map tasks non running map cache create cache task split meta info max level logging enabled log debug init tasks num maps num map tasks size non running map cache non running map cache size job id set launch time launch
1015	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobStory.java	unrelated	package org apache hadoop mapred this proxy job story zombie job customized submission time because simulation submission time totally produced simulator original submission time job trace ignored simulator job story implements job story job story job submission time simulator job story job story job time job job submission time time get submission time return submission time input split get input splits return job get input splits job conf get job conf return job get job conf task attempt info get map task attempt info adjusted task number task attempt number locality return job get map task attempt info adjusted task number task attempt number locality string get name return job get name org apache hadoop mapreduce job id get job id return job get job id get number maps return job get number maps get number reduces return job get number reduces task attempt info get task attempt info task type task type task number task attempt number return job get task attempt info task type task number task attempt number task info get task info task type task type task number return job get task info task type task number string get user return job get user pre job history constants values get outcome return job get outcome string get queue name return job get queue name
1016	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobStoryProducer.java	unrelated	package org apache hadoop mapred this creates link job story objects trace file rumen format it proxy link zombie job producer adjusts submission time aligned simulation time simulator job story producer implements job story producer zombie job producer producer first job start time relative time boolean first job true simulator job story producer path path zombie cluster cluster first job start time configuration conf throws io exception path cluster first job start time conf system nano time simulator job story producer path path zombie cluster cluster first job start time configuration conf seed throws io exception producer new zombie job producer path cluster conf seed first job start time first job start time filter jobs fed simulator for filter killed jobs facilitate debugging job story get next job filtered throws io exception true zombie job job producer get next job job null return null job get outcome pre job history constants values killed continue job get number maps continue job get num logged maps continue return job job story get next job throws io exception job story job get next job filtered job null return null first job first job false relative time job get submission time first job start time return new simulator job story job job get submission time relative time close throws io exception producer close
1017	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobSubmissionPolicy.java	unrelated	package org apache hadoop mapred job submission policies the set policies closed encapsulated link simulator job submission policy the handling submission policies embedded link simulator engine various events enum simulator job submission policy replay trace following job inter arrival rate faithfully replay ignore submission time keep submitting jobs cluster saturated stress submitting jobs sequentially serial string job submission policy mumak job submission policy simulator job submission policy get policy configuration conf string policy conf get job submission policy replay name return value of policy upper case
1018	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorJobTracker.java	heartbeat	package org apache hadoop mapred link simulator job tracker extends link job tracker it implements link inter tracker protocol protocols simulator job tracker extends job tracker a queue cleaning jobs memory the length queue always less constant specified jobs in mumak memory linked list job id cleanup queue the simulator clock maintains current simulation time always synchronized time maintained engine simulator clock clock null log log log factory get log simulator job tracker this constant used specify many jobs maintained memory mumak simulator jobs in mumak memory the simulator engine data structure engine drives simulator simulator engine engine null synchronized reset engine clock simulator engine engine simulator clock clock simulator job tracker engine engine simulator job tracker clock clock in addition standard job conf object constructor simulator job tracker requires start time simulation reference simulator engine object the clock job tracker set start time simulator job tracker job conf conf simulator clock clock simulator engine simulator engine throws io exception invoke super constructor flag indicates simulation super conf clock true reset engine clock simulator engine clock cleanup queue new linked list job id starts job tracker given configuration given time it also starts job notifier thread initialize clock simulator job tracker start tracker job conf conf start time simulator engine engine throws io exception simulator job tracker result null try simulator clock sim clock new simulator clock start time result new simulator job tracker conf sim clock engine result task scheduler set task tracker manager result catch io exception e log warn error starting tracker string utils stringify exception e result null job end notifier start notifier return result start simulator job tracker given configuration creating simulator engine pretty much used debugging simulator job tracker start tracker job conf conf start time throws io exception interrupted exception return start tracker conf start time new simulator engine offer service throws interrupted exception io exception task scheduler start log info started task scheduler synchronized state state running returns simulator clock object simulator job tracker clock get clock assert engine get current time clock get time engine time engine get current time job tracker time clock get time return clock overriding get clean task reports function original job tracker since setup cleanup tasks synchronized task report get cleanup task reports job id jobid return null overriding since support queue acls queue acls info get queue acls for current user throws io exception return null overriding since simulate setup cleanup tasks synchronized task report get setup task reports job id jobid return null synchronized job status submit job job id job id string job submit dir credentials ts throws io exception boolean logging enabled log debug enabled logging enabled log debug submit job jobname job id jobs contains key job id job already running start twice logging enabled log debug job job id get id already present return jobs get job id get status job story job story simulator job cache get job id job story null throw new illegal argument exception job found simulator job cache job id validate and set clock job story get
1019	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorLaunchTaskAction.java	unrelated	package org apache hadoop mapred this used augment link launch task action run time statistics task state successful failed simulator launch task action extends launch task action run time resource usage task task attempt info task attempt info constructs simulator launch task action object link task simulator launch task action task task task attempt info task attempt info super task task attempt info task attempt info get resource usage model task task attempt info get task attempt info return task attempt info string string return get class get name task id get task get task id
1020	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorTaskTracker.java	heartbeat	package org apache hadoop mapred explicitly use new api older mapred task attempt id deprecated this simulates link task tracker its main purpose call heartbeat simulated job tracker apropriately updated statuses tasks assigned the events emitted consumed heartbeat event task attempt completion event internal naming convention accept dispatches simulation events process event methods heartbeat dispactches task tracker actions handle action methods simulator task tracker implements simulator event listener default host name string default host name unknown default task tracker name string default tracker name tracker unknown localhost default number map slots per task tracker default map slots default number reduce slots per task tracker default reduce slots default range heartbeat response perturbations milliseconds default heartbeat fuzz the name task tracker protected string task tracker name the name host task tracker running protected string host name the http port simulated task tracker reports jobtracker protected http port number map slots protected max map slots number reduce slots protected max reduce slots the job tracker task tracker slave protected inter tracker protocol job tracker state bookkeeping information tasks assigned task tracker contains information running completed yet reported tasks we manage mark sweep garbage collector manner we insert tasks launch mark completion remove completed tasks heartbeat reports we use linked hash map instead hash map order iteration deterministic protected map task attempt id simulator task in progress tasks new linked hash map task attempt id simulator task in progress number map slots allocated tasks running state task tracker must sync tasks map used map slots number reduce slots allocated tasks running state task tracker must sync tasks map used reduce slots true job tracker heartbeat call made first one we need mimick inter tracker protocol properly boolean first heartbeat true last heartbeat response received short heartbeat response id task attempt ids task attempt completion event created task attempt got killed we use linked hash set get deterministic iterators ever use one set task attempt id orphan task completions new linked hash set task attempt id the log object send messages used debugging log log log factory get log simulator task tracker number milliseconds perturb requested heartbeat intervals simulate network latency etc if pertrubation this option also useful testing if hearbeats perturbed uniformly random integer heartbeat interval fuzz heartbeat interval fuzz including bounds heartbeat interval fuzz used randomly perturbing heartbeat timings random random constructs task tracker dl dt mumak tasktracker tracker name dd task tracker name report otherwise unused dt mumak tasktracker host name dd host name report otherwise unused dt mapred tasktracker map tasks maximum dd number map slots dt mapred tasktracker reduce tasks maximum dd number reduce slots dt mumak tasktracker heartbeat fuzz dd perturbation heartbeats none else perturbations uniformly randomly generated heartbeat fuzz heartbeat fuzz including bounds dl simulator task tracker inter tracker protocol job tracker configuration conf task tracker name conf get mumak tasktracker tracker name default tracker name log debug simulator task tracker constructor task tracker name task tracker name job tracker job tracker host name conf get mumak tasktracker host name default host name max map slots conf get int
1021	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorTaskTrackerStatus.java	heartbeat	package org apache hadoop mapred this exists pass current simulation time job tracker heartbeat call simulator task tracker status extends task tracker status the virtual simulation time hearbeat call transmitting task tracker satus occured current simulation time constructs simulator task tracker status object all parameters link task tracker status the extra heartbeat call transmitting task tracker status occured simulator task tracker status string tracker name string host http port list task status task reports failures max map tasks max reduce tasks current simulation time super tracker name host http port task reports failures max map tasks max reduce tasks current simulation time current simulation time returns current time simulation get current simulation time return current simulation time
1022	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\SimulatorThreadWakeUpEvent.java	unrelated	package org apache hadoop mapred simulator thread wake up event extends simulator event simulator thread wake up event simulator event listener listener timestamp super listener timestamp
1023	mapreduce\src\contrib\mumak\src\java\org\apache\hadoop\mapred\TaskAttemptCompletionEvent.java	unrelated	package org apache hadoop mapred this used simulator task trackers signaling task attempt finishes the rationale redundant event sent way possible monitor task completion events centrally engine t ts used call heartbeat job tracker right task completed called crazy heartbeats waiting heartbeat interval if wanted simulate need decouple task completion monitoring periodic heartbeats task attempt completion event extends simulator event the status completed task task status status constructs task completion event task status status get run state must either state succeeded state failed task attempt completion event simulator event listener listener task status status super listener status get finish time status status returns status task task status get status return status protected string real to string return super real to string task id status get task id
1024	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\DistributedRaidFileSystem.java	unrelated	package org apache hadoop hdfs this implementation hadoop raid filesystem this file system wraps instance distributed file system if file corrupted file system uses parity blocks regenerate bad block distributed raid file system extends filter file system alternate locations used read access decode info alternates configuration conf stripe length distributed raid file system throws io exception distributed raid file system file system fs throws io exception super fs alternates null stripe length information required decoding source file decode info path dest path erasure code type type configuration conf stripe length decode info configuration conf erasure code type type path dest path conf conf type type dest path dest path stripe length raid node get stripe length conf decoder create decoder type erasure code type xor return new xor decoder conf stripe length else type erasure code type rs return new reed solomon decoder conf stripe length raid node rs parity length conf return null initialize raid file system initialize uri name configuration conf throws io exception conf conf class clazz conf get class fs raid underlyingfs impl distributed file system clazz null throw new io exception no file system fs raid underlyingfs impl fs file system reflection utils new instance clazz null super initialize name conf find stripe length configured stripe length raid node get stripe length conf stripe length log info dfs raid stripe length incorrectly defined stripe length ignoring return put xor rs alternates alternates new decode info path xor path raid node xor destination path conf fs alternates new decode info conf erasure code type xor xor path path rs path raid node rs destination path conf fs alternates new decode info conf erasure code type rs rs path returns underlying filesystem file system get file system throws io exception return fs fs data input stream open path f buffer size throws io exception ext fs data input stream fd new ext fs data input stream conf alternates f stripe length buffer size return fd close throws io exception fs null try fs close catch io exception ie might already closed ignore super close layered filesystem input stream this input stream tries reading alternate locations encoumters read errors primary location ext fs data input stream extends fs data input stream underlying block file holds block need outer file path path offset within path block starts actual file offset offset within outer file block starts original file offset length block length blk sz outer file length underlying block path path actual file offset original file offset length path path actual file offset actual file offset original file offset original file offset length length create input stream wraps reads positions seeking ext fs input stream extends fs input stream extents good underlying data read underlying block underlying blocks current offset fs data input stream current stream underlying block current block byte one bytebuff new byte next location distributed raid file system lfs path path file status stat decode info alternates buffersize configuration conf stripe length ext fs input stream configuration conf distributed raid file system lfs decode info alternates path path
1025	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\RaidDFSUtil.java	unrelated	package org apache hadoop hdfs raid dfs util returns corrupt blocks file list located block corrupt blocks in file distributed file system dfs string path offset length throws io exception list located block corrupt new linked list located block located blocks located blocks get block locations dfs path offset length located block b located blocks get located blocks b corrupt b get locations length b get block size corrupt add b return corrupt located blocks get block locations distributed file system dfs string path offset length throws io exception return dfs get client namenode get block locations path offset length make successive calls list corrupt files obtain corrupt files string get corrupt files distributed file system dfs throws io exception set string corrupt files new hash set string remote iterator path cfb dfs list corrupt file blocks new path cfb next corrupt files add cfb next uri get path return corrupt files array new string corrupt files size
1026	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\blockmanagement\BlockPlacementPolicyRaid.java	unrelated	package org apache hadoop hdfs server blockmanagement this block placement policy spreads group blocks used raid recovering this important availability blocks this used multiple threads it thread safe block placement policy raid extends block placement policy log log log factory get log block placement policy raid configuration conf stripe length xor parity length rs parity length string xor prefix null string rs prefix null string raid temp prefix null string raidrs temp prefix null string raid har temp prefix null string raidrs har temp prefix null fs namesystem namesystem null block placement policy default default policy cached located blocks cached located blocks cached full path names cached full path names inherit doc initialize configuration conf fs cluster stats stats network topology cluster map conf conf stripe length raid node get stripe length conf rs parity length raid node rs parity length conf xor parity length try xor prefix raid node xor destination path conf uri get path rs prefix raid node rs destination path conf uri get path catch io exception e xor prefix null xor prefix raid node default raid location rs prefix null rs prefix raid node default raidrs location throws class cast exception cannot cast namesystem fs namesystem stats cached located blocks new cached located blocks namesystem cached full path names new cached full path names namesystem raid temp prefix raid node xor temp prefix conf raidrs temp prefix raid node rs temp prefix conf raid har temp prefix raid node xor har temp prefix conf raidrs har temp prefix raid node rs har temp prefix conf default policy new block placement policy default conf stats cluster map datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes blocksize return choose target src path num of replicas writer chosen nodes null blocksize datanode descriptor choose target string src path num of replicas datanode descriptor writer list datanode descriptor chosen nodes boolean return chosen nodes hash map node node excluded nodes blocksize try file type type get file type src path type file type not raid return default policy choose target src path num of replicas writer chosen nodes blocksize excluded nodes null excluded nodes new hash map node node add excluded nodes src path type excluded nodes datanode descriptor result default policy choose target num of replicas writer chosen nodes return chosen nodes excluded nodes blocksize add added block locations block locations cache so rest blocks know locations cached located blocks get src path add new located block new extended block result return result catch exception e log debug error happend choosing datanode write string utils stringify exception e return default policy choose target src path num of replicas writer chosen nodes blocksize verify block placement string src path located block blk min racks return default policy verify block placement src path blk min racks inherit doc datanode descriptor choose replica to delete fs inode info inode block block short replication factor collection datanode descriptor first collection datanode descriptor second datanode descriptor chosen node null try string path cached full path
1027	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\datanode\RaidBlockSender.java	unrelated	package org apache hadoop hdfs server datanode reads block disk sends recipient raid block sender implements java io closeable fs constants log log data node log log client trace log data node client trace log extended block block block read the visible length replica replica visible length input stream block in data stream block in position updated using transfer to data input stream checksum in checksum datastream data checksum checksum checksum stream offset starting position read end offset ending position bytes per checksum chunk size checksum size checksum size boolean corrupt checksum ok need verify checksum boolean chunk offset ok need send chunk offset seqno sequence number packet boolean transfer to allowed true boolean block read fully set whole block read boolean verify checksum true check verified reading string client trace fmt format client trace log message minimum buffer used sending data clients used transfer to enabled kb large it could larger sure much improvement min buffer with transferto volatile chunk checksum last chunk checksum null raid block sender extended block block block length start offset length boolean corrupt checksum ok boolean chunk offset ok boolean verify checksum boolean transfer to allowed data input stream metadata in input stream factory stream factory throws io exception block block length start offset length corrupt checksum ok chunk offset ok verify checksum transfer to allowed metadata in stream factory null raid block sender extended block block block length start offset length boolean corrupt checksum ok boolean chunk offset ok boolean verify checksum boolean transfer to allowed data input stream metadata in input stream factory stream factory string client trace fmt throws io exception try block block chunk offset ok chunk offset ok corrupt checksum ok corrupt checksum ok verify checksum verify checksum replica visible length block length transfer to allowed transfer to allowed client trace fmt client trace fmt corrupt checksum ok metadata in null checksum in metadata in read handle common header for version block metadata header header block metadata header read header checksum in short version header get version version fs dataset metadata version log warn wrong version version metadata file block ignoring checksum header get checksum else log warn could find metadata file block this decides buffer size use buffer size checksum data checksum new data checksum data checksum checksum null if bytes per checksum large metadata file mostly corrupted for truncate bytes perchecksum block length bytes per checksum checksum get bytes per checksum bytes per checksum bytes per checksum replica visible length checksum data checksum new data checksum checksum get checksum type math max replica visible length bytes per checksum checksum get bytes per checksum checksum size checksum get checksum size length length replica visible length end offset block length start offset start offset end offset length start offset end offset string msg offset start offset length length match block block block len end offset log warn send block msg throw new io exception msg offset start offset start offset bytes per checksum length make sure end offset points end checksumed chunk tmp len start offset length tmp len bytes per
1028	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\hdfs\server\namenode\NameNodeRaidUtil.java	unrelated	package org apache hadoop hdfs server namenode utilities used raid accessing name node name node raid util accessing fs directory get file info hdfs file status get file info fs directory dir string src boolean resolve link throws unresolved link exception return dir get file info src resolve link accessing fs namesystem get file info hdfs file status get file info fs namesystem namesystem string src boolean resolve link throws access control exception unresolved link exception return namesystem get file info src resolve link accessing fs namesystem get block locations located blocks get block locations fs namesystem namesystem string src offset length boolean access time boolean need block token throws file not found exception unresolved link exception io exception return namesystem get block locations src offset length access time need block token
1029	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\BlockFixer.java	unrelated	package org apache hadoop raid contains core functionality block fixer configuration options raid blockfix classname name block fixer implementation use raid blockfix interval interval checks corrupt files raid blockfix history interval interval fixing file raid blockfix read timeout read time raid blockfix write timeout write time block fixer extends configured implements runnable string blockfix classname raid blockfix classname string blockfix interval raid blockfix interval string blockfix history interval raid blockfix history interval string blockfix read timeout raid blockfix read timeout string blockfix write timeout raid blockfix write timeout default blockfix interval min default blockfix history interval mins block fixer create block fixer configuration conf throws class not found exception try default distributed block fixer class block fixer class conf get class blockfix classname dist block fixer block fixer assignable from block fixer class throw new class not found exception implementation blockfixer constructor constructor block fixer class get constructor new class configuration return block fixer constructor new instance conf catch no such method exception e throw new class not found exception cannot construct blockfixer e catch instantiation exception e throw new class not found exception cannot construct blockfixer e catch illegal access exception e throw new class not found exception cannot construct blockfixer e catch invocation target exception e throw new class not found exception cannot construct blockfixer e num files fixed volatile boolean running true interval checks corrupt files protected block fix interval interval fixing file protected history interval block fixer configuration conf super conf block fix interval get conf get long blockfix interval default blockfix interval history interval get conf get long blockfix history interval default blockfix history interval run returns number files fixed block fixer synchronized files fixed return num files fixed increments number files fixed block fixer protected synchronized incr files fixed num files fixed increments number files fixed block fixer protected synchronized incr files fixed incr incr throw new illegal argument exception cannot increment negative value incr num files fixed incr boolean source file path p string dest prefixes string path str p uri get path string dest prefix dest prefixes path str starts with dest prefix return false return true filter unfixable source files iterator path throws io exception string xor prefix raid node xor destination path get conf uri get path xor prefix ends with path separator xor prefix path separator string rs prefix raid node rs destination path get conf uri get path rs prefix ends with path separator rs prefix path separator string dest prefixes new string xor prefix rs prefix next path p next source file p dest prefixes raid node xor parity for source p get conf null raid node rs parity for source p get conf null remove implements actual fixing functionality keep separate distributed block fixer use block fixer helper extends configured log log log factory get log block fixer block fixer helper string xor prefix string rs prefix xor encoder xor encoder xor decoder xor decoder reed solomon encoder rs encoder reed solomon decoder rs decoder block fixer helper configuration conf throws io exception super conf xor prefix raid
1030	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ConfigManager.java	unrelated	package org apache hadoop raid maintains configuration xml file read memory config manager log log log factory get log org apache hadoop raid config manager time wait checks config file reload interval time wait successive runs policies rescan interval har partfile size distraid max jobs distraid max files time wait config file modified reloading done prevent loading file fully written reload wait configuration conf hadoop configuration string config file name path config xml file last reload attempt last time tried reload config file last successful reload last time successfully reloaded config boolean last reload attempt failed false reload interval reload interval periodicity time runs policies har partfile size max jobs per policy max jobs running simultaneously job max files per job max files raided job reload configuration boolean reload thread reload thread volatile boolean running false collection configured policies collection policy list policies new array list policy list config manager configuration conf throws io exception sax exception raid configuration exception class not found exception parser configuration exception conf conf config file name conf get raid config file reload conf get boolean raid config reload true reload interval conf get long raid config reload interval reload interval periodicity conf get long raid policy rescan interval rescan interval har partfile size conf get long raid har partfile size har partfile size max jobs per policy conf get int raid distraid max jobs distraid max jobs max files per job conf get int raid distraid max files distraid max files config file name null string msg no raid config file given conf hadoop raid utility cannot run aborting log warn msg throw new io exception msg reload configs last successful reload raid node last reload attempt raid node running true reload config file loaded returns true file reloaded synchronized boolean reload configs if necessary time raid node time last reload attempt reload interval last reload attempt time try file file new file config file name last modified file last modified last modified last successful reload time last modified reload wait reload configs last successful reload time last reload attempt failed false return true catch exception e last reload attempt failed log error failed reload config file use existing configuration e last reload attempt failed true return false updates memory data structures config file this file expected following whitespace separated format configuration src path prefix hdfs hadoop myhost com user warehouse u full policy name raid scan weekly dest path hdfs dfsname myhost com archive dest path parent policy raid scan monthly parent policy property name target replication name value value description rai ding decrease replication factor file value description property property name meta replication name value value description replication factor raid meta file description property property name stripe length name value value description number blocks raid together description property policy src path configuration blank lines lines starting ignored reload configs throws io exception parser configuration exception sax exception class not found exception raid configuration exception config file name null return file file new file config file name file exists throw new raid configuration exception configuration
1031	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\Decoder.java	unrelated	package org apache hadoop raid represents generic decoder used read file corrupt blocks using parity file this concrete subclasses need implement fix erased block decoder log log log factory get log org apache hadoop raid decoder protected configuration conf protected stripe size protected parity size protected random rand protected buf size protected byte read bufs protected byte write bufs decoder configuration conf stripe size parity size conf conf stripe size stripe size parity size parity size rand new random buf size conf get int raid decoder bufsize read bufs new byte stripe size parity size write bufs new byte parity size allocate buffers allocate buffers stripe size parity size read bufs new byte buf size parity size write bufs new byte buf size configure buffers block size buf size block size buf size block size allocate buffers else block size buf size buf size block size l heuristic buf size buf size buf size math min buf size allocate buffers the generate decoded file using good portion source file parity file different fs case parity file part har archive additional errors source file discovered decode process source file success decode file file system fs path src file file system parity fs path parity file error offset path decoded file throws io exception log info create decoded file error src file error offset file status src stat fs get file status src file block size src stat get block size configure buffers block size move offset start block error offset error offset block size block size create decoded file fs data output stream fs create decoded file false conf get int io file buffer size src stat get replication src stat get block size open source file fs data input stream fs open src file conf get int io file buffer size start copying data block block offset offset src stat get len offset block size limit math min block size src stat get len offset bytes already copied offset error offset try fs open src file conf get int io file buffer size seek offset raid utils copy bytes read bufs limit assert get pos offset limit log info copied till get pos src file continue catch block missing exception e log warn encountered bme src file offset bytes already copied get pos offset catch checksum exception e log warn encountered ce src file offset bytes already copied get pos offset if offset error offset got exception recover block starting offset fix erased block fs src file parity fs parity file block size offset bytes already copied limit close try fs set owner decoded file src stat get owner src stat get group fs set permission decoded file src stat get permission fs set times decoded file src stat get modification time src stat get access time catch exception exc log warn didn manage copy meta information exc ignoring recovers corrupt block local file different fs case parity file part har archive additional errors source file discovered decode process this prevent writing beyond end file recover block to file file system src
1032	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DirectoryTraversal.java	pooling	package org apache hadoop raid implements depth first traversal using stack object the traversal stopped time state traversal saved directory traversal log log log factory get log org apache hadoop raid directory traversal file system fs list file status paths path idx next path process stack node stack new stack node executor service executor num threads a file filter object used choose files directory traversal file filter boolean check file status f throws io exception represents directory node directory traversal node file status path path node represents file status elements elements node idx node file status path file status elements path path elements elements boolean next return idx elements length file status next return elements idx file status path return path constructor directory traversal file system fs list file status start paths fs start paths directory traversal file system fs list file status start paths num threads fs fs paths start paths path idx num threads num threads executor executors new fixed thread pool num threads list file status get filtered files file filter filter limit list file status filtered new array list file status we need semaphore block number running workitems equal number threads fixed thread pool limits number threads queue size this way limit memory usage semaphore slots new semaphore num threads true synchronized filtered filtered size limit break filter file work item work null try node next get next directory node next null break work new filter file work item filter next filtered slots slots acquire catch interrupted exception ie break catch io exception e break executor execute work try wait submitted items finish slots acquire num threads if traversal finished shutdown executor done traversal executor shutdown executor await termination time unit hours catch interrupted exception ie return filtered filter file work item implements runnable file filter filter node dir list file status filtered semaphore slots filter file work item file filter filter node dir list file status filtered semaphore slots slots slots filter filter dir dir filtered filtered run try log info initiating file filtering dir path get path file status f dir elements f file continue filter check f synchronized filtered filtered add f catch exception e log error error directory traversal string utils stringify exception e finally slots release return next file file status get next file throws io exception check traversal done done traversal if traversal done check stack empty stack empty if stack empty look top node node node stack peek check top node element node next file status element node next is next element directory element dir it file return return element next element directory push stack continue try push new node element catch file not found exception e ignore move next element continue else top node next element pop continue stack pop continue if stack empty paths paths empty file status next paths remove path idx next dir return next try push new node next catch file not found exception e continue break return null gets next directory tree the algorithm returns deeper directories first file status get next directory throws io
1033	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistBlockFixer.java	scheduler	package org apache hadoop raid distributed block fixer uses map reduce jobs fix corrupt files configuration options raid blockfix filespertask number corrupt files fix single map reduce task e one mapper node raid blockfix fairscheduler pool pool use block fixer jobs raid blockfix maxpendingfiles maximum number files fix simultaneously dist block fixer extends block fixer volatile sufficient since block fixer thread updates num jobs running threads may read volatile num jobs running string work dir prefix blockfixer string in file suffix string part prefix part string blockfix files per task raid blockfix filespertask string blockfix max pending files raid blockfix maxpendingfiles string blockfix pool raid blockfix fairscheduler pool mapred fairscheduler pool used local configuration passed block fixing job string mapred pool mapred fairscheduler pool default number files fix task default blockfix files per task l default number files fix simultaneously default blockfix max pending files l protected log log log factory get log dist block fixer number files fix task files per task number files fix simultaneously max pending files number files fixed right pending files pool name use may null case special pool used string pool name last check time simple date format date format new simple date format yyyy mm dd hh mm ss map string corrupt file info file index new hash map string corrupt file info map job list corrupt file info job index new hash map job list corrupt file info enum counter files succeeded files failed files noaction dist block fixer configuration conf super conf files per task dist block fixer files per task get conf max pending files dist block fixer max pending files get conf pending files l pool name conf get blockfix pool start due first iteration last check time system current time millis block fix interval determines many files fix single task protected files per task configuration conf return conf get long blockfix files per task default blockfix files per task determines many files fix simultaneously protected max pending files configuration conf return conf get long blockfix max pending files default blockfix max pending files runs block fixer periodically run running check time run block fixer system current time millis last check time block fix interval last check time try check and fix blocks catch interrupted exception ignore log info interrupted catch exception e log exceptions keep running log error string utils stringify exception e catch error e log error string utils stringify exception e throw e try sleep remainder interval sleep period last check time system current time millis block fix interval sleep period l running try thread sleep sleep period catch interrupted exception ignore log info interrupted checks corrupt blocks fixes check and fix blocks start time throws io exception interrupted exception class not found exception check jobs pending files max pending files return list path corrupt files get corrupt files filter unfixable source files corrupt files iterator string start time str date format format new date start time log info found corrupt files size corrupt files corrupt files size string job name blockfixer start time start job job name
1034	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistRaid.java	scheduler	package org apache hadoop raid dist raid extends configured protected log log log factory get log dist raid string name dist raid string job dir label name job dir op list block size block size control file short op list replication replication factor control file string ops per task raid distraid opspertask default ops per task sync file max simple date format date form new simple date format yyyy mm dd hh mm enum counter files succeeded files failed processed blocks processed size meta blocks meta size dist raid configuration conf super conf random random new random protected string get random id return integer string random next int integer max value helper holds policy paths raid policy path pair policy info policy list file status src paths raid policy path pair policy info policy list file status src paths policy policy src paths src paths list raid policy path pair raid policy path pair list new array list raid policy path pair job running job string last report null responsible generating splits src file list dist raid input format extends sequence file input format text policy info produce splits greater quotient total size number splits requested the handle configuration object number splits requested list input split get splits job context job throws io exception configuration conf job get configuration we create one input file so get first file first input directory path dir get input paths job file system fs dir get file system conf file status input files fs list status dir path input file input files get path list input split splits new array list input split sequence file reader new sequence file reader conf reader file input file prev l ops per task conf get int ops per task default ops per task try text key new text policy info value new policy info count count src next key value curr get position delta curr prev count ops per task count splits add new file split input file prev delta string null prev curr finally close remaining fs get file status input file get len prev remaining splits add new file split input file prev remaining string null return splits the mapper raiding files dist raid mapper extends mapper text policy info text text boolean ignore failures false failcount succeedcount statistics st new statistics string get count string return succeeded succeedcount failed failcount run file operation map text key policy info policy context context throws io exception interrupted exception try configuration job conf context get configuration log info raiding file key string policy policy path p new path key string file status fs p get file system job conf get file status p st clear raid node raid job conf policy fs st context succeedcount context get counter counter processed blocks increment st num processed blocks context get counter counter processed size increment st processed size context get counter counter meta blocks increment st num meta blocks context get counter counter meta size increment st meta size context get counter counter files succeeded increment catch io exception e failcount
1035	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\DistRaidNode.java	unrelated	package org apache hadoop raid implementation link raid node uses map reduce jobs raid files dist raid node extends raid node log log log factory get log dist raid node daemon thread monitor raid job progress job monitor job monitor null daemon job monitor thread null dist raid node configuration conf throws io exception super conf job monitor new job monitor conf job monitor thread new daemon job monitor job monitor thread start inherit docs join super join try job monitor thread null job monitor thread join catch interrupted exception ie nothing inherit docs stop stop requested return super stop job monitor null job monitor running false job monitor thread null job monitor thread interrupt inherit docs raid files policy info info list file status paths throws io exception we already checked job policy running so start new job dist raid dr new dist raid conf add paths distributed raiding dr add raid paths info paths boolean started dr start dist raid started job monitor monitor job info get name dr inherit docs get running jobs for policy string policy name return job monitor running jobs count policy name
1036	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\Encoder.java	unrelated	package org apache hadoop raid represents generic encoder generate parity file source file this concrete subclasses need implement encode file impl encoder log log log factory get log org apache hadoop raid encoder protected configuration conf protected stripe size protected parity size protected random rand protected buf size protected byte read bufs protected byte write bufs a acts sink data similar dev null null output stream extends output stream write byte b throws io exception write b throws io exception write byte b len throws io exception encoder configuration conf stripe size parity size conf conf stripe size stripe size parity size parity size rand new random buf size conf get int raid encoder bufsize read bufs new byte stripe size write bufs new byte parity size allocate buffers allocate buffers stripe size read bufs new byte buf size parity size write bufs new byte buf size configure buffers block size buf size block size buf size block size allocate buffers else block size buf size buf size block size l heuristic buf size buf size buf size math min buf size allocate buffers the use generate parity file this method called multiple times encoder object thus allowing reuse buffers allocated encoder object encode file file system fs path src file file system parity fs path parity file short parity repl progressable reporter throws io exception file status src stat fs get file status src file src size src stat get len block size src stat get block size configure buffers block size create tmp file write first path tmp dir get parity temp path parity fs mkdirs tmp dir throw new io exception could create tmp dir tmp dir path parity tmp new path tmp dir parity file get name rand next long fs data output stream parity fs create parity tmp true conf get int io file buffer size parity repl block size try encode file to stream fs src file src size block size reporter close null log info wrote temp parity file parity tmp delete destination exists parity fs exists parity file parity fs delete parity file false parity fs mkdirs parity file get parent parity fs rename parity tmp parity file string msg unable rename file parity tmp parity file throw new io exception msg log info wrote parity file parity file finally null close parity fs delete parity tmp false recovers corrupt block parity file local file the encoder generates parity size parity blocks source file stripe since want one parity blocks function creates null outputs blocks discarded recover parity block to file file system fs path src file src size block size path parity file corrupt offset file local block file throws io exception output stream new file output stream local block file try recover parity block to stream fs src file src size block size parity file corrupt offset finally close recovers corrupt block parity file local file the encoder generates parity size parity blocks source file stripe since want one parity blocks function creates null outputs blocks discarded recover parity block to stream
1037	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ErasureCode.java	unrelated	package org apache hadoop raid erasure code encodes given message significant bits the number data bits symbol size the number elements message stripe size significant bits the number parity bits symbol size the number elements code parity size encode message parity generates missing portions data first part array in integer relevant portion present least significant bits the number elements data stripe size parity size decode data erased locations erased values the number elements message stripe size the number elements code parity size number bits symbol symbol size
1038	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\GaloisField.java	unrelated	package org apache hadoop raid implementation galois field arithmetics p elements the input must unsigned integers galois field log table pow table mul table div table field size primitive period primitive polynomial field size good byte based system default field size primitive polynomial x x x x default primitive polynomial map integer galois field instances new hash map integer galois field get object performs galois field arithmetics galois field get instance field size primitive polynomial key field size x ffff primitive polynomial x ffff galois field gf synchronized instances gf instances get key gf null gf new galois field field size primitive polynomial instances put key gf return gf get object performs galois field arithmetics default setting galois field get instance return get instance default field size default primitive polynomial galois field field size primitive polynomial assert field size assert primitive polynomial field size field size primitive period field size primitive polynomial primitive polynomial log table new field size pow table new field size mul table new field size field size div table new field size field size value pow pow field size pow pow table pow value log table value pow value value value field size value value primitive polynomial building multiplication table field size j j field size j j mul table j continue z log table log table j z z primitive period z primitive period z z pow table z mul table j z building division table field size j j field size j div table j continue z log table log table j z z z primitive period z z pow table z div table j z return number elements field get field size return field size return primitive polynomial gf get primitive polynomial return primitive polynomial compute sum two fields add x assert x x get field size get field size return x compute multiplication two fields multiply x assert x x get field size get field size return mul table x compute division two fields divide x assert x x get field size get field size return div table x compute power n field power x n assert x x get field size n return x return x log table x n x primitive period return pow table x x x primitive period return pow table x given vandermonde matrix v j x j vector solve z vz the output z placed replaced output vector solve vandermonde system x solve vandermonde system x x length given vandermonde matrix v j x j vector solve z vz the output z placed replaced output vector solve vandermonde system x len assert x length len length len len j len j j j j mul table x j len j j len j j div table j x j x j j j len j j j j compute multiplication two polynomials the index array corresponds power entry for example p constant term polynomial p multiply p q len p length q length result new len len result p length j j q length j result j add
1039	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\HarIndex.java	unrelated	package org apache hadoop raid represents contents har index file the har assumed comprising raid parity files directories har index string index file name index list index entry entries new linked list index entry represents information single line har index file index entry string file name name file part file start offset start offset within part file length length file within part file mtime modification time file string part file name name part file index entry string file name start offset length mtime string part file name file name file name start offset start offset length length mtime mtime part file name part file name string string return file name file name start offset start offset length length mtime mtime part file name part file name constructor reads contents index file har index input stream max throws io exception line reader line reader new line reader text text new text nread nread max n line reader read line text nread n string line text string try parse line line catch unsupported encoding exception e throw new io exception unsupported encoding exception reading nread bytes parses line extracts relevant information parse line string line throws unsupported encoding exception string splits line split boolean dir dir equals splits true false dir splits length string name url decoder decode splits utf string part name url decoder decode splits utf start index long parse long splits length long parse long splits string newsplits url decoder decode splits utf split newsplits null newsplits length mtime long parse long newsplits index entry entry new index entry name start index length mtime part name entries add entry finds index entry corresponding har part file offset index entry find entry string part name part file offset index entry e entries boolean name match part name equals e part file name boolean range part file offset e start offset part file offset e start offset e length name match range return e return null finds index entry corresponding file archive index entry find entry by file name string file name index entry e entries file name equals e file name return e return null
1040	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\JobMonitor.java	unrelated	package org apache hadoop raid periodically monitors status jobs registered jobs submitted policy name kept list list kept map policy name key list value job monitor implements runnable log log log factory get log org apache hadoop raid job monitor volatile boolean running true map string list dist raid jobs job monitor interval volatile jobs monitored volatile jobs succeeded job monitor configuration conf job monitor interval conf get long raid jobmonitor interval jobs new java util hash map string list dist raid run running try log info job monitor thread continuing run monitor catch throwable e log error job monitor encountered exception string utils stringify exception e all expected exceptions caught monitor it better exit prevent raid node submitting jobs since number running jobs never decrease return periodically checks status running map reduce jobs monitor running string keys null make copy names current jobs synchronized jobs keys jobs key set array new string check jobs we want block access jobs prevent new jobs added this safe job monitor run code remove job jobs thus elements keys valid values map string list dist raid finished jobs new hash map string list dist raid string key keys for policy monitored get list jobs running dist raid job list copy null synchronized jobs list dist raid job list jobs get key synchronized job list job list copy job list array new dist raid job list size the code actually contacts job tracker synchronized uses copies list jobs dist raid job job list copy check running job try boolean complete job check complete complete add job finished jobs key job job successful jobs succeeded catch io exception ioe if error consider job finished add job finished jobs key job finished jobs size string key finished jobs key set list dist raid finished job list finished jobs get key iterate finished jobs remove jobs remove job takes care locking dist raid job finished job list remove job jobs key job try thread sleep job monitor interval catch interrupted exception ie running jobs count string key count synchronized jobs jobs contains key key list dist raid job list jobs get key synchronized job list count job list size return count monitor job string key dist raid job add job jobs key job jobs monitored jobs monitored return jobs monitored jobs succeeded return jobs succeeded add job map string list dist raid jobs map string job name dist raid job synchronized jobs map list dist raid list null jobs map contains key job name list jobs map get job name else list new linked list dist raid jobs map put job name list synchronized list list add job remove job map string list dist raid jobs map string job name dist raid job synchronized jobs map jobs map contains key job name list dist raid list jobs map get job name synchronized list iterator dist raid list iterator next dist raid val next val job remove list size jobs map remove job name
1041	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\LocalBlockFixer.java	unrelated	package org apache hadoop raid this fixes source file blocks using parity file parity file blocks using source file it periodically fetches list corrupt files namenode figures location bad block reading corrupt file local block fixer extends block fixer log log log factory get log local block fixer java util hash map string java util date history block fixer helper helper local block fixer configuration conf throws io exception super conf history new java util hash map string java util date helper new block fixer helper get conf run running try log info local block fixer continuing run fix catch exception e log error string utils stringify exception e catch error err log error exiting encountering string utils stringify exception err throw err fix throws interrupted exception io exception running sleep proceeding fix files thread sleep block fix interval purge history older history interval purge history list path corrupt files get corrupt files filter unfixable source files corrupt files iterator corrupt files empty if corrupt files retry time continue log info found corrupt files size corrupt files helper sort corrupt files corrupt files path src path corrupt files running break try boolean fixed helper fix file src path log info adding src path history history put src path string new java util date fixed incr files fixed catch io exception ie log error hit error processing src path string utils stringify exception ie do nothing move next file we maintain history fixed files fixed file may appear list corrupt files loop around quickly this function removes old items history recognize files actually become corrupt since fixed purge history java util date cut off new java util date system current time millis history interval list string remove new java util array list string string key history key set java util date item history get key item cut off remove add key string key remove log info removing key history history remove key list path get corrupt files throws io exception distributed file system dfs helper get dfs new path string files raid dfs util get corrupt files dfs list path corrupt files new linked list path string f files path p new path f history contains key p string corrupt files add p raid utils filter trash get conf corrupt files return corrupt files
1042	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\LocalRaidNode.java	unrelated	package org apache hadoop raid implementation link raid node performs raiding locally local raid node extends raid node log log log factory get log local raid node local raid node configuration conf throws io exception super conf inherit docs raid files policy info info list file status paths throws io exception raid conf info paths inherit docs get running jobs for policy string policy name return
1043	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ParityInputStream.java	unrelated	package org apache hadoop raid wraps multiple input streams provides input stream xor streams parity input stream extends input stream default bufsize input stream streams byte xor byte buf buf size remaining available read pos parity input stream input stream streams parity block size byte buf byte xor assert buf length xor length buf size buf length streams streams remaining parity block size buf buf xor xor read throws io exception make available available return ret xor read pos read pos available return ret read byte b len throws io exception make available available return ret math min len available ret b xor read pos read pos ret available ret return ret close throws io exception input stream streams close send contents stream sink drain output stream sink progressable reporter throws io exception true make available available break sink write xor read pos available available reporter null reporter progress make bytes available reading internal buffer make available throws io exception available remaining return read bytes first stream xorlen math min remaining buf size read exact streams xor xorlen read bytes streams xor streams length read exact streams buf xorlen j j xorlen j xor j buf j remaining xorlen available xorlen read pos read pos read exact input stream byte bufs read throws io exception tread tread read read read bufs tread read tread read if stream ends fill zeros arrays fill bufs tread read byte tread read else tread read assert tread read
1044	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidConfigurationException.java	unrelated	package org apache hadoop raid thrown config file link cron node malformed raid configuration exception extends exception serial version uid l raid configuration exception string message super message
1045	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidFilter.java	unrelated	package org apache hadoop raid raid filter statistics num raided num too new size too new num too small size too small aggregate statistics num raided num raided num too new num too new size too new size too new num too small num too small size too small size too small string string return num raided num raided num too new num too new size too new size too new num too small num too small size too small size too small time based filter extends configured implements directory traversal file filter target repl path raid dest prefix mod time period start time statistics stats new statistics string current src path null mod time periods new string src paths new string time based filter configuration conf path dest prefix target repl start time mod time period super conf raid dest prefix dest prefix target repl target repl start time start time mod time period mod time period time based filter configuration conf path dest prefix policy info info list policy info policies start time statistics stats super conf raid dest prefix dest prefix target repl integer parse int info get property target replication mod time period long parse long info get property mod time period start time start time stats stats current src path info get src path uri get path initialize other paths policies initialize other paths list policy info policies array list policy info tmp new array list policy info policies remove policies src path current src path matching prefix length length current src path the policies remaining ones could better select file chosen current policy iterator policy info tmp iterator next string src next get src path uri get path src compare to current src path remove continue match len matching prefix length src current src path match len current src path length remove sort reverse lexicographic order collections sort tmp new comparator compare object object return policy info get src path uri get path compare to policy info get src path uri get path src paths new string tmp size mod time periods new src paths length src paths length src paths tmp get get src path uri get path mod time periods long parse long tmp get get property mod time period boolean check file status f throws io exception choose for current policy f return false if source file fewer equal blocks skip block size f get block size block size f get len stats num too small stats size too small f get len return false boolean select false try object ppair raid node get parity file raid dest prefix f get path get conf is valid parity file ppair null is source target replication f get replication target repl select file replication set select true else stats num raided nothing select file select false else no parity file f get modification time mod time period start time if file new choose raiding select true else select false stats num too new stats size too new f get len catch java io file not
1046	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidNode.java	unrelated	package org apache hadoop raid a base implements link raid protocol use raid classname specify implementation use raid node implements raid protocol configuration add default resource hdfs default xml configuration add default resource hdfs site xml configuration add default resource mapred default xml configuration add default resource mapred site xml log log log factory get log org apache hadoop raid raid node sleep time l seconds default port default stripe length parity length rs code default stripe length rs parity length default string rs parity length key hdfs raidrs paritylength string stripe length key hdfs raid stripe length string default raid location raid string raid location key hdfs raid locations string default raid tmp location tmp raid string raid tmp location key fs raid tmpdir string default raid har tmp location tmp raid har string raid har tmp location key fs raid hartmpdir string default raidrs location raidrs string raidrs location key hdfs raidrs locations string default raidrs tmp location tmp raidrs string raidrs tmp location key fs raidrs tmpdir string default raidrs har tmp location tmp raidrs har string raidrs har tmp location key fs raidrs hartmpdir string har suffix raid har pattern parity har partfile pattern pattern compile har suffix part string raidnode classname key raid classname rpc server server server rpc server address inet socket address server address null used testing purposes protected boolean stop requested false configuration manager config manager config mgr hadoop configuration protected configuration conf protected boolean initialized are initialized protected volatile boolean running are running deamon thread trigger policies daemon trigger thread null deamon thread delete obsolete parity files purge monitor purge monitor null daemon purge thread null deamon thread har raid directories daemon har thread null daemon thread fix corrupt files block fixer block fixer null daemon block fixer thread null statistics raw hdfs blocks this counts replicas block statistics num processed blocks total blocks encountered namespace processed size disk space occupied blocks remaining size total disk space post raid num meta blocks total blocks metafile meta size total disk space meta files clear num processed blocks processed size remaining size num meta blocks meta size string string save processed size remaining size meta size savep processed size savep save processed size string msg num processed blocks num processed blocks processed size processed size post raid size remaining size num meta blocks num meta blocks meta size meta size save raw disk space savep return msg startup options enum startup option test test regular regular string name null startup option string arg name arg string get name return name start raid node p the raid node started one following startup options ul li link startup option regular regular normal raid node startup li ul the option passed via configuration field tt fs raidnode startup tt the conf modified reflect actual ports raid node running user passes port code zero code conf raid node configuration conf throws io exception try initialize conf catch io exception e log error string utils stringify exception e stop throw e catch exception e stop throw new io exception e
1047	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidShell.java	unrelated	package org apache hadoop raid a link raid shell allows browsing configured raid policies raid shell extends configured implements tool configuration add default resource hdfs default xml configuration add default resource hdfs site xml log log log factory get log org apache hadoop raid shell raid protocol raidnode raid protocol rpc raidnode user group information ugi volatile boolean client running true configuration conf start raid shell p the raid shell connects specified raid node performs basic configuration options raid shell configuration conf throws io exception super conf conf conf initialize rpc configuration conf inet socket address address throws io exception ugi user group information get current user rpc raidnode create rpc raidnode address conf ugi raidnode create raidnode rpc raidnode initialize local configuration conf throws io exception ugi user group information get current user raid protocol create raidnode configuration conf throws io exception return create raidnode raid node get address conf conf raid protocol create raidnode inet socket address raid node addr configuration conf throws io exception return create raidnode create rpc raidnode raid node addr conf user group information get current user raid protocol create rpc raidnode inet socket address raid node addr configuration conf user group information ugi throws io exception log debug raid shell connecting raid node addr return raid protocol rpc get proxy raid protocol raid protocol version id raid node addr ugi conf net utils get socket factory conf raid protocol raid protocol create raidnode raid protocol rpc raidnode throws io exception retry policy create policy retry policies retry up to maximum count with fixed sleep time unit milliseconds map class extends exception retry policy remote exception to policy map new hash map class extends exception retry policy map class extends exception retry policy exception to policy map new hash map class extends exception retry policy exception to policy map put remote exception retry policies retry by remote exception retry policies try once then fail remote exception to policy map retry policy method policy retry policies retry by exception retry policies try once then fail exception to policy map map string retry policy method name to policy map new hash map string retry policy method name to policy map put create method policy return raid protocol retry proxy create raid protocol rpc raidnode method name to policy map check open throws io exception client running io exception result new io exception raid node closed throw result close connection raid node synchronized close throws io exception client running client running false rpc stop proxy rpc raidnode displays format commands print usage string cmd string prefix usage java raid shell get simple name show config equals cmd system err println usage java raid shell show config else recover equals cmd system err println usage java raid shell recover src path corrupt offset else recover blocks equals cmd system err println usage java raid shell recover blocks path path else system err println usage java raid shell system err println show config system err println help cmd system err println recover src path corrupt offset system err println recover blocks
1048	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\RaidUtils.java	unrelated	package org apache hadoop raid raid utils a link progressable nothing we could used reporter null would introduce dependency mapreduce dummy progressable implements progressable do nothing progress removes files matching trash file pattern filter trash configuration conf list path files remove files trash string trash pattern conf get raid blockfixer trash pattern user trash iterator path files iterator next string path str next string pattern matches trash pattern path str remove read till end input stream byte buf boolean eof ok throws io exception read buf length num read num read read nread read buf num read read num read nread eof ok eof hit fill zeros arrays fill buf num read read byte num read read else eof hit throw throw new io exception premature eof else num read nread copy bytes input stream output stream byte buf count throws io exception bytes read bytes read count read math min buf length count bytes read io utils read fully buf read bytes read read write buf read zero input stream extends input stream implements seekable positioned readable end offset pos zero input stream end offset end offset end offset pos read throws io exception pos end offset pos return return available throws io exception return end offset pos get pos throws io exception return pos seek seek offset throws io exception seek offset end offset pos seek offset else throw new io exception illegal offset pos boolean seek to new source target pos throws io exception return false read position byte buffer offset length throws io exception count position end offset count length position buffer offset count count return count read fully position byte buffer offset length throws io exception count position end offset count length position buffer offset count count count length throw new io exception premature eof read fully position byte buffer throws io exception read fully position buffer buffer length
1049	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonCode.java	unrelated	package org apache hadoop raid reed solomon code implements erasure code stripe size parity size generating polynomial primitive root primitive power galois field gf galois field get instance err signature parity symbol locations data buff reed solomon code stripe size parity size assert stripe size parity size gf get field size stripe size stripe size parity size parity size err signature new parity size parity symbol locations new parity size data buff new parity size stripe size parity size parity symbol locations primitive power new stripe size parity size compute powers primitive root stripe size parity size primitive power gf power primitive root compute generating polynomial gen poly new parity size poly primitive power poly gen gf multiply gen poly generating polynomial generating roots generating polynomial gen encode message parity assert message length stripe size parity length parity size parity size data buff stripe size data buff parity size message gf remainder data buff generating polynomial parity size parity data buff decode data erased location erased value erased location length return assert erased location length erased value length erased location length data erased location erased location length err signature primitive power erased location erased value gf substitute data primitive power gf solve vandermonde system err signature erased value erased location length stripe size return stripe size parity size return parity size symbol size return math round math log gf get field size math log
1050	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonDecoder.java	unrelated	package org apache hadoop raid reed solomon decoder extends decoder log log log factory get log org apache hadoop raid reed solomon decoder erasure code reed solomon code reed solomon decoder configuration conf stripe size parity size super conf stripe size parity size reed solomon code new reed solomon code stripe size parity size protected fix erased block file system fs path src file file system parity fs path parity file block size error offset bytes to skip limit output stream throws io exception fs data input stream inputs new fs data input stream stripe size parity size erased locations build inputs fs src file parity fs parity file error offset inputs block idx in stripe error offset block size stripe size erased location to fix parity size block idx in stripe write fixed block inputs erased locations erased location to fix bytes to skip limit protected build inputs file system fs path src file file system parity fs path parity file error offset fs data input stream inputs throws io exception log info building inputs recover block starting error offset file status src stat fs get file status src file block size src stat get block size block idx error offset block size stripe idx block idx stripe size log info file size src stat get len block size block size block idx block idx stripe idx stripe idx array list integer erased locations new array list integer first open streams parity blocks parity size offset block size stripe idx parity size fs data input stream parity fs open parity file conf get int io file buffer size seek offset log info adding parity file offset input inputs now open streams data blocks parity size parity size stripe size offset block size stripe idx stripe size parity size offset error offset log info src file offset known error adding zeros input inputs new fs data input stream new raid utils zero input stream offset block size erased locations add else offset src stat get len log info src file offset past file size adding zeros input inputs new fs data input stream new raid utils zero input stream offset block size else fs data input stream fs open src file conf get int io file buffer size seek offset log info adding src file offset input inputs erased locations size parity size string msg too many erased locations erased locations size log error msg throw new io exception msg locs new erased locations size locs length locs erased locations get return locs decode inputs provided write output write fixed block fs data input stream inputs erased locations erased location to fix skip bytes limit output stream throws io exception log info need write limit skip bytes bytes erased location index erased location to fix tmp new inputs length decoded new erased locations length discard skip bytes loop number skipped written bytes less max written skip bytes written limit erased locations read from inputs inputs erased locations limit decoded length erased locations length decoded new erased locations length write math min buf
1051	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\ReedSolomonEncoder.java	unrelated	package org apache hadoop raid reed solomon encoder extends encoder log log log factory get log org apache hadoop raid reed solomon encoder erasure code reed solomon code reed solomon encoder configuration conf stripe size parity size super conf stripe size parity size reed solomon code new reed solomon code stripe size parity size protected encode stripe input stream blocks stripe start offset block size output stream outs progressable reporter throws io exception data new stripe size code new parity size encoded encoded block size encoded buf size read data block buf size blocks length raid utils read till end blocks read bufs true encode data read j j buf size j perform encode read bufs write bufs j data code now data write send temp files parity size outs write write bufs buf size reporter null reporter progress perform encode byte read bufs byte write bufs idx data code parity size code stripe size data read bufs idx x ff reed solomon code encode data code parity size write bufs idx byte code path get parity temp path return new path raid node rs temp prefix conf
1052	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\XORDecoder.java	unrelated	package org apache hadoop raid xor decoder extends decoder log log log factory get log org apache hadoop raid xor decoder xor decoder configuration conf stripe size super conf stripe size protected fix erased block file system fs path src file file system parity fs path parity file block size error offset bytes to skip limit output stream throws io exception log info fixing block src file error offset skipping bytes to skip limit limit file status src stat fs get file status src file array list fs data input stream xorinputs new array list fs data input stream fs data input stream parity file in parity fs open parity file parity file in seek parity offset error offset block size xorinputs add parity file in error block offset error offset block size block size src offsets stripe offsets error offset block size src offsets length src offsets error block offset log info skipping block src file error block offset continue src offsets src stat get len fs data input stream fs open src file seek src offsets xorinputs add fs data input stream inputs xorinputs array new fs data input stream null parity input stream recovered new parity input stream inputs limit read bufs write bufs recovered skip bytes to skip recovered drain null protected stripe offsets error offset block size offsets new stripe size stripe idx error offset block size stripe size start offset of stripe stripe idx stripe size block size stripe size offsets start offset of stripe block size return offsets protected parity offset error offset block size stripe idx error offset block size stripe size return stripe idx block size
1053	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\XOREncoder.java	unrelated	package org apache hadoop raid xor encoder extends encoder log log log factory get log org apache hadoop raid xor encoder xor encoder configuration conf stripe size super conf stripe size protected encode stripe input stream blocks stripe start offset block size output stream outs progressable reporter throws io exception log info peforming xor parity input stream parity in new parity input stream blocks block size read bufs write bufs try parity in drain outs reporter finally parity in close path get parity temp path return new path raid node unraid tmp directory conf
1054	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\PolicyInfo.java	unrelated	package org apache hadoop raid protocol maintains information one policy policy info implements writable log log log factory get log org apache hadoop raid protocol policy info protected simple date format date format new simple date format yyyy mm dd hh mm ss path src path specified src path string policy name name policy erasure code type code type erasure code used string description a verbose description policy configuration conf hadoop configuration properties properties policy dependent properties reentrant read write lock plock protects policy operations enum erasure code type xor rs erasure code type string string xor string equals ignore case return xor rs string equals ignore case return rs return null create empty object policy info conf null policy name description src path null properties new properties plock new reentrant read write lock create metadata describes policy policy info string policy name configuration conf conf conf policy name policy name description src path null properties new properties plock new reentrant read write lock copy fields another policy info copy from policy info conf null conf conf policy name null policy name length policy name policy name description null description length description description code type null code type code type src path null src path src path object key properties key set string skey string key properties set property skey properties get property skey sets input path policy applied set src path string throws io exception src path new path src path src path make qualified src path get file system conf set erasure code type used policy set erasure code string code code type erasure code type string code set description policy set description string des description des sets internal property set property string name string value properties set property name value returns value internal property string get property string name return properties get property name get name policy string get name return policy name get destination path policy erasure code type get erasure code return code type get src path path get src path return src path get expanded unglobbed forms src paths path get src path expanded throws io exception file system fs src path get file system conf globbing src path file status gpaths fs glob status src path gpaths null return null path values new path gpaths length gpaths length path p gpaths get path values p make qualified fs return values convert policy printable form string string string buffer buff new string buffer buff append policy name policy name n buff append source path src path n buff append erasure code code type n enumeration e properties property names e more elements string name string e next element buff append name properties get property name n description length len math min description length string sub description substring len trim sub sub replace all n buff append description sub n return buff string writable register ctor writable factories set factory policy info new writable factory writable new instance return new policy info write data output throws io exception text write string src path string text write string
1055	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\PolicyList.java	unrelated	package org apache hadoop raid protocol maintains informatiom policies belong category these policies applied one time cannot run simultaneously policy list implements writable log log log factory get log org apache hadoop raid protocol policy list list policy info category list policies path src path create new category policies policy list category new linked list policy info src path null add new policy category add policy info info category add info set src path configuration conf string src throws io exception src path new path src src path src path make qualified src path get file system conf path get src path return src path returns policies category collection policy info get all return category writable register ctor writable factories set factory policy list new writable factory writable new instance return new policy list write data output throws io exception write int category size policy info p category p write read fields data input throws io exception count read int count policy info p new policy info p read fields add p
1056	mapreduce\src\contrib\raid\src\java\org\apache\hadoop\raid\protocol\RaidProtocol.java	unrelated	package org apache hadoop raid protocol raid protocol used user code link org apache hadoop raid raid shell communicate raid node user code manipulate configured policies raid protocol extends versioned protocol compared previous version following changes introduced only latest change reflected new protocol introduced version id l get listing configured policies return categories configured policies policy list get all policies throws io exception unraid specified input path this called specified file corrupted this call move specified file file old recover raid subsystem string recover file string input path corrupt offset throws io exception
1057	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\AutoInputFormat.java	unrelated	package org apache hadoop streaming an link input format tries deduce types input files automatically it currently handle text sequence files auto input format extends file input format text input format text input format new text input format sequence file input format seq file input format new sequence file input format configure job conf job text input format configure job sequence file input format configure method record reader get record reader input split split job conf job reporter reporter throws io exception file split file split file split split file system fs file system get file split get path uri job fs data input stream fs open file split get path byte header new byte record reader reader null try read fully header catch eof exception eof reader text input format get record reader split job reporter finally close header s header e header q reader seq file input format get record reader split job reporter else reader text input format get record reader split job reporter return reader
1058	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\DumpTypedBytes.java	unrelated	package org apache hadoop streaming utility program fetches files match given pattern dumps content stdout typed bytes this works files handled link org apache hadoop streaming auto input format dump typed bytes implements tool configuration conf dump typed bytes configuration conf conf conf dump typed bytes new configuration configuration get conf return conf set conf configuration conf conf conf the main driver code dump typed bytes code run string args throws exception args length system err println too arguments print usage return path pattern new path args file system fs pattern get file system get conf fs set verify checksum true path p file util stat paths fs glob status pattern pattern list file status input files new array list file status file status status fs get file status p status directory file status files fs list status p collections add all input files files else input files add status return dump typed bytes input files return print usage system println usage hadoop prefix bin hadoop jar hadoop streaming jar dumptb glob pattern system println dumps files match given pattern standard output typed bytes system println the files text sequence files dump given list files standard output typed bytes dump typed bytes list file status files throws io exception job conf job new job conf get conf data output stream dout new data output stream system auto input format auto input format new auto input format file status file status files file split split new file split file status get path file status get len file status get block size string null record reader rec reader null try rec reader auto input format get record reader split job reporter null object key rec reader create key object value rec reader create value rec reader next key value key instanceof writable typed bytes writable output get dout write writable key else typed bytes output get dout write key value instanceof writable typed bytes writable output get dout write writable value else typed bytes output get dout write value finally rec reader null rec reader close dout flush return main string args throws exception dump typed bytes dumptb new dump typed bytes res tool runner run dumptb args system exit res
1059	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\Environment.java	unrelated	package org apache hadoop streaming this used get current environment host machines running map reduce this assumes setting environment streaming allowed windows ix linuz freebsd sunos solaris hp ux environment extends properties serial version uid l environment throws io exception extend code fit operating environments expect run http lopica sourceforge net os html string command null string os system get property os name string lower os os lower case os index of windows command cmd c set else lower os index of ix lower os index of linux lower os index of freebsd lower os index of sunos lower os index of solaris lower os index of hp ux command env else lower os starts with mac os x lower os starts with darwin command env else add others command null throw new runtime exception operating system os supported read environment variables process pid runtime get runtime exec command buffered reader new buffered reader new input stream reader pid get input stream true string line read line line null break p line index of p string name line substring p string value line substring p set property name value close try pid wait for catch interrupted exception e throw new io exception e get message used runtime exec string cmdarray string envp string array string arr new string super size enumeration object super keys more elements string key string next element string val string get key arr key val return arr map string string map map string string map new hash map string string enumeration object super keys more elements string key string next element string val string get key map put key val return map string get host string host get property host host null host always environment try host inet address get local host get host name catch io exception io io print stack trace return host
1060	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\HadoopStreaming.java	unrelated	package org apache hadoop streaming the main entry point usually invoked script bin hadoop jar hadoop streaming jar args hadoop streaming main string args throws exception args length system err println no arguments given print usage system exit return status string cmd args string remaining args arrays copy of range args args length cmd equals ignore case dumptb dump typed bytes dumptb new dump typed bytes return status tool runner run dumptb remaining args else cmd equals ignore case loadtb load typed bytes loadtb new load typed bytes return status tool runner run loadtb remaining args else cmd equals ignore case streamjob stream job job new stream job return status tool runner run job remaining args else backward compatibility stream job job new stream job return status tool runner run job args return status system err println streaming command failed system exit return status print usage system println usage hadoop prefix bin hadoop jar hadoop streaming jar options system println options system println dumptb glob pattern dumps files match given pattern system println standard output typed bytes system println loadtb path reads typed bytes standard input stores system println sequence file specified path system println streamjob args runs streaming job given arguments
1061	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\JarBuilder.java	unrelated	package org apache hadoop streaming this main generating job jar hadoop streaming jobs it includes files specified file option includes jar also hadoop streaming user level appplication hadoop streaming needed job also included job jar jar builder jar builder set verbose boolean v verbose v merge list src names list src unjar string dst jar throws io exception string source null jar output stream jar out null jar file jar source null jar out new jar output stream new file output stream dst jar boolean throwing false try src names null iterator iter src names iterator iter next source string iter next file fsource new file source string base get base path in jar out source fsource exists throwing true throw new file not found exception fsource get absolute path fsource directory add directory jar out base fsource else add file stream jar out base fsource src unjar null iterator iter src unjar iterator iter next source string iter next jar source new jar file source add jar entries jar out jar source jar source close finally try jar out close catch zip exception z throwing throw new io exception z string protected string file extension string file leaf pos file last index of leaf pos file length return string leaf name file substring leaf pos dot pos leaf name last index of dot pos return string ext leaf name substring dot pos return ext protected string get base path in jar out string source file task runner unjar append classpath lib string ext file extension source file ext equals return else ext equals jar ext equals zip return lib else return add jar entries jar output stream dst jar file src throws io exception enumeration entries src entries jar entry entry null entries more elements entry jar entry entries next element entry get name starts with meta inf continue input stream src get input stream entry add named stream dst entry get name add named stream jar output stream dst string name input stream throws io exception verbose system err println jar builder add named stream name try dst put next entry new jar entry name bytes read bytes read read buffer buff size dst write buffer bytes read catch zip exception ze ze get message index of duplicate entry verbose system err println ze skip duplicate entry name else throw ze finally close dst flush dst close entry add file stream jar output stream dst string jar base name file file throws io exception file input stream new file input stream file try string name jar base name file get name add named stream dst name finally close add directory jar output stream dst string jar base name file dir depth throws io exception file contents dir list files contents null contents length file f contents string f base name depth dir get name jar base name length f base name jar base name f base name f directory add directory dst f base name f depth else add file stream dst f base name f test program main string
1062	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\LoadTypedBytes.java	unrelated	package org apache hadoop streaming utility program reads typed bytes standard input stores sequence file path given argument load typed bytes implements tool configuration conf load typed bytes configuration conf conf conf load typed bytes new configuration configuration get conf return conf set conf configuration conf conf conf the main driver code load typed bytes code run string args throws exception args length system err println too arguments print usage return path path new path args file system fs path get file system get conf fs exists path system err println given path exists already return typed bytes input tbinput new typed bytes input new data input stream system sequence file writer writer sequence file create writer fs conf path typed bytes writable typed bytes writable try typed bytes writable key new typed bytes writable typed bytes writable value new typed bytes writable byte raw key tbinput read raw raw key null byte raw value tbinput read raw key set raw key raw key length value set raw value raw value length writer append key value raw key tbinput read raw finally writer close return print usage system println usage hadoop prefix bin hadoop jar hadoop streaming jar loadtb path system println reads typed bytes standard input stores sequence file system println specified path main string args throws exception load typed bytes loadtb new load typed bytes res tool runner run loadtb args system exit res
1063	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PathFinder.java	unrelated	package org apache hadoop streaming maps relative pathname absolute pathname using path environment path finder string pathenv pathnames string path sep path separator string file sep file separator directory construct path finder object using path java path path finder pathenv system get property java path path sep system get property path separator file sep system get property file separator construct path finder object using path specified system environment variable path finder string envpath pathenv system getenv envpath path sep system get property path separator file sep system get property file separator appends specified component path list prepend path component string str pathenv str path sep pathenv returns full path name file listed path file get absolute path string filename pathenv null path sep null file sep null return null val string classvalue pathenv path sep val classvalue index of path sep classvalue length extract entry pathenv string entry classvalue substring val trim file f new file entry f directory entry pathenv directory see required file directory f new file entry file sep filename see filename matches read f file f read return f classvalue classvalue substring val trim return null main string args throws io exception args length system println usage java path finder filename system exit path finder finder new path finder path file file finder get absolute path args file null system println full path name file get canonical path
1064	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeCombiner.java	unrelated	package org apache hadoop streaming pipe combiner extends pipe reducer string get pipe command job conf job string str job get stream combine streamprocessor try str null return url decoder decode str utf catch unsupported encoding exception e system err println stream combine streamprocessor jobconf found return null boolean get do pipe return get pipe command job null
1065	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapper.java	unrelated	package org apache hadoop streaming a generic mapper bridge it delegates operations external program via stdin stdout pipe mapper extends pipe map red implements mapper boolean ignore key false boolean skipping false byte map output field separator byte map input field separator num of map output key fields string get pipe command job conf job string str job get stream map streamprocessor str null return str try return url decoder decode str utf catch unsupported encoding exception e system err println stream map streamprocessor jobconf found return null boolean get do pipe return true configure job conf job super configure job disable auto increment counter for streaming processed records could different equal less records input skip bad records set auto incr mapper proc count job false skipping job get boolean mr job config skip records false map input writer class get canonical name equals text input writer get canonical name string input format class name job get class mapred input format text input format get canonical name ignore key job get boolean stream map input ignore key input format class name equals text input format get canonical name try map output field separator job get stream map output field separator get bytes utf map input field separator job get stream map input field separator get bytes utf num of map output key fields job get int stream num map output key fields catch unsupported encoding exception e throw new runtime exception the current system support utf encoding e do not declare default constructor map red creates reflectively map object key object value output collector output reporter reporter throws io exception outerr threads throwable null map red finished throw new io exception mr output mr err thread failed outerr threads throwable try hadoop num rec read maybe log record hadoop tool num exceptions ignore key writer write key key writer write value value skipping flush streams every record input running skip mode buffer records surrounding bad record client out flush else num rec skipped catch io exception io num exceptions num exceptions num rec written min rec written to enable skip terminate failure log info get context io map red finished throw io else terminate success swallow input records although stream processor failed closed close map red finished byte get input separator return map input field separator byte get field separator return map output field separator get num of key fields return num of map output key fields input writer create input writer throws io exception return super create input writer map input writer class output reader create output reader throws io exception return super create output reader map output reader class
1066	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapRed.java	unrelated	package org apache hadoop streaming shared functionality pipe mapper pipe reducer pipe map red protected log log log factory get log pipe map red get name returns configuration configuration get configuration return job returns data output client input written data output get client output return client out returns data input client output read data input get client input return client in returns input separator used byte get input separator returns field separator used byte get field separator returns number key fields get num of key fields returns command spawned subprocess mapper reducer operations delegate string get pipe command job conf job boolean get do pipe outside singleq doubleq buffer size string split args string args array list arg list new array list char ch args char array clen ch length state outside argstart c c clen c boolean last c clen last state state boolean end token false last ch c state outside state singleq else state singleq state outside end token state last state else ch c state outside state doubleq else state doubleq state outside end token state last state else ch c state outside end token true last end token c argstart unquoted space else string args substring argstart c arg list add argstart c last state state return string arg list array new string configure job conf job try string argv get pipe command job join delay job get long stream joindelay milli job job map input writer class job get class stream map input writer text input writer input writer map output reader class job get class stream map output reader text output reader output reader reduce input writer class job get class stream reduce input writer text input writer input writer reduce output reader class job get class stream reduce output reader text output reader output reader non zero exit is failure job get boolean stream non zero exit failure true pipe get do pipe pipe return set stream job details job string argv split split args argv string prog argv split file current dir new file get absolute file new file prog absolute hope executable else file util chmod new file current dir prog string x argv split an absolute path preexisting valid path task trackers a relative path converted absolute pathname looking path env variable if still fails look tasktracker local working directory new file argv split absolute path finder finder new path finder path finder prepend path component current dir string file f finder get absolute path argv split f null argv split f get absolute path f null log info pipe map red exec arrays list argv split environment child env environment stream util env clone add job conf to environment job child env add environment child env job get stream addenvironment add tmpdir environment variable value java io tmpdir env put child env tmpdir system get property java io tmpdir start process process builder builder new process builder argv split builder environment put all child env map sim builder start client out new data output stream new buffered output stream sim
1067	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeMapRunner.java	unrelated	package org apache hadoop streaming pipe map runner k v k v extends map runner k v k v run record reader k v input output collector k v output reporter reporter throws io exception pipe mapper pipe mapper pipe mapper get mapper pipe mapper start output threads output reporter super run input output reporter
1068	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\PipeReducer.java	unrelated	package org apache hadoop streaming a generic reducer bridge it delegates operations external program via stdin stdout pipe reducer extends pipe map red implements reducer byte reduce out field separator byte reduce input field separator num of reduce output key fields boolean skipping false string get pipe command job conf job string str job get stream reduce streamprocessor str null return str try return url decoder decode str utf catch unsupported encoding exception e system err println stream reduce streamprocessor jobconf found return null boolean get do pipe string argv get pipe command job currently null identity reduce reduce none map outputs return argv null stream job reduce none equals argv configure job conf job super configure job disable auto increment counter for streaming processed records could different equal less records input skip bad records set auto incr reducer proc count job false skipping job get boolean mr job config skip records false try reduce out field separator job get stream reduce output field separator get bytes utf reduce input field separator job get stream reduce input field separator get bytes utf num of reduce output key fields job get int stream num reduce output key fields catch unsupported encoding exception e throw new runtime exception the current system support utf encoding e reduce object key iterator values output collector output reporter reporter throws io exception init pipe thread null start output threads output reporter try values next writable val writable values next num rec read maybe log record pipe outerr threads throwable null map red finished throw new io exception mr output mr err thread failed outerr threads throwable writer write key key writer write value val else identity reduce output collect key val pipe skipping flush streams every record input running skip mode buffer records surrounding bad record client out flush catch io exception io common reason get failure subprocess document fact possible string extra info try exit val sim exit value exit val extra info subprocess exited successfully n else extra info subprocess exited error code exit val n catch illegal thread state exception e hmm child still running go figure extra info subprocess still running n map red finished throw new io exception extra info get context io get message close map red finished byte get input separator return reduce input field separator byte get field separator return reduce out field separator get num of key fields return num of reduce output key fields input writer create input writer throws io exception return super create input writer reduce input writer class output reader create output reader throws io exception return super create output reader reduce output reader class
1069	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamBaseRecordReader.java	unrelated	package org apache hadoop streaming shared functionality hadoop streaming formats a custom reader defined record reader constructor selected option bin hadoop streaming inputreader stream base record reader implements record reader text text protected log log log factory get log stream base record reader get name custom job conf properties prefixed namespace string conf ns stream recordreader stream base record reader fs data input stream file split split reporter reporter job conf job file system fs throws io exception split split start split get start length split get length end start length split name split get path get name reporter reporter job job fs fs status max record chars job get int conf ns statuschars record reader api read record implementation call num rec stats end boolean next text key text value throws io exception returns current position input synchronized get pos throws io exception return get pos close future operations synchronized close throws io exception close get progress throws io exception end start return f else return get pos start end start text create key return new text text create value return new text stream base record reader api implementation seek forward first byte next record the initial byte offset stream arbitrary seek next record boundary throws io exception num rec stats byte record start len throws io exception num rec num rec next status rec string record str new string record start math min len status max record chars utf next status rec string status get status record str log info status reporter set status status last mem string get status char sequence record pos try pos get pos catch io exception io string rec str record length status max record chars rec str record sub sequence status max record chars else rec str record string string unqual split split get path get name split get start split get length string status hstr stream util host num rec pos pos unqual split processing record rec str status split name return status fs data input stream file split split start end length string split name reporter reporter job conf job file system fs num rec next status rec status max record chars
1070	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamInputFormat.java	unrelated	package org apache hadoop streaming an input format selects record reader based job conf property this used non standard record reader stream xml record reader for standard record readers appropriate input format used stream input format extends key value text input format record reader text text get record reader input split generic split job conf job reporter reporter throws io exception string c job get stream recordreader c null c index of line record reader return super get record reader generic split job reporter handling non standard record reader likely stream xml record reader file split split file split generic split log info get record reader start split split reporter set status split string open file seek start split file system fs split get path get file system job fs data input stream fs open split get path factory dispatch based available params class reader class reader class stream util good class or null job c null reader class null throw new runtime exception class found c constructor ctor try ctor reader class get constructor new class fs data input stream file split reporter job conf file system catch no such method exception nsm throw new runtime exception nsm record reader text text reader try reader record reader text text ctor new instance new object split reporter job fs catch exception nsm throw new runtime exception nsm return reader
1071	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamJob.java	unrelated	package org apache hadoop streaming all client side work happens jar packaging map red job submission monitoring stream job implements tool protected log log log factory get log stream job get name string reduce none none streaming cli implementation command line parser parser new basic parser options options configuration using link set conf configuration run link run string stream job string argv boolean may exit argv argv config new configuration stream job setup options config new configuration configuration get conf return config set conf configuration conf config conf run string args throws exception try argv args init pre process args parse argv print usage print usage detailed usage return post process args set job conf catch illegal argument exception ex ignore since log already printed print log debug mode log debug error streaming job ex return return submit and monitor job this method creates streaming job given argument list the created object used submitted jobtracker execution job agent job control job conf create job string argv throws io exception stream job job new stream job job argv argv job init job pre process args job parse argv job post process args job set job conf return job job conf this method actually intializes job conf submits job jobtracker go throws io exception try return run argv catch exception ex throw new io exception ex get message protected init try env new environment catch io exception io throw new runtime exception io pre process args verbose false add task environment post process args throws io exception input specs size fail required argument input name output null fail required argument output msg add task environment add task environment string package file package files file f new file package file f file shipped canon files add f get canonical path msg shipped canon files shipped canon files careful names map cmd unqualify if local path map cmd com cmd unqualify if local path com cmd red cmd unqualify if local path red cmd string unqualify if local path string cmd throws io exception cmd null else string prog cmd string args cmd index of prog cmd substring args cmd substring string prog canon try prog canon new file prog get canonical path catch io exception io prog canon prog boolean shipped shipped canon files contains prog canon msg shipped shipped prog canon shipped change path simple filename that way pipe map red calls runtime exec look excutable task working dir and task runner unjars job jar prog new file prog get name args length cmd prog args else cmd prog msg cmd cmd return cmd parse argv command line cmd line null try cmd line parser parse options argv catch exception oe log error oe get message exit usage argv length info equals argv cmd line null detailed usage cmd line option info cmd line option help detailed usage print usage true return verbose cmd line option verbose background cmd line option background debug cmd line option debug debug debug string values cmd line get option values input values null values length string input values
1072	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamKeyValUtil.java	unrelated	package org apache hadoop streaming stream key val util find first occured tab utf encoded find tab byte utf start length start start length utf byte return return find first occured tab utf encoded find tab byte utf return org apache hadoop util utf byte array utils find nth byte utf utf length byte split utf byte array key value assuming delimilator splitpos split key val byte utf start length text key text val split pos separator length throws io exception split pos start split pos start length throw new illegal argument exception split pos must range start start length split pos key len split pos start val len start length split pos separator length key set utf start key len val set utf split pos separator length val len split utf byte array key value assuming delimilator splitpos split key val byte utf start length text key text val split pos throws io exception split key val utf start length key val split pos split utf byte array key value assuming delimilator splitpos split key val byte utf text key text val split pos separator length throws io exception split key val utf utf length key val split pos separator length split utf byte array key value assuming delimilator splitpos split key val byte utf text key text val split pos throws io exception split key val utf utf length key val split pos read utf encoded line data input stream read line line reader line reader text throws io exception clear return line reader read line
1073	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamUtil.java	unrelated	package org apache hadoop streaming utilities used streaming stream util it may seem strange silently switch behaviour string classname reason simplified usage pre mapper classname program instead explicit usage mapper program javamapper classname mapper javamapper mutually exclusive repeat reducer combiner pre class good class or null configuration conf string name string default package class clazz null try clazz conf get class by name name catch class not found exception cnf clazz null name index of default package null name default package name try clazz conf get class by name name catch class not found exception cnf return clazz string find in classpath string name return find in classpath name stream util get class loader string find in classpath string name class loader loader string rel path name rel path rel path replace rel path java net url url loader get resource rel path string code path url null boolean jar url get protocol equals jar code path url string code path starts with jar code path code path substring jar length code path starts with file code path code path substring file length jar a jar spec remove suffix path jar package class bang code path last index of code path code path substring bang else a spec remove package class portion pos code path last index of rel path pos throw new illegal argument exception invalid code path name name code path code path code path code path substring pos else code path null return code path string qualify host string url try return qualify host new url url string catch io exception io return url url qualify host url url try inet address inet address get by name url get host string qual host get canonical host name url q new url url get protocol qual host url get port url get file return q catch io exception io return url string regexp specials string regexp escape string plain string buffer buf new string buffer char ch plain char array csup ch length c c csup c regexp specials index of ch c buf append buf append ch c return buf string string slurp file f throws io exception len f length byte buf new byte len file input stream new file input stream f string contents null try read buf len contents new string buf utf finally close return contents string slurp hadoop path p file system fs throws io exception len fs get file status p get len byte buf new byte len fs data input stream fs open p string contents null try read fully get pos buf contents new string buf utf finally close return contents environment env string host try env new environment host env get host catch io exception io io print stack trace environment env env null return env try env new environment catch io exception io io print stack trace return env boolean local job tracker job conf job return job get jt config jt ipc address local equals local
1074	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\StreamXmlRecordReader.java	unrelated	package org apache hadoop streaming a way interpret xml fragments mapper input records values xml subtrees delimited configurable tags keys could value certain attribute xml subtree left stream processor application the name value properties stream xml record reader understands string begin chars marking beginning record string end chars marking end record maxrec maximum record size lookahead maximum lookahead sync cdata boolean slowmatch stream xml record reader extends stream base record reader stream xml record reader fs data input stream file split split reporter reporter job conf job file system fs throws io exception super split reporter job fs begin mark check job get conf ns begin end mark check job get conf ns end max rec size job get int conf ns maxrec look ahead job get int conf ns lookahead max rec size synched false slow match job get boolean conf ns slowmatch false slow match begin pat make pattern c data or mark begin mark end pat make pattern c data or mark end mark init init throws io exception log info stream base record reader init start start end end length length start get pos start get pos start get pos start get pos seek start pos start bin new buffered input stream seek next record boundary num next synchronized boolean next text key text value throws io exception num next pos end return false data output buffer buf new data output buffer read until match begin return false pos end read until match end buf return false there one elem key value splitting done byte record new byte buf get length system arraycopy buf get data record record length num rec stats record record length key set record value set return true seek next record boundary throws io exception read until match begin boolean read until match begin throws io exception slow match return slow read until match begin pat false null else return fast read until match begin mark false null boolean read until match end data output buffer buf throws io exception slow match return slow read until match end pat true buf else return fast read until match end mark true buf boolean slow read until match pattern mark pattern boolean pat data output buffer buf or null throws io exception byte buf new byte math max look ahead max rec size read bin mark math max look ahead max rec size mark invalidate read read bin read buf read return false string sbuf new string buf read utf matcher match mark pattern matcher sbuf first match start na first match end na buf pos state synched cdata out cdata unk match find buf pos input match group null input cdata begin else match group null input cdata end first match start na doc cdata doc keep else input record maybe input record maybe first match start na first match start match start first match end match end state next state state input match start state record accept break buf pos match end state cdata unk synched true boolean matched first match start na state
1075	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\IdentifierResolver.java	unrelated	package org apache hadoop streaming io this used resolve identifier required io by extending pointing property tt stream io identifier resolver tt extension additional io added external code identifier resolver note identifiers case insensitive string text id text string raw bytes id rawbytes string typed bytes id typedbytes class extends input writer input writer class null class extends output reader output reader class null class output key class null class output value class null resolves given identifier this method called calling getters resolve string identifier identifier equals ignore case raw bytes id set input writer class raw bytes input writer set output reader class raw bytes output reader set output key class bytes writable set output value class bytes writable else identifier equals ignore case typed bytes id set input writer class typed bytes input writer set output reader class typed bytes output reader set output key class typed bytes writable set output value class typed bytes writable else assume text id set input writer class text input writer set output reader class text output reader set output key class text set output value class text returns resolved link input writer class extends input writer get input writer class return input writer class returns resolved link output reader class extends output reader get output reader class return output reader class returns resolved output key class get output key class return output key class returns resolved output value class get output value class return output value class sets link input writer protected set input writer class class extends input writer input writer class input writer class input writer class sets link output reader protected set output reader class class extends output reader output reader class output reader class output reader class sets output key protected set output key class class output key class output key class output key class sets output value protected set output value class class output value class output value class output value class
1076	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\InputWriter.java	unrelated	package org apache hadoop streaming io abstract base write client input input writer k v initializes input writer this method called calling methods initialize pipe map red pipe map red throws io exception nothing yet might change future writes input key write key k key throws io exception writes input value write value v value throws io exception
1077	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\OutputReader.java	unrelated	package org apache hadoop streaming io abstract base read client output output reader k v initializes output reader this method called calling methods initialize pipe map red pipe map red throws io exception nothing yet might change future read next key value pair outputted client boolean read key value throws io exception returns current key k get current key throws io exception returns current value v get current value throws io exception returns last output client string string get last output
1078	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\RawBytesInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input raw bytes raw bytes input writer extends input writer writable writable data output client out byte array output stream buffer out data output stream buffer data out initialize pipe map red pipe map red throws io exception super initialize pipe map red client out pipe map red get client output buffer out new byte array output stream buffer data out new data output stream buffer out write key writable key throws io exception write raw bytes key write value writable value throws io exception write raw bytes value write raw bytes writable writable throws io exception writable instanceof bytes writable bytes writable bw bytes writable writable byte bytes bw get bytes length bw get length client out write int length client out write bytes length else buffer out reset writable write buffer data out byte bytes buffer out byte array client out write int bytes length client out write bytes
1079	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\RawBytesOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output raw bytes raw bytes output reader extends output reader bytes writable bytes writable data input client in byte bytes bytes writable key bytes writable value initialize pipe map red pipe map red throws io exception super initialize pipe map red client in pipe map red get client input key new bytes writable value new bytes writable boolean read key value throws io exception length read length length return false key set read bytes length length length read length value set read bytes length length return true bytes writable get current key throws io exception return key bytes writable get current value throws io exception return value string get last output bytes null return new bytes writable bytes string else return null read length throws io exception try return client in read int catch eof exception eof return byte read bytes length throws io exception bytes new byte length client in read fully bytes return bytes
1080	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TextInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input text text input writer extends input writer object object data output client out byte input separator initialize pipe map red pipe map red throws io exception super initialize pipe map red client out pipe map red get client output input separator pipe map red get input separator write key object key throws io exception write utf key client out write input separator write value object value throws io exception write utf value client out write n write object output stream using utf encoding write utf object object throws io exception byte bval val size object instanceof bytes writable bytes writable val bytes writable object bval val get bytes val size val get length else object instanceof text text val text object bval val get bytes val size val get length else string sval object string bval sval get bytes utf val size bval length client out write bval val size
1081	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TextOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output text text output reader extends output reader text text line reader line reader byte bytes data input client in configuration conf num key fields byte separator text key text value text line initialize pipe map red pipe map red throws io exception super initialize pipe map red client in pipe map red get client input conf pipe map red get configuration num key fields pipe map red get num of key fields separator pipe map red get field separator line reader new line reader input stream client in conf key new text value new text line new text boolean read key value throws io exception line reader read line line return false bytes line get bytes split key val bytes line get length key value line clear return true text get current key throws io exception return key text get current value throws io exception return value string get last output bytes null try return new string bytes utf catch unsupported encoding exception e return undecodable else return null split utf line key value split key val byte line length text key text val throws io exception need find num key fields separators pos utf byte array utils find bytes line length separator k k num key fields pos k pos utf byte array utils find bytes line pos separator length length separator try pos key set line length val set else stream key val util split key val line length key val pos separator length catch character coding exception e throw new io exception string utils stringify exception e
1082	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TypedBytesInputWriter.java	unrelated	package org apache hadoop streaming io input writer writes client input typed bytes typed bytes input writer extends input writer object object typed bytes output tb out typed bytes writable output tbw out initialize pipe map red pipe map red throws io exception super initialize pipe map red data output client out pipe map red get client output tb out new typed bytes output client out tbw out new typed bytes writable output client out write key object key throws io exception write typed bytes key write value object value throws io exception write typed bytes value write typed bytes object value throws io exception value instanceof writable tbw out write writable value else tb out write value
1083	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\streaming\io\TypedBytesOutputReader.java	unrelated	package org apache hadoop streaming io output reader reads client output typed bytes typed bytes output reader extends output reader typed bytes writable typed bytes writable byte bytes data input client in typed bytes writable key typed bytes writable value typed bytes input initialize pipe map red pipe map red throws io exception super initialize pipe map red client in pipe map red get client input key new typed bytes writable value new typed bytes writable new typed bytes input client in boolean read key value throws io exception bytes read raw bytes null return false key set bytes bytes length bytes read raw value set bytes bytes length return true typed bytes writable get current key throws io exception return key typed bytes writable get current value throws io exception return value string get last output bytes null return new typed bytes writable bytes string else return null
1084	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\Type.java	unrelated	package org apache hadoop typedbytes the possible type codes enum type codes supported types bytes byte bool int long float double string vector list map application specific codes writable low level codes marker code type code code code
1085	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesInput.java	unrelated	package org apache hadoop typedbytes provides functionality reading typed bytes typed bytes input data input typed bytes input set data input data input thread local tb in new thread local protected synchronized object initial value return new typed bytes input get thread local typed bytes input supplied link data input typed bytes input get data input typed bytes input bin typed bytes input tb in get bin set data input return bin creates new instance typed bytes input typed bytes input data input reads typed bytes sequence converts java object the first byte interpreted type code right number subsequent bytes read depending obtained type object read throws io exception code try code read unsigned byte catch eof exception eof return null code type bytes code return new buffer read bytes else code type byte code return read byte else code type bool code return read bool else code type int code return read int else code type long code return read long else code type float code return read float else code type double code return read double else code type string code return read string else code type vector code return read vector else code type list code return read list else code type map code return read map else code type marker code return null else code code application specific typecodes return new buffer read bytes else throw new runtime exception unknown type reads typed bytes sequence the first byte interpreted type code right number subsequent bytes read depending obtained type reached byte read raw throws io exception code try code read unsigned byte catch eof exception eof return null code type bytes code return read raw bytes else code type byte code return read raw byte else code type bool code return read raw bool else code type int code return read raw int else code type long code return read raw long else code type float code return read raw float else code type double code return read raw double else code type string code return read raw string else code type vector code return read raw vector else code type list code return read raw list else code type map code return read raw map else code type marker code return null else code code application specific typecodes return read raw bytes code else throw new runtime exception unknown type reads type byte returns corresponding link type type read type throws io exception code try code read unsigned byte catch eof exception eof return null type type type values type code code return type return null skips type byte boolean skip type throws io exception try read byte return true catch eof exception eof return false reads bytes following code type bytes code code byte read bytes throws io exception length read int byte bytes new byte length read fully bytes return bytes reads raw bytes following custom code byte read raw bytes code throws io exception length read int byte bytes new byte length bytes byte code bytes byte xff length bytes byte xff length bytes
1086	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesOutput.java	unrelated	package org apache hadoop typedbytes provides functionality writing typed bytes typed bytes output data output typed bytes output set data output data output thread local tb out new thread local protected synchronized object initial value return new typed bytes output get thread local typed bytes output supplied link data output link data output typed bytes output get data output typed bytes output bout typed bytes output tb out get bout set data output return bout creates new instance typed bytes output typed bytes output data output writes java object typed bytes sequence write object obj throws io exception obj instanceof buffer write bytes buffer obj else obj instanceof byte write byte byte obj else obj instanceof boolean write bool boolean obj else obj instanceof integer write int integer obj else obj instanceof long write long long obj else obj instanceof float write float float obj else obj instanceof double write double double obj else obj instanceof string write string string obj else obj instanceof array list write vector array list obj else obj instanceof list write list list obj else obj instanceof map write map map obj else throw new runtime exception cannot write objects type writes raw sequence typed bytes write raw byte bytes throws io exception write bytes writes raw sequence typed bytes write raw byte bytes offset length throws io exception write bytes offset length writes bytes array typed bytes sequence using given typecode length write bytes byte bytes code length throws io exception write code write int length write bytes length writes bytes array typed bytes sequence using given typecode write bytes byte bytes code throws io exception write bytes bytes code bytes length writes bytes array typed bytes sequence write bytes byte bytes throws io exception write bytes bytes type bytes code writes bytes buffer typed bytes sequence write bytes buffer buffer throws io exception write bytes buffer get type bytes code buffer get count writes byte typed bytes sequence write byte byte b throws io exception write type byte code write b writes boolean typed bytes sequence write bool boolean b throws io exception write type bool code write boolean b writes integer typed bytes sequence write int throws io exception write type int code write int writes typed bytes sequence write long throws io exception write type long code write long writes typed bytes sequence write float f throws io exception write type float code write float f writes typed bytes sequence write double throws io exception write type double code write double writes typed bytes sequence write string string throws io exception write type string code writable utils write string writes vector typed bytes sequence write vector array list vector throws io exception write vector header vector size object obj vector write obj writes vector header write vector header length throws io exception write type vector code write int length writes list typed bytes sequence write list list list throws io exception write list header object obj list write obj write list footer writes list header write list header throws io exception write type
1087	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesRecordInput.java	unrelated	package org apache hadoop typedbytes serializer records writes typed bytes typed bytes record input implements record input typed bytes input typed bytes record input set typed bytes input typed bytes input thread local tb in new thread local protected synchronized object initial value return new typed bytes record input get thread local typed bytes record input supplied link typed bytes input link typed bytes input typed bytes record input get typed bytes input typed bytes record input bin typed bytes record input tb in get bin set typed bytes input return bin get thread local typed bytes record input supplied link data input link data input typed bytes record input get data input return get typed bytes input get creates new instance typed bytes record input typed bytes record input typed bytes input creates new instance typed bytes record input typed bytes record input data input new typed bytes input boolean read bool string tag throws io exception skip type return read bool buffer read buffer string tag throws io exception skip type return new buffer read bytes byte read byte string tag throws io exception skip type return read byte read double string tag throws io exception skip type return read double read float string tag throws io exception skip type return read float read int string tag throws io exception skip type return read int read long string tag throws io exception skip type return read long string read string string tag throws io exception skip type return read string start record string tag throws io exception skip type index start vector string tag throws io exception skip type return new typed bytes index read vector header index start map string tag throws io exception skip type return new typed bytes index read map header end record string tag throws io exception end vector string tag throws io exception end map string tag throws io exception typed bytes index implements index nelems typed bytes index nelems nelems nelems boolean done return nelems incr nelems
1088	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesRecordOutput.java	unrelated	package org apache hadoop typedbytes deserialized records reads typed bytes typed bytes record output implements record output typed bytes output typed bytes record output set typed bytes output typed bytes output thread local tb out new thread local protected synchronized object initial value return new typed bytes record output get thread local typed bytes record input supplied link typed bytes output link typed bytes output typed bytes record output get typed bytes output typed bytes record output bout typed bytes record output tb out get bout set typed bytes output return bout get thread local typed bytes record output supplied link data output link data output typed bytes record output get data output return get typed bytes output get creates new instance typed bytes record output typed bytes record output typed bytes output creates new instance typed bytes record output typed bytes record output data output new typed bytes output write bool boolean b string tag throws io exception write bool b write buffer buffer buf string tag throws io exception write bytes buf get write byte byte b string tag throws io exception write byte b write double string tag throws io exception write double write float f string tag throws io exception write float f write int string tag throws io exception write int write long string tag throws io exception write long write string string string tag throws io exception write string start record record r string tag throws io exception write list header start vector array list v string tag throws io exception write vector header v size start map tree map string tag throws io exception write map header size end record record r string tag throws io exception write list footer end vector array list v string tag throws io exception end map tree map string tag throws io exception
1089	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritable.java	unrelated	package org apache hadoop typedbytes writable typed bytes typed bytes writable extends bytes writable create typed bytes writable typed bytes writable super create typed bytes writable given byte array initial value typed bytes writable byte bytes super bytes set typed bytes given java object set value object obj try byte array output stream baos new byte array output stream typed bytes output tbo typed bytes output get new data output stream baos tbo write obj byte bytes baos byte array set bytes bytes length catch io exception e throw new runtime exception e get typed bytes java object object get value try byte array input stream bais new byte array input stream get bytes typed bytes input tbi typed bytes input get new data input stream bais object obj tbi read return obj catch io exception e throw new runtime exception e get type code embedded first byte type get type byte bytes get bytes bytes null bytes length return null type type type values type code bytes return type return null generate suitable representation string string return get value string
1090	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritableInput.java	unrelated	package org apache hadoop typedbytes provides functionality reading typed bytes writable objects typed bytes writable input implements configurable typed bytes input configuration conf typed bytes writable input conf new configuration set typed bytes input typed bytes input thread local tb in new thread local protected synchronized object initial value return new typed bytes writable input get thread local typed bytes writable input supplied link typed bytes input link typed bytes input typed bytes writable input get typed bytes input typed bytes writable input bin typed bytes writable input tb in get bin set typed bytes input return bin get thread local typed bytes writable input supplied link data input link data input typed bytes writable input get data input return get typed bytes input get creates new instance typed bytes writable input typed bytes writable input typed bytes input creates new instance typed bytes writable input typed bytes writable input data input din new typed bytes input din writable read throws io exception type type read type type null return null switch type case bytes return read bytes case byte return read byte case bool return read boolean case int return read v int case long return read v long case float return read float case double return read double case string return read text case vector return read array case map return read map case writable return read writable default throw new runtime exception unknown type class extends writable read type throws io exception type type read type type null return null switch type case bytes return bytes writable case byte return byte writable case bool return boolean writable case int return v int writable case long return v long writable case float return float writable case double return double writable case string return text case vector return array writable case map return map writable case writable return writable default throw new runtime exception unknown type bytes writable read bytes bytes writable bw throws io exception byte bytes read bytes bw null bw new bytes writable bytes else bw set bytes bytes length return bw bytes writable read bytes throws io exception return read bytes null byte writable read byte byte writable bw throws io exception bw null bw new byte writable bw set read byte return bw byte writable read byte throws io exception return read byte null boolean writable read boolean boolean writable bw throws io exception bw null bw new boolean writable bw set read bool return bw boolean writable read boolean throws io exception return read boolean null int writable read int int writable iw throws io exception iw null iw new int writable iw set read int return iw int writable read int throws io exception return read int null v int writable read v int v int writable iw throws io exception iw null iw new v int writable iw set read int return iw v int writable read v int throws io exception return read v int null long writable read long long writable lw throws io exception lw null lw new long writable
1091	mapreduce\src\contrib\streaming\src\java\org\apache\hadoop\typedbytes\TypedBytesWritableOutput.java	unrelated	package org apache hadoop typedbytes provides functionality writing writable objects typed bytes typed bytes writable output typed bytes output typed bytes writable output set typed bytes output typed bytes output thread local tb out new thread local protected synchronized object initial value return new typed bytes writable output get thread local typed bytes writable input supplied link typed bytes output link typed bytes output typed bytes writable output get typed bytes output typed bytes writable output bout typed bytes writable output tb out get bout set typed bytes output return bout get thread local typed bytes writable output supplied link data output link data output typed bytes writable output get data output return get typed bytes output get creates new instance typed bytes writable output typed bytes writable output typed bytes output creates new instance typed bytes writable output typed bytes writable output data output dout new typed bytes output dout write writable w throws io exception w instanceof typed bytes writable write typed bytes typed bytes writable w else w instanceof bytes writable write bytes bytes writable w else w instanceof byte writable write byte byte writable w else w instanceof boolean writable write boolean boolean writable w else w instanceof int writable write int int writable w else w instanceof v int writable write v int v int writable w else w instanceof long writable write long long writable w else w instanceof v long writable write v long v long writable w else w instanceof float writable write float float writable w else w instanceof double writable write double double writable w else w instanceof text write text text w else w instanceof array writable write array array writable w else w instanceof map writable write map map writable w else w instanceof sorted map writable write sorted map sorted map writable w else w instanceof record write record record w else write writable w last resort write typed bytes typed bytes writable tbw throws io exception write raw tbw get bytes tbw get length write bytes bytes writable bw throws io exception byte bytes arrays copy of range bw get bytes bw get length write bytes bytes write byte byte writable bw throws io exception write byte bw get write boolean boolean writable bw throws io exception write bool bw get write int int writable iw throws io exception write int iw get write v int v int writable viw throws io exception write int viw get write long long writable lw throws io exception write long lw get write v long v long writable vlw throws io exception write long vlw get write float float writable fw throws io exception write float fw get write double double writable dw throws io exception write double dw get write text text throws io exception write string string write array array writable aw throws io exception writable writables aw get write vector header writables length writable writable writables write writable write map map writable mw throws io exception write map header mw size map entry writable writable entry mw entry
1092	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\DiagnosticTest.java	unrelated	package org apache hadoop vaidya this base extended diagnostic test it implements runnable required multiple tests run parallel diagnostic test implements runnable highval mediumval lowval job statistics passed diagnostic test evaluated job statistics job execution stats element test config element impact level boolean evaluated boolean test passed checks test already evaluated job execution statistics boolean evaluated return evaluated if impact level returned evaluate method less success threshold test passed negative else failed positive inturn indicates problem job performance boolean istest passed return test passed initialize globals init globals job statistics job execution stats element test config element job execution stats job execution stats test config element test config element returns prescription advice formated text based evaluation diagnostic test condition evaluate method individual test implement if value returned null prescription advice printed provided test config file string get prescription this method prints reference details support test result individual test needs implement information printed specific individual test string get reference details evaluates diagnostic condition returns impact level value typically method calculates impact diagnosed condition job performance note boolean conditions either evaluate job statistics job execution stats get title information test set test config file string get title throws exception return xml utils get element value title test config element get description information set test config file string get description throws exception return xml utils get element value description test config element get importance value set test config file get importance throws exception xml utils get element value importance test config element equals ignore case high return highval else xml utils get element value importance test config element equals ignore case medium return mediumval else return lowval returns impact level test condition this value calculated returned evaluate method get impact level throws exception evaluated throw new exception test evaluated return truncate impact level get severity level specified test config file get severity level throws exception return truncate get importance get impact level get success threshold specified test config file get success threshold throws exception x double parse double xml utils get element value success threshold test config element return truncate x creates returns report element test based test evaluation results element get report element document doc node parent throws exception if test evaluated yet throw exception evaluated throw new exception test evaluated if means first test print job information node report elementx doc create element job information element parent append child report elementx insert jobtrackerid node itemx doc create element job tracker id report elementx append child itemx node valuex doc create text node job execution stats get string value job keys jobtrackerid itemx append child valuex insert jobname itemx doc create element job name report elementx append child itemx valuex doc create text node job execution stats get string value job keys jobname itemx append child valuex insert jobtype itemx doc create element job type report elementx append child itemx valuex doc create text node job execution stats get string value job keys jobtype itemx append child valuex insert user itemx doc create element user report elementx append child itemx valuex doc create text
1093	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\JobDiagnoser.java	unrelated	package org apache hadoop vaidya this base driver job diagnostics various specialty drivers tests specific aspects job problems e g post ex performance diagnoser extends base job diagnoser xml document containing report elements one rule tested document report document get report return report constructor it initializes report document job diagnoser throws exception initialize report document make ready add child report elements document builder builder null document builder factory factory document builder factory new instance try builder factory new document builder report builder new document catch parser configuration exception e e print stack trace insert root element element root element report create element post ex performance diagnostic report report append child root print report document console print report xml utils print dom report save report document specified report file save report string filename xml utils write xml to file filename report
1094	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\postexdiagnosis\PostExPerformanceDiagnoser.java	unrelated	package org apache hadoop vaidya postexdiagnosis this acts driver rule engine executing post execution performance diagnostics tests map reduce job it prints saves diagnostic report xml document post ex performance diagnoser extends job diagnoser string job history file null input stream tests conf file is null string report file null string job conf file null data available analysts write post execution performance diagnostic rules job statistics job execution statistics get report file diagnostic report saved string get report file return report file get job history log file used collecting job counters string get job history file return job history file get test configuration file diagnostic tests registered configuration information input stream get tests conf file is return tests conf file is set test configuration file set tests conf file is input stream tests conf file is tests conf file is tests conf file is counters statistics information job statistics get job execution statistics return job execution statistics if specified default path hadoop prefix contrib vaidya pxpd tests config xml post ex performance diagnoser string job conf file string job history file input stream tests conf file is string report file throws exception job history file job history file tests conf file is tests conf file is report file report file job conf file job conf file read job information necessary post performance analysis job conf job conf new job conf job info job info read job information job conf job execution statistics new job statistics job conf job info read populate job statistics information job info read job information job conf job conf throws exception convert input strings url url job conf file url new url job conf file url job history file url new url job history file read job configuration job conf file url job conf add resource job conf file url read job history file build job counters evaluate diagnostic rules job history parser parser job info job info job history file url get protocol equals hdfs parser new job history parser file system get job conf job history file url get path job info parser parse else job history file url get protocol equals file parser new job history parser file system get local job conf job history file url get path job info parser parse else throw new exception malformed url protocol job history file url get protocol return job info print help print help system println usage system println post ex performance diagnoser jobconf fileurl joblog fileurl testconf filepath report filepath system println system println jobconf fileurl file path job configuration file e g job xxxx conf xml it hdfs system println local file system it specified url format system println e g local file file localhost users hadoop user job conf xml system println e g hdfs file hdfs namenode port users hadoop user hodlogs job conf xml system println system println joblog fileurl file path job history log file it hdfs local file system system println it specified url format system println system println testconf filepath optional file path performance advisor tests configuration file
1095	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\JobStatistics.java	unrelated	package org apache hadoop vaidya statistics job job statistics implements job statistics interface pattern parsing counters pattern pattern pattern compile job configuration job conf job conf set job conf job conf job conf job conf job conf todo add job conf job array aggregated job level counters job history parser job info job info job stats java util hashtable enum string job job conf get job conf return job conf get job counters type get long value enum key job get key null return else return long parse long job get key get job counters type double get double value enum key job get key null return else return double parse double job get key get job counters type string string get string value enum key job get key null return else return job get key set key value type set value enum key value job put key long string value set key value type set value enum key value job put key double string value set key value type string set value enum key string value job put key value map task list sorted task id array list map task statistics map task list new array list map task statistics reduce task list sorted task id array list reduce task statistics reduce task list new array list reduce task statistics ctor job statistics job conf job conf job info job info throws parse exception job conf job conf job info job info job new hashtable enum string populate job job job info populate map reduce task lists map task list reduce task list job info get all tasks add job type map reduce map only get long value job keys total reduces job put job keys jobtype map only else job put job keys jobtype map reduce populate map reduce task lists array list map task statistics map task list array list reduce task statistics reduce task list map task id task info task map throws parse exception num tasks task map entry set size do need lists list task attempt info successful map attempt list new array list task attempt info list task attempt info successful reduce attempt list new array list task attempt info job history parser task info task info task map values task info get task type equals task type map map task statistics map t new map task statistics task attempt info successful attempt get last successful task attempt task info map t set value map task keys task id successful attempt get attempt id get task id string map t set value map task keys attempt id successful attempt get attempt id string map t set value map task keys hostname successful attempt get tracker name map t set value map task keys task type successful attempt get task type string map t set value map task keys status successful attempt get task status string map t set value map task keys start time successful attempt get start time map t set value map task keys finish time successful attempt get finish time map t
1096	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\JobStatisticsInterface.java	unrelated	package org apache hadoop vaidya statistics job job statistics interface get job configuration job xml values job conf get job conf get job counters type get long value enum key get job counters type double get double value enum key get job counters type string string get string value enum key set key value type set value enum key value set key value type set value enum key valye set key value type string set value enum key string value if sort key null default map tasks sorted using map task ids array list map task statistics get map task list enum map task sort key key data type data type if sort key null default reduce tasks sorted using task ids array list reduce task statistics get reduce task list enum reduce task sort key key data type data type print job execution statistics print job execution statistics job task statistics key data types enum key data type string long double job keys enum job keys jobtrackerid jobid jobname jobtype user submit time conf path launch time total maps total reduces status finish time finished maps finished reduces failed maps failed reduces launched maps launched reduces racklocal maps datalocal maps hdfs bytes read hdfs bytes written file bytes read file bytes written combine output records combine input records reduce input groups reduce input records reduce output records map input records map output records map input bytes map output bytes map hdfs bytes written jobconf job priority shuffle bytes spilled records map task keys enum map task keys task id task type start time status finish time hdfs bytes read hdfs bytes written file bytes read file bytes written combine output records combine input records output records input records input bytes output bytes num attempts attempt id hostname splits spilled records tracker name state string http port error execution time reduce task keys enum reduce task keys task id task type start time status finish time hdfs bytes read hdfs bytes written file bytes read file bytes written combine output records combine input records output records input records num attempts attempt id hostname shuffle finish time sort finish time input groups tracker name state string http port splits shuffle bytes spilled records execution time
1097	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\MapTaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job map task statistics extends task statistics map task statistics extends task statistics
1098	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\ReduceTaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job reduce task statistics extends task statistics reduce task statistics extends task statistics
1099	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\statistics\job\TaskStatistics.java	unrelated	package org apache hadoop vaidya statistics job task statistics stores task statistics enum string key value pairs hashtable enum string task new hashtable enum string get long key value get long value enum key task get key null return else return long parse long task get key get key type double get double value enum key task get key null return else return double parse double task get key get key type string string get string value enum key task get key null return else return task get key set key value set value enum key value task put key long string value set key value set value enum key value task put key double string value set string key value set value enum key string value task put key value print key values pairs task print keys java util set map entry enum string task task entry set size task size java util iterator map entry enum string kv task iterator size map entry enum string entry map entry enum string kv next enum key entry get key string value entry get value system println key key name value value
1100	mapreduce\src\contrib\vaidya\src\java\org\apache\hadoop\vaidya\util\XMLUtils.java	unrelated	package org apache hadoop vaidya util sample utility work dom document xml utils prints specified node prints children print dom node node type node get node type switch type print document element case node document node system print xml version print dom document node get document element break print element attributes case node element node system println system print system print node get node name named node map attrs node get attributes attrs get length node attr attrs item system print attr get node name trim attr get node value trim system print node list children node get child nodes children null len children get length len print dom children item break handle entity reference nodes case node entity reference node system print system print node get node name trim system print break print cdata sections case node cdata section node system print cdata system print node get node value trim system print break print text case node text node system println system print node get node value trim break print processing instruction case node processing instruction node system print system print node get node name trim string data node get node value trim system print system print data system print break type node element node system println system print system print node get node name trim system print get value first element given node name string get element value string element name element element throws exception string value null node list child nodes element get elements by tag name element name element cn element child nodes item value cn get first child get node value trim value child nodes item get node value trim value null throw new exception no element found given name element name return value parse xml file create document document parse input stream fs document document null initiate document builder factory document builder factory factory document builder factory new instance to get validating parser factory set validating false to get one understands namespaces factory set namespace aware true try get document builder document builder builder factory new document builder parse load memory document document builder parse new file file name document builder parse fs return document catch sax parse exception spe error generated parser system err println n parsing error line spe get line number uri spe get system id system err println spe get message use contained exception exception x spe spe get exception null x spe get exception x print stack trace catch sax exception sxe error generated parsing exception x sxe sxe get exception null x sxe get exception x print stack trace catch parser configuration exception pce parser specified options built pce print stack trace catch io exception ioe i o error ioe print stack trace return null this method writes dom document file write xml to file string filename document document try prepare dom document writing source source new dom source document prepare output file file file new file filename result result new stream result file write dom document file get transformer transformer xformer transformer factory new instance new transformer write file
1101	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaConfiguration.java	unrelated	package org apache hadoop vertica a container configuration property names jobs vertica input output the job configured using methods link vertica input format link vertica output format alternatively properties set configuration proper values string string string string string string string string vertica configuration vertica version constants integer version class name vertica jdbc driver string vertica driver class com vertica driver host names connect selected random string hostnames prop mapred vertica hostnames name database connect string database prop mapred vertica database user name vertica string username prop mapred vertica username password vertica string password prop mapred vertica password host names connect selected random string output hostnames prop mapred vertica hostnames output name database connect string output database prop mapred vertica database output user name vertica string output username prop mapred vertica username output password vertica string output password prop mapred vertica password output query run input string query prop mapred vertica input query query run retrieve parameters string query param prop mapred vertica input query paramquery static parameters query string query params prop mapred vertica input query params optional input delimiter streaming string input delimiter prop mapred vertica input delimiter optional input terminator streaming string input terminator prop mapred vertica input terminator whether marshal dates strings string date string mapred vertica date output table name string output table name prop mapred vertica output table name definition output table types string output table def prop mapred vertica output table def whether drop tables string output table drop mapred vertica output table drop optional output format delimiter string output delimiter prop mapred vertica output delimiter optional output format terminator string output terminator prop mapred vertica output terminator override sleep timer optimize poll new projetions refreshed string optimize poll timer prop mapred vertica optimize poll sets vertica database connection information link configuration configuration one hosts vertica cluster name vertica database vertica database username vertica database password configure vertica configuration conf string hostnames string database string username string password conf set strings hostnames prop hostnames conf set database prop database conf set username prop username conf set password prop password sets vertica database connection information link configuration configuration one hosts source cluster name source vertica database source vertica database source vertica database one hosts output cluster name output vertica database target vertica database target vertica database configure vertica configuration conf string hostnames string database string username string password string output hostnames string output database string output username string output password configure vertica conf hostnames database username password conf set strings output hostnames prop output hostnames conf set output database prop output database conf set output username prop output username conf set output password prop output password configuration conf default record terminator writing output vertica string record terminater u default delimiter writing output vertica string delimiter u defulat optimize poll timeout optimize poll timer vertica configuration configuration conf conf conf configuration get configuration return conf returns connection random host vertica cluster true connection writing connection get connection boolean output throws io exception class not found exception sql exception try class name vertica driver class catch class not found
1102	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaInputFormat.java	unrelated	package org apache hadoop vertica input formatter returns results query executed vertica the key record number within result set mapper the value vertica record uses similar jdbc result sets returning values vertica input format extends input format long writable vertica record set input query job query run vertica set input job job string input query job set input format class vertica input format vertica configuration config new vertica configuration job get configuration config set input query input query set parameterized input query job query returns parameters sql query parameters specified question marks sql query returns parameters input query set input job job string input query string segment params query job set input format class vertica input format vertica configuration config new vertica configuration job get configuration config set input query input query config set params query segment params query set input query number comma delimited literal list parameters sql query parameters specified question marks numer comma delimited strings literal parameters substitute input query set input job job string input query string segment params throws io exception transform param set array date format datefmt date format get date instance collection list object params new hash set list object string str params segment params list object param new array list object string str param str params split str param str param trim str param char at str param char at str param length param add str param substring str param length else try param add datefmt parse str param catch parse exception e try param add integer parse int str param catch number format exception e throw new io exception error parsing argument str param params add param set input job input query params set input query collection parameter lists sql query parameters specified question marks collection ordered lists subtitute input query set input job job string inpu query collection list object segment params throws io exception job set input format class vertica input format vertica configuration config new vertica configuration job get configuration config set input query inpu query config set input params segment params inherit doc record reader long writable vertica record create record reader input split split task attempt context context throws io exception try return new vertica record reader vertica input split split context get configuration catch exception e e print stack trace return null inherit doc list input split get splits job context context throws io exception return vertica util get splits context
1103	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaInputSplit.java	unrelated	package org apache hadoop vertica input split reading data vertica vertica input split extends input split implements writable log log log factory get log vertica input split prepared statement stmt null connection connection null vertica configuration vtconfig null string input query null list object segment params null start end inherit doc vertica input split log trace input split default constructor set input query list parameters substitute evaluating query sql query run list parameters substitute query logical starting record number logical ending record number vertica input split string input query list object segment params start end log trace input split constructor query params input query input query segment params segment params start start end end inherit doc configure configuration conf throws exception log trace input split configured vtconfig new vertica configuration conf connection vtconfig get connection false connection set auto commit true connection set transaction isolation connection transaction read committed return parameters used input query list object get segment params return segment params run query executed returns input mapper result set execute query throws exception log trace input split execute query length get length length input query select from input query limited limit offset connection null throw new exception cannot execute query connection stmt connection prepare statement input query segment params null object param segment params stmt set object param length stmt set long length stmt set long start result set rs stmt execute query return rs inherit doc close throws sql exception stmt close get start return start get end return end get length throws io exception todo figureout return length start end return end start inherit doc string get locations throws io exception return new string inherit doc configuration get configuration return vtconfig get configuration inherit doc read fields data input throws io exception input query text read string param count read long param count vertica record record new vertica record record read fields segment params record get values start read long end read long inherit doc write data output throws io exception text write string input query segment params null segment params size write long segment params size vertica record record new vertica record segment params true record write else write long write long start write long end
1104	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaOutputFormat.java	unrelated	package org apache hadoop vertica output formatter loading reducer output vertica vertica output format extends output format text vertica record string delimiter vertica configuration delimiter string terminator vertica configuration record terminater set output table set output job job string table name set output job table name false set output table whether drop loading set output job job string table name boolean drop table set output job table name drop table string null set output table whether drop loading create table specification exist list column definitions foo bar varchar set output job job string table name boolean drop table string table def vertica configuration vtconfig new vertica configuration job get configuration vtconfig set output table name table name vtconfig set output table def table def vtconfig set drop table drop table todo handle collection output tables vertica table inherit doc check output specs job context context throws io exception vertica util check output specs context get configuration vertica configuration vtconfig new vertica configuration context get configuration delimiter vtconfig get output delimiter terminator vtconfig get output record terminator test check specs connect db true testing check output specs job context context boolean test throws io exception vertica util check output specs context get configuration vertica configuration vtconfig new vertica configuration context get configuration delimiter vtconfig get output delimiter terminator vtconfig get output record terminator inherit doc record writer text vertica record get record writer task attempt context context throws io exception vertica configuration config new vertica configuration context get configuration string name context get job name todo use explicit date formats string table config get output table name string copy stmt copy table from stdin delimiter delimiter record terminator terminator stream name name direct try connection conn config get connection true return new vertica record writer conn copy stmt table delimiter terminator catch exception e throw new io exception e vertica record get value configuration conf throws exception vertica configuration config new vertica configuration conf string table config get output table name connection conn config get connection true return new vertica record writer conn table config get output delimiter config get output record terminator get value optionally called end job optimize newly created loaded tables useful new tables k records optimize configuration conf throws exception vertica configuration vtconfig new vertica configuration conf connection conn vtconfig get connection true todo consider tables skip tables non temp projections string table name vtconfig get output table name statement stmt conn create statement result set rs null string buffer design tables new string buffer table name hash set string tables with temp new hash set string fully qualify table name defaults table table name index of table name table name add single output table tables with temp add table name map table name set projection names hash map string collection string table proj new hash map string collection string rs stmt execute query select schemaname anchortablename projname vt projection rs next string ptable rs get string rs get string table proj contains key ptable table proj put ptable new hash set string table proj get ptable add rs
1105	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecord.java	unrelated	package org apache hadoop vertica serializable record records returned written vertica vertica record implements writable result set results null result set meta data meta null columns list integer types null list object values null list string names null boolean date string string delimiter vertica configuration delimiter string terminator vertica configuration record terminater date format datefmt new simple date format yyyy m mdd date format timefmt new simple date format h hmmss date format tmstmpfmt new simple date format yyyy m mdd h hmmss date format sqlfmt new simple date format yyyy mm dd hh mm ss list object get values return values list integer get types return types create new vertica record query result set result set returned running input split query true dates marshaled strings vertica record result set results boolean date string throws sql exception results results date string date string meta results get meta data columns meta get column count names new array list string columns types new array list integer columns values new array list object columns columns names add meta get catalog name types add meta get column type values add null vertica record types new array list integer values new array list object vertica record list string names list integer types names names types types values new array list object suppress warnings unused integer type types values add null columns values size vertica record list object values boolean parse types types new array list integer values values columns values size object types test junit tests require database vertica record list string names list integer types list object values boolean date string names names types types values values date string date string columns types size types size object types object get string name throws exception names null names size throw new exception cannot set record name names initialized names index of name return get object get values size throw new index out of bounds exception index greater input size values size return values get set string name object value throws exception names null names size throw new exception cannot set record name names initialized names index of name set value set value indexed set integer object value set value false set value indexed set integer object value boolean validate values size throw new index out of bounds exception index greater input size values size validate value null integer type types get switch type case types bigint value instanceof long value instanceof integer value instanceof short value instanceof long writable value instanceof v long writable value instanceof v int writable throw new class cast exception cannot cast value get class get name long break case types integer value instanceof integer value instanceof short value instanceof v int writable throw new class cast exception cannot cast value get class get name integer break case types tinyint case types smallint value instanceof short throw new class cast exception cannot cast value get class get name short break case types real case types decimal case types numeric value instanceof big decimal throw new class cast exception cannot cast value get class
1106	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecordReader.java	unrelated	package org apache hadoop vertica vertica record reader extends record reader long writable vertica record result set results null start pos length vertica input split split null long writable key null vertica record value null vertica record reader vertica input split split configuration job throws exception run query segment split split split configure job start split get start length split get length results split execute query initialize input split split task attempt context context throws io exception interrupted exception key new long writable try pos value new vertica record results false catch sql exception e e print stack trace throw new io exception e inherit doc close throws io exception try split close catch sql exception e e print stack trace throw new io exception e get pos throws io exception return pos get progress throws io exception todo figure length would length return return pos length boolean next long writable key vertica record value throws io exception key set pos start pos try return value next catch sql exception e throw new io exception e long writable get current key throws io exception interrupted exception return key vertica record get current value throws io exception interrupted exception return value boolean next key value throws io exception interrupted exception key set pos start pos try return value next catch sql exception e throw new io exception e
1107	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaRecordWriter.java	unrelated	package org apache hadoop vertica vertica record writer extends record writer text vertica record string writer table null connection connection null statement statement null com vertica pg statement string copy stmt null string delimiter vertica configuration delimiter string terminator vertica configuration record terminater methods com vertica pg statement method start copy in null method finish copy in null method add stream to copy in null vertica record writer connection connection string copy stmt string writer table string delimiter string terminator connection connection copy stmt copy stmt writer table writer table delimiter delimiter terminator terminator try start copy in class name com vertica pg statement get method start copy in string input stream finish copy in class name com vertica pg statement get method finish copy in add stream to copy in class name com vertica pg statement get method add stream to copy in input stream catch exception e throw new runtime exception vertica formatter requies vertica jdbc driver vertica record get value throws sql exception database meta data dbmd connection get meta data string schema null string table null string schema table writer table split schema table length schema schema table table schema table else schema table length table schema table else throw new runtime exception vertica formatter requires value output table list integer types new array list integer list string names new array list string result set rs dbmd get columns null schema table null rs next names add rs get string types add rs get int vertica record record new vertica record names types return record close task attempt context context throws io exception try statement null finish copy in invoke statement statement finish copy in catch exception e throw new io exception e write text table vertica record record throws io exception table string equals writer table throw new io exception writing different table table string expecting writer table string str record record sql string delimiter terminator byte array input stream bais new byte array input stream str record get bytes try statement null statement connection create statement start copy in invoke statement copy stmt bais statement start copy in copy stmt bais else add stream to copy in invoke statement bais statement add stream to copy in bais catch exception e throw new io exception e
1108	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingInput.java	unrelated	package org apache hadoop vertica vertica streaming input extends input format text text record reader text text create record reader input split split task attempt context context throws io exception try return new vertica streaming record reader vertica input split split context get configuration catch exception e throw new io exception e list input split get splits job context context throws io exception return vertica util get splits context
1109	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingOutput.java	unrelated	package org apache hadoop vertica vertica streaming output extends output format text text log log log factory get log vertica streaming output string delimiter vertica configuration delimiter string terminator vertica configuration record terminater check output specs job context context throws io exception configuration conf context get configuration vertica util check output specs conf vertica configuration vtconfig new vertica configuration conf delimiter vtconfig get output delimiter terminator vtconfig get output record terminator log debug vertica output using delimiter delimiter terminator terminator record writer text text get record writer task attempt context context throws io exception configuration conf context get configuration vertica configuration vtconfig new vertica configuration conf string name context get job name delimiter vtconfig get output delimiter terminator vtconfig get output record terminator todo use explicit date formats string table vtconfig get output table name string copy stmt copy table from stdin delimiter delimiter record terminator terminator stream name name direct try connection conn vtconfig get connection true return new vertica streaming record writer conn copy stmt table catch exception e throw new io exception e output committer get output committer task attempt context context throws io exception interrupted exception return new file output committer file output format get output path context context
1110	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingRecordReader.java	unrelated	package org apache hadoop vertica vertica streaming record reader extends record reader text text result set results null vertica record internal record null start pos length vertica input split split null string delimiter vertica configuration delimiter string terminator vertica configuration record terminater text key new text text value new text vertica streaming record reader vertica input split split configuration conf throws exception run query segment split split split configure conf start split get start length split get length results split execute query internal record new vertica record results false vertica configuration vtconfig new vertica configuration conf delimiter vtconfig get input delimiter terminator vtconfig get input record terminator inherit doc initialize input split split task attempt context context throws io exception interrupted exception nothing inherit doc close throws io exception try split close catch sql exception e e print stack trace throw new io exception e get pos throws io exception return pos get progress throws io exception todo figure length would length return return pos length inherit doc text get current key throws io exception interrupted exception return key inherit doc text get current value throws io exception interrupted exception return value inherit doc boolean next key value throws io exception key set new long pos start string pos try internal record next value set internal record sql string delimiter terminator return true catch sql exception e e print stack trace return false
1111	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaStreamingRecordWriter.java	unrelated	package org apache hadoop vertica vertica streaming record writer extends record writer text text log log log factory get log vertica streaming record writer string writer table null connection connection null statement statement null com vertica pg statement string copy stmt null methods com vertica pg statement method start copy in null method finish copy in null method add stream to copy in null vertica streaming record writer connection connection string copy stmt string writer table connection connection copy stmt copy stmt writer table writer table try start copy in class name com vertica pg statement get method start copy in string byte array input stream finish copy in class name com vertica pg statement get method finish copy in add stream to copy in class name com vertica pg statement get method add stream to copy in byte array input stream catch exception ee throw new runtime exception vertica formatter requies vertica jdbc driver close task attempt context context throws io exception try statement null finish copy in invoke statement statement finish copy in catch exception e throw new io exception e write text table text record throws io exception table string equals writer table throw new io exception writing different table table string expecting writer table log debug enabled log debug writing record string byte array input stream bais new byte array input stream record get bytes try statement null statement connection create statement start copy in invoke statement copy stmt bais statement start copy in copy stmt bais else add stream to copy in invoke statement bais statement add stream to copy in bais catch exception e throw new io exception e
1112	mapreduce\src\contrib\vertica\src\java\org\apache\hadoop\vertica\VerticaUtil.java	unrelated	package org apache hadoop vertica vertica util log log log factory get log vertica util vertica version configuration conf boolean output throws io exception ver try vertica configuration vtconfig new vertica configuration conf connection conn vtconfig get connection output database meta data dbmd conn get meta data ver dbmd get database major version ver dbmd get database minor version catch class not found exception e throw new io exception vertica driver required use vertica input output formatters catch sql exception e throw new io exception e return ver check output specs configuration conf throws io exception vertica configuration vtconfig new vertica configuration conf string writer table vtconfig get output table name writer table null throw new io exception vertica output requires table name defined vertica configuration output table name prop string def vtconfig get output table def boolean drop table vtconfig get drop table string schema null string table null string schema table writer table split schema table length schema schema table table schema table else table schema table statement stmt null try connection conn vtconfig get connection true database meta data dbmd conn get meta data result set rs dbmd get tables null schema table null boolean table exists rs next stmt conn create statement table exists drop table vertica version conf true stmt conn create statement stmt execute truncate table writer table else version drop table exists def empty grab columns first redfine table def null rs dbmd get columns null schema table null array list string defs new array list string rs next defs add rs get string rs get string def defs array new string stmt conn create statement stmt execute drop table writer table cascade table exists false force create create table exist table exists def null throw new runtime exception table writer table exist table definition provided schema null rs dbmd get schemas null schema rs next stmt execute create schema schema string buffer tabledef new string buffer create table append writer table append string column def tabledef append column append tabledef replace tabledef length tabledef length stmt execute tabledef string todo create segmented projections stmt execute select implement temp design writer table catch exception e throw new runtime exception e finally stmt null try stmt close catch sql exception e throw new runtime exception e todo catch params required missing todo better error message count query bad list input split get splits job context context throws io exception configuration conf context get configuration num splits conf get int mapreduce job maps log debug creating splits num splits list input split splits new array list input split start end boolean limit offset true this fancy part mapping inputs figure splits get params query params vertica configuration config new vertica configuration conf string input query config get input query input query null throw new io exception vertica input requires query defined vertica configuration query prop string params query config get params query collection list object params config get input parameters todo limit needs order unique key todo parameters numsplits prep count wrapper query populate bind params string
1113	mapreduce\src\examples\org\apache\hadoop\examples\AggregateWordCount.java	unrelated	package org apache hadoop examples this example aggregated hadoop map reduce application it reads text input files breaks line words counts the output locally sorted list words count often occurred to run bin hadoop jar hadoop examples jar aggregatewordcount dir dir num of reducers textinputformat aggregate word count word count plug in class extends value aggregator base descriptor array list entry text text generate key val pairs object key object val string count type long value sum array list entry text text retv new array list entry text text string line val string string tokenizer itr new string tokenizer line itr more tokens entry text text e generate entry count type itr next token one e null retv add e return retv the main driver word count map reduce program invoke method submit map reduce job when communication problems job tracker main string args throws io exception interrupted exception class not found exception job job value aggregator job create value aggregator job args new class word count plug in class job set jar by class aggregate word count ret job wait for completion true system exit ret
1114	mapreduce\src\examples\org\apache\hadoop\examples\AggregateWordHistogram.java	unrelated	package org apache hadoop examples this example aggregated hadoop map reduce application computes histogram words input texts to run bin hadoop jar hadoop examples jar aggregatewordhist dir dir num of reducers textinputformat aggregate word histogram aggregate word histogram plugin extends value aggregator base descriptor parse given value generate aggregation id value pair per word the id type value histogram word histogram real id the value word array list entry text text generate key val pairs object key object val string words val string split array list entry text text retv new array list entry text text words length text val count new text words entry text text en generate entry value histogram word histogram val count retv add en return retv the main driver word count map reduce program invoke method submit map reduce job when communication problems job tracker main string args throws io exception interrupted exception class not found exception job job value aggregator job create value aggregator job args new class aggregate word histogram plugin job set jar by class aggregate word count ret job wait for completion true system exit ret
1115	mapreduce\src\examples\org\apache\hadoop\examples\BaileyBorweinPlouffe.java	unrelated	package org apache hadoop examples a map reduce program uses bailey borwein plouffe compute exact digits pi this program able calculate digit positions lower certain limit roughly if limit exceeded corresponding results may incorrect due overflow errors for computing higher bits pi consider using distbbp reference david h bailey peter b borwein simon plouffe on rapid computation various polylogarithmic constants math comp bailey borwein plouffe extends configured implements tool string description a map reduce program uses bailey borwein plouffe compute exact digits pi string name mapreduce bailey borwein plouffe get simple name custom job properties string working dir property name dir string hex file property name hex file string digit start property name digit start string digit size property name digit size string digit parts property name digit parts log log log factory get log bailey borwein plouffe mapper computing digits pi bbp mapper extends mapper long writable int writable long writable bytes writable compute offset th offset length th digits protected map long writable offset int writable length context context throws io exception interrupted exception log info offset offset length length compute digits byte bytes new byte length get offset get bytes length digits hex digits bytes byte digits bytes byte digits output map results context write offset new bytes writable bytes reducer concatenating map outputs bbp reducer extends reducer long writable bytes writable long writable bytes writable storing hex digits list byte hex new array list byte concatenate map outputs protected reduce long writable offset iterable bytes writable values context context throws io exception interrupted exception read map outputs bytes writable bytes values bytes get length hex add bytes get bytes log info hex size hex size write output files protected cleanup context context throws io exception interrupted exception configuration conf context get configuration path dir new path conf get working dir property file system fs dir get file system conf write hex output path hexfile new path conf get hex file property output stream new buffered output stream fs create hexfile try byte b hex write b finally close if starting digit hex value converted decimal value conf get int digit start property path outfile new path dir pi txt log info writing text output outfile output stream outputstream fs create outfile try print stream new print stream outputstream true write hex text print hex iterator pi x x println total number hexadecimal digits hex size write decimal text fraction dec new fraction hex dec digits hex size todo conservative print new iterator integer boolean next return dec digits integer next return dec times remove pi println total number decimal digits dec digits finally outputstream close print elements nice format t print print stream iterator t iterator string prefix string format elements per group groups per line string builder sb new string builder n prefix length sb append string spaces sb string print n prefix iterator next elements per group print elements per group groups per line spaces print string format format iterator next println input split link bbp input format bbp split extends input split implements writable string
1116	mapreduce\src\examples\org\apache\hadoop\examples\DBCountPageView.java	unrelated	package org apache hadoop examples this demonstrative program uses db input format reading input data database db output format writing data database br the program first creates necessary tables populates input table runs mapred job br the input data mini access log code lt url referrer time gt code schema the output number pageviews url log schema code lt url pageview gt code when called arguments program starts local hsqldb server uses database storing retrieving data db count page view extends configured implements tool log log log factory get log db count page view connection connection boolean initialized false string access field names url referrer time string pageview field names url pageview string db url jdbc hsqldb hsql localhost url access string driver class org hsqldb jdbc driver server server start hsqldb server server new server server set database path system get property test build data tmp url access server set database name url access server start create connection string driver class name string url throws exception class name driver class name connection driver manager get connection url connection set auto commit false shutdown try connection commit connection close catch throwable ex log warn exception occurred closing connection string utils stringify exception ex finally try server null server shutdown catch throwable ex log warn exception occurred shutting hsqldb string utils stringify exception ex initialize string driver class name string url throws exception initialized driver class name equals driver class start hsqldb server create connection driver class name url drop tables create tables populate access initialized true drop tables string drop access drop table access string drop pageview drop table pageview statement st null try st connection create statement st execute update drop access st execute update drop pageview connection commit st close catch sql exception ex try st null st close catch exception e create tables throws sql exception string create access create table access url varchar not null referrer varchar time bigint not null primary key url time string create pageview create table pageview url varchar not null pageview bigint not null primary key url statement st connection create statement try st execute update create access st execute update create pageview connection commit finally st close populates access table generated records populate access throws sql exception prepared statement statement null try statement connection prepare statement insert into access url referrer time values random random new random time random next int probability precision new page probability pages site string pages b c e f g j link matrix array pages indexes page links link matrix mini model user browsing la pagerank current page random next int pages length string referrer null time statement set string pages current page statement set string referrer statement set long statement execute action random next int probability precision go new page probability new page probability probability precision action new page probability current page random next int pages length random page referrer null else referrer pages current page action random next int link matrix current page length current page link matrix current page action connection commit catch sql
1117	mapreduce\src\examples\org\apache\hadoop\examples\ExampleDriver.java	unrelated	package org apache hadoop examples a description example program based human readable description example driver main string argv exit code program driver pgd new program driver try pgd add class wordcount word count a map reduce program counts words input files pgd add class aggregatewordcount aggregate word count an aggregate based map reduce program counts words input files pgd add class aggregatewordhist aggregate word histogram an aggregate based map reduce program computes histogram words input files pgd add class grep grep a map reduce program counts matches regex input pgd add class randomwriter random writer a map reduce program writes gb random data per node pgd add class randomtextwriter random text writer a map reduce program writes gb random textual data per node pgd add class sort sort a map reduce program sorts data written random writer pgd add class pi quasi monte carlo quasi monte carlo description pgd add class bbp bailey borwein plouffe bailey borwein plouffe description pgd add class distbbp dist bbp dist bbp description pgd add class pentomino distributed pentomino a map reduce tile laying program find solutions pentomino problems pgd add class secondarysort secondary sort an example defining secondary sort reduce pgd add class sudoku sudoku a sudoku solver pgd add class join join a job effects join sorted equally partitioned datasets pgd add class multifilewc multi file word count a job counts words several files pgd add class dbcount db count page view an example job count pageview counts database pgd add class teragen tera gen generate data terasort pgd add class terasort tera sort run terasort pgd add class teravalidate tera validate checking results terasort exit code pgd driver argv catch throwable e e print stack trace system exit exit code
1118	mapreduce\src\examples\org\apache\hadoop\examples\Grep.java	unrelated	package org apache hadoop examples extracts matching regexs input files counts grep extends configured implements tool grep singleton run string args throws exception args length system println grep dir dir regex group tool runner print generic command usage system return path temp dir new path grep temp integer string new random next int integer max value configuration conf get conf conf set regex mapper pattern args args length conf set regex mapper group args job grep job new job conf try grep job set job name grep search file input format set input paths grep job args grep job set mapper class regex mapper grep job set combiner class long sum reducer grep job set reducer class long sum reducer file output format set output path grep job temp dir grep job set output format class sequence file output format grep job set output key class text grep job set output value class long writable grep job wait for completion true job sort job new job conf sort job set job name grep sort file input format set input paths sort job temp dir sort job set input format class sequence file input format sort job set mapper class inverse mapper sort job set num reduce tasks write single file file output format set output path sort job new path args sort job set sort comparator class sort decreasing freq long writable decreasing comparator sort job wait for completion true finally file system get conf delete temp dir true return main string args throws exception res tool runner run new configuration new grep args system exit res
1119	mapreduce\src\examples\org\apache\hadoop\examples\Join.java	unrelated	package org apache hadoop examples given set sorted datasets keyed yielding equal partitions possible effect join datasets prior map the example facilitates to run bin hadoop jar build hadoop examples jar join r reduces format input format format output format key output key value output value join op lt inner outer gt dir dir dir join extends configured implements tool string reduces per host mapreduce join reduces per host print usage system println join r reduces format input format format output format key output key value output value join op inner outer input input output tool runner print generic command usage system return the main driver sort program invoke method submit map reduce job job tracker run string args throws exception configuration conf get conf job client client new job client conf cluster status cluster client get cluster status num reduces cluster get max reduce tasks string join reduces conf get reduces per host join reduces null num reduces cluster get task trackers integer parse int join reduces job job new job conf job set job name join job set jar by class sort job set mapper class mapper job set reducer class reducer class extends input format input format class sequence file input format class extends output format output format class sequence file output format class extends writable comparable output key class bytes writable class extends writable output value class tuple writable string op inner list string args new array list string args length try r equals args num reduces integer parse int args else format equals args input format class class name args subclass input format else format equals args output format class class name args subclass output format else key equals args output key class class name args subclass writable comparable else value equals args output value class class name args subclass writable else join op equals args op args else args add args catch number format exception except system println error integer expected instead args return print usage catch array index out of bounds exception except system println error required parameter missing args return print usage exits set user supplied possibly default job configs job set num reduce tasks num reduces args size system println error wrong number parameters return print usage file output format set output path job new path args remove args size list path plist new array list path args size string args plist add new path job set input format class composite input format job get configuration set composite input format join expr composite input format compose op input format class plist array new path job set output format class output format class job set output key class output key class job set output value class output value class date start time new date system println job started start time ret job wait for completion true date end time new date system println job ended end time system println the job took end time get time start time get time seconds return ret main string args throws exception res tool runner run new configuration
1120	mapreduce\src\examples\org\apache\hadoop\examples\MultiFileWordCount.java	unrelated	package org apache hadoop examples multi file word count example demonstrate usage multi file input format this examples counts occurrences words text files given input directory multi file word count extends configured implements tool this record keeps lt filename offset gt pairs word offset implements writable comparable offset string file name read fields data input throws io exception offset read long file name text read string write data output throws io exception write long offset text write string file name compare to object word offset word offset f file name compare to file name f return math signum offset offset return f boolean equals object obj obj instanceof word offset return compare to obj return false hash code assert false hash code designed return arbitrary constant to use link combine file input format one extend return custom link record reader combine file input format uses link combine file split my input format extends combine file input format word offset text record reader word offset text create record reader input split split task attempt context context throws io exception return new combine file record reader word offset text combine file split split context combine file line record reader record reader responsible extracting records chunk combine file split combine file line record reader extends record reader word offset text start offset offset chunk end end chunk pos current pos file system fs path path word offset key text value fs data input stream file in line reader reader combine file line record reader combine file split split task attempt context context integer index throws io exception fs file system get context get configuration path split get path index start offset split get offset index end start offset split get length index boolean skip first line false open file file in fs open path start offset skip first line true start offset file in seek start offset reader new line reader file in skip first line skip first line establish start offset start offset reader read line new text math min integer max value end start offset pos start offset initialize input split split task attempt context context throws io exception interrupted exception close throws io exception get progress throws io exception start offset end return f else return math min f pos start offset end start offset boolean next key value throws io exception key null key new word offset key file name path get name key offset pos value null value new text new size pos end new size reader read line value pos new size new size key null value null return false else return true word offset get current key throws io exception interrupted exception return key text get current value throws io exception interrupted exception return value this mapper similar one link word count map class map class extends mapper word offset text text int writable int writable one new int writable text word new text map word offset key text value context context throws io exception interrupted exception string line value string string tokenizer itr new string tokenizer line
1121	mapreduce\src\examples\org\apache\hadoop\examples\QuasiMonteCarlo.java	unrelated	package org apache hadoop examples a map reduce program estimates value pi using quasi monte carlo q mc method arbitrary integrals approximated numerically q mc methods in example use q mc method approximate integral i s f x dx s unit square x x x dimensional point f function describing inscribed circle square s f x x x f x otherwise it easy see pi equal i so approximation pi obtained i evaluated numerically there better methods computing pi we emphasize numerical approximation arbitrary integrals example for computing many digits pi consider using bbp the implementation discussed mapper generate points unit square count points inside outside inscribed circle square reducer accumulate points inside outside results mappers let num total num inside num outside the fraction num inside num total rational approximation value area circle area square i area inscribed circle pi area unit square finally estimated value pi num inside num total quasi monte carlo extends configured implements tool string description a map reduce program estimates pi using quasi monte carlo method tmp directory input output path tmp dir new path quasi monte carlo get simple name tmp dimensional halton sequence h h dimensional point index halton sequence used generate sample points pi estimation halton sequence bases p maximum number digits allowed k index x q initialize h startindex sequence begins h startindex halton sequence startindex index startindex x new k length q new k length new k length k length q new k new k k length k index x j j k j q j j q j p j k p k k j p x j q j compute next point assume current point h index compute h index next point index k length j j k j j x q j j p break j x j q j return x mapper pi estimation generate points unit square count points inside outside inscribed circle square qmc mapper extends mapper long writable long writable boolean writable long writable map method map long writable offset long writable size context context throws io exception interrupted exception halton sequence haltonsequence new halton sequence offset get num inside l num outside l size get generate points unit square point haltonsequence next point count points inside outside inscribed circle square x point point x x num outside else num inside report status context set status generated samples output map results context write new boolean writable true new long writable num inside context write new boolean writable false new long writable num outside reducer pi estimation accumulate points inside outside results mappers qmc reducer extends reducer boolean writable long writable writable comparable writable num inside num outside accumulate number points inside outside results mappers reduce boolean writable inside iterable long writable values context context throws io exception interrupted exception inside get long writable val values num inside val get else long writable val values num outside val get reduce task done write output file cleanup context context throws io exception write output file path dir new path tmp dir path file new path dir
1122	mapreduce\src\examples\org\apache\hadoop\examples\RandomTextWriter.java	unrelated	package org apache hadoop examples this program uses map reduce run distributed job interaction tasks task writes large unsorted random sequence words in order program generate data terasort words per key words per value following config xmp xml version xml stylesheet type text xsl href configuration xsl configuration property name mapreduce randomtextwriter minwordskey name value value property property name mapreduce randomtextwriter maxwordskey name value value property property name mapreduce randomtextwriter minwordsvalue name value value property property name mapreduce randomtextwriter maxwordsvalue name value value property property name mapreduce randomtextwriter totalbytes name value value property configuration xmp equivalently link random text writer also supports options ones supported link tool via command line to run bin hadoop jar hadoop version examples jar randomtextwriter format output format output random text writer extends configured implements tool string total bytes mapreduce randomtextwriter totalbytes string bytes per map mapreduce randomtextwriter bytespermap string maps per host mapreduce randomtextwriter mapsperhost string max value mapreduce randomtextwriter maxwordsvalue string min value mapreduce randomtextwriter minwordsvalue string min key mapreduce randomtextwriter minwordskey string max key mapreduce randomtextwriter maxwordskey print usage system println randomtextwriter format output format output tool runner print generic command usage system return user counters enum counters records written bytes written random text mapper extends mapper text text text text num bytes to write min words in key words in key range min words in value words in value range random random new random save configuration value need write data setup context context configuration conf context get configuration num bytes to write conf get long bytes per map min words in key conf get int min key words in key range conf get int max key min words in key min words in value conf get int min value words in value range conf get int max value min words in value given output filename write bunch random records map text key text value context context throws io exception interrupted exception item count num bytes to write generate key value words key min words in key words in key range random next int words in key range words value min words in value words in value range random next int words in value range text key words generate sentence words key text value words generate sentence words value write sentence context write key words value words num bytes to write key words get length value words get length update counters progress etc context get counter counters bytes written increment key words get length value words get length context get counter counters records written increment item count context set status wrote record item count num bytes to write bytes left context set status done item count records text generate sentence words string buffer sentence new string buffer string space words sentence append words random next int words length sentence append space return new text sentence string this main routine launching distributed random write job it runs maps node node writes gig data dfs file the reduce anything run string args throws exception args length return print usage configuration conf get conf job client client
1123	mapreduce\src\examples\org\apache\hadoop\examples\RandomWriter.java	unrelated	package org apache hadoop examples this program uses map reduce run distributed job interaction tasks task write large unsorted random binary sequence file bytes writable in order program generate data terasort byte keys byte values following config xmp xml version xml stylesheet type text xsl href configuration xsl configuration property name mapreduce randomwriter minkey name value value property property name mapreduce randomwriter maxkey name value value property property name mapreduce randomwriter minvalue name value value property property name mapreduce randomwriter maxvalue name value value property property name mapreduce randomwriter totalbytes name value value property configuration xmp equivalently link random writer also supports options ones supported link generic options parser via command line random writer extends configured implements tool string total bytes mapreduce randomwriter totalbytes string bytes per map mapreduce randomwriter bytespermap string maps per host mapreduce randomwriter mapsperhost string max value mapreduce randomwriter maxvalue string min value mapreduce randomwriter minvalue string min key mapreduce randomwriter minkey string max key mapreduce randomwriter maxkey user counters enum counters records written bytes written a custom input format creates virtual inputs single map random input format extends input format text text generate requested number file splits filename set filename output file list input split get splits job context job throws io exception list input split result new array list input split path dir file output format get output path job num splits job get configuration get int mr job config num maps num splits result add new file split new path dir dummy split string null return result return single record filename filename taken file split random record reader extends record reader text text path name text key null text value new text random record reader path p name p initialize input split split task attempt context context throws io exception interrupted exception boolean next key value name null key new text key set name get name name null return true return false text get current key return key text get current value return value close get progress return f record reader text text create record reader input split split task attempt context context throws io exception interrupted exception return new random record reader file split split get path random mapper extends mapper writable comparable writable bytes writable bytes writable num bytes to write min key size key size range min value size value size range random random new random bytes writable random key new bytes writable bytes writable random value new bytes writable randomize bytes byte data offset length offset length offset data byte random next int given output filename write bunch random records map writable comparable key writable value context context throws io exception interrupted exception item count num bytes to write key length min key size key size range random next int key size range random key set size key length randomize bytes random key get bytes random key get length value length min value size value size range random next int value size range random value set size value length randomize bytes random value get bytes random value get length context write
1124	mapreduce\src\examples\org\apache\hadoop\examples\SecondarySort.java	unrelated	package org apache hadoop examples this example hadoop map reduce application it reads text input files must contain two integers per line the output sorted first second number grouped first number to run bin hadoop jar build hadoop examples jar secondarysort dir dir secondary sort define pair integers writable they serialized byte comparable format int pair implements writable comparable int pair first second set left right values set left right first left second right get first return first get second return second read two integers encoded min value min value max value read fields data input throws io exception first read int integer min value second read int integer min value write data output throws io exception write int first integer min value write int second integer min value hash code return first second boolean equals object right right instanceof int pair int pair r int pair right return r first first r second second else return false a comparator compares serialized int pair comparator extends writable comparator comparator super int pair compare byte b byte b return compare bytes b b register comparator writable comparator define int pair new comparator compare to int pair first first return first first else second second return second second else return partition based first part pair first partitioner extends partitioner int pair int writable get partition int pair key int writable value num partitions return math abs key get first num partitions compare first part pair reduce called value first part first grouping comparator implements raw comparator int pair compare byte b byte b return writable comparator compare bytes b integer size b integer size compare int pair int pair get first r get first return r r read two integers line generate key value pair left right right map class extends mapper long writable text int pair int writable int pair key new int pair int writable value new int writable map long writable key text value context context throws io exception interrupted exception string tokenizer itr new string tokenizer value string left right itr more tokens left integer parse int itr next token itr more tokens right integer parse int itr next token key set left right value set right context write key value a reducer emits sum input values reduce extends reducer int pair int writable text int writable text separator new text text first new text reduce int pair key iterable int writable values context context throws io exception interrupted exception context write separator null first set integer string key get first int writable value values context write first value main string args throws exception configuration conf new configuration string args new generic options parser conf args get remaining args args length system err println usage secondarysrot system exit job job new job conf secondary sort job set jar by class secondary sort job set mapper class map class job set reducer class reduce group partition first pair job set partitioner class first partitioner job set grouping comparator class first grouping comparator map output int pair int writable job set map
1125	mapreduce\src\examples\org\apache\hadoop\examples\Sort.java	unrelated	package org apache hadoop examples this trivial map reduce program absolutely nothing use framework fragment sort input values to run bin hadoop jar build hadoop examples jar sort r reduces format input format format output format key output key value output value total order pcnt num samples max splits dir dir sort k v extends configured implements tool string reduces per host mapreduce sort reducesperhost job job null print usage system println sort r reduces format input format format output format key output key value output value total order pcnt num samples max splits input output tool runner print generic command usage system return the main driver sort program invoke method submit map reduce job job tracker run string args throws exception configuration conf get conf job client client new job client conf cluster status cluster client get cluster status num reduces cluster get max reduce tasks string sort reduces conf get reduces per host sort reduces null num reduces cluster get task trackers integer parse int sort reduces class extends input format input format class sequence file input format class extends output format output format class sequence file output format class extends writable comparable output key class bytes writable class extends writable output value class bytes writable list string args new array list string input sampler sampler k v sampler null args length try r equals args num reduces integer parse int args else format equals args input format class class name args subclass input format else format equals args output format class class name args subclass output format else key equals args output key class class name args subclass writable comparable else value equals args output value class class name args subclass writable else total order equals args pcnt double parse double args num samples integer parse int args max splits integer parse int args max splits max splits integer max value sampler new input sampler random sampler k v pcnt num samples max splits else args add args catch number format exception except system println error integer expected instead args return print usage catch array index out of bounds exception except system println error required parameter missing args return print usage exits set user supplied possibly default job configs job new job conf job set job name sorter job set jar by class sort job set mapper class mapper job set reducer class reducer job set num reduce tasks num reduces job set input format class input format class job set output format class output format class job set output key class output key class job set output value class output value class make sure exactly parameters left args size system println error wrong number parameters args size instead return print usage file input format set input paths job args get file output format set output path job new path args get sampler null system println sampling input effect total order sort job set partitioner class total order partitioner path input dir file input format get input paths job input dir input dir make qualified input dir get file
1126	mapreduce\src\examples\org\apache\hadoop\examples\WordCount.java	unrelated	package org apache hadoop examples word count tokenizer mapper extends mapper object text text int writable int writable one new int writable text word new text map object key text value context context throws io exception interrupted exception string tokenizer itr new string tokenizer value string itr more tokens word set itr next token context write word one int sum reducer extends reducer text int writable text int writable int writable result new int writable reduce text key iterable int writable values context context throws io exception interrupted exception sum int writable val values sum val get result set sum context write key result main string args throws exception configuration conf new configuration string args new generic options parser conf args get remaining args args length system err println usage wordcount system exit job job new job conf word count job set jar by class word count job set mapper class tokenizer mapper job set combiner class int sum reducer job set reducer class int sum reducer job set output key class text job set output value class int writable file input format add input path job new path args file output format set output path job new path args system exit job wait for completion true
1127	mapreduce\src\examples\org\apache\hadoop\examples\dancing\DancingLinks.java	unrelated	package org apache hadoop examples dancing a generic solver tile laying problems using knuth dancing link algorithm it provides fast backtracking data structure problems expressed sparse boolean matrix goal select subset rows column exactly true the application gives column name row named set columns true solutions passed back giving selected rows names the type parameter column name application column names dancing links column name log log log factory get log dancing links get name a cell table left right links form doubly linked lists directions it also includes link column head node column name node column name left node column name right node column name node column name column header column name head node node column name node column name r node column name u node column name column header column name left right r u head node null null null null null column headers record name column number rows satisfy column the names provided application anything the size used heuristic picking next column explore column header column name extends node column name column name name size column header column name n name n size head column header null the head table left right head unsatisfied column header objects column header column name head the complete list columns list column header column name columns dancing links head new column header column name null head left head head right head head head head head columns new array list column header column name add column table solutions add column column name name boolean primary column header column name top new column header column name name top top top top primary node column name tail head left tail right top top left tail top right head head left top else top left top top right top columns add top add column table add column column name name add column name true get number columns get number columns return columns size get name given column string get column name index return columns get index name string add row table add row boolean values node column name prev null values length values column header column name top columns get top size node column name bottom top node column name node new node column name null null bottom top top bottom node top node prev null node column name front prev right node left prev node right front prev right node front left node else node left node node right node prev node applications implement receive solutions problems solution acceptor column name a callback return solution application selected row solution list list column name value find column fewest choices column header column name find best column low size integer max value column header column name result null column header column name current column header column name head right current head current size low size low size current size result current current column header column name current right return result hide column table cover column column header column name col log debug cover col head name remove column col right left col left col left right col right node
1128	mapreduce\src\examples\org\apache\hadoop\examples\dancing\DistributedPentomino.java	unrelated	package org apache hadoop examples dancing launch distributed pentomino solver it generates complete list prefixes length n unique prefix separate line a prefix sequence n integers denote index row choosen column order note next column heuristically choosen solver dependant previous choice that file given input map reduce the output key value move prefix solution text text distributed pentomino extends configured implements tool pent depth pent width pent height default maps each map takes line represents prefix move finds solutions start prefix the output prefix key solution value pent map extends mapper writable comparable text text text width height depth pentomino pent text prefix string context context for solution generate prefix representation solution the solution starts newline output looks like prefix solution solution catcher implements dancing links solution acceptor pentomino column name solution list list pentomino column name answer string board pentomino stringify solution width height answer try context write prefix string new text n board context get counter pent get category answer increment catch io exception e system err println string utils stringify exception e catch interrupted exception ie system err println string utils stringify exception ie break prefix moves sequence integer row ids selected column order find solutions prefix map writable comparable key text value context context throws io exception prefix string value string tokenizer itr new string tokenizer prefix string string prefix new depth idx itr more tokens string num itr next token prefix idx integer parse int num pent solve prefix setup context context context context configuration conf context get configuration depth conf get int pentomino depth pent depth width conf get int pentomino width pent width height conf get int pentomino height pent height pent pentomino reflection utils new instance conf get class pentomino class one sided pentomino conf pent initialize width height pent set printer new solution catcher create input file possible combinations given depth create input directory file system fs path dir pentomino pent depth throws io exception fs mkdirs dir list splits pent get splits depth path input new path dir part print stream file new print stream new buffered output stream fs create input prefix splits prefix length file print file print prefix file print n file close return fs get file status input get len launch solver x board one sided pentominos this takes hours nodes cpus node splits job maps reduce main string args throws exception res tool runner run new configuration new distributed pentomino args system exit res run string args throws exception args length system println pentomino output tool runner print generic command usage system return configuration conf get conf width conf get int pentomino width pent width height conf get int pentomino height pent height depth conf get int pentomino depth pent depth class extends pentomino pent class conf get class pentomino class one sided pentomino pentomino num maps conf get int mr job config num maps default maps path output new path args path input new path output input file system file sys file system get conf try job job new job conf file input format set input
1129	mapreduce\src\examples\org\apache\hadoop\examples\dancing\OneSidedPentomino.java	unrelated	package org apache hadoop examples dancing of normal pentominos distinct shapes flipped this includes variants flippable shapes unflippable shapes total pieces clearly boards must boxes hold solutions one sided pentomino extends pentomino one sided pentomino one sided pentomino width height super width height define one sided pieces the flipped pieces name capital letter protected initialize pieces pieces add new piece x x xxx x false one rotation pieces add new piece v x x xxx false four rotations pieces add new piece xxx x x false four rotations pieces add new piece w x xx xx false four rotations pieces add new piece u x x xxx false four rotations pieces add new piece xxxxx false two rotations pieces add new piece f xx xx x false four rotations pieces add new piece p xx xx x false four rotations pieces add new piece z xx x xx false two rotations pieces add new piece n xx xxx false four rotations pieces add new piece x xxxx false four rotations pieces add new piece x xxxx false four rotations pieces add new piece f xx xx x false four rotations pieces add new piece p xx xx x false four rotations pieces add new piece z xx x xx false two rotations pieces add new piece n xx xxx false four rotations pieces add new piece y x xxxx false four rotations pieces add new piece l x xxxx false four rotations solve x puzzle main string args pentomino model new one sided pentomino solutions model solve system println solutions solutions found
1130	mapreduce\src\examples\org\apache\hadoop\examples\dancing\Pentomino.java	unrelated	package org apache hadoop examples dancing pentomino string depth mapreduce pentomino depth string width mapreduce pentomino width string height mapreduce pentomino height string class mapreduce pentomino this marker types i expect get back column names protected column name nothing maintain information puzzle piece protected piece implements column name string name boolean shape rotations boolean flippable piece string name string shape boolean flippable rotations name name rotations rotations flippable flippable string tokenizer parser new string tokenizer shape list boolean lines new array list boolean parser more tokens string token parser next token boolean line new boolean token length line length line token char at x lines add line shape new boolean lines size lines size shape lines get string get name return name get rotations return rotations boolean get flippable return flippable flip boolean flip x max flip return max x else return x boolean get shape boolean flip rotate boolean result rotate height shape length width shape length result new boolean height boolean flip x rotate boolean flip y flip rotate height result new boolean width x x width x result x shape flip flip y height flip flip x x width else height shape length width shape length result new boolean height boolean flip x rotate boolean flip y flip rotate height result new boolean width x x width x result x shape flip flip x x width flip flip y height return result a point puzzle board this represents placement piece given point board point implements column name x point x x x convert solution puzzle returned model represents placement pieces onto board string stringify solution width height list list column name solution string picture new string height width string buffer result new string buffer piece placement list column name row solution go find piece placed piece piece null column name item row item instanceof piece piece piece item break point piece placed mark piece name column name item row item instanceof point point p point item picture p p x piece get name put together picture length x x picture length x result append picture x result append n return result string enum solution category upper left mid x mid y center find whether solution x upper left quadrant x midline midline center solution category get category list list column name names piece x piece null find x piece piece p pieces x equals p name x piece p break find row containing x list column name row names row contains x piece figure x located low x width high x low height high column name col row col instanceof point x point col x point col x low x low x x x high x high x x low low high high boolean mid x low x high x width boolean mid low high height mid x mid return solution category center else mid x return solution category mid x else mid return solution category mid y break return solution category upper left a solution printer writes solution stdout solution printer implements dancing links solution acceptor column name
1131	mapreduce\src\examples\org\apache\hadoop\examples\dancing\Sudoku.java	unrelated	package org apache hadoop examples dancing this uses dancing links algorithm knuth solve sudoku puzzles it solved x puzzles seconds sudoku the preset values board board x value x board the size board size the size sub squares cells across square x size the size sub squares celss square y size this marker columns created sudoku solver protected column name nothing a containing representation solution string stringify solution size list list column name solution picture new size size string buffer result new string buffer go rows selected model build picture solution list column name row solution x num column name item row item instanceof column constraint x column constraint item column num column constraint item num else item instanceof row constraint row constraint item row picture x num build size x x size x result append picture x result append result append n return result string an acceptor get solutions puzzle generated print console solution printer implements dancing links solution acceptor column name size solution printer size size size a debugging aid prints raw information dancing link columns selected row raw write list solution iterator itr solution iterator itr next iterator subitr list itr next iterator subitr next system print subitr next string system println solution list list column name names system println stringify solution size names set puzzle board given size boards may asymmetric squares always divided cells wide tall for example x puzzle make sub squares x cells wide cells tall clearly means board made x sub squares sudoku input stream stream throws io exception buffered reader file new buffered reader new input stream reader stream string line file read line list result new array list line null string tokenizer tokenizer new string tokenizer line size tokenizer count tokens col new size tokenizer more elements string word tokenizer next token equals word col else col integer parse int word result add col line file read line size result size board result array new size square y size math sqrt size square x size size square y size file close a constraint number appear column column constraint implements column name column constraint num column num num column column num column string string return num column column a constraint number appear row row constraint implements column name row constraint num row num num row row num row string string return num row row a constraint number appear square square constraint implements column name square constraint num x num num x x num x string string return num square x a constraint cell used cell constraint implements column name cell constraint x x x x string string return cell x create row places num cell x boolean generate row boolean row values x num clear scratch array row values length row values false find square coordinates x box x square x size box square y size mark column row values x size num true mark row row values size size size num true mark square row values size size x box square x size box size num true mark cell row values size
1132	mapreduce\src\examples\org\apache\hadoop\examples\pi\Combinable.java	unrelated	package org apache hadoop examples pi a combinable object combined objects combinable t extends comparable t combine t combine t
1133	mapreduce\src\examples\org\apache\hadoop\examples\pi\Container.java	unrelated	package org apache hadoop examples pi a container contains element container t t get element
1134	mapreduce\src\examples\org\apache\hadoop\examples\pi\DistBbp.java	unrelated	package org apache hadoop examples pi a map reduce program uses bbp type method compute exact binary digits pi this program designed computing n th bit pi large n say n for computing lower bits pi consider using bbp the actually computation done dist sum jobs the steps launching jobs initialize parameters create list sums read computed values given local directory remove computed values sums partition remaining sums computation jobs submit computation jobs cluster wait results write job outputs given local directory combine job outputs print pi bits the command line format hadoop org apache hadoop examples pi dist bbp b n threads n jobs type n part remote dir local dir and parameters b the number bits skip e compute b th position n threads the number working threads n jobs the number jobs per sum type map side job r reduce side job x mix type n part the number parts per job remote dir remote directory submitting jobs local dir local directory storing output files note may take time finish jobs b large if program killed middle execution command different remote dir used resume execution for example suppose use following command compute th bit pi hadoop org apache hadoop examples pi dist bbp x remote local output it uses threads summit jobs concurrent jobs each sum totally sums partitioned jobs the jobs executed map side reduce side each job parts the remote directory jobs remote local directory storing output local output depends cluster configuration may take many days finish entire execution if execution killed may resume hadoop org apache hadoop examples pi dist bbp x remote b local output dist bbp extends configured implements tool string description a map reduce program uses bbp type formula compute exact bits pi util timer timer new util timer true inherit doc run string args throws exception parse arguments args length dist sum parameters count return util print usage args get class get name b parameters list n b the number bits skip e compute b th position parameters description b util args dist sum parameters parameters dist sum parameters parse args b throw new illegal argument exception b b util print bit skipped b util println parameters util println initialize sums dist sum distsum new dist sum distsum set conf get conf distsum set parameters parameters boolean verbose get conf get boolean parser verbose property false map parameter list task result existings new parser verbose parse parameters local dir get path null parser combine existings list task result tr existings values collections sort tr util println map bellard parameter bellard sum sums bellard get sums b parameters n jobs existings util println execute computations execute distsum sums compute pi sums pi bellard compute pi b sums util print bit skipped b util println util pi pi bellard bit terms b return execute dist sum computations execute dist sum distsum map bellard parameter bellard sum sums throws exception list computation computations new array list computation bellard parameter p bellard parameter values summation sums get p get value null computations add distsum new computation
1135	mapreduce\src\examples\org\apache\hadoop\examples\pi\DistSum.java	unrelated	package org apache hadoop examples pi the main computing sums using map reduce jobs a sum partitioned jobs a job may executed map side reduce side a map side job multiple maps zero reducer a reduce side job one map multiple reducers depending clusters status runtime mix type job may executed either side dist sum extends configured implements tool log log log factory get log dist sum string name dist sum get simple name string n parts mapreduce pi name n parts dist sum job parameters parameters count string list n threads n jobs type n part remote dir local dir string description n n threads the number working threads n n jobs the number jobs per sum n type map side job r reduce side job x mix type n n part the number parts per job n remote dir remote directory submitting jobs n local dir local directory storing output files number worker threads n threads number jobs n jobs number parts per job n parts the machine used computation machine machine the remote job directory string remote dir the local output directory file local dir parameters machine machine n threads n jobs n parts string remote dir file local dir machine machine n threads n threads n jobs n jobs n parts n parts remote dir remote dir local dir local dir inherit doc string string return nn threads n threads nn jobs n jobs nn parts n parts machine nremote dir remote dir nlocal dir local dir parse parameters parameters parse string args args length count throw new illegal argument exception args length count count args length args length args arrays list args n threads integer parse int args n jobs integer parse int args string type args n parts integer parse int args string remote dir args file local dir new file args equals type r equals type x equals type throw new illegal argument exception type type equal r x else n parts throw new illegal argument exception n parts n parts else n jobs throw new illegal argument exception n jobs n jobs else n threads throw new illegal argument exception n threads n threads util check directory local dir return new parameters equals type map side instance r equals type reduce side instance mix machine instance n threads n jobs n parts remote dir local dir abstract machine job execution machine initialize job init job job throws io exception inherit doc string string return get class get simple name compute sigma compute summation sigma task input output context null writable task result context throws io exception interrupted exception string log info sigma sigma context set status start system current time millis sigma compute duration system current time millis start task result result new task result sigma duration log info result result context set status context write null writable get result split summations summation split extends input split implements writable container summation string empty summation sigma summation split summation split summation sigma sigma sigma inherit doc summation get element return sigma inherit doc get length
1136	mapreduce\src\examples\org\apache\hadoop\examples\pi\Parser.java	unrelated	package org apache hadoop examples pi a parsing outputs parser string verbose property pi parser verbose boolean verbose parser boolean verbose verbose verbose println string verbose util println parse line parse line string line map parameter list task result log info line line map entry string task result e dist sum task result line e null list task result sums get parameter get e get key sums null throw new illegal argument exception sums null line line e e sums add e get value parse file directory tree parse file f map parameter list task result sums throws io exception f directory println process directory f file child f list files parse child sums else f get name ends with txt println parse file f map parameter list task result new tree map parameter list task result parameter p parameter values put p new array list task result buffered reader new buffered reader new file reader f try string line line read line null try parse line line catch runtime exception e util err println line line throw e finally close parameter p parameter values list task result combined util combine get p combined empty println p size combined size task result r combined println r sums get p add all get p parse path map parameter list task result parse string f throws io exception map parameter list task result new tree map parameter list task result parameter p parameter values put p new array list task result parse new file f log info string replace n parameter p parameter values put p get p return parse input write results map parameter list task result parse string inputpath string outputdir throws io exception parse input util print n parsing inputpath util flush map parameter list task result parsed parse inputpath util println done write results outputdir null util print n writing outputdir util flush parameter p parameter values list task result results parsed get p collections sort results print writer new print writer new file writer new file outputdir p txt true try results size println dist sum task result p results get finally close util println done return parsed combine results t extends combinable t map parameter t combine map parameter list t map parameter t combined new tree map parameter t parameter p parameter values list t results util combine get p util format p results null util println null else results size util println results string replace n else t r results get combined put p r util println r return combined main main string args throws io exception args length args length util print usage args parser get name b inputpath outputdir b util args string inputpath args string outputdir args length args null read input map parameter list task result parsed new parser true parse inputpath outputdir map parameter task result combined combine parsed duration task result r combined values duration r get duration print pi pi bellard compute pi b combined util print bit skipped b util println util pi pi bellard bit terms b
1137	mapreduce\src\examples\org\apache\hadoop\examples\pi\SummationWritable.java	unrelated	package org apache hadoop examples pi a writable summation summation writable implements writable comparable summation writable container summation summation sigma summation writable summation writable summation sigma sigma sigma inherit doc string string return get class get simple name sigma inherit doc summation get element return sigma read sigma conf summation read class clazz configuration conf return summation value of conf get clazz get simple name sigma write sigma conf write summation sigma class clazz configuration conf conf set clazz get simple name sigma sigma string read summation data input summation read data input throws io exception summation writable new summation writable read fields return get element inherit doc read fields data input throws io exception arithmetic progression n arithmetic progression writable read arithmetic progression e arithmetic progression writable read sigma new summation n e read boolean sigma set value read double write sigma data output write summation sigma data output throws io exception arithmetic progression writable write sigma n arithmetic progression writable write sigma e double v sigma get value v null write boolean false else write boolean true write double v inherit doc write data output throws io exception write sigma inherit doc compare to summation writable return sigma compare to sigma inherit doc boolean equals object obj obj return true else obj null obj instanceof summation writable summation writable summation writable obj return compare to throw new illegal argument exception obj null obj null obj get class obj get class not supported hash code throw new unsupported operation exception a writable arithmetic progression arithmetic progression writable read arithmetic progression data input arithmetic progression read data input throws io exception return new arithmetic progression read char read long read long read long write arithmetic progression data output write arithmetic progression ap data output throws io exception write char ap symbol write long ap value write long ap delta write long ap limit
1138	mapreduce\src\examples\org\apache\hadoop\examples\pi\TaskResult.java	unrelated	package org apache hadoop examples pi a map task results reduce task results task result implements container summation combinable task result writable summation sigma duration task result task result summation sigma duration sigma sigma duration duration inherit doc summation get element return sigma get duration return duration inherit doc compare to task result return sigma compare to sigma inherit doc boolean equals object obj obj return true else obj null obj instanceof task result task result task result obj return compare to throw new illegal argument exception obj null obj null obj get class obj get class not supported hash code throw new unsupported operation exception inherit doc task result combine task result summation sigma combine sigma return null null new task result duration duration inherit doc read fields data input throws io exception sigma summation writable read duration read long inherit doc write data output throws io exception summation writable write sigma write long duration inherit doc string string return sigma sigma duration duration util millis string duration covert string task result task result value of string j index of duration j throw new illegal argument exception j j summation sigma summation value of util parse string variable sigma substring j j j index of j throw new illegal argument exception j j duration util parse long variable duration substring j return new task result sigma duration
1139	mapreduce\src\examples\org\apache\hadoop\examples\pi\Util.java	pooling	package org apache hadoop examples pi utility methods util output stream print stream system error stream print stream err system timer timer boolean accumulative start system current time millis previous start timer constructor timer boolean accumulative accumulative accumulative stack trace element stack thread current thread get stack trace stack trace element e stack stack length println e started new date start same tick null tick return tick null tick synchronized tick string system current time millis delta accumulative start previous null format dms n delta millis string delta flush previous return delta covert milliseconds string string millis string n n return millis string n else n return n ms string builder b new string builder millis n l millis b append string format millis n return b insert n append string b insert string format n l n return b insert n string b insert string format n l n return b insert n string b insert n l days n l b insert days day days insert days n l b insert n year years insert n return b string covert string this support comma separated number format string return long parse long trim replace covert string comma separated number format string n n return n string builder b new string builder n n n b insert string format n return n b string parse variable parse long variable string name string return parse string variable name parse variable string parse string variable string name string starts with name throw new illegal argument exception starts with name name name return substring name length execute callables number threads t e extends callable t execute n threads list e callables throws interrupted exception execution exception executor service executor executors new fixed thread pool n threads list future t futures executor invoke all callables future t f futures f get print usage messages print usage string args string usage err println args arrays list args err println err println usage java usage err println tool runner print generic command usage err return combine list items t extends combinable t list t combine collection t items list t sorted new array list t items sorted size return sorted collections sort sorted list t combined new array list t items size t prev sorted get sorted size t curr sorted get t c curr combine prev c null prev c else combined add prev prev curr combined add prev return combined check local directory check directory file dir dir exists dir mkdirs throw new illegal argument exception dir mkdirs dir dir dir directory throw new illegal argument exception dir dir directory simple date format date format new simple date format yyyy m mdd h hmmss sss create writer local file print writer create writer file dir string prefix throws io exception check directory dir file f new file dir prefix date format format new date system current time millis txt f exists return new print writer new file writer f try thread sleep catch interrupted exception e print bits skipped message print bit skipped b println
1140	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\ArithmeticProgression.java	unrelated	package org apache hadoop examples pi math an arithmetic progression arithmetic progression implements comparable arithmetic progression a symbol char symbol starting value value difference terms delta ending value limit constructor arithmetic progression char symbol value delta limit delta throw new illegal argument exception delta symbol symbol value value delta delta limit limit inherit doc boolean equals object obj obj return true else obj null obj instanceof arithmetic progression arithmetic progression arithmetic progression obj symbol symbol throw new illegal argument exception symbol symbol return value value delta delta limit limit throw new illegal argument exception obj null obj null obj get class obj get class not supported hash code throw new unsupported operation exception inherit doc compare to arithmetic progression symbol symbol throw new illegal argument exception symbol symbol delta delta throw new illegal argument exception delta delta limit limit return does contain boolean contains arithmetic progression symbol symbol throw new illegal argument exception symbol symbol delta delta value value return get steps get steps else delta return value value limit limit else delta return value value limit limit return false skip steps skip steps steps throw new illegal argument exception steps steps steps return value steps delta get number steps get steps return limit value delta inherit doc string string return symbol value value delta delta limit limit convert string arithmetic progression arithmetic progression value of string j index of delta value util parse long variable value substring j j j index of limit delta util parse long variable delta substring j j limit util parse long variable limit substring return new arithmetic progression char at value delta limit
1141	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Bellard.java	unrelated	package org apache hadoop examples pi math bellard bbp type pi formula sum n infty n n n n n n n n n references david h bailey peter b borwein simon plouffe on rapid computation various polylogarithmic constants math comp fabrice bellard a new formula compute n th binary digit pi available http fabrice bellard free fr pi bellard parameters sums enum parameter sum k infty k k k k k p false p false p p p p sum k infty k k k k k k k k k k k p true p false p false p false p true p p p p p p p p p p boolean isplus j delta n delta e offset e parameter boolean isplus j delta n offset e isplus isplus j j delta n delta n delta e offset e offset e parameter parameter p isplus p isplus j p j p delta n delta n p delta n delta e p delta e offset e p offset e p delta e get parameter represented string parameter get string trim char at p substring string parts split d parts length string name p parts parts parameter p values p name equals name return p throw new illegal argument exception parts arrays list parts the sums bellard formula sum implements container summation iterable summation accuracy bit parameter parameter summation sigma summation parts tail tail constructor t extends container summation sum b parameter p n parts list t existing b throw new illegal argument exception b b n parts throw new illegal argument exception n parts n parts p j p offset e e b p delta e p offset e n p delta n p j parameter p sigma new summation n p delta n e p delta e parts partition sigma n parts existing tail new tail n e t extends container summation summation partition summation sigma n parts list t existing list summation parts new array list summation existing null existing empty parts add all arrays list sigma partition n parts else steps per part sigma get steps n parts list summation remaining sigma remaining terms existing summation remaining n get steps steps per part parts add all arrays list partition n container summation c existing parts add c get element collections sort parts return parts array new summation parts size inherit doc string string n summation parts get value null n return get class get simple name parameter sigma remaining n set value sigma set value summation get value null throw new illegal argument exception get value n sigma sigma n contains sigma sigma contains throw new illegal argument exception contains sigma sigma contains n sigma sigma n sigma set value get value get value sigma get value sigma get value null parts length modular add mod parts compute sigma set value modular add mod sigma get value tail compute return parameter isplus inherit doc summation get element sigma get value null parts length parts get value null modular add mod parts get value parts length sigma
1142	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\LongLong.java	unrelated	package org apache hadoop examples pi math support bit integer arithmetic long long bits per long mid bits per long size bits per long full mask l bits per long lower mask full mask mid upper mask lower mask mid set values long long set return and operation mask return mask shift right operation shift right n return bits per long n n plus equal operation long long plus equal long long return convert big integer big integer big integer return big integer value of shift left bits per long add big integer value of inherit doc string string remainder bits per long return string format x x remainder bits per long remainder compute b store result r long long multiplication long long r b x lower mask x upper mask mid b lower mask b upper mask mid x x u x x v x tmp u result u v tmp mid full mask result v tmp mid return result lower lower mask upper upper mask mid b lower b lower mask b upper b upper mask mid tmp lower b upper upper b lower r lower b lower tmp mid full mask r upper b upper tmp mid return r
1143	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Modular.java	unrelated	package org apache hadoop examples pi math modular arithmetics modular max sqrt long math sqrt long max value compute e mod n mod e n half long number of leading zeros n full half ones half r mask long highest one bit e mask mask r max sqrt long r r r n r n else r overflow high r half low r ones r r r n r n high high high n n full n n low r else high low n n half n n r r n r n r r n r n e mask r r n r n return r given x return x mod add mod x x return x x x x x given x return x mod mod inverse x x return b c x u v w q w c w q c u q w return u u u v q b q c w c q w q u c return b q v
1144	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Montgomery.java	unrelated	package org apache hadoop examples pi math montgomery method references richard crandall carl pomerance prime numbers a computational perspective springer verlag peter montgomery modular multiplication without trial division math comp montgomery protected product product new product protected n protected n i n protected r protected r r protected set modular initialize object montgomery set n n throw new illegal argument exception n n n n n r long highest one bit n n i r modular mod inverse n r r r long number of trailing zeros r return compute mod n n odd mod p r n x p x n x n mask long highest one bit mask mask p product p p mask p product p x return product p product long long x new long long long long x n i new long long long long n new long long c long long multiplication x c x n r x r n r long long multiplication x n i x r n i r long long multiplication n n z n plus equal x shift right return z n z z n
1145	mapreduce\src\examples\org\apache\hadoop\examples\pi\math\Summation.java	unrelated	package org apache hadoop examples pi math represent summation sum frac e mod n n summation implements container summation combinable summation variable n summation arithmetic progression n variable e summation arithmetic progression e double value null constructor summation arithmetic progression n arithmetic progression e n get steps e get steps throw new illegal argument exception n get steps e get steps n n get steps n get steps n n n e get steps e get steps e e n n e e constructor summation value n delta n value e delta e limit e value n delta n value n delta n value e limit e delta e value e delta e limit e constructor summation value n delta n limit n value e delta e limit e new arithmetic progression n value n delta n limit n new arithmetic progression e value e delta e limit e inherit doc summation get element return return number steps summation get steps return e get steps return value summation double get value return value set value summation set value v value v inherit doc string string return n e value null value double to long bits value inherit doc boolean equals object obj obj return true obj null obj instanceof summation summation summation obj return n equals n e equals e throw new illegal argument exception obj null obj null obj get class obj get class not supported hash code throw new unsupported operation exception covert string summation summation value of string j index of j throw new illegal argument exception j j arithmetic progression n arithmetic progression value of substring j j j index of j throw new illegal argument exception j j arithmetic progression e arithmetic progression value of substring j summation sigma new summation n e j length string value util parse string variable value substring sigma set value value index of double bits to double long parse long value double parse double value return sigma compute value summation compute value null value n limit max modular compute modular compute montgomery return value max modular l compute value using link modular mod compute modular e e value n n value e e limit e e delta modular add mod modular mod e n n n n delta return montgomery montgomery new montgomery compute value using link montgomery mod compute montgomery e e value n n value e e limit e e delta modular add mod montgomery set n mod e n n n delta return inherit doc compare to summation de e compare to e de return de return n compare to n inherit doc summation combine summation n delta n delta e delta e delta throw new illegal argument exception n delta n delta e delta e delta n n e limit e value n limit n value v modular add mod value value summation new summation new arithmetic progression n symbol n value n delta n limit new arithmetic progression e symbol e value e delta e limit set value v return return null find remaining terms t
1146	mapreduce\src\examples\org\apache\hadoop\examples\terasort\GenSort.java	unrelated	package org apache hadoop examples terasort a single process data generator terasort data based gensort c version mar chris nyberg chris nyberg ordinal com gen sort generate binary record suitable sort benchmarks except penny sort generate record byte rec buf unsigned rand unsigned record number generate byte key using high bytes bit random number rec buf rand get byte add bytes break rec buf x rec buf x convert bit record number bits ascii hexadecimal next bytes record rec buf byte record number get hex digit add bytes break data rec buf byte x rec buf byte x rec buf byte x aa rec buf byte x bb add bytes filler based low bits random number rec buf rec buf rec buf rec buf byte rand get hex digit add bytes break data rec buf byte x cc rec buf byte x dd rec buf byte x ee rec buf byte x ff big integer make big integer x byte data new byte data byte x return new big integer data big integer ninety five new big integer generate ascii record suitable sort benchmarks including penny sort generate ascii record byte rec buf unsigned rand unsigned record number generate byte ascii key using mostly high bits temp rand get high temp use biginteger avoid negative sign problem big integer big temp make big integer temp rec buf byte big temp mod ninety five value temp big temp divide ninety five value else rec buf byte temp temp rec buf byte temp temp temp rand get low temp big integer big temp make big integer temp rec buf byte big temp mod ninety five value temp big temp divide ninety five value else rec buf byte temp temp rec buf byte temp add bytes break rec buf rec buf convert bit record number bits ascii hexadecimal next bytes record rec buf byte record number get hex digit add bytes break data rec buf rec buf add bytes filler based low bits random number rec buf rec buf rec buf rec buf byte rand get hex digit add bytes break data rec buf r nice windows rec buf n usage print stream system println usage gensort c b starting rec num num recs file name println generate ascii records required penny sort joule sort println these records also alternative input println sort benchmarks without flag binary records println generated contain highest density randomness println byte key println c calculate sum crc checksums println generated records send standard error println b n set beginning record generated n by default println first record generated record println num recs the number sequential records generate println file name the name file write records n println example generate ascii records starting record println file named pennyinput println gensort pennyinput n println example generate binary records beginning record println file named partition println gensort b partition system exit output records output stream boolean use ascii unsigned first record number unsigned records to generate unsigned checksum throws io exception byte row new byte unsigned record number new unsigned first record number
1147	mapreduce\src\examples\org\apache\hadoop\examples\terasort\Random16.java	unrelated	package org apache hadoop examples terasort this implements bit linear congruential generator specifically x recently issued bit random number seed random number already generated next number generated x equal x x c mod x ed fc da df fccf c x the coefficient suggested pierre l ecuyer tables linear congruential generators different sizes good lattice structure mathematics computation pp http www ams org mcom s s pdf the constant c meets simple suggestion reference odd there also facility quickly advancing state generator fixed number steps facilitates parallel generation this based rand c chris nyberg chris nyberg ordinal com random the gen array contain powers linear congruential generator the index contain coefficient c constant generator that generator f x gen x gen c mod all structs first contain c comprise square previous function f x gen x gen c mod f x gen x gen c mod f x gen x gen c mod random constant unsigned unsigned c random constant string left string right new unsigned left c new unsigned right random constant gen array new random constant new random constant ed fc da df fccf new random constant bce bdf c ed eb e ae e e b edfe c b new random constant f dd db bd dfbe c b f b c new random constant af f f ffe efc abfaca e ca edb f dfbf new random constant b f b f f ef c c f bd af c b bafa f new random constant c ad cb cd b ba b ca c c c db e new random constant dab f ee e c f b ae bd c eae c fc new random constant cf fe de fd dbdff b cdb f new random constant ecb c b f c bc db fdb e cd edcecb f new random constant c e c aa e bf f c cd e c af dafe new random constant ae f fbece c bef dccb ab c fc new random constant b cb c cb ff ed ae de e b ad b cd c cbf new random constant f fc b e f e fbc b bbfd ed e c c f new random constant df fc f dd f b ee c fa e fe bd fe new random constant f ea e f e ef cc c fe aae fc new random constant e e f c c de b abff f f dd c bf new random constant b dc e cd fbc e b f new random constant e ec e fafcbbb f ffc ff cb f acf b fe new random constant c e bf c c ddfb ef ab fb fb e fc new random constant eb ee fb c dbc e c de c caa f bf new random constant b f f f fe e fbc b cb cfe c f f new random constant ea f e f c bc ae b f c b c fe new random constant c db b c bfa ef c
1148	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraChecksum.java	unrelated	package org apache hadoop examples terasort tera checksum extends configured implements tool checksum mapper extends mapper text text null writable unsigned unsigned checksum new unsigned unsigned sum new unsigned checksum crc new pure java crc map text key text value context context throws io exception crc reset crc update key get bytes key get length crc update value get bytes value get length checksum set crc get value sum add checksum cleanup context context throws io exception interrupted exception context write null writable get sum checksum reducer extends reducer null writable unsigned null writable unsigned reduce null writable key iterable unsigned values context context throws io exception interrupted exception unsigned sum new unsigned unsigned val values sum add val context write key sum usage throws io exception system err println terasum dir report dir run string args throws exception job job job get instance new cluster get conf get conf args length usage return tera input format set input paths job new path args file output format set output path job new path args job set job name tera sum job set jar by class tera checksum job set mapper class checksum mapper job set reducer class checksum reducer job set output key class null writable job set output value class unsigned force single reducer job set num reduce tasks job set input format class tera input format return job wait for completion true main string args throws exception res tool runner run new configuration new tera checksum args system exit res
1149	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraGen.java	unrelated	package org apache hadoop examples terasort generate official gray sort input data set the user specifies number rows output directory runs map reduce program generate data the format data ul li bytes key constant bytes bytes rowid constant bytes bytes filler constant bytes li the rowid right justified row id hex number ul p to run program b bin hadoop jar hadoop examples jar teragen dir b tera gen extends configured implements tool log log log factory get log tera sort enum counters checksum string num rows mapreduce terasort num rows an input format assigns ranges longs mapper range input format extends input format long writable null writable an input split consisting range numbers range input split extends input split implements writable first row row count range input split range input split offset length first row offset row count length get length throws io exception return string get locations throws io exception return new string read fields data input throws io exception first row writable utils read v long row count writable utils read v long write data output throws io exception writable utils write v long first row writable utils write v long row count a record reader generate range numbers range record reader extends record reader long writable null writable start row finished rows total rows long writable key null range record reader initialize input split split task attempt context context throws io exception interrupted exception start row range input split split first row finished rows total rows range input split split row count close throws io exception nothing long writable get current key return key null writable get current value return null writable get get progress throws io exception return finished rows total rows boolean next key value key null key new long writable finished rows total rows key set start row finished rows finished rows return true else return false record reader long writable null writable create record reader input split split task attempt context context throws io exception return new range record reader create desired number splits dividing number rows mappers list input split get splits job context job total rows get number of rows job num splits job get configuration get int mr job config num maps log info generating total rows using num splits list input split splits new array list input split current row split split num splits split goal math ceil total rows split num splits splits add new range input split current row goal current row current row goal return splits get number of rows job context job return job get configuration get long num rows set number of rows job job num rows job get configuration set long num rows num rows the mapper given row number generate appropriate output line sort gen mapper extends mapper long writable null writable text text text key new text text value new text unsigned rand null unsigned row id null unsigned checksum new unsigned checksum crc new pure java crc unsigned total new unsigned unsigned one new unsigned byte buffer new byte tera input
1150	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraInputFormat.java	scheduler	package org apache hadoop examples terasort an input format reads first characters line key rest line value both key value represented text tera input format extends file input format text text string partition filename partition lst string num partitions mapreduce terasort num partitions string sample size mapreduce terasort partitions sample key length value length record length key length value length mr job config last context null list input split last result null tera file split extends file split string locations tera file split tera file split path file start length string hosts super file start length hosts locations hosts protected set locations string hosts locations hosts string get locations return locations string string string buffer result new string buffer result append get path result append result append get start result append length result append get length string host get locations result append result append host return result string text sampler implements indexed sortable array list text records new array list text compare j text left records get text right records get j return left compare to right swap j text left records get text right records get j records set j left records set right add key text key synchronized records add new text key find split points given sample the sample keys sorted sampled find even split points partitions the returned keys start respective partitions text create partitions num partitions num records records size system println making num partitions num records sampled records num partitions num records throw new illegal argument exception requested partitions input keys num partitions num records new quick sort sort records size step size num records num partitions text result new text num partitions num partitions result records get math round step size return result use input splits take samples input generate sample keys by default reads keys locations input sorts picks n keys generate n equally sized partitions write partition file job context job path part file throws io exception interrupted exception system current time millis configuration conf job get configuration tera input format format new tera input format text sampler sampler new text sampler partitions job get num reduce tasks sample size conf get long sample size list input split splits format get splits job system current time millis system println computing input splits took ms samples math min conf get int num partitions splits size system println sampling samples splits splits size records per sample sample size samples sample step splits size samples thread sampler reader new thread samples take n samples different parts input samples idx sampler reader new thread sampler reader idx set daemon true run records try task attempt context context new task attempt context impl job get configuration new task attempt id record reader text text reader format create record reader splits get sample step idx context reader initialize splits get sample step idx context reader next key value sampler add key new text reader get current key records records per sample records break catch io exception ie system err println got exception reading splits string utils stringify exception ie
1151	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraOutputFormat.java	unrelated	package org apache hadoop examples terasort an output format writes key value appended together tera output format extends file output format text text string final sync attribute mapreduce terasort sync output committer committer null set requirement sync stream closed set final sync job context job boolean new value job get configuration set boolean final sync attribute new value does user want sync close boolean get final sync job context job return job get configuration get boolean final sync attribute false tera record writer extends record writer text text boolean sync false fs data output stream tera record writer fs data output stream job context job sync get final sync job synchronized write text key text value throws io exception write key get bytes key get length write value get bytes value get length close task attempt context context throws io exception sync sync close check output specs job context job throws invalid job conf exception io exception ensure output directory set path dir get output path job dir null throw new invalid job conf exception output directory set job conf record writer text text get record writer task attempt context job throws io exception path file get default work file job file system fs file get file system job get configuration fs data output stream file out fs create file return new tera record writer file out job output committer get output committer task attempt context context throws io exception committer null path output get output path context committer new tera output committer output context return committer tera output committer extends file output committer tera output committer path output path task attempt context context throws io exception super output path context commit job job context job context setup job job context job context setup task task attempt context task context
1152	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraScheduler.java	scheduler	package org apache hadoop examples terasort tera scheduler string use mapreduce terasort use terascheduler log log log factory get log tera scheduler split splits list host hosts new array list host slots per host remaining splits file split real splits null split string filename boolean assigned false list host locations new array list host split string filename filename filename string string string buffer result new string buffer result append filename result append host host locations result append host hostname result append return result string host string hostname list split splits new array list split host string hostname hostname hostname string string string buffer result new string buffer result append splits size result append result append hostname return result string list string read file string filename throws io exception list string result new array list string buffered reader new buffered reader new file reader filename string line read line line null result add line line read line close return result tera scheduler string split filename string node filename throws io exception slots per host get hosts map string host host ids new hash map string host string host name read file node filename host host new host host name hosts add host host ids put host name host read blocks list string split lines read file split filename splits new split split lines size remaining splits string line split lines string tokenizer itr new string tokenizer line split new split new split itr next token splits remaining splits new split itr more tokens host host host ids get itr next token new split locations add host host splits add new split tera scheduler file split real splits configuration conf throws io exception real splits real splits slots per host conf get int tt config tt map slots map string host host table new hash map string host splits new split real splits length file split real split real splits split split new split real split get path string splits remaining splits split string hostname real split get locations host host host table get hostname host null host new host hostname host table put hostname host hosts add host host splits add split split locations add host host pick best host host result null splits integer max value host host hosts host splits size splits result host splits host splits size result null hosts remove result log debug picking result return result pick best splits host host tasks to pick math min slots per host math ceil remaining splits hosts size split best new split tasks to pick split cur host splits log debug examine cur filename cur locations size tasks to pick best null best locations size cur locations size tasks to pick j tasks to pick j j best j best j best cur chosen blocks remove locations tasks to pick best null log debug best best filename host best locations splits remove best best locations clear best locations add host best assigned true remaining splits non chosen blocks remove host split cur host splits cur assigned cur locations remove host solve
1153	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraSort.java	unrelated	package org apache hadoop examples terasort generates sampled split points launches job waits finish p to run program b bin hadoop jar hadoop examples jar terasort dir dir b tera sort extends configured implements tool log log log factory get log tera sort string simple partitioner mapreduce terasort simplepartitioner string output replication mapreduce terasort output replication a partitioner splits text keys roughly equal partitions global sorted order total order partitioner extends partitioner text text implements configurable trie node trie text split points configuration conf a generic trie node trie node level trie node level level level find partition text key print print stream strm throws io exception get level return level an inner trie node contains children based next character inner trie node extends trie node trie node child new trie node inner trie node level super level find partition text key level get level key get length level return child find partition key return child key get bytes level xff find partition key set child idx trie node child child idx child print print stream strm throws io exception ch ch ch get level strm print strm print ch strm println child ch null child ch print strm a leaf trie node compares figure given key belongs lower upper leaf trie node extends trie node lower upper text split points leaf trie node level text split points lower upper super level split points split points lower lower upper upper find partition text key lower upper split points compare to key return return upper print print stream strm throws io exception get level strm print strm print lower strm print strm println upper read cut points given sequence file text read partitions file system fs path p configuration conf throws io exception reduces conf get int mr job config num reduces text result new text reduces data input stream reader fs open p reduces result new text result read fields reader reader close return result given sorted set cut points build trie find correct partition quickly trie node build trie text splits lower upper text prefix max depth depth prefix get length depth max depth lower upper return new leaf trie node depth splits lower upper inner trie node result new inner trie node depth text trial new text prefix append extra byte prefix trial append new byte current bound lower ch ch ch trial get bytes depth byte ch lower current bound current bound upper splits current bound compare to trial break current bound trial get bytes depth byte ch result child ch build trie splits lower current bound trial max depth pick rest trial get bytes depth byte result child build trie splits current bound upper trial max depth return result set conf configuration conf try file system fs file system get local conf conf conf path part file new path tera input format partition filename split points read partitions fs part file conf trie build trie split points split points length new text catch io exception ie throw new illegal argument exception read paritions file ie configuration get conf
1154	mapreduce\src\examples\org\apache\hadoop\examples\terasort\TeraValidate.java	unrelated	package org apache hadoop examples terasort generate mapper per file checks make sure keys sorted within file the mapper also generates file begin first key file end last key the reduce verifies start end items order any output reduce problem report p to run program b bin hadoop jar hadoop examples jar teravalidate dir report dir b p if output something wrong output reduce problem report tera validate extends configured implements tool text error new text error text checksum new text checksum string textify bytes text bytes writable b new bytes writable b set get bytes get length return b string validate mapper extends mapper text text text text text last key string filename unsigned checksum new unsigned unsigned tmp new unsigned checksum crc new pure java crc get part input name string get filename file split split return split get path get name map text key text value context context throws io exception interrupted exception last key null file split fs file split context get input split filename get filename fs context write new text filename begin key last key new text else key compare to last key context write error new text misorder filename textify bytes last key textify bytes key compute crc key value add sum crc reset crc update key get bytes key get length crc update value get bytes value get length tmp set crc get value checksum add tmp last key set key cleanup context context throws io exception interrupted exception last key null context write new text filename end last key context write checksum new text checksum string check boundaries output files making sure boundary keys always increasing also passes error reports along intact validate reducer extends reducer text text text text boolean first key true text last key new text text last value new text reduce text key iterable text values context context throws io exception interrupted exception error equals key text val values context write key val else checksum equals key unsigned tmp new unsigned unsigned sum new unsigned text val values tmp set val string sum add tmp context write checksum new text sum string else text value values iterator next first key first key false else value compare to last value context write error new text bad key partitioning n file last key key textify bytes last value n file key key textify bytes value last key set key last value set value usage throws io exception system err println teravalidate dir report dir run string args throws exception job job job get instance new cluster get conf get conf args length usage return tera input format set input paths job new path args file output format set output path job new path args job set job name tera validate job set jar by class tera validate job set mapper class validate mapper job set reducer class validate reducer job set output key class text job set output value class text force single reducer job set num reduce tasks force single split file input format set min input split size job
1155	mapreduce\src\examples\org\apache\hadoop\examples\terasort\Unsigned16.java	unrelated	package org apache hadoop examples terasort an unsigned byte integer supports addition multiplication left shifts unsigned implements writable hi lo unsigned hi lo unsigned hi lo unsigned unsigned hi hi lo lo boolean equals object instanceof unsigned unsigned unsigned return hi hi lo lo return false hash code return lo parse hex unsigned string throws number format exception set set number hex set string throws number format exception hi lo last digit xfl length digit get hex digit char at last digit hi throw new number format exception overflowed bytes hi hi lo last digit lo lo digit set number given set lo hi map hexadecimal character digit get hex digit char ch throws number format exception ch ch return ch ch ch f return ch ch a ch f return ch a throw new number format exception ch valid hex digit unsigned ten new unsigned unsigned decimal string throws number format exception unsigned result new unsigned unsigned tmp new unsigned length char ch char at ch ch throw new number format exception ch valid decimal digit digit ch result multiply ten tmp set digit result add tmp return result return number hex string string hi return long hex string lo else string builder result new string builder result append long hex string hi string lo string long hex string lo lo string length result append result append lo string return result string get given byte number byte get byte b b b b return byte hi b else return byte lo b return get hexadecimal digit given position char get hex digit p byte digit get byte p p digit digit xf digit return char digit else return char a digit get high bytes get high return hi get low bytes get low return lo multiple current number byte unsigned integer overflow detected result low bytes result the numbers divided bit chunks product two chucks fits unsigned bits multiply unsigned b divide left bit chunks left new left lo xffffffffl left lo left hi xffffffffl left hi divide right bit chunks right new right b lo x fffffffl right b lo x fffffffl right b lo b hi x fffffffl right b hi x fffffffl right b hi clear cur value set unsigned tmp new unsigned r r r prod left right r prod r tmp set prod tmp shift left add tmp add given number current number add unsigned b sum hi sum lo reshibit hibit hibit sum hi hi b hi hibit lo x l hibit b lo x l sum lo lo b lo reshibit sum lo x l hibit hibit hibit hibit reshibit sum hi add carry bit hi sum hi lo sum lo shift number given number bit positions the number low order bits result shift left bits bits bits hi bits hi lo bits lo bits else bits hi lo bits lo else hi lo read fields data input throws io exception hi read long lo read long write data output throws io exception write long hi write long lo
1156	mapreduce\src\java\org\apache\hadoop\filecache\DistributedCache.java	unrelated	package org apache hadoop filecache distribute application specific large read files efficiently p code distributed cache code facility provided map reduce framework cache files text archives jars etc needed applications p p applications specify files via urls hdfs http cached via link org apache hadoop mapred job conf the code distributed cache code assumes files specified via urls already present link file system path specified url accessible every machine cluster p p the framework copy necessary files slave node tasks job executed node its efficiency stems fact files copied per job ability cache archives un archived slaves p p code distributed cache code used distribute simple read data text files complex types archives jars etc archives zip tar tgz tar gz files un archived slave nodes jars may optionally added classpath tasks rudimentary software distribution mechanism files execution permissions optionally users also direct symlink distributed cache file working directory task p p code distributed cache code tracks modification timestamps cache files clearly cache files modified application externally job executing p p here illustrative example use code distributed cache code p p blockquote pre setting cache application copy requisite files code file system code bin hadoop fs copy from local lookup dat myapp lookup dat bin hadoop fs copy from local map zip myapp map zip bin hadoop fs copy from local mylib jar myapp mylib jar bin hadoop fs copy from local mytar tar myapp mytar tar bin hadoop fs copy from local mytgz tgz myapp mytgz tgz bin hadoop fs copy from local mytargz tar gz myapp mytargz tar gz setup application code job conf code job conf job new job conf distributed cache add cache file new uri myapp lookup dat lookup dat job distributed cache add cache archive new uri myapp map zip job distributed cache add file to class path new path myapp mylib jar job distributed cache add cache archive new uri myapp mytar tar job distributed cache add cache archive new uri myapp mytgz tgz job distributed cache add cache archive new uri myapp mytargz tar gz job use cached files link org apache hadoop mapred mapper link org apache hadoop mapred reducer map class extends map reduce base implements mapper lt k v k v gt path local archives path local files configure job conf job get cached archives files local archives distributed cache get local cache archives job local files distributed cache get local cache files job map k key v value output collector lt k v gt output reporter reporter throws io exception use data cached archives files output collect k v pre blockquote p it also common use distributed cache using link org apache hadoop util generic options parser this includes methods used users specifically mentioned example well link distributed cache add archive to class path path configuration well methods intended use map reduce framework e g link org apache hadoop mapred job client distributed cache extends org apache hadoop mapreduce filecache distributed cache
1157	mapreduce\src\java\org\apache\hadoop\filecache\package-info.java	unrelated	package org apache hadoop filecache
1158	mapreduce\src\java\org\apache\hadoop\mapred\ACLsManager.java	audit	package org apache hadoop mapred manages map reduce cluster administrators access checks job level operations queue level operations uses job ac ls manager access checks job level operations queue manager queue operations ac ls manager log log log factory get log ac ls manager mr owner user started mapreduce cluster ugi user group information mr owner mapreduce cluster administrators access control list admin acl job ac ls manager job ac ls manager queue manager queue manager boolean acls enabled ac ls manager configuration conf job ac ls manager job ac ls manager queue manager queue manager throws io exception mr owner user group information get current user admin acl new access control list conf get mr config mr admins admin acl add user mr owner get short user name string deprecated super group conf get mr config mr supergroup deprecated super group null log warn mr config mr supergroup deprecated use mr config mr admins instead admin acl add group deprecated super group acls enabled conf get boolean mr config mr acls enabled false job ac ls manager job ac ls manager queue manager queue manager user group information get mr owner return mr owner access control list get admins acl return admin acl job ac ls manager get job ac ls manager return job ac ls manager is calling user admin mapreduce cluster e either cluster owner cluster administrator boolean mr admin user group information caller ugi admin acl user allowed caller ugi return true return false check ac ls user passed operation ul li if ac ls disabled allow users li li otherwise operation job operation eg submit job queue allow cluster owner started cluster b cluster administrators c members queue submit job acl queue li li if operation job operation allow job owner b cluster owner started cluster c cluster administrators members queue admins acl queue e members job acl job operation li ul check access job in progress job user group information caller ugi operation operation throws access control exception string queue job get profile get queue name string job id job get job id string job status job status job get status string job owner job status get username access control list job acl job status get job ac ls get operation job acl needed check access job id caller ugi queue operation job owner job acl check ac ls user passed job operation ul li if ac ls disabled allow users li li otherwise allow job owner b cluster owner started cluster c cluster administrators members job acl job operation li ul check access job status job status user group information caller ugi string queue operation operation throws access control exception string job id job status get job id string string job owner job status get username access control list job acl job status get job ac ls get operation job acl needed if acls enabled check caller ugi job owner queue admin cluster admin part job acl check access job id caller ugi queue operation job owner job acl check ac ls user passed operation
1159	mapreduce\src\java\org\apache\hadoop\mapred\AdminOperationsProtocol.java	unrelated	package org apache hadoop mapred protocol admin operations this framework not to be used by users directly server principal jt config jt user name admin operations protocol extends versioned protocol version initial version added refresh queue acls version added node refresh facility version changed refresh queue acls refresh queues version id l refresh queues used jobtracker scheduler access control lists queue states refreshed refresh queues throws io exception refresh node list link job tracker refresh nodes throws io exception
1160	mapreduce\src\java\org\apache\hadoop\mapred\AuditLogger.java	audit	package org apache hadoop mapred manages map reduce audit logs audit logs provides information authorization authentication events success failure audit log format written key value pairs audit logger log log log factory get log audit logger enum keys user operation target result ip permissions description constants string success success string failure failure string key val separator char pair separator some constants used others using audit logger some commonly used targets string jobtracker job tracker some commonly used operations string refresh queue refresh queue string refresh nodes refresh nodes some commonly used descriptions string unauthorized user unauthorized user a helper api creating audit log successful event this factored testing purpose string create success log string user string operation string target string builder b new string builder start keys user user b add remote ip b add keys operation operation b add keys target target b add keys result constants success b return b string create readable parseable audit log successful event commonly operated targets jobs job tracker queues etc br br note link audit logger uses tabs key val delimiter hence value fields contains tabs log success string user string operation string target log info enabled log info create success log user operation target a helper api creating audit log failure event this factored testing purpose string create failure log string user string operation string perm string target string description string builder b new string builder start keys user user b add remote ip b add keys operation operation b add keys target target b add keys result constants failure b add keys description description b add keys permissions perm b return b string create readable parseable audit log failed event commonly operated targets jobs job tracker queues etc failed br br note link audit logger uses tabs key val delimiter hence value fields contains tabs log failure string user string operation string perm string target string description log warn enabled log warn create failure log user operation perm target description a helper api add remote ip address add remote ip string builder b inet address ip server get remote ip ip address null testcases ip null add keys ip ip get host address b adds first key val pair passed builder following format key value start keys key string value string builder b b append key name append constants key val separator append value appends key val pair passed builder following format pair delim key value add keys key string value string builder b b append constants pair separator append key name append constants key val separator append value
1161	mapreduce\src\java\org\apache\hadoop\mapred\BackupStore.java	unrelated	package org apache hadoop mapred code backup store code utility used support mark reset functionality values iterator p it two caches memory cache file cache values stored iterated mark on reset values retrieved caches framework moves memory cache file cache memory cache becomes full backup store k v log log log factory get log backup store get name max vint size eof marker size max vint size task attempt id tid memory cache mem cache file cache file cache list segment k v segment list new linked list segment k v read segment index first segment offset current kv offset next kv offset data input buffer current key null data input buffer current value new data input buffer data input buffer current disk value new data input buffer boolean more false boolean reset false boolean clear mark flag false boolean last segment eof false backup store configuration conf task attempt id taskid throws io exception buffer percent conf get float job context reduce markreset buffer percent f buffer percent buffer percent throw new io exception job context reduce markreset buffer percent buffer percent max size math min runtime get runtime max memory buffer percent integer max value support absolute size also tmp conf get int job context reduce markreset buffer size tmp max size tmp mem cache new memory cache max size file cache new file cache conf tid taskid log info created new backup store memory max size write given k v cache write memcache space available else write filecache write data input buffer key data input buffer value throws io exception assert key null value null file cache active file cache write key value return mem cache reserve space key value mem cache write key value else file cache activate file cache write key value mark throws io exception we read one kv pair advance next if next read next kv pair new segment user called next kv reset read segment index previous segment next kv offset assert read segment index assert current kv offset read segment index drop segments current active segment iterator segment k v itr segment list iterator itr next segment k v itr next read segment index break close itr remove log debug dropping segment first segment offset offset current segment need start reading next reset first segment offset current kv offset read segment index log debug setting firs segment offset current kv offset reset throws io exception create new segment previously written records already reset mode reset file cache active file cache create in disk segment else mem cache create in memory segment reset true reset segments correct position next read begin segment list size segment k v segment list get memory offset first segment offset get reader reset offset else close reader reinit reader first segment offset get reader disable checksum validation current kv offset first segment offset next kv offset read segment index more false last segment eof false log debug reset first segment offset first segment offset segment list size segment list size boolean next throws io exception last segment eof return
1162	mapreduce\src\java\org\apache\hadoop\mapred\BasicTypeSorterBase.java	unrelated	package org apache hadoop mapred this implements sort using primitive arrays data structures called basic type sorter base basic type sorter base implements buffer sorter protected output buffer key val buffer buffer used storing key values protected start offsets array used store start offsets keys key val buffer protected key lengths array used store lengths keys protected value lengths array used store value lengths protected pointers array start offsets indices this sorted end contain sorted array indices offsets protected raw comparator comparator comparator map output protected count number key values overhead arrays memory keyoffsets keylengths value lengths indices start offsets array pointers array ignored partpointers list buffered key val overhead initial array size maintain max lengths key val encounter during iteration sorted results create data output buffer return keys the max size data output buffer max keylength encounter expose value model memory accurately max key length max val length reference progressable object sending keep alive protected progressable reporter implementation methods sorter base configure job conf conf comparator conf get output key comparator set progressable progressable reporter reporter reporter add key value record offset key length val length add start offset key start offsets array length key lengths array start offsets null count start offsets length grow start offsets count record offset key lengths count key length key length max key length max key length key length val length max val length max val length val length value lengths count val length pointers count count count set input buffer output buffer buffer store reference key val buffer need read sort key val buffer buffer get memory utilized total length arrays max key val length max size data output buffers iteration sorted keys start offsets null return start offsets length buffered key val overhead max key length max val length else nothing yet return raw key value iterator sort close set count also reuse arrays since want maintain consistency memory model count start offsets null key lengths null value lengths null pointers null max key length max val length release large key value buffer gc necessary collect away key val buffer null grow curr length start offsets null curr length start offsets length new length curr length start offsets grow start offsets new length key lengths grow key lengths new length value lengths grow value lengths new length pointers grow pointers new length grow old new length result new new length old null system arraycopy old result old length return result basic type sorter base implementation methods raw key value iterator these methods must invoked iterate key vals sort done mr sort result iterator implements raw key value iterator count pointers start offsets key lengths val lengths curr start offset index curr index in pointers output buffer key val buffer data output buffer key new data output buffer in mem uncompressed bytes value new in mem uncompressed bytes mr sort result iterator output buffer key val buffer pointers start offsets key lengths val lengths count pointers length pointers pointers start offsets start offsets key lengths key lengths val lengths val lengths key val buffer
1163	mapreduce\src\java\org\apache\hadoop\mapred\BufferSorter.java	unrelated	package org apache hadoop mapred this provides generic sort implemented specific sort algorithms the use case following a user writes key value records buffer finally wants sort buffer this defines methods user update implementation offsets records lengths keys values the user gives reference buffer latter wishes sort records written buffer far typically user decides point sort happen based memory consumed far buffer data structures maintained implementation that method provided get memory consumed far datastructures implementation buffer sorter extends job configurable pass progressable object sort call progress sorting set progressable progressable reporter when key value added particular offset key value buffer method invoked user impl sort update datastructures add key value recordoffset key length val length the user invokes method set buffer specific sort algorithm indirectly sort generally sort algorithm impl access buffer via comparators sort offset indices buffer set input buffer output buffer buffer the framework invokes method get memory consumed far implementation get memory utilized framework decides actually sort raw key value iterator sort framework invokes signal sorter cleanup close
1164	mapreduce\src\java\org\apache\hadoop\mapred\Child.java	unrelated	package org apache hadoop mapred the main child processes child log log log factory get log child volatile task attempt id taskid null volatile boolean cleanup main string args throws throwable log debug child starting job conf default conf new job conf set tcp nodelay default conf set boolean ipc client tcpnodelay true string host args port integer parse int args inet socket address address new inet socket address host port task attempt id first taskid task attempt id name args string log location args sleep longer count jvm id int integer parse int args jvm id jvm id new jvm id first taskid get job id first taskid get task type task type map jvm id int load token cache storage string job token file system getenv get user group information hadoop token file location credentials credentials token cache load tokens job token file default conf log debug loading token keys credentials number of secret keys file job token file token job token identifier jt token cache get job token credentials jt set service new text address get address get host address address get port user group information current user group information get current user current add token jt create task umbilical protocol actual task owner user group information task owner user group information create remote user first taskid get job id string task owner add token jt set credentials default conf set credentials credentials task umbilical protocol umbilical task owner as new privileged exception action task umbilical protocol task umbilical protocol run throws exception return task umbilical protocol rpc get proxy task umbilical protocol task umbilical protocol version id address default conf num tasks to execute signifies limit num tasks executed runtime get runtime add shutdown hook new thread run try taskid null task log sync logs log location taskid cleanup catch throwable throwable thread new thread run every often wake sync logs track logs currently running task true try thread sleep taskid null task log sync logs log location taskid cleanup catch interrupted exception ie catch io exception iee log error error sync logs iee system exit set name thread sync logs set daemon true start string pid shell windows pid system getenv get jvm pid jvm context context new jvm context jvm id pid idle loop count task task null user group information child ugi null try true taskid null jvm task task umbilical get task context task die break else task get task null taskid null idle loop count sleep longer count sleep bigger interval receive tasks thread sleep else thread sleep continue idle loop count task task get task taskid task get task id cleanup task task cleanup task reset statistics task file system clear statistics create index file log files viewable immediately task log sync logs log location taskid cleanup create job conf set credentials job conf job new job conf task get job file job set credentials default conf get credentials set job token file task task set job token secret job token secret manager create secret key jt get password setup child configs
1165	mapreduce\src\java\org\apache\hadoop\mapred\CleanupQueue.java	unrelated	package org apache hadoop mapred cleanup queue log log log factory get log cleanup queue path cleanup thread cleanup thread create singleton path clean queue it used delete paths directories files separate thread this constructor creates clean thread also starts daemon callers instantiate one cleanup queue per jvm use deleting paths use link cleanup queue add to queue path deletion context add paths deletion cleanup queue synchronized path cleanup thread cleanup thread null cleanup thread new path cleanup thread contains info related path file dir deleted path deletion context string full path full path file dir file system fs path deletion context file system fs string full path fs fs full path full path protected string get path for cleanup return full path makes path subdirectories recursively fully deletable protected enable path for cleanup throws io exception do nothing default subclasses provide enabling deletion adds paths queue paths deleted cleanup thread add to queue path deletion context contexts cleanup thread add to queue contexts protected boolean delete path path deletion context context throws io exception context enable path for cleanup log debug enabled log debug trying delete context full path context fs exists new path context full path return context fs delete new path context full path true return true currently used tests protected boolean queue empty return cleanup thread queue size path cleanup thread extends thread cleanup queue deletes files directories paths queued linked blocking queue path deletion context queue new linked blocking queue path deletion context path cleanup thread set name directory file cleanup thread set daemon true start add to queue path deletion context contexts path deletion context context contexts try queue put context catch interrupted exception ie run log debug enabled log debug get name started path deletion context context null true try context queue take delete path delete path context log warn cleanup thread unable delete path context full path else log debug enabled log debug deleted context full path catch interrupted exception log warn interrupted deletion context full path return catch exception e log warn error deleting path context full path e
1166	mapreduce\src\java\org\apache\hadoop\mapred\Clock.java	unrelated	package org apache hadoop mapred a clock mocked testing clock get time return system current time millis
1167	mapreduce\src\java\org\apache\hadoop\mapred\ClusterStatus.java	unrelated	package org apache hadoop mapred status information current state map reduce cluster p code cluster status code provides clients information ol li size cluster li li name trackers li li task capacity cluster li li the number currently running map reduce tasks li li state code job tracker code li li details regarding black listed trackers li ol p p clients query latest code cluster status code via link job client get cluster status p cluster status implements writable class encapsulates information blacklisted tasktracker the information includes tasktracker name reasons getting blacklisted the string method print information whitespace separated fashion enable parsing black list info implements writable string tracker name string reason for black listing string black list report black list info gets blacklisted tasktracker name string get tracker name return tracker name gets reason tasktracker blacklisted string get reason for black listing return reason for black listing sets blacklisted tasktracker name set tracker name string tracker name tracker name tracker name sets reason tasktracker blacklisted set reason for black listing string reason for black listing reason for black listing reason for black listing gets descriptive report tasktracker blacklisted string get black list report return black list report sets descriptive report tasktracker blacklisted blacklisted set black list report string black list report black list report black list report read fields data input throws io exception tracker name text read string reason for black listing text read string black list report text read string write data output throws io exception text write string tracker name text write string reason for black listing text write string black list report print information related blacklisted tasktracker whitespace separated fashion the method changes newlines report describing tasktracker blacklisted enabling better parsing string string string builder sb new string builder sb append tracker name sb append sb append reason for black listing sb append sb append black list report replace n return sb string num active trackers collection string active trackers new array list string num blacklisted trackers num excluded nodes tt expiry interval map tasks reduce tasks max map tasks max reduce tasks job tracker status status collection black list info blacklisted trackers info new array list black list info cluster status construct new cluster status cluster status trackers blacklists tt expiry interval maps reduces max maps max reduces job tracker status status trackers blacklists tt expiry interval maps reduces max maps max reduces status construct new cluster status cluster status trackers blacklists tt expiry interval maps reduces max maps max reduces job tracker status status num decommissioned nodes num active trackers trackers num blacklisted trackers blacklists num excluded nodes num decommissioned nodes tt expiry interval tt expiry interval map tasks maps reduce tasks reduces max map tasks max maps max reduce tasks max reduces status status construct new cluster status cluster status collection string active trackers collection black list info blacklisted trackers tt expiry interval maps reduces max maps max reduces job tracker status status active trackers blacklisted trackers tt expiry interval maps reduces max maps max reduces status construct new cluster status cluster cluster status
1168	mapreduce\src\java\org\apache\hadoop\mapred\CommitTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker commit output task commit task action extends task tracker action task attempt id task id commit task action super action type commit task task id new task attempt id commit task action task attempt id task id super action type commit task task id task id task attempt id get task id return task id write data output throws io exception task id write read fields data input throws io exception task id read fields
1169	mapreduce\src\java\org\apache\hadoop\mapred\CompletedJobStatusStore.java	unrelated	package org apache hadoop mapred persists retrieves job info job dfs p if retain time zero jobs persisted p a daemon thread cleans job info files older retain time p the retain time set persist jobstatus hours configuration variable hours completed job status store implements runnable boolean active string job info dir retain time file system fs string job info store dir jobtracker jobs info ac ls manager acls manager log log log factory get log completed job status store hour sleep time hour fs permission job status store dir permission fs permission create immutable short rwxr x completed job status store configuration conf ac ls manager acls manager throws io exception active conf get boolean jt config jt persist jobstatus true active retain time conf get int jt config jt persist jobstatus hours hour job info dir conf get jt config jt persist jobstatus dir job info store dir path path new path job info dir set fs fs path get file system conf fs exists path fs mkdirs path new fs permission job status store dir permission throw new io exception completed job status store mkdirs failed create path string else file status stat fs get file status path fs permission actual stat get permission stat dir throw new disk error exception directory path string fs action user actual get user action user implies fs action read throw new disk error exception directory readable path string user implies fs action write throw new disk error exception directory writable path string retain time retain time zero stored jobstatuses deleted delete job status dirs acls manager acls manager log info completed job store activated configured retain time retain time job info dir job info dir else log info completed job store inactive indicates job status persistency active boolean active return active run retain time true delete job status dirs try thread sleep sleep time catch interrupted exception ex break delete job status dirs try current time job tracker get clock get time noinspection for loop replaceable by for each file status job info fs list status new path job info dir try current time job info get modification time retain time log info retiring job status store job info get path fs delete job info get path true catch io exception ie log warn could housekeeping job info get path job info ie get message ie catch io exception ie log warn could obtain job info files ie get message ie path get info file path job id job id return new path job info dir job id info persists job dfs store job in progress job active retain time job id job id job get status get job id path job status file get info file path job id try fs data output stream data out fs create job status file job get status write data out job get profile write data out job get counters write data out task completion event events job get task completion events integer max value data out write int events length task completion event event
1170	mapreduce\src\java\org\apache\hadoop\mapred\CompositeTaskTrackerInstrumentation.java	unrelated	package org apache hadoop mapred this task tracker instrumentation subclass forwards events receives list instrumentation objects thus used attack multiple instrumentation objects task tracker composite task tracker instrumentation extends task tracker instrumentation list task tracker instrumentation instrumentations composite task tracker instrumentation task tracker tt list task tracker instrumentation instrumentations super tt instrumentations instrumentations package getter methods tests list task tracker instrumentation get instrumentations return instrumentations complete task task attempt id task tracker instrumentation tti instrumentations tti complete task timedout task task attempt id task tracker instrumentation tti instrumentations tti timedout task task failed ping task attempt id task tracker instrumentation tti instrumentations tti task failed ping report task launch task attempt id file stdout file stderr task tracker instrumentation tti instrumentations tti report task launch stdout stderr report task end task attempt id task tracker instrumentation tti instrumentations tti report task end status update task task task status task status task tracker instrumentation tti instrumentations tti status update task task status
1171	mapreduce\src\java\org\apache\hadoop\mapred\Counters.java	unrelated	package org apache hadoop mapred a set named counters p code counters code represent global counters defined either map reduce framework applications each code counter code link enum type p p code counters code bunched link group comprising counters particular code enum code counters implements writable iterable counters group log log log factory get log counters char group open char group close char counter open char counter close char unit open char unit close char chars to escape group open group close counter open counter close unit open unit close log log log factory get log counters downgrade new link org apache hadoop mapreduce counters old counters counters downgrade org apache hadoop mapreduce counters new counters counters old counters new counters org apache hadoop mapreduce counter group new group new counters string group name new group get name group old group old counters get group group name org apache hadoop mapreduce counter new counter new group counter old counter old group get counter for name new counter get name old counter set display name new counter get display name old counter increment new counter get value return old counters a counter record comprising name value counter extends org apache hadoop mapreduce counter counter counter string name string display name value super name display name increment value set display name string new name super set display name new name returns compact stringified version counter format actual name display name value synchronized string make escaped compact string first obtain strings need escaping this help us determine buffer length apriori string escaped name escape get name string escaped disp name escape get display name current value get value length escaped name length escaped disp name length length for following delimiting characters string builder builder new string builder length builder append counter open add counter name builder append unit open builder append escaped name builder append unit close add display name builder append unit open builder append escaped disp name builder append unit close add value builder append unit open builder append current value builder append unit close builder append counter close return builder string checks content equality two basic counters synchronized boolean content equals counter c return equals c what current value counter synchronized get counter return get value code group code counters comprising counters particular counter link enum p code group code handles localization name counter names p group implements writable iterable counter string group name string display name map string counter subcounters new hash map string counter optional resource bundle localization group counter names resource bundle bundle null group string group name try bundle get resource bundle group name catch missing resource exception never mind group name group name display name localize counter group name group name log debug enabled log debug creating group group name bundle null nothing bundle returns specified resource bundle throws exception resource bundle get resource bundle string enum class name string bundle name enum class name replace return resource bundle get bundle bundle name returns raw name group this name enum group counters string get name return group
1172	mapreduce\src\java\org\apache\hadoop\mapred\DefaultTaskController.java	unrelated	package org apache hadoop mapred the default implementation controlling tasks this provides implementation launching killing tasks need run tasktracker hence many initializing cleanup methods required br default task controller extends task controller log log log factory get log default task controller launch new jvm task this method launches new jvm task executing jvm command using link shell shell command executor launch task jvm task controller task controller context context throws io exception initialize task context jvm env env context env list string wrapped command task log capture out and error env setup env vargs env stdout env stderr env log size true shell command executor shexec new shell command executor wrapped command array new string env work dir env env set shell command executor later use context sh exec shexec shexec execute initialize task environment since tasks launched tasktracker user method action perform initialize task task controller task controller context context the default task controller need set permissions proper execution so dummy method return no need anything need dont need anything extra task tracker done initialize job job initialization context context terminate task task controller context context shell command executor shexec context sh exec shexec null process process shexec get process shell windows currently use setsid windows so kill process alone process null process destroy else in addition task jvm kill subprocesses also string pid context pid pid null process tree setsid available process tree terminate process group pid else process tree terminate process pid kill task task controller context context shell command executor shexec context sh exec shexec null shell windows we send kill process signal case windows already done process destroy terminate task jvm return string pid context pid pid null process tree setsid available process tree kill process group pid else process tree kill process pid dump task stack task controller context context shell command executor shexec context sh exec shexec null shell windows we use signals windows return string pid context pid pid null send sigquit get stack dump process tree setsid available process tree sig quit process group pid else process tree sig quit process pid initialize distributed cache file distributed cache file context context throws io exception path localized unique dir context get localized unique dir try setting recursive execute permission localized dir log info doing chmod localdir localized unique dir file util chmod localized unique dir string x true catch interrupted exception ie log warn exception chmod localized unique dir ie throw new io exception ie initialize user initialization context context do nothing run debug script debug script context context throws io exception list string wrapped command task log capture debug out context args context stdout run script shell command executor shexec new shell command executor wrapped command array new string context work dir shexec execute exit code shexec get exit code exit code throw new io exception task debug script exit nonzero status exit code enables task cleanup changing permissions specified path local filesystem enable task for cleanup path deletion context context throws io exception enable path for cleanup context enables job cleanup changing
1173	mapreduce\src\java\org\apache\hadoop\mapred\DeprecatedQueueConfigurationParser.java	unrelated	package org apache hadoop mapred class build queue hierarchy using deprecated conf mapred site xml generates single level queue hierarchy deprecated queue configuration parser extends queue configuration parser log log log factory get log deprecated queue configuration parser string mapred queue names key mapred queue names deprecated queue configuration parser configuration conf if configuration done return immediately deprecated conf conf return list queue listq create queues conf set acls enabled conf get boolean mr config mr acls enabled false root new queue root set name queue q listq root add child q list queue create queues configuration conf string queue name values conf get strings mapred queue names key list queue list new array list queue string name queue name values try map string access control list acls get queue acls name conf queue state state get queue state name conf queue q new queue name acls state list add q catch throwable log warn not able initialize queue name return list only applicable leaf level queues parse ac ls queue configuration queue state get queue state string name configuration conf string state val conf get full property name name state queue state running get state name return queue state get state state val check queue properties configured passed configuration if yes print deprecation warning messages boolean deprecated conf configuration conf string queues null string queue name values get queue names conf queue name values null return false else log warn configuring mapred queue names key mapred site xml hadoop site xml deprecated overshadow queue conf file name remove property configure queue hierarchy queue conf file name store queues check ac ls also configured deprecated files queues conf get strings mapred queue names key check acls defined queues null string queue queues queue acl q acl queue acl values string key full property name queue q acl get acl name string acl string conf get key acl string null log warn configuring queue ac ls mapred site xml hadoop site xml deprecated configure queue ac ls queue conf file name even one configured enough printing warning return return true return true string get queue names configuration conf string queue name values conf get mapred queue names key return queue name values parse ac ls queue configuration map string access control list get queue acls string name configuration conf hash map string access control list map new hash map string access control list queue acl q acl queue acl values string acl key full property name name q acl get acl name map put acl key new access control list conf get acl key return map
1174	mapreduce\src\java\org\apache\hadoop\mapred\DisallowedTaskTrackerException.java	unrelated	package org apache hadoop mapred this exception thrown tasktracker tries register communicate jobtracker appear list included nodes specifically excluded disallowed task tracker exception extends io exception serial version uid l disallowed task tracker exception task tracker status tracker super tasktracker denied communication jobtracker tracker get tracker name
1175	mapreduce\src\java\org\apache\hadoop\mapred\EagerTaskInitializationListener.java	pooling	package org apache hadoop mapred a link job in progress listener initializes tasks job soon job added using link job added job in progress method eager task initialization listener extends job in progress listener default num threads log log log factory get log eager task initialization listener get name used init new jobs created job init manager implements runnable run job in progress job null true try synchronized job init queue job init queue empty job init queue wait job job init queue remove thread pool execute new init job job catch interrupted exception log info job init manager thread interrupted break log info shutting thread pool thread pool shutdown now init job implements runnable job in progress job init job job in progress job job job run ttm init job job job init manager job init manager new job init manager thread job init manager thread list job in progress job init queue new array list job in progress executor service thread pool num threads task tracker manager ttm eager task initialization listener configuration conf num threads conf get int jt config jt jobinit threads default num threads thread pool executors new fixed thread pool num threads set task tracker manager task tracker manager ttm ttm ttm start throws io exception job init manager thread new thread job init manager job init manager job init manager thread set daemon true job init manager thread start terminate throws io exception job init manager thread null job init manager thread alive log info stopping job init manager thread job init manager thread interrupt try job init manager thread join catch interrupted exception ex ex print stack trace we add jip job init queue processed asynchronously handle split computation build right task tracker block mapping job added job in progress job synchronized job init queue job init queue add job resort init queue job init queue notify all sort jobs priority start time synchronized resort init queue comparator job in progress comp new comparator job in progress compare job in progress job in progress res get priority compare to get priority res get start time get start time res else res get start time get start time return res synchronized job init queue collections sort job init queue comp job removed job in progress job synchronized job init queue job init queue remove job job updated job change event event event instanceof job status change event job state changed job status change event event called job status changed job state changed job status change event event resort job queue job start time job priority changes event get event type event type start time changed event get event type event type priority changed synchronized job init queue resort init queue
1176	mapreduce\src\java\org\apache\hadoop\mapred\FileAlreadyExistsException.java	unrelated	package org apache hadoop mapred used target file already exists operation configured overwritten file already exists exception extends io exception serial version uid l file already exists exception super file already exists exception string msg super msg
1177	mapreduce\src\java\org\apache\hadoop\mapred\FileInputFormat.java	unrelated	package org apache hadoop mapred a base file based link input format p code file input format code base file based code input format code this provides generic implementation link get splits job conf subclasses code file input format code also link splitable file system path method ensure input files split processed whole link mapper instead file input format k v implements input format k v log log log factory get log file input format string num input files org apache hadoop mapreduce lib input file input format num input files split slop slop min split size path filter hidden file filter new path filter boolean accept path p string name p get name return name starts with name starts with protected set min split size min split size min split size min split size proxy path filter accepts path filters given constructor used list paths apply built hidden file filter together user provided one multi path filter implements path filter list path filter filters multi path filter list path filter filters filters filters boolean accept path path path filter filter filters filter accept path return false return true is given filename splitable usually true file stream compressed code file input format code implementations return code false code ensure individual input files never split link mapper process entire files protected boolean splitable file system fs path filename return true record reader k v get record reader input split split job conf job reporter reporter throws io exception set path filter applied input paths map reduce job set input path filter job conf conf class extends path filter filter conf set class org apache hadoop mapreduce lib input file input format pathfilter class filter path filter get path filter instance filter set input paths path filter get input path filter job conf conf class extends path filter filter class conf get class org apache hadoop mapreduce lib input file input format pathfilter class null path filter return filter class null reflection utils new instance filter class conf null add files input path recursively results the list store files the file system the input path the input filter used filter files dirs protected add input path recursively list file status result file system fs path path path filter input filter throws io exception file status stat fs list status path input filter stat directory add input path recursively result fs stat get path input filter else result add stat list input directories subclasses may e g select files matching regular expression protected file status list status job conf job throws io exception path dirs get input paths job dirs length throw new io exception no input paths specified job get tokens required file systems token cache obtain tokens for namenodes job get credentials dirs job whether need recursive look directory structure boolean recursive job get boolean mapred input dir recursive false list file status result new array list file status list io exception errors new array list io exception creates multi path filter hidden file filter user provided one list path filter filters new
1178	mapreduce\src\java\org\apache\hadoop\mapred\FileOutputCommitter.java	unrelated	package org apache hadoop mapred an link output committer commits files specified job output directory e mapreduce output fileoutputformat outputdir file output committer extends output committer log log log factory get log org apache hadoop mapred file output committer temporary directory name string temp dir name temporary string succeeded file name success string successful job output dir marker mapreduce fileoutputcommitter marksuccessfuljobs setup job job context context throws io exception job conf conf context get job conf path output path file output format get output path conf output path null path tmp dir new path output path file output committer temp dir name file system file sys tmp dir get file system conf file sys mkdirs tmp dir log error mkdirs failed create tmp dir string true job requires output dir marked successful job note default set true boolean mark output dir job conf conf return conf get boolean successful job output dir marker true commit job job context context throws io exception delete temporary folder output folder cleanup job context check output dir marking required mark output dir context get job conf create success file output folder mark output dir successful context create success file job output folder mark output dir successful job context context throws io exception job conf conf context get job conf get p path path output path file output format get output path conf output path null get filesys file system file sys output path get file system conf create file output folder mark job completion path file path new path output path succeeded file name file sys create file path close cleanup job job context context throws io exception job conf conf context get job conf clean temporary directory path output path file output format get output path conf output path null path tmp dir new path output path file output committer temp dir name file system file sys tmp dir get file system conf context get progressible progress file sys exists tmp dir file sys delete tmp dir true else log warn output path null cleanup abort job job context context run state throws io exception simply delete temporary dir p folder job cleanup job context setup task task attempt context context throws io exception file output committer setup task anything because temporary task directory created demand task writing commit task task attempt context context throws io exception path task output path get temp task output path context task attempt id attempt id context get task attempt id job conf job context get job conf task output path null file system fs task output path get file system job context get progressible progress fs exists task output path path job output path task output path get parent get parent move task outputs place move task outputs context fs job output path task output path delete temporary task specific output directory fs delete task output path true log info failed delete temporary output directory task attempt id task output path log info saved output task attempt id job output path move task outputs task attempt context context
1179	mapreduce\src\java\org\apache\hadoop\mapred\FileOutputFormat.java	unrelated	package org apache hadoop mapred a base link output format file output format k v implements output format k v set whether output job compressed set compress output job conf conf boolean compress conf set boolean org apache hadoop mapreduce lib output file output format compress compress is job output compressed code false code otherwise boolean get compress output job conf conf return conf get boolean org apache hadoop mapreduce lib output file output format compress false set link compression codec used compress job outputs compress job outputs set output compressor class job conf conf class extends compression codec codec class set compress output conf true conf set class org apache hadoop mapreduce lib output file output format compress codec codec class compression codec get link compression codec compressing job outputs job outputs class extends compression codec get output compressor class job conf conf class extends compression codec default value class extends compression codec codec class default value string name conf get org apache hadoop mapreduce lib output file output format compress codec name null try codec class conf get class by name name subclass compression codec catch class not found exception e throw new illegal argument exception compression codec name found e return codec class record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception check output specs file system ignored job conf job throws file already exists exception invalid job conf exception io exception ensure output directory set already path dir get output path job dir null job get num reduce tasks throw new invalid job conf exception output directory set job conf dir null file system fs dir get file system job normalize output directory dir fs make qualified dir set output path job dir get delegation token dir file system token cache obtain tokens for namenodes job get credentials new path dir job check existence fs exists dir throw new file already exists exception output directory dir already exists set link path output directory map reduce job map reduce job set output path job conf conf path output dir output dir new path conf get working directory output dir conf set org apache hadoop mapreduce lib output file output format outdir output dir string set link path task temporary output directory map reduce job p note task output path set framework p map reduce job set work output path job conf conf path output dir output dir new path conf get working directory output dir conf set job context task output dir output dir string get link path output directory map reduce job path get output path job conf conf string name conf get org apache hadoop mapreduce lib output file output format outdir return name null null new path name get link path task temporary output directory map reduce job id side effect files tasks side effect files p note the following valid link output committer link file output committer if code output committer code code file output committer code task temporary output directory link get output
1180	mapreduce\src\java\org\apache\hadoop\mapred\FileSplit.java	unrelated	package org apache hadoop mapred a section input file returned link input format get splits job conf passed link input format get record reader input split job conf reporter instead file split extends org apache hadoop mapreduce input split implements input split org apache hadoop mapreduce lib input file split fs protected file split fs new org apache hadoop mapreduce lib input file split constructs split file split path file start length job conf conf file start length string null constructs split host information file split path file start length string hosts fs new org apache hadoop mapreduce lib input file split file start length hosts file split org apache hadoop mapreduce lib input file split fs fs fs the file containing split data path get path return fs get path the position first byte file process get start return fs get start the number bytes file process get length return fs get length string string return fs string writable methods write data output throws io exception fs write read fields data input throws io exception fs read fields string get locations throws io exception return fs get locations
1181	mapreduce\src\java\org\apache\hadoop\mapred\HeartbeatResponse.java	heartbeat	package org apache hadoop mapred the response sent link job tracker hearbeat sent periodically link task tracker heartbeat response implements writable configurable configuration conf null short response id heartbeat interval task tracker action actions heartbeat response heartbeat response short response id task tracker action actions response id response id actions actions heartbeat interval jt config jt heartbeat interval min default set response id short response id response id response id short get response id return response id set actions task tracker action actions actions actions task tracker action get actions return actions set conf configuration conf conf conf configuration get conf return conf set heartbeat interval interval heartbeat interval interval get heartbeat interval return heartbeat interval write data output throws io exception write short response id write int heartbeat interval actions null writable utils write v int else writable utils write v int actions length task tracker action action actions writable utils write enum action get action id action write read fields data input throws io exception response id read short heartbeat interval read int length writable utils read v int length actions new task tracker action length length task tracker action action type action type writable utils read enum task tracker action action type actions task tracker action create action action type actions read fields else actions null
1182	mapreduce\src\java\org\apache\hadoop\mapred\ID.java	unrelated	package org apache hadoop mapred a general identifier internally stores id integer this super link job id link task id link task attempt id id extends org apache hadoop mapreduce id constructs id object given id id super id protected id
1183	mapreduce\src\java\org\apache\hadoop\mapred\IFile.java	pooling	package org apache hadoop mapred code i file code simple key len value len key value format intermediate map outputs map reduce there code writer code write map outputs format code reader code read files format i file log log log factory get log i file eof marker end file marker code i file writer code write intermediate map outputs writer k extends object v extends object fs data output stream boolean output stream false start fs data output stream raw out compression output stream compressed out compressor compressor boolean compress output false decompressed bytes written compressed bytes written count records written disk num records written counters counter written records counter i file output stream checksum out class k key class class v value class serializer k key serializer serializer v value serializer data output buffer buffer new data output buffer writer configuration conf file system fs path file class k key class class v value class compression codec codec counters counter writes counter throws io exception conf fs create file key class value class codec writes counter output stream true protected writer counters counter writes counter written records counter writes counter writer configuration conf fs data output stream class k key class class v value class compression codec codec counters counter writes counter throws io exception written records counter writes counter checksum out new i file output stream raw out start raw out get pos codec null compressor codec pool get compressor codec compressor null compressor reset compressed out codec create output stream checksum out compressor new fs data output stream compressed out null compress output true else log warn could obtain compressor codec pool new fs data output stream checksum out null else new fs data output stream checksum out null key class key class value class value class key class null serialization factory serialization factory new serialization factory conf key serializer serialization factory get serializer key class key serializer open buffer value serializer serialization factory get serializer value class value serializer open buffer writer configuration conf file system fs path file throws io exception conf fs file null null null null close throws io exception when i file writer created backup store key value set so check closing serializers key class null key serializer close value serializer close write eof marker key value length writable utils write v int eof marker writable utils write v int eof marker decompressed bytes written writable utils get v int size eof marker flush stream flush compress output flush compressed out finish compressed out reset state close underlying stream iff output stream close else write checksum checksum out finish compressed bytes written raw out get pos start compress output return back compressor codec pool return compressor compressor compressor null null written records counter null written records counter increment num records written append k key v value throws io exception key get class key class throw new io exception wrong key key get class key class value get class value class throw new io exception wrong value value get class value class append key
1184	mapreduce\src\java\org\apache\hadoop\mapred\IFileInputStream.java	unrelated	package org apache hadoop mapred a checksum input stream used i files used validate checksum files created link i file output stream i file input stream extends input stream input stream the input stream verified checksum length the total length input file data length data checksum sum current offset byte b new byte byte csum null checksum size boolean disable checksum validation false create checksum input stream reads i file input stream input stream len sum data checksum new data checksum data checksum checksum crc integer max value checksum size sum get checksum size length len data length length checksum size close input stream note need read end stream validate checksum close throws io exception current offset data length byte new byte math min integer max value data length current offset current offset data length n read length n throw new eof exception could validate checksum close skip n throws io exception throw new io exception skip supported i file input stream get position return current offset data length data length current offset get size return checksum size read bytes stream at eof checksum validated checksum bytes passed back buffer read byte b len throws io exception current offset data length return return read b len read bytes stream at eof checksum validated sent back last four bytes buffer the caller handle bytes appropriately read with checksum byte b len throws io exception current offset length return else current offset data length if previous read drained data return checksum note checksum validation would happened earlier read len to copy checksum size current offset data length len len to copy len to copy len system arraycopy csum current offset data length b len to copy current offset len to copy return len to copy bytes read read b len current offset data length len bytes read checksum size system arraycopy csum b bytes read checksum size bytes read checksum size current offset checksum size return bytes read read byte b len throws io exception if trying read past end data read left data current offset len data length len data length current offset bytes read read b len bytes read throw new checksum exception checksum error sum update b bytes read current offset bytes read disable checksum validation return bytes read current offset data length the last four bytes checksum strip verify csum new byte checksum size io utils read fully csum checksum size sum compare csum throw new checksum exception checksum error return bytes read read throws io exception b read b return upgrade b misinterpret first bit byte sign bit result x ff b return result byte get checksum return csum disable checksum validation disable checksum validation true
1185	mapreduce\src\java\org\apache\hadoop\mapred\IFileOutputStream.java	unrelated	package org apache hadoop mapred a checksum output stream checksum contents file calculated appended end file close stream used i files i file output stream extends filter output stream the output stream checksummed data checksum sum byte barray boolean closed false boolean finished false create checksum output stream writes bytes given stream i file output stream output stream super sum data checksum new data checksum data checksum checksum crc integer max value barray new byte sum get checksum size close throws io exception closed return closed true finish close finishes writing data output stream writing checksum bytes end the underlying stream closed finish throws io exception finished return finished true sum write value barray false write barray sum get checksum size flush write bytes stream write byte b len throws io exception sum update b len write b len write b throws io exception barray byte b x ff write barray
1186	mapreduce\src\java\org\apache\hadoop\mapred\IndexCache.java	unrelated	package org apache hadoop mapred index cache job conf conf total memory allowed atomic integer total memory used new atomic integer log log log factory get log index cache concurrent hash map string index information cache new concurrent hash map string index information linked blocking queue string queue new linked blocking queue string index cache job conf conf conf conf total memory allowed conf get int tt config tt index cache log info index cache created max memory total memory allowed this method gets index information given map id reduce it reads index file cache already present already present cache index record get index information string map id reduce path file name string expected index owner throws io exception index information info cache get map id info null info read index file to cache file name map id expected index owner else synchronized info null info map spill record try info wait catch interrupted exception e throw new io exception interrupted waiting construction e log debug index cache hit map id map id found info map spill record size info map spill record size reduce throw new io exception invalid request map id map id reducer reduce index info length info map spill record size return info map spill record get index reduce index information read index file to cache path index file name string map id string expected index owner throws io exception index information info index information new ind new index information info cache put if absent map id new ind null synchronized info null info map spill record try info wait catch interrupted exception e throw new io exception interrupted waiting construction e log debug index cache hit map id map id found return info log debug index cache miss map id map id found spill record tmp null try tmp new spill record index file name conf expected index owner catch throwable e tmp new spill record cache remove map id throw new io exception error reading index file e finally synchronized new ind new ind map spill record tmp new ind notify all queue add map id total memory used add and get new ind get size total memory allowed free index information return new ind this method removes map cache it called map output tracker discarded remove map string map id index information info cache remove map id info null total memory used add and get info get size queue remove map id log warn map id map id found queue else log info map id map id found cache bring memory usage total memory allowed synchronized free index information total memory used get total memory allowed string queue remove index information info cache remove info null total memory used add and get info get size index information spill record map spill record get size return map spill record null map spill record size map task map output index record length
1187	mapreduce\src\java\org\apache\hadoop\mapred\InputFormat.java	unrelated	package org apache hadoop mapred code input format code describes input specification map reduce job p the map reduce framework relies code input format code job p ol li validate input specification job li split input file logical link input split assigned individual link mapper li li provide link record reader implementation used glean input records logical code input split code processing link mapper li ol p the default behavior file based link input format typically sub link file input format split input logical link input split based total size bytes input files however link file system blocksize input files treated upper bound input splits a lower bound split size set via href doc root mapred default html mapreduce input fileinputformat split minsize mapreduce input fileinputformat split minsize p p clearly logical splits based input size insufficient many applications since record boundaries respected in cases application also implement link record reader lies responsibilty respect record boundaries present record oriented view logical code input split code individual task input format k v logically split set input files job p each link input split assigned individual link mapper processing p p note the split logical split inputs input files physically split chunks for e g split could lt input file path start offset gt tuple input split get splits job conf job num splits throws io exception get link record reader given link input split p it responsibility code record reader code respect record boundaries processing logical split present record oriented view individual task p record reader k v get record reader input split split job conf job reporter reporter throws io exception
1188	mapreduce\src\java\org\apache\hadoop\mapred\InputSplit.java	unrelated	package org apache hadoop mapred code input split code represents data processed individual link mapper p typically presents byte oriented view input responsibility link record reader job process present record oriented view input split extends writable get total number bytes data code input split code get length throws io exception get list hostnames input split located located array code string code string get locations throws io exception
1189	mapreduce\src\java\org\apache\hadoop\mapred\InterTrackerProtocol.java	heartbeat	package org apache hadoop mapred protocol task tracker central job tracker use communicate the job tracker server implements protocol server principal jt config jt user name client principal tt config tt user name inter tracker protocol extends versioned protocol version introduced replace emit hearbeat poll for new task poll for task with closed job link heartbeat task tracker status boolean boolean boolean short version changed task report hadoop version introduced removes locate map outputs instead uses get task completion events figure finished maps fetch outputs version adds max tasks task tracker status hadoop version replaces max tasks max map tasks max reduce tasks task tracker status hadoop version heartbeat response added next heartbeat interval version changes counter representation hadoop version changes task status representation hadoop version changes job id get task completion events version changes counters representation hadoop version added call get build version hadoop version replaced get filesystem name get system dir hadoop version changed format task task status hadoop version adds resource status task tracker status hadoop version changed format task task status hadoop version changed status message due changes task status version changed heartbeat piggyback job tracker restart information task tracker synchronize version changed status message due changes task status hadoop version changed information reported task tracker status resource status corresponding accessor methods hadoop version replaced parameter initial contact restarted heartbeat method hadoop version added parameter initial contact heartbeat method hadoop version changed format task task status hadoop version job i ds passed response job tracker restart version modified task id aware new task types version added num required slots task status mapreduce version adding node health status task status mapreduce version adding user name serialized task use tt version adding available memory cpu usage information tt task tracker status mapreduce version id l trackers ok unknown tasktracker called regularly link task tracker update status tasks within job tracker link job tracker responds link heartbeat response directs link task tracker undertake series actions see link org apache hadoop mapred task tracker action action type link task tracker must also indicate whether first interaction since state refresh acknowledge last response received link job tracker restarted code false code otherwise refresh code false code otherwise ready accept new tasks run link task tracker fresh instructions heartbeat response heartbeat task tracker status status boolean restarted boolean initial contact boolean accept new tasks short response id throws io exception the task tracker calls discern find files referred job tracker string get filesystem name throws io exception report problem job tracker remote side report task tracker error string task tracker string error class string error message throws io exception get task completion events jobid starting event id returns empty aray events available task completion event get task completion events job id jobid event id max events throws io exception grab jobtracker system directory path job specific files placed string get system dir returns build version job tracker string get build version throws io exception
1190	mapreduce\src\java\org\apache\hadoop\mapred\InvalidFileTypeException.java	unrelated	package org apache hadoop mapred used file type differs desired file type like getting file directory expected or wrong file type invalid file type exception extends io exception serial version uid l invalid file type exception super invalid file type exception string msg super msg
1191	mapreduce\src\java\org\apache\hadoop\mapred\InvalidInputException.java	unrelated	package org apache hadoop mapred this wraps list problems input user get list problems together instead finding fixing one one invalid input exception extends io exception serial version uid l list io exception problems create exception given list invalid input exception list io exception probs problems probs get complete list problems reported list io exception get problems return problems get summary message problems found string get message string buffer result new string buffer iterator io exception itr problems iterator itr next result append itr next get message itr next result append n return result string
1192	mapreduce\src\java\org\apache\hadoop\mapred\InvalidJobConfException.java	unrelated	package org apache hadoop mapred this exception thrown jobconf misses mendatory attributes value attributes invalid invalid job conf exception extends io exception serial version uid l invalid job conf exception super invalid job conf exception string msg super msg
1193	mapreduce\src\java\org\apache\hadoop\mapred\JobACLsManager.java	unrelated	package org apache hadoop mapred job ac ls manager configuration conf job ac ls manager configuration conf conf conf boolean ac ls enabled return conf get boolean mr config mr acls enabled false construct job ac ls configuration kept memory if authorization disabled jt nothing constructed empty map returned map job acl access control list construct job ac ls configuration conf map job acl access control list acls new hash map job acl access control list don construct anything authorization disabled ac ls enabled return acls job acl acl name job acl values string acl config name acl name get acl name string acl configured conf get acl config name acl configured null if ac ls configured grant access anyone so job owner cluster administrator stuff acl configured acls put acl name new access control list acl configured return acls if authorization enabled checks whether user caller ugi authorized perform operation specified job operation job checking user job owner part job acl specific job operation ul li the owner job operation job li li for users groups job acls checked li ul boolean check access user group information caller ugi job acl job operation string job owner access control list job acl string user caller ugi get short user name ac ls enabled return true allow job owner operation job user equals job owner job acl user allowed caller ugi return true return false
1194	mapreduce\src\java\org\apache\hadoop\mapred\JobChangeEvent.java	unrelated	package org apache hadoop mapred link job change event used capture state changes job a job change state w r priority progress run state etc job change event job in progress jip job change event job in progress jip jip jip get job object change reported job in progress get job in progress return jip
1195	mapreduce\src\java\org\apache\hadoop\mapred\JobClient.java	scheduler	package org apache hadoop mapred code job client code primary user job interact link job tracker code job client code provides facilities submit jobs track progress access component tasks reports logs get map reduce cluster status information etc p the job submission process involves ol li checking input output specifications job li li computing link input split job li li setup requisite accounting information link distributed cache job necessary li li copying job jar configuration map reduce system directory distributed file system li li submitting job code job tracker code optionally monitoring status li ol p normally user creates application describes various facets job via link job conf uses code job client code submit job monitor progress p here example use code job client code p p blockquote pre create new job conf job conf job new job conf new configuration my job specify various job specific parameters job set job name myjob job set input path new path job set output path new path job set mapper class my job my mapper job set reducer class my job my reducer submit job poll progress job complete job client run job job pre blockquote p id job control job control p at times clients would chain map reduce jobs accomplish complex tasks cannot done via single map reduce job this fairly easy since output job typically goes distributed file system used input next job p p however also means onus ensuring jobs complete success failure lies squarely clients in situations various job control options ol li link run job job conf submits job returns job completed li li link submit job job conf submits job poll returned handle link running job query status make scheduling decisions li li link job conf set job end notification uri string setup notification job completion thus avoiding polling li ol p job client extends cli enum task status filter none killed failed succeeded all task status filter task output filter task status filter failed config util load resources a networked job implementation running job it holds job profile object provide info interacts remote service provide certain functionality networked job implements running job job job we store job profile timestamp last acquired job profile if job null cannot perform tasks the job might null job tracker completely forgotten job eg hours job completes networked job job status status cluster cluster throws io exception job job get instance cluster status new job conf status get job file networked job job job throws io exception job job configuration get configuration return job get configuration an identifier job job id get id return job id downgrade job get job id rather use link get id string get job id return get id string the user specified job name string get job name return job get job name the name job file string get job file return job get job file a url job status seen string get tracking url return job get tracking url a indicating map work completed map progress throws io exception try return job map progress catch
1196	mapreduce\src\java\org\apache\hadoop\mapred\JobConf.java	scheduler	package org apache hadoop mapred a map reduce job configuration p code job conf code primary user describe map reduce job hadoop framework execution the framework tries faithfully execute job described code job conf code however ol li some configuration parameters might marked href doc root org apache hadoop conf configuration html final params administrators hence cannot altered li li while job parameters straight forward set e g link set num reduce tasks parameters interact subtly rest framework job configuration relatively complex user control finely e g link set num map tasks li ol p p code job conf code typically specifies link mapper combiner link partitioner link reducer link input format link output format implementations used etc p optionally code job conf code used specify advanced facets job code comparator code used files put link distributed cache whether intermediate job outputs compressed debugability via user provided scripts link set map debug script string link set reduce debug script string post processing task logs task stdout stderr syslog etc p p here example configure job via code job conf code p p blockquote pre create new job conf job conf job new job conf new configuration my job specify various job specific parameters job set job name myjob file input format set input paths job new path file output format set output path job new path job set mapper class my job my mapper job set combiner class my job my reducer job set reducer class my job my reducer job set input format sequence file input format job set output format sequence file output format pre blockquote p job conf extends configuration log log log factory get log job conf config util load resources link mapred job reduce memory mb property string mapred task maxvmem property mapred task maxvmem string upper limit on task vmem property mapred task limit maxvmem string mapred task default maxvmem property mapred task default maxvmem string mapred task maxpmem property mapred task maxpmem a value set memory related configuration options indicates options turned disabled memory limit l property name configuration property mapreduce cluster local dir string mapred local dir property mr config local dir name queue jobs submitted queue name mentioned string default queue name default string mapred job map memory mb property job context map memory mb string mapred job reduce memory mb property job context reduce memory mb pattern default unpacking behavior job jars pattern unpack jar pattern default pattern compile lib configuration key set java command line options child map reduce tasks java opts task tracker child processes the following symbol present interpolated taskid it replaced current task id any occurrences go unchanged for example enable verbose gc logging file named taskid tmp set heap maximum gigabyte pass value xmx verbose gc xloggc tmp taskid gc the configuration variable link mapred task ulimit used control maximum virtual memory child processes the configuration variable link mapred task env used pass environment variables child processes link mapred reduce task java opts string mapred task java opts mapred child java opts configuration key set java command
1197	mapreduce\src\java\org\apache\hadoop\mapred\JobConfigurable.java	unrelated	package org apache hadoop mapred that may configured job configurable initializes new instance link job conf configure job conf job
1198	mapreduce\src\java\org\apache\hadoop\mapred\JobContext.java	unrelated	package org apache hadoop mapred job context extends org apache hadoop mapreduce job context get job configuration job conf get job conf get progress mechanism reporting progress progressable get progressible
1199	mapreduce\src\java\org\apache\hadoop\mapred\JobContextImpl.java	unrelated	package org apache hadoop mapred job context impl extends org apache hadoop mapreduce task job context impl implements job context job conf job progressable progress job context impl job conf conf org apache hadoop mapreduce job id job id progressable progress super conf job id job conf progress progress job context impl job conf conf org apache hadoop mapreduce job id job id conf job id reporter null get job configuration job conf get job conf return job get progress mechanism reporting progress progressable get progressible return progress
1200	mapreduce\src\java\org\apache\hadoop\mapred\JobEndNotifier.java	unrelated	package org apache hadoop mapred job end notifier log log log factory get log job end notifier get name thread thread volatile boolean running blocking queue job end status info queue new delay queue job end status info start notifier running true thread new thread new runnable run try running send notification queue take catch interrupted exception irex running log error thread ended unexpectedly irex send notification job end status info notification try code http notification notification get uri code throw new io exception invalid response status code code catch io exception ioex log error notification failure notification ioex notification configure for retry try queue put notification catch interrupted exception iex log error notification queuing error notification iex catch exception ex log error notification failure notification ex thread start stop notifier running false thread interrupt job end status info create notification job conf conf job status status job end status info notification null string uri conf get job end notification uri uri null make logic first notification identical retry retry attempts conf get int job context end notification retries retry interval conf get int job context end notification retrie interval uri contains job id uri uri replace job id status get job id string uri contains job status string status str status get run state job status succeeded succeeded status get run state job status failed failed killed uri uri replace job status status str notification new job end status info uri retry attempts retry interval return notification register notification job conf job conf job status status job end status info notification create notification job conf status notification null try queue put notification catch interrupted exception iex log error notification queuing failure notification iex http notification string uri throws io exception uri url new uri uri false http client client new http client http method method new get method url get escaped uri method set request header accept return client execute method method use local job runner without using thread queue simple synchronous way local runner notification job conf conf job status status job end status info notification create notification conf status notification null notification configure for retry try code http notification notification get uri code throw new io exception invalid response status code code else break catch io exception ioex log error notification error notification get uri ioex catch exception ex log error notification error notification get uri ex try thread sleep notification get retry interval catch interrupted exception iex log error notification retry error notification iex job end status info implements delayed string uri retry attempts retry interval delay time job end status info string uri retry attempts retry interval uri uri retry attempts retry attempts retry interval retry interval delay time system current time millis string get uri return uri get retry attempts return retry attempts get retry interval return retry interval get delay time return delay time boolean configure for retry boolean retry false get retry attempts retry true delay time system current time millis retry interval retry attempts return retry get delay time unit unit n delay
1201	mapreduce\src\java\org\apache\hadoop\mapred\JobID.java	unrelated	package org apache hadoop mapred job id represents immutable unique identifier job job id consists two parts first part represents jobtracker identifier job id jobtracker map defined for cluster setup jobtracker start time local setting local second part job id job number br an example job id code job code represents third job running jobtracker started code code p applications never construct parse job id strings rather use appropriate constructors link name string method job id extends org apache hadoop mapreduce job id constructs job id object job id string jt identifier id super jt identifier id job id downgrade new job id old one job id downgrade org apache hadoop mapreduce job id old old instanceof job id return job id old else return new job id old get jt identifier old get id job id read data input throws io exception job id job id new job id job id read fields return job id construct job id object given job id name string str throws illegal argument exception return job id org apache hadoop mapreduce job id name str returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching job run jobtracker started would use pre job id get task i ds pattern null pre return pre job pre string get job i ds pattern string jt identifier integer job id string builder builder new string builder job append separator builder append get job i ds pattern wo prefix jt identifier job id return builder string string builder get job i ds pattern wo prefix string jt identifier integer job id string builder builder new string builder jt identifier null builder append jt identifier else builder append append separator append builder append separator append job id null id format format job id return builder
1202	mapreduce\src\java\org\apache\hadoop\mapred\JobInfo.java	unrelated	package org apache hadoop mapred represents basic information saved per job job tracker receives submit job request the information saved job tracker recover incomplete jobs upon restart job info implements writable org apache hadoop mapreduce job id id text user path job submit dir job info job info org apache hadoop mapreduce job id id text user path job submit dir id id user user job submit dir job submit dir get job id org apache hadoop mapreduce job id get job id return id get configured job user name text get user return user get job submission directory path get job submit dir return job submit dir read fields data input throws io exception id new org apache hadoop mapreduce job id id read fields user new text user read fields job submit dir new path writable utils read string write data output throws io exception id write user write writable utils write string job submit dir string
1203	mapreduce\src\java\org\apache\hadoop\mapred\JobInProgress.java	audit	package org apache hadoop mapred job in progress maintains info keeping job straight narrow it keeps job profile latest job status plus set tables bookkeeping tasks job in progress used kill issued job initializing kill interrupted exception extends interrupted exception serial version uid l kill interrupted exception string msg super msg log log log factory get log job in progress job profile profile job status status path job file null path local job file null task in progress maps new task in progress task in progress reduces new task in progress task in progress cleanup new task in progress task in progress setup new task in progress num map tasks num reduce tasks memory per map memory per reduce volatile num slots per map volatile num slots per reduce max task failures per tracker counters track currently running finished failed map reduce task attempts running map tasks running reduce tasks finished map tasks finished reduce tasks failed map tasks failed reduce tasks default completed maps percent for reduce slowstart f completed maps for reduce slowstart running map tasks speculative tasks need capture speculative tasks separately speculative map tasks speculative reduce tasks map failures percent reduce failures percent failed map ti ps failed reduce ti ps volatile boolean launched cleanup false volatile boolean launched setup false volatile boolean job killed false volatile boolean job failed false boolean job setup cleanup needed boolean task cleanup needed job priority priority job priority normal protected job tracker jobtracker protected credentials token storage job history job history network topology node set ti ps map node list task in progress non running map cache map network topology node set running ti ps map node set task in progress running map cache a list non local non running maps list task in progress non local maps a set non local running maps set task in progress non local running maps a list non running reduce ti ps list task in progress non running reduces a set running reduce ti ps set task in progress running reduces a list cleanup tasks map task attempts launched list task attempt id map cleanup tasks new linked list task attempt id a list cleanup tasks reduce task attempts launched list task attempt id reduce cleanup tasks new linked list task attempt id max level a special value indicating link find new map task task tracker status schedule available map tasks job including speculative tasks cache level a special value indicating link find new map task task tracker status schedule switch speculative map tasks job non local cache level task completion event tracker list task completion event task completion events the maximum percentage trackers cluster added blacklist cluster blacklist percent the maximum percentage fetch failures allowed map max allowed fetch failures percent no tasktrackers cluster volatile cluster size the tasktrackers conf get max task failures per tracker tasks failed volatile flaky task trackers map tracker host name task failures map string integer tracker to failures map new tree map string integer confine estimation algorithms oracle jip queries resource estimator resource estimator start time launch
1204	mapreduce\src\java\org\apache\hadoop\mapred\JobInProgressListener.java	unrelated	package org apache hadoop mapred a listener changes link job in progress job lifecycle link job tracker job in progress listener invoked new job added link job tracker job added job in progress job throws io exception invoked job removed link job tracker job removed job in progress job invoked job updated link job tracker this change job tracker using link job change event job updated job change event event
1205	mapreduce\src\java\org\apache\hadoop\mapred\JobPriority.java	unrelated	package org apache hadoop mapred used describe priority running job enum job priority very high high normal low very low
1206	mapreduce\src\java\org\apache\hadoop\mapred\JobProfile.java	unrelated	package org apache hadoop mapred a job profile map reduce primitive tracks job whether living dead job profile implements writable register ctor writable factories set factory job profile new writable factory writable new instance return new job profile string user job id jobid string job file string url string name string queue name construct empty link job profile job profile jobid new job id construct link job profile userid jobid job config file job details url job name job profile string user org apache hadoop mapreduce job id jobid string job file string url string name user jobid job file url name job conf default queue name construct link job profile userid jobid job config file job details url job name job profile string user org apache hadoop mapreduce job id jobid string job file string url string name string queue name user user jobid job id downgrade jobid job file job file url url name name queue name queue name job profile string user string jobid string job file string url string name user job id name jobid job file url name get user id string get user return user get job id job id get job id return jobid string get job id return jobid string get configuration file job string get job file return job file get link web ui details job url get url try return new url url catch io exception ie return null get user specified job name string get job name return name get name queue job submitted string get queue name return queue name writable write data output throws io exception jobid write text write string job file text write string url text write string user text write string name text write string queue name read fields data input throws io exception jobid read fields job file text read string url text read string user text read string name text read string queue name text read string
1207	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueClient.java	unrelated	package org apache hadoop mapred code job queue client code provided user order get job queue related information link job tracker it provides facility list job queues present ability view list jobs within specific job queue job queue client extends configured implements tool job client jc job queue client job queue client job conf conf throws io exception set conf conf init job conf conf throws io exception set conf conf jc new job client conf run string argv throws exception exitcode argv length display usage return exitcode string cmd argv boolean display queue list false boolean display queue info with jobs false boolean display queue info without jobs false boolean display queue acls info for current user false list equals cmd display queue list true else showacls equals cmd display queue acls info for current user true else info equals cmd argv length argv equals show jobs display queue info without jobs true else argv length argv equals show jobs display queue info with jobs true else display usage cmd return exitcode else display usage cmd return exitcode else display usage cmd return exitcode job conf conf new job conf get conf init conf display queue list display queue list exitcode else display queue info without jobs display queue info argv false exitcode else display queue info with jobs display queue info argv true exitcode else display queue acls info for current user display queue acls info for current user exitcode return exitcode format print information passed job queue print job queue info job queue info job queue info writer writer throws io exception job queue info null writer write no queue found n writer flush return writer write string format queue name n job queue info get queue name writer write string format queue state n job queue info get queue state writer write string format scheduling info n job queue info get scheduling info list job queue info child queues job queue info get children child queues null child queues size writer write string format child queues child queues size job queue info child queue child queues get writer write string format child queue get queue name child queues size writer write string format writer write n writer write string format n writer flush display queue list throws io exception job queue info root queues jc get root queues list job queue info queues expand queue list root queues job queue info queue queues print job queue info queue new print writer system expands hierarchy queues gives list queues depth first order list job queue info expand queue list job queue info root queues list job queue info queues new array list job queue info job queue info queue root queues queues add queue queue get children null job queue info child queues queue get children array new job queue info queues add all expand queue list child queues return queues method used display information pertaining single job queue registered link queue manager display jobs determine boolean display queue info string queue boolean show jobs throws io exception job
1208	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueInfo.java	unrelated	package org apache hadoop mapred class contains information regarding job queues maintained hadoop map reduce framework job queue info extends queue info default constructor job queue info job queue info super construct new job queue info object using queue name scheduling information passed queue job queue info string queue name string scheduling info super queue name scheduling info job queue info queue info queue queue get queue name queue get scheduling info set queue state queue get state get state name set queue children queue get queue children set properties queue get properties set job statuses queue get job statuses set queue name job queue info protected set queue name string queue name super set queue name queue name set scheduling information associated particular job queue protected set scheduling info string scheduling info super set scheduling info scheduling info set state queue protected set queue state string state super set state queue state get state state string get queue state return super get state string protected set children list job queue info children list queue info list new array list queue info job queue info q children list add q super set queue children list list job queue info get children list job queue info list new array list job queue info queue info q super get queue children list add job queue info q return list protected set properties properties props super set properties props add child link job queue info link job queue info modify fully qualified name child link job queue info reflect hierarchy only testing add child job queue info child list job queue info children get children children add child set children children remove child link job queue info this also resets queue name child fully qualified name simple queue name only testing remove child job queue info child list job queue info children get children children remove child set children children protected set job statuses org apache hadoop mapreduce job status stats super set job statuses stats
1209	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueJobInProgressListener.java	unrelated	package org apache hadoop mapred a link job in progress listener maintains jobs managed queue by default queue fifo possible use custom queue ordering using link job queue job in progress listener map constructor job queue job in progress listener extends job in progress listener a groups information link job in progress necessary scheduling job job scheduling info job priority priority start time job id id job scheduling info job in progress jip jip get status job scheduling info job status status priority status get job priority start time status get start time id status get job id job priority get priority return priority get start time return start time job id get job id return id comparator job scheduling info fifo job queue comparator new comparator job scheduling info compare job scheduling info job scheduling info res get priority compare to get priority res get start time get start time res else res get start time get start time res res get job id compare to get job id return res map job scheduling info job in progress job queue job queue job in progress listener new tree map job scheduling info job in progress fifo job queue comparator for clients want provide job priorities protected job queue job in progress listener map job scheduling info job in progress job queue job queue collections synchronized map job queue returns synchronized view job queue collection job in progress get job queue return job queue values job added job in progress job job queue put new job scheduling info job get status job job removed job completes job removed job in progress job job completed job scheduling info old info job queue remove old info synchronized job updated job change event event job in progress job event get job in progress event instanceof job status change event check ordering job changed for priority start time change job ordering job status change event status event job status change event event job scheduling info old info new job scheduling info status event get old status status event get event type event type priority changed status event get event type event type start time changed make priority change reorder jobs job old info else status event get event type event type run state changed check job complete run state status event get new status get run state run state job status succeeded run state job status failed run state job status killed job completed old info reorder jobs job in progress job job scheduling info old info synchronized job queue job queue remove old info job queue put new job scheduling info job job
1210	mapreduce\src\java\org\apache\hadoop\mapred\JobQueueTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler keeps jobs queue priority order fifo default job queue task scheduler extends task scheduler min cluster size for padding log log log factory get log job queue task scheduler protected job queue job in progress listener job queue job in progress listener protected eager task initialization listener eager task initialization listener pad fraction job queue task scheduler job queue job in progress listener new job queue job in progress listener synchronized start throws io exception super start task tracker manager add job in progress listener job queue job in progress listener eager task initialization listener set task tracker manager task tracker manager eager task initialization listener start task tracker manager add job in progress listener eager task initialization listener synchronized terminate throws io exception job queue job in progress listener null task tracker manager remove job in progress listener job queue job in progress listener eager task initialization listener null task tracker manager remove job in progress listener eager task initialization listener eager task initialization listener terminate super terminate synchronized set conf configuration conf super set conf conf pad fraction conf get float jt config jt task alloc pad fraction f eager task initialization listener new eager task initialization listener conf synchronized list task assign tasks task tracker task tracker throws io exception task tracker status task tracker status task tracker get status cluster status cluster status task tracker manager get cluster status num task trackers cluster status get task trackers cluster map capacity cluster status get max map tasks cluster reduce capacity cluster status get max reduce tasks collection job in progress job queue job queue job in progress listener get job queue get map reduce counts current tracker tracker map capacity task tracker status get max map slots tracker reduce capacity task tracker status get max reduce slots tracker running maps task tracker status count map tasks tracker running reduces task tracker status count reduce tasks assigned tasks list task assigned tasks new array list task compute running pending map reduce task numbers across pool remaining reduce load remaining map load synchronized job queue job in progress job job queue job get status get run state job status running remaining map load job desired maps job finished maps job schedule reduces remaining reduce load job desired reduces job finished reduces compute load factor maps reduces map load factor cluster map capacity map load factor remaining map load cluster map capacity reduce load factor cluster reduce capacity reduce load factor remaining reduce load cluster reduce capacity in steps allocate first map tasks appropriate reduce tasks appropriate we go jobs order job arrival jobs get serviced predecessors serviced we assign tasks current task tracker given machine workload less maximum load kind task however cluster close getting loaded e enough padding speculative executions etc schedule highest priority task e task job highest priority tracker current map capacity math min math ceil map load factor tracker map capacity tracker map capacity available map slots tracker current map capacity tracker running maps boolean exceeded map padding
1211	mapreduce\src\java\org\apache\hadoop\mapred\JobStatus.java	unrelated	package org apache hadoop mapred describes current status job this intended comprehensive piece data for look job profile job status extends org apache hadoop mapreduce job status running org apache hadoop mapreduce job status state running get value succeeded org apache hadoop mapreduce job status state succeeded get value failed org apache hadoop mapreduce job status state failed get value prep org apache hadoop mapreduce job status state prep get value killed org apache hadoop mapreduce job status state killed get value string unknown unknown string run states unknown running succeeded failed prep killed helper method get human readable state job string get job run state state state state run states length return unknown return run states state org apache hadoop mapreduce job status state get enum state switch state case return org apache hadoop mapreduce job status state running case return org apache hadoop mapreduce job status state succeeded case return org apache hadoop mapreduce job status state failed case return org apache hadoop mapreduce job status state prep case return org apache hadoop mapreduce job status state killed return null job status create job status object given jobid job status job id jobid map progress reduce progress cleanup progress run state string user string job name string job file string tracking url jobid map progress reduce progress cleanup progress run state job priority normal user job name job file tracking url create job status object given jobid job status job id jobid map progress reduce progress run state string user string job name string job file string tracking url jobid map progress reduce progress f run state user job name job file tracking url create job status object given jobid job status job id jobid map progress reduce progress cleanup progress run state job priority jp string user string job name string job file string tracking url jobid f map progress reduce progress cleanup progress run state jp user job name job file tracking url create job status object given jobid job status job id jobid setup progress map progress reduce progress cleanup progress run state job priority jp string user string job name string job file string tracking url jobid setup progress map progress reduce progress cleanup progress run state jp user job name default job file tracking url create job status object given jobid job status job id jobid setup progress map progress reduce progress cleanup progress run state job priority jp string user string job name string queue string job file string tracking url super jobid setup progress map progress reduce progress cleanup progress get enum run state org apache hadoop mapreduce job priority value of jp name user job name queue job file tracking url job status downgrade org apache hadoop mapreduce job status stat job status old new job status job id downgrade stat get job id stat get setup progress stat get map progress stat get reduce progress stat get cleanup progress stat get state get value job priority value of stat get priority name stat get username stat get job name stat get
1212	mapreduce\src\java\org\apache\hadoop\mapred\JobStatusChangeEvent.java	unrelated	package org apache hadoop mapred link job status change event tracks change job status job status change w r run state e prep running failed killed succeeded start time priority note job times change job get restarted job status change event extends job change event events job status lead job status change enum event type run state changed start time changed priority changed job status old status job status new status event type event type job status change event job in progress jip event type event type job status old status job status new status super jip old status old status new status new status event type event type create link job status change event indicating state changed note assume state change doesnt care old state job status change event job in progress jip event type event type job status status jip event type status status returns event type caused state change event type get event type return event type get old job status job status get old status return old status get new job status result events job status get new status return new status
1213	mapreduce\src\java\org\apache\hadoop\mapred\JobTracker.java	audit	package org apache hadoop mapred job tracker central location submitting tracking mr jobs network environment job tracker implements mr constants inter tracker protocol client protocol task tracker manager refresh user mappings protocol refresh authorization policy protocol admin operations protocol get user mappings protocol jt config config util load resources tasktracker expiry interval delegation token gc interval hour delegation token secret manager secret manager the interval one fault tracker discarded faults update faulty tracker interval the maximum percentage trackers cluster added blacklist across jobs max blacklist percent a tracker blacklisted across jobs number blacklists x average number blacklists x blacklist threshold average blacklist threshold the maximum number blacklists tracker tracker could blacklisted across jobs max blacklists per tracker approximate number heartbeats could arrive job tracker second num heartbeats in second default num heartbeats in second min num heartbeats in second scaling factor heartbeats used testing heartbeats scaling factor min heartbeats scaling factor f default heartbeats scaling factor f minimum interval heartbeats regardless cluster size heartbeat interval min enum state initializing running state state state initializing fs access retry period string job info file job info dns to switch mapping dns to switch mapping network topology cluster map new network topology num task cache levels max level cache tasks link nodes at max level using key set link concurrent hash map safely written iterated via separate threads note it iterated single thread feasible since iteration done link job in progress link job tracker lock set node nodes at max level collections new set from map new concurrent hash map node boolean task scheduler task scheduler list job in progress listener job in progress listeners new copy on write array list job in progress listener list service plugin plugins system directory completely owned job tracker fs permission system dir permission fs permission create immutable short rwx system files permission fs permission system file permission fs permission create immutable short rwx clock clock null clock default clock new clock job history job history job token secret manager job token secret manager new job token secret manager job token secret manager get job token secret manager return job token secret manager mr async disk service async disk service returns delegation token secret manager instance job tracker delegation token secret manager get delegation token secret manager return secret manager a client tried submit job job tracker ready illegal state exception extends io exception serial version uid l illegal state exception string msg super msg atomic integer next job id new atomic integer log log log factory get log job tracker returns job tracker clock note correct clock implementation obtained job tracker initialized if job tracker initialized default clock e link clock returned clock get clock return clock null default clock clock return jt job history handle job history get job history return job history start job tracker given configuration the conf modified reflect actual ports job tracker running user passes port code zero code job tracker start tracker job conf conf throws io exception interrupted exception return start tracker conf default clock job tracker start tracker job conf
1214	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerClientProtocolProvider.java	unrelated	package org apache hadoop mapred job tracker client protocol provider extends client protocol provider client protocol create configuration conf throws io exception string framework conf get mr config framework name framework null framework equals classic return null string tracker conf get jt config jt ipc address local local equals tracker return create rpc proxy job tracker get address conf conf return null client protocol create inet socket address addr configuration conf throws io exception return create rpc proxy addr conf client protocol create rpc proxy inet socket address addr configuration conf throws io exception return client protocol rpc get proxy client protocol client protocol version id addr user group information get current user conf net utils get socket factory conf client protocol close client protocol client protocol throws io exception rpc stop proxy client protocol
1215	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerInstrumentation.java	heartbeat	package org apache hadoop mapred job tracker instrumentation protected job tracker tracker job tracker instrumentation job tracker jt job conf conf tracker jt launch map task attempt id task attempt id complete map task attempt id task attempt id failed map task attempt id task attempt id launch reduce task attempt id task attempt id complete reduce task attempt id task attempt id failed reduce task attempt id task attempt id submit job job conf conf job id id complete job job conf conf job id id terminate job job conf conf job id id finalize job job conf conf job id id add waiting maps job id id task dec waiting maps job id id task add waiting reduces job id id task dec waiting reduces job id id task set map slots slots set reduce slots slots add black listed map slots slots dec black listed map slots slots add black listed reduce slots slots dec black listed reduce slots slots add reserved map slots slots dec reserved map slots slots add reserved reduce slots slots dec reserved reduce slots slots add occupied map slots slots dec occupied map slots slots add occupied reduce slots slots dec occupied reduce slots slots failed job job conf conf job id id killed job job conf conf job id id add prep job job conf conf job id id dec prep job job conf conf job id id add running job job conf conf job id id dec running job job conf conf job id id add running maps tasks dec running maps tasks add running reduces tasks dec running reduces tasks killed map task attempt id task attempt id killed reduce task attempt id task attempt id add trackers trackers dec trackers trackers add black listed trackers trackers dec black listed trackers trackers set decommissioned trackers trackers heartbeat speculate map task attempt id task attempt id speculate reduce task attempt id task attempt id launch data local map task attempt id task attempt id launch rack local map task attempt id task attempt id
1216	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerMetricsInst.java	heartbeat	package org apache hadoop mapred job tracker metrics inst extends job tracker instrumentation implements updater metrics record metrics record num map tasks launched num map tasks completed num map tasks failed num reduce tasks launched num reduce tasks completed num reduce tasks failed num jobs submitted num jobs completed num waiting maps num waiting reduces num speculative maps num speculative reduces num data local maps num rack local maps cluster status fields volatile num map slots volatile num reduce slots num black listed map slots num black listed reduce slots num reserved map slots num reserved reduce slots num occupied map slots num occupied reduce slots num jobs failed num jobs killed num jobs preparing num jobs running num running maps num running reduces num map tasks killed num reduce tasks killed num trackers num trackers black listed num trackers decommissioned could well month worth heartbeats reasonable assumptions job tracker improvements num heartbeats l job tracker metrics inst job tracker tracker job conf conf super tracker conf string session id conf get session id initiate jvm metrics jvm metrics init job tracker session id create record map reduce metrics metrics context context metrics util get context mapred metrics record metrics util create record context jobtracker metrics record set tag session id session id context register updater since object registered updater method called periodically e g every seconds updates metrics context unused synchronized metrics record set metric map slots num map slots metrics record set metric reduce slots num reduce slots metrics record incr metric blacklisted maps num black listed map slots metrics record incr metric blacklisted reduces num black listed reduce slots metrics record incr metric maps launched num map tasks launched metrics record incr metric maps completed num map tasks completed metrics record incr metric maps failed num map tasks failed metrics record incr metric reduces launched num reduce tasks launched metrics record incr metric reduces completed num reduce tasks completed metrics record incr metric reduces failed num reduce tasks failed metrics record incr metric jobs submitted num jobs submitted metrics record incr metric jobs completed num jobs completed metrics record incr metric waiting maps num waiting maps metrics record incr metric waiting reduces num waiting reduces metrics record incr metric speculative maps num speculative maps metrics record incr metric speculative reduces num speculative reduces metrics record incr metric datalocal maps num data local maps metrics record incr metric racklocal maps num rack local maps metrics record incr metric reserved map slots num reserved map slots metrics record incr metric reserved reduce slots num reserved reduce slots metrics record incr metric occupied map slots num occupied map slots metrics record incr metric occupied reduce slots num occupied reduce slots metrics record incr metric jobs failed num jobs failed metrics record incr metric jobs killed num jobs killed metrics record incr metric jobs preparing num jobs preparing metrics record incr metric jobs running num jobs running metrics record incr metric running maps num running maps metrics record incr metric running reduces num running reduces metrics record incr metric maps killed num map
1217	mapreduce\src\java\org\apache\hadoop\mapred\JobTrackerStatistics.java	unrelated	package org apache hadoop mapred collects job tracker statistics job tracker statistics statistics collector collector map string task tracker stat tt stats new hash map string task tracker stat job tracker statistics collector new statistics collector collector start synchronized task tracker added string name task tracker stat stat tt stats get name stat null stat new task tracker stat name tt stats put name stat synchronized task tracker removed string name task tracker stat stat tt stats remove name stat null stat remove synchronized task tracker stat get task tracker stat string name return tt stats get name task tracker stat string total tasks key stat total tasks stat string succeeded tasks key stat succeeded tasks stat string health check failed key stat health check failed stat task tracker stat string tracker name total tasks key tracker name total tasks total tasks stat collector create stat total tasks key succeeded tasks key tracker name succeeded tasks succeeded tasks stat collector create stat succeeded tasks key health check failed key tracker name healthcheckfailed health check failed stat collector create stat health check failed key synchronized incr total tasks total tasks stat inc synchronized incr succeeded tasks succeeded tasks stat inc synchronized incr health check failed health check failed stat inc synchronized remove collector remove stat total tasks key collector remove stat succeeded tasks key collector remove stat health check failed key
1218	mapreduce\src\java\org\apache\hadoop\mapred\JSPUtil.java	unrelated	package org apache hadoop mapred jsp util lru based cache map string job info job history cache new linked hash map string job info log log log factory get log jsp util wraps link job in progress object contains boolean job view access allowed this usage js ps servlets job with view access check job in progress job null true user authorized view job boolean view allowed true job with view access check job in progress job job job job in progress get job return job boolean view job allowed return view allowed set view access boolean view allowed view allowed view allowed validates current user view job if user authorized view job method modify response forwards error page returns job view job access flag set false view job access flag callers method check flag decide view allowed job null job given jobid doesnot exist job tracker job with view access check check access and get job job tracker jt job id jobid http servlet request request http servlet response response throws servlet exception io exception job in progress job jt get job jobid job with view access check job new job with view access check job string user request get remote user user null job null jt ac ls enabled user group information ugi user group information create remote user user try ugi as new privileged exception action void void run throws io exception servlet exception checks job view permission jt get ac ls manager check access job ugi operation view job details return null catch access control exception e string err msg user ugi get short user name failed view jobid br br e get message hr href jobtracker jsp go back job tracker br jsp util set error and forward err msg request response job set view access false catch interrupted exception e string err msg interrupted trying access jobid hr href jobtracker jsp go back job tracker br jsp util set error and forward err msg request response job set view access false return job sets error code sc unauthorized response forwards error page contains error message back link set error and forward string err msg http servlet request request http servlet response response throws servlet exception io exception request set attribute error msg err msg request dispatcher dispatcher request get request dispatcher job authorization error jsp response set status http servlet response sc unauthorized dispatcher forward request response method used process request job page based request received for example like changing priority selected jobs boolean process buttons http servlet request request http servlet response response job tracker tracker throws io exception interrupted exception servlet exception string user request get remote user actions allowed tracker conf request get parameter kill jobs null string jobs request get parameter values job check box jobs null boolean authorized false string err msg user user failed kill following job br br string job jobs job id job id job id name job user null user group information ugi user group information create remote user user try ugi as new privileged exception action
1219	mapreduce\src\java\org\apache\hadoop\mapred\JvmContext.java	unrelated	package org apache hadoop mapred jvm context implements writable log log log factory get log jvm context jvm id jvm id string pid jvm context jvm id new jvm id pid jvm context jvm id id string pid jvm id id pid pid read fields data input throws io exception jvm id read fields pid text read string write data output throws io exception jvm id write text write string pid
1220	mapreduce\src\java\org\apache\hadoop\mapred\JVMId.java	unrelated	package org apache hadoop mapred jvm id extends id boolean map job id job id string jvm jvm number format id format number format get instance id format set grouping used false id format set minimum integer digits jvm id job id job id boolean map id super id map map job id job id jvm id string jt identifier job id boolean map id new job id jt identifier job id map id jvm id job id new job id boolean map jvm return map job id get job id return job id boolean equals object null return false get class equals get class jvm id jvm id return id id map map job id equals job id else return false compare task in progress ids first job ids tip numbers reduces defined greater maps compare to org apache hadoop mapreduce id jvm id jvm id job comp job id compare to job id job comp map map return id id else return map else return job comp string string return append to new string builder jvm string add unique id given string builder protected string builder append to string builder builder return job id append to builder append separator append map r append separator append id format format id hash code return job id hash code id read fields data input throws io exception super read fields job id read fields map read boolean write data output throws io exception super write job id write write boolean map construct jvm id object given jvm id name string str throws illegal argument exception str null return null try string parts str split parts length parts equals jvm boolean map false parts equals map true else parts equals r map false else throw new exception return new jvm id parts integer parse int parts map integer parse int parts catch exception ex fall throw new illegal argument exception task id str properly formed
1221	mapreduce\src\java\org\apache\hadoop\mapred\JvmManager.java	scheduler	package org apache hadoop mapred jvm manager log log log factory get log jvm manager jvm manager for type map jvm manager jvm manager for type reduce jvm manager jvm env construct jvm env list string setup vector string vargs file stdout file stderr log size file work dir map string string env job conf conf return new jvm env setup vargs stdout stderr log size work dir env conf jvm manager task tracker tracker map jvm manager new jvm manager for type tracker get max current map tasks true tracker reduce jvm manager new jvm manager for type tracker get max current reduce tasks false tracker jvm manager for type get jvm manager for type task type type type equals task type map return map jvm manager else type equals task type reduce return reduce jvm manager return null stop map jvm manager stop reduce jvm manager stop boolean jvm known jvm id jvm id jvm id map jvm return map jvm manager jvmknown jvm id else return reduce jvm manager jvmknown jvm id saves pid given task jvm set pid to jvm jvm id jvm id string pid jvm id map jvm map jvm manager set pid for jvm jvm id pid else reduce jvm manager set pid for jvm jvm id pid returns pid task string get pid task runner null get task null get task map task return map jvm manager get pid by running task else return reduce jvm manager get pid by running task return null launch jvm task runner jvm env env get task map task map jvm manager reap jvm env else reduce jvm manager reap jvm env task in progress get task for jvm jvm id jvm id throws io exception jvm id map jvm return map jvm manager get task for jvm jvm id else return reduce jvm manager get task for jvm jvm id task finished task runner tr tr get task map task map jvm manager task finished tr else reduce jvm manager task finished tr task killed task runner tr tr get task map task map jvm manager task killed tr else reduce jvm manager task killed tr dump stack task runner tr tr get task map task map jvm manager dump stack tr else reduce jvm manager dump stack tr kill jvm jvm id jvm id jvm id map map jvm manager kill jvm jvm id else reduce jvm manager kill jvm jvm id adds task work dir cleanup queue task tracker asynchronous deletion work dir delete work dir task tracker tracker task task throws io exception tracker get cleanup thread add to queue task tracker build task controller task path deletion contexts tracker get local file system tracker get local files tracker get job conf task true work dir tracker get task controller jvm manager for type mapping jvm i ds running tasks map jvm id task runner jvm to running task new hash map jvm id task runner mapping tasks jvm i ds map task runner jvm id running task to jvm new hash map task
1222	mapreduce\src\java\org\apache\hadoop\mapred\JvmTask.java	unrelated	package org apache hadoop mapred task abstraction serialized implements writable jvm task implements writable task boolean die jvm task task boolean die die die jvm task task get task return boolean die return die write data output throws io exception write boolean die null write boolean true write boolean map task write else write boolean false read fields data input throws io exception die read boolean boolean task coming read boolean task coming boolean map read boolean map new map task else new reduce task read fields
1223	mapreduce\src\java\org\apache\hadoop\mapred\KeyValueLineRecordReader.java	unrelated	package org apache hadoop mapred this treats line input key value pair separated separator character the separator specified config file attribute name mapreduce input keyvaluelinerecordreader key value separator the default separator tab character link org apache hadoop mapreduce lib input key value line record reader instead key value line record reader implements record reader text text line record reader line record reader byte separator byte long writable dummy key text inner value class get key class return text text create key return new text text create value return new text key value line record reader configuration job file split split throws io exception line record reader new line record reader job split dummy key line record reader create key inner value line record reader create value string sep str job get mapreduce input keyvaluelinerecordreader key value separator separator byte sep str char at find separator byte utf start length byte sep return org apache hadoop mapreduce lib input key value line record reader find separator utf start length sep read key value pair line synchronized boolean next text key text value throws io exception byte line null line len line record reader next dummy key inner value line inner value get bytes line len inner value get length else return false line null return false pos find separator line line len separator org apache hadoop mapreduce lib input key value line record reader set key value key value line line len pos return true get progress throws io exception return line record reader get progress synchronized get pos throws io exception return line record reader get pos synchronized close throws io exception line record reader close
1224	mapreduce\src\java\org\apache\hadoop\mapred\KeyValueTextInputFormat.java	unrelated	package org apache hadoop mapred an link input format plain text files files broken lines either linefeed carriage return used signal end line each line divided key value parts separator byte if byte exists key entire line value empty link org apache hadoop mapreduce lib input key value text input format instead key value text input format extends file input format text text implements job configurable compression codec factory compression codecs null configure job conf conf compression codecs new compression codec factory conf protected boolean splitable file system fs path file compression codec codec compression codecs get codec file null codec return true return codec instanceof splittable compression codec record reader text text get record reader input split generic split job conf job reporter reporter throws io exception reporter set status generic split string return new key value line record reader job file split generic split
1225	mapreduce\src\java\org\apache\hadoop\mapred\KillJobAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker kill task job cleanup resources kill job action extends task tracker action job id job id kill job action super action type kill job job id new job id kill job action job id job id super action type kill job job id job id job id get job id return job id write data output throws io exception job id write read fields data input throws io exception job id read fields
1226	mapreduce\src\java\org\apache\hadoop\mapred\KillTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker kill task kill task action extends task tracker action task attempt id task id kill task action super action type kill task task id new task attempt id kill task action task attempt id task id super action type kill task task id task id task attempt id get task id return task id write data output throws io exception task id write read fields data input throws io exception task id read fields
1227	mapreduce\src\java\org\apache\hadoop\mapred\LaunchTaskAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker launch new task launch task action extends task tracker action task task launch task action super action type launch task launch task action task task super action type launch task task task task get task return task write data output throws io exception write boolean task map task task write read fields data input throws io exception boolean map task read boolean map task task new map task else task new reduce task task read fields
1228	mapreduce\src\java\org\apache\hadoop\mapred\LimitTasksPerJobTaskScheduler.java	scheduler	package org apache hadoop mapred a link task scheduler limits maximum number tasks running job the limit set means link jt config jt runningtasks per job property limit tasks per job task scheduler extends job queue task scheduler log log log factory get log org apache hadoop mapred task limited job queue task scheduler max tasks per job limit tasks per job task scheduler super synchronized start throws io exception super start queue manager queue manager task tracker manager get queue manager string queue name queue manager get job queue infos get queue name queue manager set scheduler info queue name maximum tasks per job string value of max tasks per job synchronized set conf configuration conf super set conf conf max tasks per job conf get long jt config jt runningtasks per job long max value max tasks per job string msg jt config jt runningtasks per job set zero negative value aborting log fatal msg throw new runtime exception msg synchronized list task assign tasks task tracker task tracker throws io exception task tracker status task tracker status task tracker get status num task trackers task tracker manager get cluster status get task trackers collection job in progress job queue job queue job in progress listener get job queue task task stats current task tracker map tasks number task tracker status count map tasks reduce tasks number task tracker status count reduce tasks maximum map tasks number task tracker status get max map slots maximum reduce tasks number task tracker status get max reduce slots statistics whole cluster most approximate concurrency max map and reduce load get max map and reduce load maximum map tasks number maximum reduce tasks number maximum map load max map and reduce load maximum reduce load max map and reduce load begin at step when step loop starts many map tasks wrt max tasks per job when step loop starts many reduce tasks wrt max tasks per job when step loop starts many map tasks when step loop starts many reduce tasks it may seem would improve loop queuing jobs cannot start steps max tasks per job using queue step a first thing notice time current algorithm logarithmic sum p k k n n number jobs p probability job exceed limits the probability cache useful would similar p n e n whereas size time spent manage would ln n so good idea max tasks per job long max value begin at step else begin at step list task assigned tasks new array list task schedule tasks step begin at step step step if reached maximum load step go next step step map tasks number maximum map load step step reduce tasks number maximum reduce load continue for job start tasks synchronized job queue job in progress job job queue ignore non running jobs job get status get run state job status running continue check exceeding global limits step step job running maps job running reduces max tasks per job continue step step task job obtain new map task task tracker status num task trackers task
1229	mapreduce\src\java\org\apache\hadoop\mapred\LineRecordReader.java	pooling	package org apache hadoop mapred treats keys offset file value line link org apache hadoop mapreduce lib input line record reader instead line record reader implements record reader long writable text log log log factory get log line record reader get name compression codec factory compression codecs null start pos end line reader fs data input stream file in seekable file position max line length compression codec codec decompressor decompressor a provides line reader input stream line reader extends org apache hadoop util line reader line reader input stream super line reader input stream buffer size super buffer size line reader input stream configuration conf throws io exception super conf line reader input stream byte record delimiter super record delimiter line reader input stream buffer size byte record delimiter super buffer size record delimiter line reader input stream configuration conf byte record delimiter throws io exception super conf record delimiter line record reader configuration job file split split throws io exception job split null line record reader configuration job file split split byte record delimiter throws io exception max line length job get int org apache hadoop mapreduce lib input line record reader max line length integer max value start split get start end start split get length path file split get path compression codecs new compression codec factory job codec compression codecs get codec file open file seek start split file system fs file get file system job file in fs open file compressed input decompressor codec pool get decompressor codec codec instanceof splittable compression codec split compression input stream c in splittable compression codec codec create input stream file in decompressor start end splittable compression codec read mode byblock new line reader c in job record delimiter start c in get adjusted start end c in get adjusted end file position c in take pos compressed stream else new line reader codec create input stream file in decompressor job record delimiter file position file in else file in seek start new line reader file in job record delimiter file position file in if first split always throw away first record always except last split read one extra line next method start start read line new text max bytes to consume start pos start line record reader input stream offset end offset max line length offset end offset max line length null line record reader input stream offset end offset max line length byte record delimiter max line length max line length new line reader record delimiter start offset pos offset end end offset file position null line record reader input stream offset end offset configuration job throws io exception offset end offset job null line record reader input stream offset end offset configuration job byte record delimiter throws io exception max line length job get int org apache hadoop mapreduce lib input line record reader max line length integer max value new line reader job record delimiter start offset pos offset end end offset file position null long writable create key return new long writable text create value return new
1230	mapreduce\src\java\org\apache\hadoop\mapred\LinuxTaskController.java	unrelated	package org apache hadoop mapred a link task controller runs task jv ms user submits job this executes setuid executable implement methods link task controller including launching task jvm killing needed also initializing finalizing task environment p the setuid executable launched using command line p p task controller mapreduce job user name command command args p p mapreduce job user name name owner submits job p p command one cardinal value link linux task controller task controller commands enumeration p p command args depends command launched p in addition running killing tasks also sets appropriate access directories files used tasks linux task controller extends task controller log log log factory get log linux task controller name executable script contain child jvm command line see write command details string command file taskjvm sh path setuid executable string task controller exe task controller expected hadoop prefix bin directory file hadoop bin new file system getenv hadoop prefix bin task controller exe new file hadoop bin task controller get absolute path linux task controller super list commands setuid script execute enum task controller commands initialize user initialize job initialize distributedcache file launch task jvm initialize task terminate task jvm kill task jvm run debug script sigquit task jvm enable task for cleanup enable job for cleanup setup throws io exception super setup check permissions task controller binary running plainly if permissions correct returns error code else returns something else bugs also present string task controller cmd new string get task controller executable path shell command executor sh exec new shell command executor task controller cmd try sh exec execute catch exit code exception e exit code sh exec get exit code exit code log warn exit code checking binary permissions exit code log output sh exec get output throw new io exception task controller setup failed invalid permissions ownership exit code exit code e launch task jvm run owner job this method launches task jvm executing setuid executable switch user run task also initialization first task setuid process launch launch task jvm task controller task controller context context throws io exception jvm env env context env get jvm command line string cmd line task log build command line env setup env vargs env stdout env stderr env log size true string buffer sb new string buffer export environment variable child command setuid setgid binaries would getting environmental variables begin ld entry string string entry env env entry set sb append export sb append entry get key sb append sb append entry get value sb append n sb append cmd line write command file task specific cache directory write command sb string get task cache directory context context env work dir call taskcontroller right parameters list string launch task jvm args build launch task args context context env work dir shell command executor sh exec build task controller executor task controller commands launch task jvm env conf get user launch task jvm args env work dir env env context sh exec sh exec try sh exec execute catch exception e exit code sh exec get exit
1231	mapreduce\src\java\org\apache\hadoop\mapred\LocalClientProtocolProvider.java	unrelated	package org apache hadoop mapred local client protocol provider extends client protocol provider client protocol create configuration conf throws io exception string framework conf get mr config framework name framework null framework equals local return null local equals conf get jt config jt ipc address local conf set int mapreduce job maps return new local job runner conf return null client protocol create inet socket address addr configuration conf return null local job runner use socket close client protocol client protocol clean required
1232	mapreduce\src\java\org\apache\hadoop\mapred\LocalJobRunner.java	pooling	package org apache hadoop mapred implements map reduce locally process debugging local job runner implements client protocol log log log factory get log local job runner the maximum number map tasks run parallel local job runner string local max maps mapreduce local map tasks maximum file system fs hash map job id job jobs new hash map job id job job conf conf atomic integer map tasks new atomic integer reduce tasks random rand new random job tracker instrumentation metrics null string job dir local runner counters empty counters new counters get protocol version string protocol client version return client protocol version id protocol signature get protocol signature string protocol client version client methods hash throws io exception return protocol signature get protocol signature protocol client version client methods hash job extends thread implements task umbilical protocol the job directory system job client places job configurations this analogous job tracker system directory path system job dir path system job file the job directory task analagous task job directory path local job dir path local job file job id id job conf job num map tasks partial map progress counters map counters counters reduce counters job status status list task attempt id map ids collections synchronized list new array list task attempt id job profile profile file system local fs boolean killed false tracker distributed cache manager tracker distributerd cache manager task distributed cache manager task distributed cache manager get protocol version string protocol client version return task umbilical protocol version id protocol signature get protocol signature string protocol client version client methods hash throws io exception return protocol signature get protocol signature protocol client version client methods hash job job id jobid string job submit dir throws io exception system job dir new path job submit dir system job file new path system job dir job xml id jobid job conf conf new job conf system job file local fs file system get local conf local job dir local fs make qualified conf get local path job dir local job file new path local job dir id xml manage distributed cache if files copied trigger local file written tracker distributerd cache manager new tracker distributed cache manager conf new default task controller task distributed cache manager tracker distributerd cache manager new task distributed cache manager conf task distributed cache manager setup new local dir allocator mr config local dir new file system job dir string archive archive distributed cache get symlink conf this supported largely child subprocess cwd local job runner fresh slate rather user working directory this complicated logic setup work dir creates symlinks jarfile configuration log warn local job runner support symlinking current working dir setup symlinks distributed cache task runner setup work dir conf new file local job dir uri get absolute file write configuration file instead copying system job file write since setup may updated output stream local fs create local job file try conf write xml finally close job new job conf local job file job current object thread wrap loader task distributed cache manager get class
1233	mapreduce\src\java\org\apache\hadoop\mapred\MapFileOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes link map file link org apache hadoop mapreduce lib output map file output format instead map file output format extends file output format writable comparable writable record writer writable comparable writable get record writer file system ignored job conf job string name progressable progress throws io exception get path temporary output file path file file output format get task output path job name file system fs file get file system job compression codec codec null compression type compression type compression type none get compress output job find kind compression compression type sequence file output format get output compression type job find right codec class extends compression codec codec class get output compressor class job default codec codec reflection utils new instance codec class job ignore progress parameter since map file local map file writer new map file writer job fs file string job get output key class subclass writable comparable job get output value class subclass writable compression type codec progress return new record writer writable comparable writable write writable comparable key writable value throws io exception append key value close reporter reporter throws io exception close open output generated format map file reader get readers file system ignored path dir configuration conf throws io exception return org apache hadoop mapreduce lib output map file output format get readers dir conf get entry output generated k extends writable comparable v extends writable writable get entry map file reader readers partitioner k v partitioner k key v value throws io exception part partitioner get partition key value readers length return readers part get key value
1234	mapreduce\src\java\org\apache\hadoop\mapred\MapOutputFile.java	unrelated	package org apache hadoop mapred manipulate working area transient store maps reduces this used map reduce tasks identify directories need write read intermediate files the callers methods child space see mapreduce cluster local dir task tracker job cache job id attempt id this used task tracker space map output file job conf conf string reduce input file format string map map output file local dir allocator dir alloc new local dir allocator mr config local dir return path local map output file created earlier path get output file throws io exception return dir alloc get local path to read task tracker output path separator file conf create local map output file name path get output file for write size throws io exception return dir alloc get local path for write task tracker output path separator file size conf return path local map output index file created earlier path get output index file throws io exception return dir alloc get local path to read task tracker output path separator file index conf create local map output index file name path get output index file for write size throws io exception return dir alloc get local path for write task tracker output path separator file index size conf return local map spill file created earlier path get spill file spill number throws io exception return dir alloc get local path to read task tracker output spill spill number conf create local map spill file name path get spill file for write spill number size throws io exception return dir alloc get local path for write task tracker output spill spill number size conf return local map spill index file created earlier path get spill index file spill number throws io exception return dir alloc get local path to read task tracker output spill spill number index conf create local map spill index file name path get spill index file for write spill number size throws io exception return dir alloc get local path for write task tracker output spill spill number index size conf return local reduce input file created earlier path get input file map id throws io exception return dir alloc get local path to read string format reduce input file format string task tracker output integer value of map id conf create local reduce input file name path get input file for write org apache hadoop mapreduce task id map id size throws io exception return dir alloc get local path for write string format reduce input file format string task tracker output map id get id size conf removes files related task remove all throws io exception conf delete local files task tracker output set conf configuration conf conf instanceof job conf conf job conf conf else conf new job conf conf
1235	mapreduce\src\java\org\apache\hadoop\mapred\Mapper.java	unrelated	package org apache hadoop mapred maps input key value pairs set intermediate key value pairs p maps individual tasks transform input records intermediate records the transformed intermediate records need type input records a given input pair may map zero many output pairs p p the hadoop map reduce framework spawns one map task link input split generated link input format job code mapper code implementations access link job conf job via link job configurable configure job conf initialize similarly use link closeable close method de initialization p p the framework calls link map object object output collector reporter key value pair code input split code task p p all intermediate values associated given output key subsequently grouped framework passed link reducer determine output users control grouping specifying code comparator code via link job conf set output key comparator class class p p the grouped code mapper code outputs partitioned per code reducer code users control keys hence records go code reducer code implementing custom link partitioner p users optionally specify code combiner code via link job conf set combiner class class perform local aggregation intermediate outputs helps cut amount data transferred code mapper code code reducer code p the intermediate grouped outputs always stored link sequence file applications specify intermediate outputs compressed link compression codec used via code job conf code p p if job href doc root org apache hadoop mapred job conf html reducer none zero reduces output code mapper code directly written link file system without grouping keys p p example p p blockquote pre my mapper lt k extends writable comparable v extends writable gt extends map reduce base implements mapper lt k v k v gt enum my counters num records string map task id string input file records configure job conf job map task id job get job context task attempt id input file job get job context map input file map k key v val output collector lt k v gt output reporter reporter throws io exception process lt key value gt pair assume takes let framework know alive kicking reporter progress process increment lt key value gt pairs processed records increment counters reporter incr counter num records every records update application level status records reporter set status map task id processed records input file input file output result output collect key val pre blockquote p p applications may write custom link map runnable exert greater control map processing e g multi threaded code mapper code etc p mapper k v k v extends job configurable closeable maps single input key value pair intermediate key value pair p output pairs need types input pairs a given input pair may map zero many output pairs output pairs collected calls link output collector collect object object p p applications use link reporter provided report progress indicate alive in scenarios application takes insignificant amount time process individual key value pairs crucial since framework might assume task timed kill task the way avoiding set href doc root mapred default html mapreduce task timeout mapreduce task timeout high enough value
1236	mapreduce\src\java\org\apache\hadoop\mapred\MapReduceBase.java	unrelated	package org apache hadoop mapred base link mapper link reducer implementations p provides default op implementations methods non trivial applications need p map reduce base implements closeable job configurable default implementation nothing close throws io exception default implementation nothing configure job conf job
1237	mapreduce\src\java\org\apache\hadoop\mapred\MapReducePolicyProvider.java	unrelated	package org apache hadoop mapred link policy provider map reduce protocols map reduce policy provider extends policy provider service map reduce services new service new service security inter tracker protocol acl inter tracker protocol new service security job submission protocol acl client protocol new service security task umbilical protocol acl task umbilical protocol new service security refresh policy protocol acl refresh authorization policy protocol new service security refresh user mappings protocol acl refresh user mappings protocol new service security admin operations protocol acl admin operations protocol new service security get user mappings protocol acl get user mappings protocol service get services return map reduce services
1238	mapreduce\src\java\org\apache\hadoop\mapred\MapRunnable.java	unrelated	package org apache hadoop mapred expert generic link mapper p custom implementations code map runnable code exert greater control map processing e g multi threaded asynchronous mappers etc p map runnable k v k v extends job configurable start mapping input tt lt key value gt tt pairs p mapping input records output records complete method returns p run record reader k v input output collector k v output reporter reporter throws io exception
1239	mapreduce\src\java\org\apache\hadoop\mapred\MapRunner.java	unrelated	package org apache hadoop mapred default link map runnable implementation map runner k v k v implements map runnable k v k v mapper k v k v mapper boolean incr proc count configure job conf job mapper reflection utils new instance job get mapper class job increment processed counter skipping feature enabled incr proc count skip bad records get mapper max skip records job skip bad records get auto incr mapper proc count job run record reader k v input output collector k v output reporter reporter throws io exception try allocate key value instances used entries k key input create key v value input create value input next key value map pair output mapper map key value output reporter incr proc count reporter incr counter skip bad records counter group skip bad records counter map processed records finally mapper close protected mapper k v k v get mapper return mapper
1240	mapreduce\src\java\org\apache\hadoop\mapred\MapTask.java	unrelated	package org apache hadoop mapred a map task map task extends task the size record index file map outputs map output index record length task split index split meta info new task split index approx header length log log log factory get log map task get name progress map phase progress sort phase set phase task set phase task status phase map get progress set status map map task super map task string job file task attempt id task id partition task split index split index num slots required super job file task id partition num slots required split meta info split index boolean map task return true localize configuration job conf conf throws io exception super localize configuration conf task runner create runner task tracker tracker task tracker task in progress tip return new map task runner tip tracker conf write data output throws io exception super write map or reduce split meta info write split meta info null read fields data input throws io exception super read fields map or reduce split meta info read fields this wraps user record reader update counters progress records read tracked record reader k v implements record reader k v record reader k v raw in counters counter file input byte counter counters counter input record counter task reporter reporter bytes in prev bytes in curr statistics fs stats tracked record reader task reporter reporter job conf job throws io exception input record counter reporter get counter task counter map input records file input byte counter reporter get counter file input format counter bytes read reporter reporter statistics matched stats null reporter get input split instanceof file split matched stats get fs statistics file split reporter get input split get path job fs stats matched stats bytes in prev get input bytes fs stats raw in job get input format get record reader reporter get input split job reporter bytes in curr get input bytes fs stats file input byte counter increment bytes in curr bytes in prev k create key return raw in create key v create value return raw in create value synchronized boolean next k key v value throws io exception boolean ret move to next key value ret incr counters return ret protected incr counters input record counter increment protected synchronized boolean move to next k key v value throws io exception bytes in prev get input bytes fs stats boolean ret raw in next key value bytes in curr get input bytes fs stats file input byte counter increment bytes in curr bytes in prev reporter set progress get progress return ret get pos throws io exception return raw in get pos close throws io exception bytes in prev get input bytes fs stats raw in close bytes in curr get input bytes fs stats file input byte counter increment bytes in curr bytes in prev get progress throws io exception return raw in get progress task reporter get task reporter return reporter get input bytes statistics stats return stats null stats get bytes read this skips records based
1241	mapreduce\src\java\org\apache\hadoop\mapred\MapTaskCompletionEventsUpdate.java	unrelated	package org apache hadoop mapred a represents communication tasktracker child tasks w r map task completion events it also indicates whether child task reset events index map task completion events update implements writable task completion event events boolean reset map task completion events update map task completion events update task completion event events boolean reset events events reset reset boolean reset return reset task completion event get map task completion events return events write data output throws io exception write boolean reset write int events length task completion event event events event write read fields data input throws io exception reset read boolean events new task completion event read int events length events new task completion event events read fields
1242	mapreduce\src\java\org\apache\hadoop\mapred\MapTaskRunner.java	unrelated	package org apache hadoop mapred runs map task map task runner extends task runner map task runner task in progress task task tracker tracker job conf conf super task tracker conf string get child java opts job conf job conf string default value return job conf get job conf mapred map task java opts super get child java opts job conf job conf default mapred task java opts get child ulimit job conf job conf return job conf get int job conf mapred map task ulimit super get child ulimit job conf string get child env job conf job conf return job conf get job conf mapred map task env super get child env job conf level get log level job conf job conf return level level job conf get job conf mapred map task log level job conf default log level string
1243	mapreduce\src\java\org\apache\hadoop\mapred\MapTaskStatus.java	unrelated	package org apache hadoop mapred map task status extends task status map finish time sort finish time map task status map task status task attempt id taskid progress num slots state run state string diagnostic info string state string string task tracker phase phase counters counters super taskid progress num slots run state diagnostic info state string task tracker phase counters boolean get is map return true sets finish time set finish time finish time super set finish time finish time map finish time map finish time finish time set sort finish time finish time get shuffle finish time throw new unsupported operation exception get shuffle finish time supported map task set shuffle finish time shuffle finish time throw new unsupported operation exception set shuffle finish time supported map task get map finish time return map finish time set map finish time map finish time map finish time map finish time get sort finish time return sort finish time set sort finish time sort finish time sort finish time sort finish time synchronized status update task status status super status update status status get map finish time map finish time status get map finish time read fields data input throws io exception super read fields map finish time read long write data output throws io exception super write write long map finish time add fetch failed map task attempt id map task id throw new unsupported operation exception add fetch failed map supported map task
1244	mapreduce\src\java\org\apache\hadoop\mapred\Merger.java	scheduler	package org apache hadoop mapred merger utility used map reduce tasks merging memory disk segments merger log log log factory get log merger local directories local dir allocator dir alloc new local dir allocator mr config local dir k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class compression codec codec path inputs boolean delete inputs merge factor path tmp dir raw comparator k comparator progressable reporter counters counter reads counter counters counter writes counter progress merge phase throws io exception return new merge queue k v conf fs inputs delete inputs codec comparator reporter null merge key class value class merge factor tmp dir reads counter writes counter merge phase k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class compression codec codec path inputs boolean delete inputs merge factor path tmp dir raw comparator k comparator progressable reporter counters counter reads counter counters counter writes counter counters counter merged map outputs counter progress merge phase throws io exception return new merge queue k v conf fs inputs delete inputs codec comparator reporter merged map outputs counter merge key class value class merge factor tmp dir reads counter writes counter merge phase k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class list segment k v segments merge factor path tmp dir raw comparator k comparator progressable reporter counters counter reads counter counters counter writes counter progress merge phase throws io exception return merge conf fs key class value class segments merge factor tmp dir comparator reporter false reads counter writes counter merge phase k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class list segment k v segments merge factor path tmp dir raw comparator k comparator progressable reporter boolean sort segments counters counter reads counter counters counter writes counter progress merge phase throws io exception return new merge queue k v conf fs segments comparator reporter sort segments merge key class value class merge factor tmp dir reads counter writes counter merge phase k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class compression codec codec list segment k v segments merge factor path tmp dir raw comparator k comparator progressable reporter boolean sort segments counters counter reads counter counters counter writes counter progress merge phase throws io exception return new merge queue k v conf fs segments comparator reporter sort segments codec merge key class value class merge factor tmp dir reads counter writes counter merge phase k extends object v extends object raw key value iterator merge configuration conf file system fs class k key class class v value class list segment k v segments merge factor mem segments path tmp dir raw comparator k comparator progressable
1245	mapreduce\src\java\org\apache\hadoop\mapred\MergeSorter.java	unrelated	package org apache hadoop mapred this implements sort method basic type sorter base merge sort note really wrapper actual mergesort implementation util package the main intent providing setup input data util merge sort algo latter need bother various data structures created map output rather concentrate core algorithm thereby allowing easy integration mergesort implementation the bridge util merge sort comparator merge sorter extends basic type sorter base implements comparator int writable progress update frequency progress calls the sort method derived basic type sorter base overridden raw key value iterator sort merge sort new merge sort count super count count return null pointers super pointers pointers copy new count system arraycopy pointers pointers copy count merge sort pointers pointers copy count return new mr sort result iterator super key val buffer pointers copy super start offsets super key lengths super value lengths the implementation compare method comparator note comparator compare takes objects inputs values wrapped reusable int writables util merge sort compare int writable int writable j indicate making progress batch update progress calls progress update frequency progress calls else progress calls reporter progress return comparator compare key val buffer get data start offsets get key lengths get key val buffer get data start offsets j get key lengths j get add extra memory utilized sort method get memory utilized memory actually utilized considering temp array allocated sort method mergesort return super get memory utilized super count
1246	mapreduce\src\java\org\apache\hadoop\mapred\MRConstants.java	unrelated	package org apache hadoop mapred some handy constants mr constants timeouts constants counter update interval result codes success file not found the custom http header used map output length string map output length map output length the custom http header used raw map output length string raw map output length raw map output length the map task map output data transferred string from map task map task the reduce task number map output transferred string for reduce task reduce task string workdir work
1247	mapreduce\src\java\org\apache\hadoop\mapred\MultiFileInputFormat.java	unrelated	package org apache hadoop mapred an link input format returns link multi file split link get splits job conf method splits constructed files input paths each split returned contains nearly equal content length br subclasses implement link get record reader input split job conf reporter construct code record reader code code multi file split code multi file input format k v extends file input format k v input split get splits job conf job num splits throws io exception path paths file util stat paths list status job list multi file split splits new array list multi file split math min num splits paths length paths length hadoop manage splits paths lengths new paths length tot length paths length file system fs paths get file system job lengths fs get content summary paths get length tot length lengths avg length per split tot length num splits cumulative length start index num splits split size find size avg length per split cumulative length start index lengths split size hadoop manage split split size equals path split paths new path split size split lengths new split size system arraycopy paths start index split paths split size system arraycopy lengths start index split lengths split size splits add new multi file split job split paths split lengths start index split size split lengths cumulative length return splits array new multi file split splits size find size split index avg length per split cumulative length start index lengths split index lengths length return lengths length start index goal length split index avg length per split partial length accumulate till goal length start index lengths length partial length lengths partial length cumulative length goal length return start index return lengths length start index record reader k v get record reader input split split job conf job reporter reporter throws io exception
1248	mapreduce\src\java\org\apache\hadoop\mapred\MultiFileSplit.java	unrelated	package org apache hadoop mapred a sub collection input files unlike link file split multi file split represent split file split input files smaller sets the atomic unit split file br multi file split used implement link record reader reading one record per file multi file split extends combine file split multi file split multi file split job conf job path files lengths super job files lengths string get locations throws io exception hash set string host set new hash set string path file get paths file system fs file get file system get job file status status fs get file status file block location blk locations fs get file block locations status status get len blk locations null blk locations length add to set host set blk locations get hosts return host set array new string host set size add to set set string set string array string array set add string string string buffer sb new string buffer get paths length sb append get path uri get path get length get paths length sb append n return sb string
1249	mapreduce\src\java\org\apache\hadoop\mapred\NodeHealthCheckerService.java	unrelated	package org apache hadoop mapred the provides functionality checking health node reporting back service health checker asked report node health checker service log log log factory get log node health checker service absolute path health script string node health script delay node health script executed interval time time script timedout script timeout timer used schedule node health monitoring script execution timer node health script scheduler shell command executor used execute monitoring script shell command executor shexec null configuration used checker configuration conf pattern used searching output node health script string error pattern error configuration keys string health check script property tt config tt health checker script path string health check interval property tt config tt health checker interval string health check failure interval property tt config tt health checker script timeout string health check script arguments property tt config tt health checker script args end configuration keys time error message string node health script timed out msg node health script timed default frequency running node health script default health check interval default script time period default health script failure interval default health check interval boolean healthy string health report last reported time timer task timer enum health checker exit status success timed out failed with exit code failed with exception failed class used link timer periodically execute node health script node health monitor executor extends timer task string exception stack trace node health monitor executor string args array list string exec script new array list string exec script add node health script args null exec script add all arrays list args shexec new shell command executor exec script array new string exec script size null null script timeout run health checker exit status status health checker exit status success try shexec execute catch exit code exception e ignore exit code script status health checker exit status failed with exit code catch exception e log warn caught exception e get message shexec timed out status health checker exit status failed with exception else status health checker exit status timed out exception stack trace string utils stringify exception e finally status health checker exit status success errors shexec get output status health checker exit status failed report health status status method used parse output node health monitor send report address the timed script script causes io exception output ignored the node marked unhealthy ol li the node health script times li li the node health scripts output line begins error li li an exception thrown executing script li ol if script throws link io exception link exit code exception output ignored node left remaining healthy script might syntax error report health status health checker exit status status system current time millis switch status case success set health status true break case timed out set health status false node health script timed out msg break case failed with exception set health status false exception stack trace break case failed with exit code set health status true break case failed set health status false shexec get output break method check output line begins error boolean errors
1250	mapreduce\src\java\org\apache\hadoop\mapred\Operation.java	scheduler	package org apache hadoop mapred generic operation maps dependent set ac ls drive authorization operation enum operation view job counters queue acl administer jobs job acl view job view job details queue acl administer jobs job acl view job view task logs queue acl administer jobs job acl view job kill job queue acl administer jobs job acl modify job fail task queue acl administer jobs job acl modify job kill task queue acl administer jobs job acl modify job set job priority queue acl administer jobs job acl modify job submit job queue acl submit job null queue acl q acl needed job acl job acl needed operation queue acl q acl job acl job acl q acl needed q acl job acl needed job acl
1251	mapreduce\src\java\org\apache\hadoop\mapred\OutputCollector.java	unrelated	package org apache hadoop mapred collects code lt key value gt code pairs output link mapper link reducer p code output collector code generalization facility provided map reduce framework collect data output either code mapper code code reducer code e intermediate outputs output job p output collector k v adds key value pair output collect k key v value throws io exception
1252	mapreduce\src\java\org\apache\hadoop\mapred\OutputCommitter.java	unrelated	package org apache hadoop mapred code output committer code describes commit task output map reduce job p the map reduce framework relies code output committer code job p ol li setup job initialization for example create temporary output directory job initialization job li li cleanup job job completion for example remove temporary output directory job completion li li setup task temporary output li li check whether task needs commit this avoid commit procedure task need commit li li commit task output li li discard task commit li ol output committer extends org apache hadoop mapreduce output committer for framework setup job output initialization setup job job context job context throws io exception for cleaning job output job completion link abort job job context instead cleanup job job context job context throws io exception for committing job output successful job completion note invoked jobs runstate successful commit job job context job context throws io exception cleanup job job context for aborting unsuccessful job output note invoked jobs runstate link job status failed link job status killed abort job job context job context status throws io exception cleanup job job context sets output task setup task task attempt context task context throws io exception check whether task needs commit boolean needs task commit task attempt context task context throws io exception to promote task temporary output output location the task output moved job output directory commit task task attempt context task context throws io exception discard task output abort task task attempt context task context throws io exception this method implements new calling old method note input types different new old apis bridge two setup job org apache hadoop mapreduce job context job context throws io exception setup job job context job context this method implements new calling old method note input types different new old apis bridge two link abort job org apache hadoop mapreduce job context org apache hadoop mapreduce job status state instead cleanup job org apache hadoop mapreduce job context context throws io exception cleanup job job context context this method implements new calling old method note input types different new old apis bridge two commit job org apache hadoop mapreduce job context context throws io exception commit job job context context this method implements new calling old method note input types different new old apis bridge two abort job org apache hadoop mapreduce job context context org apache hadoop mapreduce job status state run state throws io exception state job status get old new job run state run state state job status failed state job status killed throw new io exception invalid job run state run state name abort job job context context state this method implements new calling old method note input types different new old apis bridge two setup task org apache hadoop mapreduce task attempt context task context throws io exception setup task task attempt context task context this method implements new calling old method note input types different new old apis bridge two boolean needs task commit org apache hadoop mapreduce task attempt context task
1253	mapreduce\src\java\org\apache\hadoop\mapred\OutputFormat.java	unrelated	package org apache hadoop mapred code output format code describes output specification map reduce job p the map reduce framework relies code output format code job p ol li validate output specification job for e g check output directory already exist li provide link record writer implementation used write output files job output files stored link file system li ol output format k v get link record writer given job record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception check validity output specification job p this validate output specification job job submitted typically checks already exist throwing exception already exists output overwritten p check output specs file system ignored job conf job throws io exception
1254	mapreduce\src\java\org\apache\hadoop\mapred\OutputLogFilter.java	unrelated	package org apache hadoop mapred this filters log files directory given it doesnt accept paths logs this used list paths output directory follows path file list file util stat paths fs list status dir new output log filter link org apache hadoop mapred utils output file utils output log filter instead output log filter implements path filter path filter log filter new utils output file utils output log filter boolean accept path path return log filter accept path
1255	mapreduce\src\java\org\apache\hadoop\mapred\Partitioner.java	unrelated	package org apache hadoop mapred partitions key space p code partitioner code controls partitioning keys intermediate map outputs the key subset key used derive partition typically hash function the total number partitions number reduce tasks job hence controls code code reduce tasks intermediate key hence record sent reduction p partitioner k v extends job configurable get paritition number given key hence record given total number partitions e number reduce tasks job p typically hash function subset key p get partition k key v value num partitions
1256	mapreduce\src\java\org\apache\hadoop\mapred\Queue.java	unrelated	package org apache hadoop mapred a storing properties job queue queue implements comparable queue log log log factory get log queue queue name string name null acls list map string access control list acls queue state queue state state queue state running an object used schedulers fill arbitrary scheduling information the string method objects called framework get string displayed ui object scheduling info set queue children properties props default constructor useful creating hierarchy the variables populated using mutator methods queue create job queue queue string name map string access control list acls queue state state name name acls acls state state return name queue string get name return name set name queue set name string name name name return ac ls queue the keys map indicate operations performed values indicate list users groups perform operation perform operations map string access control list get acls return acls set ac ls queue perform operations set acls map string access control list acls acls acls return state queue queue state get state return state set state queue set state queue state state state state return scheduling information queue object get scheduling info return scheduling info set scheduling information queue set scheduling info object scheduling info scheduling info scheduling info copy scheduling information source queue queue recursively copy scheduling info queue source queue first update children queues recursively set queue dest children get children dest children null iterator queue itr dest children iterator iterator queue itr source queue get children iterator itr next itr next copy scheduling info itr next now copy information root queue set scheduling info source queue get scheduling info add child queue child children null children new tree set queue children add child set queue get children return children set properties properties props props props properties get properties return props this methods helps traversing tree hierarchy returns list inner queues e nodes children level incase children null returns empty map this helps case creating union inner leaf queues map string queue get inner queues map string queue new hash map string queue if children return empty set this check required root node children null return check children parent queue child children check children parent add child get children null child get children size put child get name child put all child get inner queues return this method helps maintaining single data structure across queue manager now maintain list root queues done doesn return null adds leaf node map string queue get leaf queues map string queue new hash map string queue children null put name return queue child children put all child get leaf queues return compare to queue queue return name compare to queue get name boolean equals object return true instanceof queue return false return queue get name equals name string string return get name hash code return get name hash code return hierarchy link job queue info objects queue job queue info get job queue info job queue info queue info new job queue info queue info set queue name name log debug created job q info queue info
1257	mapreduce\src\java\org\apache\hadoop\mapred\QueueACL.java	scheduler	package org apache hadoop mapred enum representing access control list drives set operations performed queue enum queue acl submit job acl submit job administer jobs acl administer jobs currently acl acl administer jobs checked operations fail task kill task kill job set job priority view job todo add acl list jobs ability authenticate users ui todo add acl change acl admin tool configuring queues string acl name queue acl string acl name acl name acl name string get acl name return acl name
1258	mapreduce\src\java\org\apache\hadoop\mapred\QueueAclsInfo.java	unrelated	package org apache hadoop mapred class encapsulate queue ac ls particular user queue acls info extends org apache hadoop mapreduce queue acls info default constructor queue acls info queue acls info super construct new queue acls info object using queue name queue operations array queue acls info string queue name string operations super queue name operations queue acls info downgrade org apache hadoop mapreduce queue acls info acl return new queue acls info acl get queue name acl get operations
1259	mapreduce\src\java\org\apache\hadoop\mapred\QueueConfigurationParser.java	unrelated	package org apache hadoop mapred class parsing mapred queues xml the format consists nesting queues within queues feature called hierarchical queues the parser expects queues defined within queues tag top level element xml document creates complete queue hieararchy queue configuration parser log log log factory get log queue configuration parser boolean acls enabled false default root protected queue root null xml tags mapred queues xml string name separator string queue tag queue string acl submit job tag acl submit job string acl administer job tag acl administer jobs the value read queues config file tag used to enable queue acls job acls mapreduce cluster acls enabled set mapred site xml string acls enabled tag acls enabled string properties tag properties string state tag state string queue name tag name string queues tag queues string property tag property string key tag key string value tag value default constructor deperacated queue configuration parser queue configuration parser queue configuration parser string conf file boolean acls enabled acls enabled acls enabled file file new file conf file get absolute file file exists throw new runtime exception configuration file found conf file input stream null try new buffered input stream new file input stream file load from catch io exception ioe throw new runtime exception ioe finally io utils close stream queue configuration parser input stream xml input boolean acls enabled acls enabled acls enabled load from xml input load from input stream xml input try root load resource xml input catch parser configuration exception e throw new runtime exception e catch sax exception e throw new runtime exception e catch io exception e throw new runtime exception e set acls enabled boolean acls enabled acls enabled acls enabled boolean acls enabled return acls enabled queue get root return root set root queue root root root method load resource file generates root protected queue load resource input stream resource input throws parser configuration exception sax exception io exception document builder factory doc builder factory document builder factory new instance ignore comments inside xml file doc builder factory set ignoring comments true allow includes xml file doc builder factory set namespace aware true try doc builder factory set x include aware true catch unsupported operation exception e log info failed set set x include aware true parser doc builder factory name separator e document builder builder doc builder factory new document builder document doc null element queues node null doc builder parse resource input queues node doc get document element return parse resource queues node queue parse resource element queues node queue root node null try queues tag equals queues node get tag name log info bad conf file top level element queues throw new runtime exception no queues defined named node map nmp queues node get attributes node acls nmp get named item acls enabled tag acls null log warn configuring acls enabled tag flag queue manager queue conf file name valid this tag ignored configure mr config mr acls enabled mapred site xml see documentation mr config mr acls enabled used enabling job level authorization queue
1260	mapreduce\src\java\org\apache\hadoop\mapred\QueueManager.java	scheduler	package org apache hadoop mapred class exposes information queues maintained hadoop map reduce framework p the map reduce framework configured one queues depending scheduler configured while schedulers work one queue schedulers support multiple queues some schedulers also support notion queues within queues feature called hierarchical queues p queue names unique used key lookup queues hierarchical queues named fully qualified name q q q q child queue q q child queue q p leaf level queues queues contain queues within jobs submitted leaf level queues p queues configured various properties some properties common schedulers handled schedulers might also associate several custom properties queues these properties parsed maintained per queue framework if schedulers need complicated structure maintain configuration per queue free use facilities provided framework define mechanisms in cases likely name queue used relate common properties queue scheduler specific properties p information related queue name properties scheduling information children exposed via serializable called link job queue info p queues configured configuration file mapred queues xml to support backwards compatibility queues also configured mapred site xml however configured latter support hierarchical queues queue manager log log log factory get log queue manager map queue name queue object map string queue leaf queues new hash map string queue map string queue queues new hash map string queue string queue conf file name mapred queues xml string queue conf default file name mapred queues default xml prefix configuration queue related keys string queue conf property name prefix mapred queue resource queue acls configured queue root null represents job queue acls enabled mapreduce cluster boolean acls enabled false factory method create appropriate instance queue configuration parser p returns parser parse either deprecated property style queue configuration mapred site xml one parse hierarchical queues mapred queues xml first preference given configuration mapred site xml if queue configuration found parser parse configuration mapred queues xml created use queue configuration parser get queue configuration parser configuration conf boolean reload conf boolean acls enabled conf null conf get deprecated queue configuration parser mapred queue names key null reload conf conf reload configuration return new deprecated queue configuration parser conf else url xml in url thread current thread get context class loader get resource queue conf file name xml in url null xml in url thread current thread get context class loader get resource queue conf default file name assert xml in url null jar input stream stream null try stream xml in url open stream return new queue configuration parser new buffered input stream stream acls enabled catch io exception ioe throw new runtime exception couldn open queue configuration xml in url ioe finally io utils close stream stream queue manager acls disabled false queue manager boolean acls enabled acls enabled acls enabled initialize get queue configuration parser null false acls enabled construct new queue manager using configuration specified passed link org apache hadoop conf configuration object p this instance supports queue configuration specified mapred site xml without support hierarchical queues if queue configuration found mapred site xml look site configuration mapred queues xml supporting hierarchical queues queue manager configuration cluster conf
1261	mapreduce\src\java\org\apache\hadoop\mapred\RamManager.java	pooling	package org apache hadoop mapred code ram manager code manages memory pool configured limit ram manager reserve memory data coming given input stream else code false code boolean reserve requested size input stream throws interrupted exception return memory pool unreserve requested size
1262	mapreduce\src\java\org\apache\hadoop\mapred\RawKeyValueIterator.java	unrelated	package org apache hadoop mapred code raw key value iterator code iterator used iterate raw keys values sort merge intermediate data raw key value iterator gets current raw key data input buffer get key throws io exception gets current raw value data input buffer get value throws io exception sets current key value get key get value code false code otherwise boolean next throws io exception closes iterator underlying streams closed close throws io exception gets progress object indicating bytes processed iterator far progress get progress
1263	mapreduce\src\java\org\apache\hadoop\mapred\RecordReader.java	unrelated	package org apache hadoop mapred code record reader code reads lt key value gt pairs link input split p code record reader code typically converts byte oriented view input provided code input split code presents record oriented view link mapper link reducer tasks processing it thus assumes responsibility processing record boundaries presenting tasks keys values p record reader k v reads next key value pair input processing boolean next k key v value throws io exception create object appropriate type used key k create key create object appropriate type used value v create value returns current position input get pos throws io exception close link input split future operations close throws io exception how much input link record reader consumed e processed get progress throws io exception
1264	mapreduce\src\java\org\apache\hadoop\mapred\RecordWriter.java	unrelated	package org apache hadoop mapred code record writer code writes output lt key value gt pairs output file p code record writer code implementations write job outputs link file system record writer k v writes key value pair write k key v value throws io exception close code record writer code future operations close reporter reporter throws io exception
1265	mapreduce\src\java\org\apache\hadoop\mapred\Reducer.java	unrelated	package org apache hadoop mapred reduces set intermediate values share key smaller set values p the number code reducer code job set user via link job conf set num reduce tasks code reducer code implementations access link job conf job via link job configurable configure job conf method initialize similarly use link closeable close method de initialization p p code reducer code primary phases p ol li id shuffle shuffle p code reducer code input grouped output link mapper in phase framework code reducer code fetches relevant partition output code mapper code via http p li li id sort sort p the framework groups code reducer code inputs code key code since different code mapper code may output key stage p p the shuffle sort phases occur simultaneously e outputs fetched merged p id secondary sort secondary sort p if equivalence rules keys grouping intermediates different grouping keys reduction one may specify code comparator code via link job conf set output value grouping comparator class since link job conf set output key comparator class class used control intermediate keys grouped used conjunction simulate secondary sort values p for example say want find duplicate web pages tag url best known example you would set job like ul li map input key url li li map input value document li li map output key document checksum url pagerank li li map output value url li li partitioner checksum li li output key comparator checksum decreasing pagerank li li output value grouping comparator checksum li ul li li id reduce reduce p in phase link reduce object iterator output collector reporter method called code lt key list values code pair grouped inputs p p the output reduce task typically written link file system via link output collector collect object object p li ol p the output code reducer code b sorted b p p example p p blockquote pre my reducer lt k extends writable comparable v extends writable gt extends map reduce base implements reducer lt k v k v gt enum my counters num records string reduce task id keys configure job conf job reduce task id job get job context task attempt id reduce k key iterator lt v gt values output collector lt k v gt output reporter reporter throws io exception process values values next v value values next increment values key values process lt key value gt pair assume takes let framework know alive kicking values reporter progress process output lt key value gt output collect key value increment lt key list values gt pairs processed keys increment counters reporter incr counter num records every keys update application level status keys reporter set status reduce task id processed keys pre blockquote p reducer k v k v extends job configurable closeable reduces values given key p the framework calls method code lt key list values code pair grouped inputs output values must type input values input keys must altered the framework b reuse b key value objects passed reduce therefore application clone objects want keep copy in many cases
1266	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTask.java	unrelated	package org apache hadoop mapred a reduce task reduce task extends task register ctor writable factories set factory reduce task new writable factory writable new instance return new reduce task log log log factory get log reduce task get name num maps compression codec codec get progress set status reduce set phase task status phase shuffle phase start progress copy phase progress sort phase progress reduce phase counters counter shuffled maps counter get counters find counter task counter shuffled maps counters counter reduce shuffle bytes get counters find counter task counter reduce shuffle bytes counters counter reduce input key counter get counters find counter task counter reduce input groups counters counter reduce input value counter get counters find counter task counter reduce input records counters counter reduce output counter get counters find counter task counter reduce output records counters counter reduce combine input counter get counters find counter task counter combine input records counters counter reduce combine output counter get counters find counter task counter combine output records counters counter file output byte counter get counters find counter file output format counter bytes written a custom comparator map output files here ordering determined file size path in case files size different file paths first parameter considered smaller second one in case files size path considered equal comparator file status map output file comparator new comparator file status compare file status file status b get len b get len return else get len b get len get path string equals b get path string return else return else return a sorted set keeping set map output files disk sorted set file status map output files on disk new tree set file status map output file comparator reduce task super reduce task string job file task attempt id task id partition num maps num slots required super job file task id partition num slots required num maps num maps compression codec init codec check map outputs compressed conf get compress map output class extends compression codec codec class conf get map output compressor class default codec return reflection utils new instance codec class conf return null task runner create runner task tracker tracker task in progress tip throws io exception return new reduce task runner tip tracker conf boolean map task return false get num maps return num maps localize given job conf specific task localize configuration job conf conf throws io exception super localize configuration conf conf set num map tasks num maps write data output throws io exception super write write int num maps write number maps read fields data input throws io exception super read fields num maps read int get input files reducer path get map files file system fs boolean local throws io exception list path file list new array list path local local jobs num maps file list add map output file get input file else non local jobs file status filestatus map output files on disk file list add filestatus get path return file list array new path reduce values iterator key value extends values iterator key
1267	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTaskRunner.java	unrelated	package org apache hadoop mapred runs reduce task reduce task runner extends task runner reduce task runner task in progress task task tracker tracker job conf conf throws io exception super task tracker conf close throws io exception get task get progress set status closed string get child java opts job conf job conf string default value return job conf get job conf mapred reduce task java opts super get child java opts job conf job conf default mapred task java opts get child ulimit job conf job conf return job conf get int job conf mapred reduce task ulimit super get child ulimit job conf string get child env job conf job conf return job conf get job conf mapred reduce task env super get child env job conf level get log level job conf job conf return level level job conf get job conf mapred reduce task log level job conf default log level string
1268	mapreduce\src\java\org\apache\hadoop\mapred\ReduceTaskStatus.java	unrelated	package org apache hadoop mapred reduce task status extends task status shuffle finish time sort finish time list task attempt id failed fetch tasks new array list task attempt id reduce task status reduce task status task attempt id taskid progress num slots state run state string diagnostic info string state string string task tracker phase phase counters counters super taskid progress num slots run state diagnostic info state string task tracker phase counters object clone reduce task status clone reduce task status super clone clone failed fetch tasks new array list task attempt id failed fetch tasks return clone boolean get is map return false set finish time finish time shuffle finish time shuffle finish time finish time sort finish time sort finish time finish time super set finish time finish time get shuffle finish time return shuffle finish time set shuffle finish time shuffle finish time shuffle finish time shuffle finish time get sort finish time return sort finish time set sort finish time sort finish time sort finish time sort finish time shuffle finish time shuffle finish time sort finish time get map finish time throw new unsupported operation exception get map finish time supported reduce task set map finish time shuffle finish time throw new unsupported operation exception set map finish time supported reduce task list task attempt id get fetch failed maps return failed fetch tasks add fetch failed map task attempt id map task id failed fetch tasks add map task id synchronized status update task status status super status update status status get shuffle finish time shuffle finish time status get shuffle finish time status get sort finish time sort finish time status get sort finish time list task attempt id new fetch failed maps status get fetch failed maps failed fetch tasks null failed fetch tasks new fetch failed maps else new fetch failed maps null failed fetch tasks add all new fetch failed maps synchronized clear status super clear status failed fetch tasks clear read fields data input throws io exception super read fields shuffle finish time read long sort finish time read long failed fetch tasks read int failed fetch tasks new array list task attempt id failed fetch tasks failed fetch tasks task attempt id id new task attempt id id read fields failed fetch tasks add id write data output throws io exception super write write long shuffle finish time write long sort finish time write int failed fetch tasks size task attempt id task id failed fetch tasks task id write
1269	mapreduce\src\java\org\apache\hadoop\mapred\ReinitTrackerAction.java	unrelated	package org apache hadoop mapred represents directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker reinitialize reinit tracker action extends task tracker action reinit tracker action super action type reinit tracker write data output throws io exception read fields data input throws io exception
1270	mapreduce\src\java\org\apache\hadoop\mapred\Reporter.java	unrelated	package org apache hadoop mapred a facility map reduce applications report progress update counters status information etc p link mapper link reducer use code reporter code provided report progress indicate alive in scenarios application takes insignificant amount time process individual key value pairs crucial since framework might assume task timed kill task p applications also update link counters via provided code reporter code p reporter extends progressable a constant reporter type nothing reporter null new reporter set status string progress counter get counter enum name return null counter get counter string group string name return null incr counter enum key amount incr counter string group string counter amount input split get input split throws unsupported operation exception throw new unsupported operation exception null reporter input get progress return set status description task set status string status get link counter given group given name counter get counter enum name get link counter given group given name counter get counter string group string name increments counter identified key link enum type specified amount code enum code incremented incr counter enum key amount increments counter identified group counter name specified amount incremented incr counter string group string counter amount get link input split object map input split get input split throws unsupported operation exception get progress task progress represented number inclusive get progress
1271	mapreduce\src\java\org\apache\hadoop\mapred\ResourceEstimator.java	unrelated	package org apache hadoop mapred class responsible modeling resource consumption running tasks for temp space maps there one resource estimator per job in progress resource estimator log job in progress log log log factory get log org apache hadoop mapred resource estimator completed maps input size completed maps output size completed maps updates job in progress job threshhold to use resource estimator job in progress job job job threshhold to use job desired maps protected synchronized update with completed task task status ts task in progress tip indicates error average tip map task ts get output size completed maps updates completed maps input size tip get map input size completed maps output size ts get output size log debug enabled log debug completed maps updates completed maps updates completed maps input size completed maps input size completed maps output size completed maps output size protected synchronized get estimated total map output size completed maps updates threshhold to use return else input size job get input length job desired maps add desired maps randomwriter case blow estimate math round input size completed maps output size completed maps input size log debug enabled log debug estimate total map output estimate return estimate get estimated map output size estimate l job desired maps estimate get estimated total map output size job desired maps return estimate get estimated reduce input size job desired reduces reduce output size return else return get estimated total map output size job desired reduces estimate reduce gets equal share total map output
1272	mapreduce\src\java\org\apache\hadoop\mapred\RunningJob.java	scheduler	package org apache hadoop mapred code running job code user query details running map reduce job p clients get hold code running job code via link job client query running job details name configuration progress etc p running job get underlying job configuration configuration get configuration get job identifier job id get id rather use link get id string get job id get name job string get job name get path submitted job configuration string get job file get url job progress information displayed string get tracking url get progress job map tasks when map tasks completed function returns map progress throws io exception get progress job reduce tasks when reduce tasks completed function returns reduce progress throws io exception get progress job cleanup tasks when cleanup tasks completed function returns cleanup progress throws io exception get progress job setup tasks when setup tasks completed function returns setup progress throws io exception check job finished this non blocking call boolean complete throws io exception check job completed successfully boolean successful throws io exception blocks job complete wait for completion throws io exception returns current state job link job status get job state throws io exception kill running job blocks job tasks killed well if job longer running simply returns kill job throws io exception set priority running job set job priority string priority throws io exception get events indicating completion success failure component tasks task completion event get task completion events start from throws io exception kill indicated task attempt list otherwise killed w affecting job failure status kill task task attempt id task id boolean fail throws io exception kill task string task id boolean fail throws io exception gets counters job counters get counters throws io exception gets diagnostic messages given task attempt string get task diagnostics task attempt id taskid throws io exception get url history file archived returns empty history file available yet string get history url throws io exception check whether job removed job tracker memory retired on retire job history file copied location known link get history url boolean retired throws io exception
1273	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsBinaryInputFormat.java	unrelated	package org apache hadoop mapred input format reading keys values sequence files binary raw format link org apache hadoop mapreduce lib input sequence file as binary input format instead sequence file as binary input format extends sequence file input format bytes writable bytes writable sequence file as binary input format super record reader bytes writable bytes writable get record reader input split split job conf job reporter reporter throws io exception return new sequence file as binary record reader job file split split read records sequence file binary raw bytes sequence file as binary record reader implements record reader bytes writable bytes writable sequence file reader start end boolean done false data output buffer buffer new data output buffer sequence file value bytes vbytes sequence file as binary record reader configuration conf file split split throws io exception path path split get path file system fs path get file system conf new sequence file reader fs path conf end split get start split get length split get start get position sync split get start sync start start get position vbytes create value bytes done start end bytes writable create key return new bytes writable bytes writable create value return new bytes writable retrieve name key sequence file string get key class name return get key class name retrieve name value sequence file string get value class name return get value class name read raw bytes sequence file synchronized boolean next bytes writable key bytes writable val throws io exception done return false pos get position boolean eof next raw key buffer eof key set buffer get data buffer get length buffer reset next raw value vbytes vbytes write uncompressed bytes buffer val set buffer get data buffer get length buffer reset return done eof pos end sync seen get pos throws io exception return get position close throws io exception close return progress within input split get progress throws io exception end start return f else return math min f get position start end start
1274	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsBinaryOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes keys values link sequence file binary raw format link org apache hadoop mapreduce lib output sequence file as binary output format instead sequence file as binary output format extends sequence file output format bytes writable bytes writable inner used append raw protected writable value bytes extends org apache hadoop mapreduce lib output sequence file as binary output format writable value bytes set key link sequence file p this allows user specify key different actual link bytes writable used writing p set sequence file output key class job conf conf class class conf set class org apache hadoop mapreduce lib output sequence file as binary output format key class class object set value link sequence file p this allows user specify value different actual link bytes writable used writing p set sequence file output value class job conf conf class class conf set class org apache hadoop mapreduce lib output sequence file as binary output format value class class object get key link sequence file class extends writable comparable get sequence file output key class job conf conf return conf get class org apache hadoop mapreduce lib output sequence file as binary output format key class conf get output key class subclass writable comparable writable comparable get value link sequence file class extends writable get sequence file output value class job conf conf return conf get class org apache hadoop mapreduce lib output sequence file as binary output format value class conf get output value class subclass writable writable record writer bytes writable bytes writable get record writer file system ignored job conf job string name progressable progress throws io exception get path temporary output file path file file output format get task output path job name file system fs file get file system job compression codec codec null compression type compression type compression type none get compress output job find kind compression compression type get output compression type job find right codec class extends compression codec codec class get output compressor class job default codec codec reflection utils new instance codec class job sequence file writer sequence file create writer fs job file get sequence file output key class job get sequence file output value class job compression type codec progress return new record writer bytes writable bytes writable writable value bytes wvaluebytes new writable value bytes write bytes writable bkey bytes writable bvalue throws io exception wvaluebytes reset bvalue append raw bkey get bytes bkey get length wvaluebytes wvaluebytes reset null close reporter reporter throws io exception close check output specs file system ignored job conf job throws io exception super check output specs ignored job get compress output job get output compression type job compression type record throw new invalid job conf exception sequence file as binary output format support record compression
1275	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsTextInputFormat.java	unrelated	package org apache hadoop mapred this similar sequence file input format except generates sequence file as text record reader converts input keys values string forms calling string method link org apache hadoop mapreduce lib input sequence file as text input format instead sequence file as text input format extends sequence file input format text text sequence file as text input format super record reader text text get record reader input split split job conf job reporter reporter throws io exception reporter set status split string return new sequence file as text record reader job file split split
1276	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileAsTextRecordReader.java	unrelated	package org apache hadoop mapred this converts input keys values string forms calling string method this sequence file as text input format line record reader text input format link org apache hadoop mapreduce lib input sequence file as text record reader instead sequence file as text record reader implements record reader text text sequence file record reader writable comparable writable sequence file record reader writable comparable inner key writable inner value sequence file as text record reader configuration conf file split split throws io exception sequence file record reader new sequence file record reader writable comparable writable conf split inner key sequence file record reader create key inner value sequence file record reader create value text create key return new text text create value return new text read key value pair line synchronized boolean next text key text value throws io exception text key key text value value sequence file record reader next inner key inner value return false key set inner key string value set inner value string return true get progress throws io exception return sequence file record reader get progress synchronized get pos throws io exception return sequence file record reader get pos synchronized close throws io exception sequence file record reader close
1277	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileInputFilter.java	unrelated	package org apache hadoop mapred a allows map red job work sample sequence files the sample decided filter set job link org apache hadoop mapreduce lib input sequence file input filter instead sequence file input filter k v extends sequence file input format k v string filter class org apache hadoop mapreduce lib input sequence file input filter filter class sequence file input filter create record reader given split record reader k v get record reader input split split job conf job reporter reporter throws io exception reporter set status split string return new filter record reader k v job file split split set filter set filter class configuration conf class filter class conf set filter class filter class get name filter filter extends org apache hadoop mapreduce lib input sequence file input filter filter base filters filter base extends org apache hadoop mapreduce lib input sequence file input filter filter base implements filter records filter matching key regex regex filter extends filter base org apache hadoop mapreduce lib input sequence file input filter regex filter rf set pattern configuration conf string regex throws pattern syntax exception org apache hadoop mapreduce lib input sequence file input filter regex filter set pattern conf regex regex filter rf new org apache hadoop mapreduce lib input sequence file input filter regex filter configure filter checking configuration set conf configuration conf rf set conf conf filtering method if key matches regex return true otherwise return false boolean accept object key return rf accept key this returns percentage records the percentage determined filtering frequency f using criteria record f for example frequency one records returned percent filter extends filter base org apache hadoop mapreduce lib input sequence file input filter percent filter pf set frequency stores conf set frequency configuration conf frequency org apache hadoop mapreduce lib input sequence file input filter percent filter set frequency conf frequency percent filter pf new org apache hadoop mapreduce lib input sequence file input filter percent filter configure filter checking configuration set conf configuration conf pf set conf conf filtering method if record frequency return true otherwise return false boolean accept object key return pf accept key this returns set records examing md digest key filtering frequency f the filtering criteria md key f md filter extends filter base md len org apache hadoop mapreduce lib input sequence file input filter md filter md len org apache hadoop mapreduce lib input sequence file input filter md filter mf set filtering frequency configuration set frequency configuration conf frequency org apache hadoop mapreduce lib input sequence file input filter md filter set frequency conf frequency md filter mf new org apache hadoop mapreduce lib input sequence file input filter md filter configure filter according configuration set conf configuration conf mf set conf conf filtering method if md key frequency return true otherwise return false boolean accept object key return mf accept key filter record reader k v extends sequence file record reader k v filter filter filter record reader configuration conf file split split throws io exception super conf split instantiate filter
1278	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileInputFormat.java	unrelated	package org apache hadoop mapred an link input format link sequence file link org apache hadoop mapreduce lib input sequence file input format instead sequence file input format k v extends file input format k v sequence file input format set min split size sequence file sync interval protected file status list status job conf job throws io exception file status files super list status job files length file status file files file directory map file path data file new path file get path map file data file name file system fs file get path get file system job use data file files fs get file status data file return files record reader k v get record reader input split split job conf job reporter reporter throws io exception reporter set status split string return new sequence file record reader k v job file split split
1279	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes link sequence file link org apache hadoop mapreduce lib output sequence file output format instead sequence file output format k v extends file output format k v record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception get path temporary output file path file file output format get task output path job name file system fs file get file system job compression codec codec null compression type compression type compression type none get compress output job find kind compression compression type get output compression type job find right codec class extends compression codec codec class get output compressor class job default codec codec reflection utils new instance codec class job sequence file writer sequence file create writer fs job file job get output key class job get output value class compression type codec progress return new record writer k v write k key v value throws io exception append key value close reporter reporter throws io exception close open output generated format sequence file reader get readers configuration conf path dir throws io exception file system fs dir get file system conf path names file util stat paths fs list status dir sort names hash partitioning works arrays sort names sequence file reader parts new sequence file reader names length names length parts new sequence file reader fs names conf return parts get link compression type output link sequence file defaulting link compression type record compression type get output compression type job conf conf string val conf get org apache hadoop mapreduce lib output file output format compress type compression type record string return compression type value of val set link compression type output link sequence file link sequence file set output compression type job conf conf compression type style set compress output conf true conf set org apache hadoop mapreduce lib output file output format compress type style string
1280	mapreduce\src\java\org\apache\hadoop\mapred\SequenceFileRecordReader.java	unrelated	package org apache hadoop mapred an link record reader link sequence file sequence file record reader k v implements record reader k v sequence file reader start end boolean true protected configuration conf sequence file record reader configuration conf file split split throws io exception path path split get path file system fs path get file system conf new sequence file reader fs path conf end split get start split get length conf conf split get start get position sync split get start sync start start get position start end the key must passed link next object object class get key class return get key class the value must passed link next object object class get value class return get value class k create key return k reflection utils new instance get key class conf v create value return v reflection utils new instance get value class conf synchronized boolean next k key v value throws io exception return false pos get position boolean remaining next key null remaining get current value value pos end sync seen false else remaining return protected synchronized boolean next k key throws io exception return false pos get position boolean remaining next key null pos end sync seen false else remaining return protected synchronized get current value v value throws io exception get current value value return progress within input split get progress throws io exception end start return f else return math min f get position start end start synchronized get pos throws io exception return get position protected synchronized seek pos throws io exception seek pos synchronized close throws io exception close
1281	mapreduce\src\java\org\apache\hadoop\mapred\SkipBadRecords.java	unrelated	package org apache hadoop mapred utility skip bad records functionality it contains various settings related skipping bad records p hadoop provides optional mode execution bad records detected skipped attempts p this feature used map reduce tasks crashes deterministically certain input this happens due bugs map reduce function the usual course would fix bugs but sometimes possible perhaps bug third party libraries source code available due task never reaches completion even multiple attempts complete data task lost p p with feature small portion data lost surrounding bad record may acceptable user applications see link skip bad records set mapper max skip records configuration p p the skipping mode gets kicked certain failures see link skip bad records set attempts to start skipping configuration p p in skipping mode map reduce task maintains record range getting processed times before giving input map reduce function sends record range task tracker if task crashes task tracker knows one last reported range on attempts range get skipped p skip bad records special counters written application used framework detecting bad records for detecting bad records counters must incremented application string counter group skipping task counters number processed map records string counter map processed records map processed records number processed reduce groups string counter reduce processed groups reduce processed groups string attempts to start skipping job context skip start attempts string auto incr map proc count job context map skip incr proc count string auto incr reduce proc count job context reduce skip incr proc count string out path job context skip outdir string mapper max skip records job context map skip max records string reducer max skip groups job context reduce skip maxgroups get number task attempts after skip mode kicked when skip mode kicked tasks reports range records process next task tracker so failures tt knows ones possibly bad records on executions skipped default value get attempts to start skipping configuration conf return conf get int attempts to start skipping set number task attempts after skip mode kicked when skip mode kicked tasks reports range records process next task tracker so failures tt knows ones possibly bad records on executions skipped default value set attempts to start skipping configuration conf attempts to start skipping conf set int attempts to start skipping attempts to start skipping get flag set true link skip bad records counter map processed records incremented map runner invoking map function this value must set false applications process records asynchronously buffer input records for example streaming in cases applications increment counter default value true link skip bad records counter map processed records code false code otherwise boolean get auto incr mapper proc count configuration conf return conf get boolean auto incr map proc count true set flag set true link skip bad records counter map processed records incremented map runner invoking map function this value must set false applications process records asynchronously buffer input records for example streaming in cases applications increment counter default value true link skip bad records counter map processed records set auto incr mapper proc count configuration conf boolean auto incr
1282	mapreduce\src\java\org\apache\hadoop\mapred\SortedRanges.java	unrelated	package org apache hadoop mapred keeps ranges sorted start index the added ranges always ensured non overlapping provides skip range iterator skips ranges stored object sorted ranges implements writable log log log factory get log sorted ranges tree set range ranges new tree set range indices count get iterator skips stored ranges the iterator next call return index starting synchronized skip range iterator skip range iterator return new skip range iterator ranges iterator get indices stored ranges synchronized get indices count return indices count get sorted set ranges synchronized sorted set range get ranges return ranges add range indices it ensured added range overlap existing ranges if overlaps existing overlapping ranges removed single range superset removed ranges range added if range length anything synchronized add range range range empty return start index range get start index end index range get end index make sure overlapping ranges sorted set range head set ranges head set range head set size range previous range head set last log debug previous range previous range start index previous range get end index previous range overlaps range remove previous range ranges remove previous range indices count previous range get length expand range start index previous range get start index end index end index previous range get end index end index previous range get end index iterator range tail set it ranges tail set range iterator tail set it next range next range tail set it next log debug next range next range start index start index end index end index end index next range get start index next range overlaps range remove next range tail set it remove indices count next range get length end index next range get end index expand range end index next range get end index break else break add start index end index remove range indices if range found existing ranges existing ranges shrunk if range length anything synchronized remove range range range empty return start index range get start index end index range get end index make sure overlapping ranges sorted set range head set ranges head set range head set size range previous range head set last log debug previous range previous range start index previous range get end index previous range overlaps range narrow previous range ranges remove previous range indices count previous range get length log debug removed previous range previous range add previous range get start index start index end index previous range get end index add end index previous range get end index iterator range tail set it ranges tail set range iterator tail set it next range next range tail set it next log debug next range next range start index start index end index end index end index next range get start index next range overlaps range narrow next range tail set it remove indices count next range get length end index next range get end index add end index next range get end index break else break add start end end start range rec range new range start end start ranges add rec range
1283	mapreduce\src\java\org\apache\hadoop\mapred\SpillRecord.java	unrelated	package org apache hadoop mapred spill record backing store byte buffer buf view backing storage longs long buffer entries spill record num partitions buf byte buffer allocate num partitions map task map output index record length entries buf long buffer spill record path index file name job conf job throws io exception index file name job null spill record path index file name job conf job string expected index owner throws io exception index file name job new pure java crc expected index owner spill record path index file name job conf job checksum crc string expected index owner throws io exception file system rfs file system get local job get raw data input stream new data input stream secure io utils open for read new file index file name uri get path expected index owner null try length rfs get file status index file name get len partitions length map output index record length size partitions map output index record length buf byte buffer allocate size crc null crc reset checked input stream chk new checked input stream crc io utils read fully chk buf array size chk get checksum get value read long throw new checksum exception checksum error reading spill index index file name else io utils read fully buf array size entries buf long buffer finally close return number index record entries spill size return entries capacity map task map output index record length get spill offsets given partition index record get index partition pos partition map task map output index record length return new index record entries get pos entries get pos entries get pos set spill offsets given partition put index index record rec partition pos partition map task map output index record length entries put pos rec start offset entries put pos rec raw length entries put pos rec part length write spill record location provided write to file path loc job conf job throws io exception write to file loc job new pure java crc write to file path loc job conf job checksum crc throws io exception file system rfs file system get local job get raw checked output stream chk null fs data output stream rfs create loc try crc null crc reset chk new checked output stream crc chk write buf array write long chk get checksum get value else write buf array finally chk null chk close else close index record start offset raw length part length index record index record start offset raw length part length start offset start offset raw length raw length part length part length
1284	mapreduce\src\java\org\apache\hadoop\mapred\StatisticsCollector.java	unrelated	package org apache hadoop mapred collects statistics time windows statistics collector default period time window since start new time window since start time window last week new time window last week time window last day new time window last day time window last hour new time window last hour time window last minute new time window last minute time window default collect windows statistics collector since start statistics collector last day statistics collector last hour period boolean started map time window stat updater updaters new linked hash map time window stat updater map string stat statistics new hash map string stat statistics collector default period statistics collector period period period synchronized start started return timer timer new timer timer thread monitoring true timer task task new timer task run update millis period timer schedule at fixed rate task millis millis started true protected synchronized update stat updater c updaters values c update map time window stat updater get updaters return collections unmodifiable map updaters map string stat get statistics return collections unmodifiable map statistics synchronized stat create stat string name return create stat name default collect windows synchronized stat create stat string name time window windows statistics get name null throw new runtime exception stat name name already defined map time window time stat time stats new linked hash map time window time stat time window window windows stat updater collector updaters get window collector null since start equals window collector new stat updater else collector new time window stat updater window period updaters put window collector time stat time stat new time stat collector add time stat name time stat time stats put window time stat stat stat new stat name time stats statistics put name stat return stat synchronized stat remove stat string name stat stat statistics remove name stat null stat updater collector updaters values collector remove time stat name return stat time window string name window size update granularity time window string name window size update granularity update granularity window size throw new runtime exception invalid time window update granularity window size name name window size window size update granularity update granularity hash code return name hash code update granularity window size boolean equals object obj obj return true obj null return false get class obj get class return false time window time window obj name null name null return false else name equals name return false update granularity update granularity return false window size window size return false return true stat string name map time window time stat time stats stat string name map time window time stat time stats name name time stats time stats synchronized inc incr time stat ts time stats values ts inc incr synchronized inc inc synchronized map time window time stat get values return collections unmodifiable map time stats time stat linked list integer buckets new linked list integer value current value synchronized get value return value synchronized inc current value synchronized add bucket buckets add last current value set value to current synchronized set value to current value current value
1285	mapreduce\src\java\org\apache\hadoop\mapred\Task.java	scheduler	package org apache hadoop mapred base tasks task implements writable configurable log log log factory get log task string merged output prefix merged counters measure usage different file systems always return string array two elements first one name bytes read counter second one bytes written counter protected string get file system counter names string uri scheme string scheme uri scheme upper case return new string scheme bytes read scheme bytes written name file system counters group protected string filesystem counter group file system counters helper methods construct task output paths construct output file names output directory listing sorted lexicographically positions correspond output partitions number format number format number format get instance number format set minimum integer digits number format set grouping used false synchronized string get output name partition return part number format format partition fields string job file job configuration file string user user running job task attempt id task id unique includes job id partition id within job task status task status current status task protected job status state job run state for cleanup protected boolean job cleanup false protected boolean job setup false protected boolean task cleanup false an opaque data field used attach extra data task this used hadoop scheduler mesos associate mesos task id task recover i ds task tracker protected bytes writable extra data new bytes writable skip ranges based failed ranges previous attempts sorted ranges skip ranges new sorted ranges boolean skipping false boolean write skip recs true currently processing record start index volatile current rec start index iterator long current rec index iterator skip ranges skip range iterator resource calculator plugin resource calculator null init cpu cumulative time protected job conf conf protected map output file map output file new map output file protected local dir allocator dir alloc max retries protected job context job context protected task attempt context task context protected org apache hadoop mapreduce output format output format protected org apache hadoop mapreduce output committer committer protected counters counter spilled records counter protected counters counter failed shuffle counter protected counters counter merged map outputs counter num slots required protected task umbilical protocol umbilical protected secret key token secret protected gc time updater gc updater constructors task task status task status create task status map task task id new task attempt id spilled records counter counters find counter task counter spilled records failed shuffle counter counters find counter task counter failed shuffle merged map outputs counter counters find counter task counter merged map outputs gc updater new gc time updater task string job file task attempt id task id partition num slots required job file job file task id task id partition partition num slots required num slots required task status task status create task status map task task id f num slots required task status state unassigned map task task status phase map task status phase shuffle counters spilled records counter counters find counter task counter spilled records failed shuffle counter counters find counter task counter failed shuffle merged map outputs counter counters find counter task counter merged map outputs gc
1286	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptContext.java	unrelated	package org apache hadoop mapred instead task attempt context extends org apache hadoop mapreduce task attempt context task attempt id get task attempt id progressable get progressible job conf get job conf
1287	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptContextImpl.java	unrelated	package org apache hadoop mapred instead task attempt context impl extends org apache hadoop mapreduce task task attempt context impl implements task attempt context reporter reporter task attempt context impl job conf conf task attempt id taskid conf taskid reporter null task attempt context impl job conf conf task attempt id taskid reporter reporter super conf taskid reporter reporter get task attempt id task attempt id get task attempt id return task attempt id super get task attempt id progressable get progressible return reporter job conf get job conf return job conf get configuration get progress return reporter get progress counter get counter enum counter name return reporter get counter counter name counter get counter string group name string counter name return reporter get counter group name counter name report progress progress reporter progress set current status task given set status string status set status string status reporter set status status
1288	mapreduce\src\java\org\apache\hadoop\mapred\TaskAttemptID.java	unrelated	package org apache hadoop mapred task attempt id represents immutable unique identifier task attempt each task attempt one particular instance map reduce task identified task id task attempt id consists parts first part link task id task attempt id belongs second part task attempt number br an example task attempt id code attempt code represents zeroth task attempt fifth map task third job running jobtracker started code code p applications never construct parse task attempt id strings rather use appropriate constructors link name string method task attempt id extends org apache hadoop mapreduce task attempt id constructs task attempt id object given link task id task attempt id task id task id id super task id id constructs task id object given parts task attempt id string jt identifier job id boolean map task id id jt identifier job id map task type map task type reduce task id id constructs task id object given parts task attempt id string jt identifier job id task type type task id id new task id jt identifier job id type task id id task attempt id super new task id downgrade new task attempt id old one task attempt id downgrade org apache hadoop mapreduce task attempt id old old instanceof task attempt id return task attempt id old else return new task attempt id task id downgrade old get task id old get id task id get task id return task id super get task id job id get job id return job id super get job id task attempt id read data input throws io exception task attempt id task id new task attempt id task id read fields return task id construct task attempt id object given task attempt id name string str throws illegal argument exception return task attempt id org apache hadoop mapreduce task attempt id name str returns regex pattern matches task attempt i ds arguments given null case part regex generic for example obtain regex matching task attempt i ds jobtracker job first map task would use pre task attempt id get task attempt i ds pattern null null true null pre return pre attempt pre string get task attempt i ds pattern string jt identifier integer job id boolean map integer task id integer attempt id return get task attempt i ds pattern jt identifier job id map task type map task type reduce task id attempt id returns regex pattern matches task attempt i ds arguments given null case part regex generic for example obtain regex matching task attempt i ds jobtracker job first map task would use pre task attempt id get task attempt i ds pattern null null task type map null pre return pre attempt pre string get task attempt i ds pattern string jt identifier integer job id task type type integer task id integer attempt id string builder builder new string builder attempt append separator builder append get task attempt i ds pattern wo prefix jt identifier job id type task id attempt id return builder string string builder get
1289	mapreduce\src\java\org\apache\hadoop\mapred\TaskCompletionEvent.java	unrelated	package org apache hadoop mapred this used track task completion events job tracker link org apache hadoop mapreduce task completion event instead task completion event extends org apache hadoop mapreduce task completion event enum status failed killed succeeded obsolete tipfailed task completion event empty array new task completion event default constructor writable task completion event super constructor event id created externally incremented per event job incrementally starting task completion event event id task attempt id task id id within job boolean map status status string task tracker http super event id task id id within job map org apache hadoop mapreduce task completion event status value of status name task tracker http task completion event downgrade org apache hadoop mapreduce task completion event event return new task completion event event get event id task attempt id downgrade event get task attempt id event id within job event map task status value of event get status name event get task tracker http returns task id string get task id return get task attempt id string returns task id task attempt id get task attempt id return task attempt id downgrade super get task attempt id returns enum status sucess status failure status get task status return status value of super get status name sets task id set task id string task id set task attempt id task attempt id name task id sets task id protected set task attempt id task attempt id task id super set task attempt id task id set task status protected set task status status status super set task status org apache hadoop mapreduce task completion event status value of status name set task completion time protected set task run time task completion time super set task run time task completion time set event id assigned incrementally starting protected set event id event id super set event id event id set task tracker http location protected set task tracker http string task tracker http super set task tracker http task tracker http
1290	mapreduce\src\java\org\apache\hadoop\mapred\TaskController.java	unrelated	package org apache hadoop mapred controls initialization finalization clean tasks also launching killing task jv ms this defines api initializing finalizing cleaning tasks also launching killing task jv ms subclasses implement logic required performing actual actions br task controller implements configurable configuration conf log log log factory get log task controller configuration get conf return conf the list directory paths specified variable configs local dir this used determine among list directories picked storing data particular task protected string mapred local dirs set conf configuration conf conf conf mapred local dirs conf get trimmed strings mr config local dir sets permissions following directories configured disks ul li mapreduce cluster local directories li li hadoop log directories li ul setup throws io exception file system local fs file system get local conf string local dir mapred local dirs set mapreduce cluster local directories file mapredlocal dir new file local dir mapredlocal dir directory mapredlocal dir mkdirs log warn unable create mapreduce cluster local directory mapredlocal dir get path else local fs set permission new path mapredlocal dir get canonical path new fs permission short set user log directory file task log task log get user log dir task log directory task log mkdirs log warn unable create task log directory task log get path else local fs set permission new path task log get canonical path new fs permission short disk checker check dir task log get user log dir take task controller specific actions initialize job this involves setting appropriate permissions job files secure files accessible user tasks initialize job job initialization context context throws io exception take task controller specific actions initialize distributed cache file this involves setting appropriate permissions files secure accessible owners initialize distributed cache file distributed cache file context context throws io exception launch task jvm this method defines jvm launched run task each task controller also link initialize task task controller context inside method initialize task launching this reasons task controller specific optimizations w r combining initialization launching tasks launch task jvm task controller context context throws io exception top level cleanup task jvm method ol li sends graceful termiante signal task jvm allow subprocesses cleanup li li sends forceful kill signal task jvm terminating sub processes forcefully li ol destroy task jvm task controller context context send sigterm try ask polite exit terminate task context try thread sleep context sleeptime before sigkill catch interrupted exception e log warn sleep interrupted string utils stringify exception e kill task context perform initializing actions required task run for instance method used setup appropriate access permissions files directories used tasks tasks use job cache log distributed cache directories files part functioning typically files shared daemon tasks so task controller launching tasks different users implement method setup appropriate ownership permissions directories files initialize task task controller context context throws io exception task exec context task executed task task contains task information required task controller task controller context extends task exec context shell command executor sh exec shell executor executing jvm task information used context used launching new tasks jvm env env jvm
1291	mapreduce\src\java\org\apache\hadoop\mapred\TaskGraphServlet.java	unrelated	package org apache hadoop mapred the servlet outputs svg graphics map reduce task statuses task graph servlet extends http servlet serial version uid l height graph w margins width height graph w margins height margin space axis ymargin margin space x axis xmargin one third f f get http servlet request request http servlet response response throws servlet exception io exception response set content type image svg xml job tracker tracker job tracker get servlet context get attribute job tracker string job id str request get parameter jobid job id str null return job id job id job id name job id str verify user view access job job with view access check job jsp util check access and get job tracker job id request response job view job allowed return user authorized view job boolean map map equals ignore case request get parameter type task report reports map tracker get map task reports job id tracker get reduce task reports job id reports null reports length return num tasks reports length tasks per bar math ceil num tasks num bars math ceil num tasks tasks per bar w math max num bars bar width math min w num bars min px max px bars per notch math ceil bar width w w num bars bars per notch total width w xmargin draw white rectangle print writer response get writer print xml version standalone n doctype svg public w c dtd svg en n http www w org graphics svg dtd svg dtd n xml stylesheet type text css href hadoop css n n svg width print total width print height print height ymargin print version n xmlns http www w org svg n n axes print line xmargin xmargin height ymargin ymargin black print line xmargin w xmargin height ymargin height ymargin black borderlines print line w xmargin w xmargin height ymargin ymargin cccccc print line xmargin w xmargin ymargin ymargin cccccc string colors new string dd e aaaaff determine notch interval using number digits num tasks x notch interval math ceil num tasks x offset x notch count task bar graph bar cnt tasks per bar bar cnt bar cnt bars per notch x offset x bar cnt bar width xmargin x offset x axis notches x notch interval x notch count print line x x height ymargin height ymargin black print text x height ymargin string value of x notch interval x notch count middle reports length break map progress get map avarage progress tasks per bar reports bar height math ceil height progress height bar height ymargin print rect bar width bar height x colors else progresses get reduce avarage progresses tasks per bar reports draw three bars stacked copy sort reduce prev height j j j bar height height progresses j bar height height fix rounding error bar height height height bar height ymargin prev height prev height bar height print rect bar width bar height x colors j axis notches print line xmargin xmargin ymargin height ymargin height black print text xmargin ymargin height string value
1292	mapreduce\src\java\org\apache\hadoop\mapred\TaskID.java	unrelated	package org apache hadoop mapred task id represents immutable unique identifier map reduce task each task id encompasses multiple attempts made execute map reduce task uniquely indentified task attempt id task id consists parts first part link job id task in progress belongs second part task id either r representing whether task map task reduce task and third part task number br an example task id code task code represents fifth map task third job running jobtracker started code code p applications never construct parse task id strings rather use appropriate constructors link name string method task id extends org apache hadoop mapreduce task id constructs task id object given link job id task id org apache hadoop mapreduce job id job id boolean map id job id map task type map task type reduce id constructs task in progress id object given parts task id string jt identifier job id boolean map id jt identifier job id map task type map task type reduce id constructs task id object given link job id task id org apache hadoop mapreduce job id job id task type type id super job id type id constructs task in progress id object given parts task id string jt identifier job id task type type id new job id jt identifier job id type id task id super new job id task type reduce downgrade new task id old one task id downgrade org apache hadoop mapreduce task id old old instanceof task id return task id old else return new task id job id downgrade old get job id old get task type old get id task id read data input throws io exception task id tip id new task id tip id read fields return tip id job id get job id return job id super get job id returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching first map task jobtracker job would use pre task id get task i ds pattern null null true pre return pre task pre integer string get task i ds pattern string jt identifier integer job id boolean map integer task id return get task i ds pattern jt identifier job id map task type map task type reduce task id returns regex pattern matches task i ds arguments given null case part regex generic for example obtain regex matching first map task jobtracker job would use pre task id get task i ds pattern null null true pre return pre task pre string get task i ds pattern string jt identifier integer job id task type type integer task id string builder builder new string builder task append separator append get task i ds pattern wo prefix jt identifier job id type task id return builder string string builder get task i ds pattern wo prefix string jt identifier integer job id task type type integer task id string builder builder new string builder builder append job id get job i ds pattern wo prefix jt
1293	mapreduce\src\java\org\apache\hadoop\mapred\TaskInProgress.java	scheduler	package org apache hadoop mapred task in progress maintains info needed task lifetime owning job a given task might speculatively executed reexecuted need level indirection running id br a given task in progress contains multiple taskids might executing one time that allows speculative execution a taskid never recycled a tip allocates enough taskids account speculation failures ever handle once tip dead task in progress max task execs max nonspec tasks run concurrently max task attempts speculative lag num attempts per restart log log log factory get log task in progress defines tip string job file null task split meta info split info num maps partition job tracker jobtracker job history job history task id id job in progress job num slots required status tip success event number num task failures num killed tasks progress old progress rate string state last dispatch time recent time task attempt given tt exec start time started first task attempt exec finish time completes boolean failed false boolean killed false max skip records failed ranges failed ranges new failed ranges volatile boolean skipping false boolean job cleanup false boolean job setup false the next usable taskid tip next task id the taskid took tip success task attempt id successful task id the first taskid tip task attempt id first task id map task id task tracker id contains tasks currently runnings tree map task attempt id string active tasks new tree map task attempt id string all attempt ids tip tree set task attempt id tasks new tree set task attempt id job conf conf map task attempt id list string task diagnostic data new tree map task attempt id list string map task id task status tree map task attempt id task status task statuses new tree map task attempt id task status map task id task tracker id contains cleanup attempts ran tree map task attempt id string cleanup tasks new tree map task attempt id string tree set string machines where failed new tree set string tree set task attempt id tasks reported closed new tree set task attempt id list tasks kill taskid fail tree map task attempt id boolean tasks to kill new tree map task attempt id boolean task commit taskattemptid task attempt id task to commit volatile counters counters new counters hash map task attempt id long dispatch time map new hash map task attempt id long string user constructor map task task in progress job id jobid string job file task split meta info split job tracker jobtracker job conf conf job in progress job partition num slots required job file job file split info split jobtracker jobtracker job job conf conf partition partition max skip records skip bad records get mapper max skip records conf num slots required num slots required set max task attempts init jobid jobtracker null job history jobtracker get job history user job get user constructor reduce task task in progress job id jobid string job file num maps partition job tracker jobtracker job conf conf job in progress job num slots required job
1294	mapreduce\src\java\org\apache\hadoop\mapred\TaskLog.java	unrelated	package org apache hadoop mapred a simple logger handle task specific user logs this uses system property code hadoop log dir code task log log log log factory get log task log string userlogs dir name userlogs file log dir new file get base log dir userlogs dir name get absolute file local fs set used write to index file local file system local fs null log dir exists boolean b log dir mkdirs b log warn mkdirs failed ignoring file get task log file task attempt id taskid boolean cleanup log name filter return new file get attempt dir taskid cleanup filter string file get real task log file location task attempt id taskid boolean cleanup log name filter log file detail try get log file detail taskid filter cleanup catch io exception ie log error get task log file detail threw exception ie return null return new file location filter string log file detail string location log dir string location start length log file detail get log file detail task attempt id taskid log name filter boolean cleanup throws io exception file index file get index file taskid cleanup buffered reader fis new buffered reader new input stream reader secure io utils open for read index file obtain log dir owner taskid null format index file log dir dir task logs really stored stdout start offset stdout file length stderr start offset stderr file length syslog start offset syslog file length log file detail new log file detail string str fis read line str null file anything throw new io exception index file log taskid exist location str substring str index of log file detail location log file detail location length special cases debugout profile files they guaranteed associated task attempt since jvm reuse disabled profiling debugging enabled filter equals log name debugout filter equals log name profile length new file location filter string length start fis close return str fis read line str null look exact line containing logname str contains filter string str str substring filter string length string start and len str split start long parse long start and len length long parse long start and len break str fis read line fis close return file get tmp index file task attempt id taskid boolean cleanup return new file get attempt dir taskid cleanup log tmp file get index file task attempt id taskid boolean cleanup return new file get attempt dir taskid cleanup log index obtain owner log dir this determined checking job log directory string obtain log dir owner task attempt id taskid throws io exception configuration conf new configuration file system raw file system get local conf get raw path job log dir new path get job dir taskid get job id get absolute path file status job stat raw get file status job log dir return job stat get owner string get base log dir return system get property hadoop log dir file get attempt dir task attempt id taskid boolean cleanup string cleanup suffix cleanup cleanup return new file get job dir
1295	mapreduce\src\java\org\apache\hadoop\mapred\TaskLogAppender.java	unrelated	package org apache hadoop mapred a simple log j appender task child map reduce system logs task log appender extends file appender string task id task id managed string rather task id object log j configure configuration log j properties integer max events queue logging event tail null boolean cleanup system properties passed jvm runner string iscleanup property hadoop tasklog iscleanup string logsize property hadoop tasklog total log file size string taskid property hadoop tasklog taskid activate options synchronized set options from system properties max events tail new linked list logging event set file task log get task log file task attempt id name task id cleanup task log log name syslog string set append true super activate options the task runner passes options system properties set options setters already called synchronized set options from system properties cleanup null string prop value system get property iscleanup property false cleanup boolean value of prop value task id null task id system get property taskid property max events null string prop value system get property logsize property set total log file size long value of prop value append logging event event synchronized tail null super append event else tail size max events tail remove tail add event flush qw flush synchronized close tail null logging event event tail super append event super close getter setter methods log j synchronized string get task id return task id synchronized set task id string task id task id task id event size synchronized get total log file size return max events event size synchronized set total log file size log size max events log size event size set whether task cleanup attempt true task cleanup attempt false otherwise synchronized set is cleanup boolean cleanup cleanup cleanup get whether task cleanup attempt synchronized boolean get is cleanup return cleanup
1296	mapreduce\src\java\org\apache\hadoop\mapred\TaskLogServlet.java	unrelated	package org apache hadoop mapred a servlet run task trackers provide task logs via http task log servlet extends http servlet serial version uid l log log log factory get log task log boolean task log task attempt id task id boolean cleanup task log log name type file f task log get task log file task id cleanup type return f read construct task log url string get task log url string task tracker host name string http port string task attempt id return http task tracker host name http port tasklog attemptid task attempt id print task log http servlet response response output stream task attempt id task id start end boolean plain text task log log name filter boolean cleanup throws io exception plain text write br b u filter logs u b br n pre n get bytes try input stream task log reader new task log reader task id filter start end cleanup byte b new byte result true result task log reader read b result plain text write b result else html quoting quote html chars b result else break task log reader close plain text write pre td tr table hr br n get bytes catch io exception ioe filter task log log name debugout plain text write pre hr br n get bytes nothing else string msg failed retrieve filter log task task id log warn msg ioe response send error http servlet response sc gone msg validates given user job view permissions job conf contains job owner job view ac ls we allow job owner super user e mr owner cluster administrators users groups specified configuration using mapreduce job acl view job view job check access for task logs job conf conf string user string job id task tracker tracker throws access control exception tracker ac ls enabled return buiild job view acl reading conf access control list job view acl tracker get job ac ls manager construct job ac ls conf get job acl view job read job queue name conf string queue conf get queue name build queue admins acl reading conf access control list queue admins acl new access control list conf get full property name queue queue acl administer jobs get acl name string job owner conf get job context user name user group information caller ugi user group information create remote user user check user queue admin cluster admin job owner member job view acl queue admins acl user allowed caller ugi tracker get ac ls manager check access job id caller ugi queue operation view task logs job owner job view acl builds job conf object reading job acls xml file this load default resources returns null job acls xml userlogs jobid local file system this happen restart cluster job level authorization enabled disabled earlier cluster viewing task logs old jobs e jobs finished earlier unsecure cluster job conf get conf from job ac ls file job id job id path job acls file path new path task log get job dir job id string task tracker
1297	mapreduce\src\java\org\apache\hadoop\mapred\TaskMemoryManagerThread.java	unrelated	package org apache hadoop mapred manages memory usage tasks running tt kills task trees overflow step memory limits task memory manager thread extends thread log log log factory get log task memory manager thread task tracker task tracker monitoring interval max memory allowed for all tasks max rss memory allowed for all tasks map task attempt id process tree info process tree info map map task attempt id process tree info tasks to be added list task attempt id tasks to be removed string memory usage string memory usage process tree task id virutal bytes limit bytes physical bytes limit bytes task memory manager thread task tracker task tracker task tracker get total memory allotted for tasks on tt l task tracker get job conf get long tt config tt memory manager monitoring interval l task tracker task tracker reserved rss memory task tracker get reserved physical memory on tt total physical memory on tt task tracker get total physical memory on tt reserved rss memory job conf disabled memory limit total physical memory on tt job conf disabled memory limit max rss memory allowed for all tasks job conf disabled memory limit else max rss memory allowed for all tasks total physical memory on tt reserved rss memory mainly test purposes note tasktracker variable set task memory manager thread max memory allowed for all tasks monitoring interval set name get class get name process tree info map new hash map task attempt id process tree info tasks to be added new hash map task attempt id process tree info tasks to be removed new array list task attempt id max memory allowed for all tasks max memory allowed for all tasks job conf disabled memory limit max memory allowed for all tasks monitoring interval monitoring interval add task task attempt id tid mem limit mem limit physical synchronized tasks to be added log debug tracking process tree tid first time process tree info pt info new process tree info tid null null mem limit mem limit physical tasks to be added put tid pt info remove task task attempt id tid synchronized tasks to be removed tasks to be removed add tid process tree info task attempt id tid string pid procfs based process tree p tree mem limit mem limit physical process tree info task attempt id tid string pid procfs based process tree p tree mem limit mem limit physical tid tid pid pid p tree p tree mem limit mem limit mem limit physical mem limit physical task attempt id get tid return tid string get pid return pid set pid string pid pid pid procfs based process tree get process tree return p tree set process tree procfs based process tree p tree p tree p tree get mem limit return mem limit get mem limit physical return mem limit physical run log info starting thread get class true print process trees debugging log debug enabled string buffer tmp new string buffer process tree info p process tree info map values tmp append p get pid tmp
1298	mapreduce\src\java\org\apache\hadoop\mapred\TaskReport.java	unrelated	package org apache hadoop mapred a report state task task report extends org apache hadoop mapreduce task report task report super creates new task report object task report task id taskid progress string state string diagnostics start time finish time counters counters taskid progress state diagnostics null start time finish time counters creates new task report object task report task id taskid progress string state string diagnostics tip status current status start time finish time counters counters super taskid progress state diagnostics current status start time finish time new org apache hadoop mapreduce counters counters task report downgrade org apache hadoop mapreduce task report report return new task report task id downgrade report get task id report get progress report get state report get diagnostics report get current status report get start time report get finish time counters downgrade report get task counters task report downgrade array org apache hadoop mapreduce task report reports list task report ret new array list task report org apache hadoop mapreduce task report report reports ret add downgrade report return ret array new task report the id task task id get task id return task id downgrade super get task id counters get counters return counters downgrade super get task counters set successful attempt id task set successful attempt task attempt id super set successful attempt id get attempt id took task completion task attempt id get successful task attempt return task attempt id downgrade super get successful task attempt id set running attempt task set running task attempts collection task attempt id running attempts collection org apache hadoop mapreduce task attempt id attempts new array list org apache hadoop mapreduce task attempt id task attempt id id running attempts attempts add id super set running task attempt ids attempts get running task attempt i ds task collection task attempt id get running task attempts collection task attempt id attempts new array list task attempt id org apache hadoop mapreduce task attempt id id super get running task attempt ids attempts add task attempt id downgrade id return attempts set finish time task protected set finish time finish time super set finish time finish time set start time task protected set start time start time super set start time start time
1299	mapreduce\src\java\org\apache\hadoop\mapred\TaskRunner.java	unrelated	package org apache hadoop mapred base runs task separate process tasks run separate process order isolate map reduce system code bugs user supplied map reduce functions task runner extends thread log log log factory get log task runner volatile boolean killed false task tracker task in progress tip task object lock new object volatile boolean done false exit code boolean exit code set false string system path separator system get property path separator task tracker tracker task distributed cache manager task distributed cache manager protected job conf conf jvm manager jvm manager task runner task tracker task in progress tip task tracker tracker job conf conf tip tip tip get task tracker tracker conf conf jvm manager tracker get jvm manager instance task get task return task tracker task in progress get task in progress return tip task tracker get tracker return tracker jvm manager get jvm manager return jvm manager called task output longer needed this method run parent process child exits it execute user code system code close throws io exception get java command line options child map reduce tasks via link job conf mapred map task java opts link job conf mapred reduce task java opts string get child java opts job conf job conf string default value return job conf get job conf mapred task java opts default value get maximum virtual memory child map reduce tasks none specified link job conf mapred map task ulimit link job conf mapred reduce task ulimit get child ulimit job conf job conf return job conf get int job conf mapred task ulimit get environment variables child map reduce tasks code null code unspecified set via link job conf mapred map task env link job conf mapred reduce task env string get child env job conf job conf return job conf get job conf mapred task env get log link level child map reduce tasks level get log level job conf job conf run string error info child error try preparing job localize archives task attempt id taskid get task id local dir allocator dir alloc new local dir allocator mr config local dir file work dir form work dir dir alloc taskid task cleanup task conf we create symlinks yet presence absence work dir actually file system matter tip get ugi as new privileged exception action void void run throws io exception task distributed cache manager tracker get tracker distributed cache manager new task distributed cache manager conf task distributed cache manager setup dir alloc work dir task tracker get private distributed cache dir conf get user task tracker get public distributed cache dir return null set child task configuration after call localization files happen task tracker process space any changes conf object not reflected child setup child task configuration dir alloc build classpath list string paths get class paths conf work dir task distributed cache manager log size task log get task log length conf build exec child jvm args vector string vargs get vm args taskid work dir paths log size tracker add to memory manager get task
1300	mapreduce\src\java\org\apache\hadoop\mapred\TaskScheduler.java	scheduler	package org apache hadoop mapred used link job tracker schedule link task link task tracker p link task scheduler typically use one link job in progress listener receive notifications jobs p it responsibility link task scheduler initialize tasks job calling link job in progress init tasks job added link job in progress listener job added job in progress called tasks job assigned link assign tasks task tracker task scheduler implements configurable protected configuration conf protected task tracker manager task tracker manager configuration get conf return conf set conf configuration conf conf conf synchronized set task tracker manager task tracker manager task tracker manager task tracker manager task tracker manager lifecycle method allow scheduler start work separate threads start throws io exception nothing lifecycle method allow scheduler stop work terminate throws io exception nothing returns tasks like task tracker execute right list task assign tasks task tracker task tracker throws io exception returns collection jobs order specific particular scheduler collection job in progress get jobs string queue name abstract queue refresher scheduler extend return instance link get queue refresher method the link refresh queues list method instance invoked link queue manager whenever gets request administrator refresh queue configuration this method documented contract link queue manager link task scheduler before calling queue refresher caller must hold lock corresponding link task scheduler generally link job tracker queue refresher refresh queue configuration scheduler this method following contract ol li before method link queue manager validation new queue configuration for e g currently addition new queues removal queues level hierarchy supported link queue manager supported schedulers li li schedulers passed list link job queue info root queues e queues top level all descendants properly linked top level queues li li schedulers use scheduler specific queue properties new root queues validate properties apply internally li li once method returns successfully schedulers assumed refresh queue properties successful throughout committed internally link queue manager it guaranteed point successful return scheduler queue refresh queue manager failed if ever abnormalities happen queue framework inconsistent need jt restart li li if scheduler throws exception link refresh queues link queue manager throws away newly read configuration retains old consistent configuration informs request issuer error appropriately li ol refresh queues list job queue info new root queues throws throwable get link queue refresher scheduler by default link queue refresher exists scheduler set null schedulers need return instance link queue refresher wish refresh queue configuration link queue manager refreshes queue configuration via administrator request queue refresher get queue refresher return null
1301	mapreduce\src\java\org\apache\hadoop\mapred\TaskStatus.java	heartbeat	package org apache hadoop mapred describes current status task this intended comprehensive piece data task status implements writable cloneable log log log factory get log task status get name enumeration reporting current phase task enum phase starting map shuffle sort reduce cleanup state task enum state running succeeded failed unassigned killed commit pending failed unclean killed unclean task attempt id taskid progress volatile state run state string diagnostic info string state string string task tracker num slots start time ms finish time output size l volatile phase phase phase starting counters counters boolean counters sorted ranges range next record range new sorted ranges range max task status size max string size testcases link get max string size control max size strings link task status note link task status never exposed clients users e map reduce hence users cannot api pass large strings link task status protected get max string size return max string size task status taskid new task attempt id num slots task status task attempt id taskid progress num slots state run state string diagnostic info string state string string task tracker phase phase counters counters taskid taskid progress progress num slots num slots run state run state set diagnostic info diagnostic info set state string state string task tracker task tracker phase phase counters counters counters true task attempt id get task id return taskid boolean get is map get num slots return num slots get progress return progress set progress progress progress progress state get run state return run state string get task tracker return task tracker set task tracker string tracker task tracker tracker set run state state run state run state run state string get diagnostic info return diagnostic info set diagnostic info string info diag info already reached max log return diagnostic info null diagnostic info length get max string size log info task diagnostic info task taskid info return diagnostic info diagnostic info null info diagnostic info concat info trim max string size needed diagnostic info null diagnostic info length get max string size log info task diagnostic info task taskid diagnostic info diagnostic info diagnostic info substring get max string size string get state string return state string set state link task status set state string string state string state string null state string length get max string size state string state string else log log info state task taskid state string trim state state string state string substring get max string size get next record range going processed task sorted ranges range get next record range return next record range set next record range going processed task set next record range sorted ranges range next record range next record range next record range get task finish time shuffle finish time sort finish time set set finish time it takes care case shuffle sort finish completed heartbeat interval reported separately task state task status failed finish time represents task failed get finish time return finish time sets finish time task status start time set passed finish time greater zero set finish time finish
1302	mapreduce\src\java\org\apache\hadoop\mapred\TaskTracker.java	authenticate	package org apache hadoop mapred task tracker process starts tracks mr tasks networked environment it contacts job tracker task assignments reporting results task tracker implements mr constants task umbilical protocol runnable tt config string mapred tasktracker vmem reserved property mapred tasktracker vmem reserved string mapred tasktracker pmem reserved property mapred tasktracker pmem reserved wait for done http port enum state normal stale interrupted denied config util load resources log log log factory get log task tracker string mr clienttrace format src src ip dest dst ip maps number maps op operation reduce id reduce id duration duration log client trace log log factory get log task tracker get name clienttrace job ac ls file created task tracker userlogs jobid directory job job localization time this used task log servlet authorizing viewing task logs job string job ac ls file job acls xml volatile boolean running true local dir allocator local dir allocator string task tracker name string local hostname inet socket address job track addr inet socket address task report address server task report server null inter tracker protocol job client tracker distributed cache manager distributed cache manager last heartbeat response received short heartbeat response id string task cleanup suffix cleanup this last status report sent tracker job tracker if rpc call succeeds status cleared tracker indicating fresh status report generated event rpc calls fails whatever reason previous status report sent task tracker status status null the system directory hdfs job files stored path system directory null the filesystem job files stored file system system fs null http server server volatile boolean shutting down false map task attempt id task in progress tasks new hash map task attempt id task in progress map task id task in progress map task attempt id task in progress running tasks null map job id running job running jobs new tree map job id running job job token secret manager job token secret manager new job token secret manager volatile map total volatile reduce total boolean started true boolean inited true mark reduce tasks shuffling rollback events index set task attempt id reset new hash set task attempt id dir df map string df local dirs df new hash map string df min space start must much space free start new tasks boolean accept new tasks true min space kill run limit kill one task make sure never receive new jobs old tasks cleaned machine full good serving map output nodes random r new random string subdir task tracker string distcachedir distcache string jobcache jobcache string output output string jarsdir jars string local split file split dta string local split meta file split info string jobfile job xml string job token file job token localized file string job local dir mr job config job local dir job conf f conf file system local fs localizer localizer max map slots max reduce slots failures ac ls manager acls manager performance related config knob send band heartbeat task completion volatile boolean oob heartbeat on task completion track number completed tasks send band heartbeat int writable finished count new
1303	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerAction.java	unrelated	package org apache hadoop mapred a generic directive link org apache hadoop mapred job tracker link org apache hadoop mapred task tracker take action task tracker action implements writable ennumeration various actions link job tracker directs link task tracker perform periodically enum action type launch new task launch task kill task kill task kill tasks job cleanup kill job reinitialize tasktracker reinit tracker ask task save output commit task a factory method create objects given link action type task tracker action create action action type action type task tracker action action null switch action type case launch task action new launch task action break case kill task action new kill task action break case kill job action new kill job action break case reinit tracker action new reinit tracker action break case commit task action new commit task action break return action action type action type protected task tracker action action type action type action type action type return link action type action type get action id return action type write data output throws io exception writable utils write enum action type read fields data input throws io exception action type writable utils read enum action type
1304	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerInstrumentation.java	unrelated	package org apache hadoop mapred task tracker instrumentation defines number instrumentation points associated task trackers by default instrumentation points nothing subclasses arbitrary instrumentation monitoring points task tracker instrumentation interfaces associated uniquely task tracker we want inner subclasses direct access associated task tracker task tracker instrumentation protected task tracker tt task tracker instrumentation task tracker tt invoked task attempt succeeds complete task task attempt id timedout task task attempt id task failed ping task attempt id called task attempt starts report task launch task attempt id file stdout file stderr called task finished report task end task attempt id called task changes status status update task task task status task status
1305	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerManager.java	heartbeat	package org apache hadoop mapred manages information link task tracker running cluster this exits primarily test link job tracker intended implemented users task tracker manager managed collection task tracker status task trackers get number of unique hosts cluster status get cluster status registers link job in progress listener updates link task tracker manager add job in progress listener job in progress listener listener unregisters link job in progress listener link task tracker manager remove job in progress listener job in progress listener listener return link queue manager manages queues link task tracker manager queue manager get queue manager return current heartbeat interval used link task tracker get next heartbeat interval kill job identified jobid kill job job id jobid throws io exception obtain job object identified jobid job in progress get job job id jobid mark task attempt identified taskid killed boolean kill task task attempt id taskid boolean fail throws io exception initialize job init job job in progress job fail job fail job job in progress job
1306	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerMetricsInst.java	unrelated	package org apache hadoop mapred task tracker metrics inst extends task tracker instrumentation implements updater metrics record metrics record num completed tasks timedout tasks tasks failed ping task tracker metrics inst task tracker super job conf conf tt get job conf string session id conf get session id initiate java vm metrics jvm metrics init task tracker session id create record task tracker metrics metrics context context metrics util get context mapred metrics record metrics util create record context tasktracker guaranteed never null metrics record set tag session id session id context register updater synchronized complete task task attempt id num completed tasks synchronized timedout task task attempt id timedout tasks synchronized task failed ping task attempt id tasks failed ping since object registered updater method called periodically e g every seconds updates metrics context unused synchronized metrics record set metric maps running tt map total metrics record set metric reduces running tt reduce total metrics record set metric map task slots short tt get max current map tasks metrics record set metric reduce task slots short tt get max current reduce tasks metrics record incr metric tasks completed num completed tasks metrics record incr metric tasks failed timeout timedout tasks metrics record incr metric tasks failed ping tasks failed ping num completed tasks timedout tasks tasks failed ping metrics record update
1307	mapreduce\src\java\org\apache\hadoop\mapred\TaskTrackerStatus.java	unrelated	package org apache hadoop mapred a task tracker status map reduce primitive keeps info task tracker the job tracker maintains set recent task tracker status objects unique task tracker knows task tracker status implements writable log log log factory get log task tracker status register ctor writable factories set factory task tracker status new writable factory writable new instance return new task tracker status string tracker name string host http port failures list task status task reports volatile last seen max map tasks max reduce tasks task tracker health status health status unavailable class representing collection resources tasktracker resource status implements writable total virtual memory total physical memory map slot memory size on tt reduce slot memory size on tt available space available virtual memory unavailable byte available physical memory unavailable byte num processors unavailable cumulative cpu time unavailable millisecond cpu frequency unavailable k hz cpu usage unavailable resource status total virtual memory job conf disabled memory limit total physical memory job conf disabled memory limit map slot memory size on tt job conf disabled memory limit reduce slot memory size on tt job conf disabled memory limit available space long max value set maximum amount virtual memory tasktracker set total virtual memory total mem total virtual memory total mem get maximum amount virtual memory tasktracker if link job conf disabled memory limit ignored used computation get total virtual memory return total virtual memory set maximum amount physical memory tasktracker bytes set total physical memory total ram total physical memory total ram get maximum amount physical memory tasktracker if link job conf disabled memory limit ignored used computation get total physical memory return total physical memory set memory size map slot tt this used jt accounting slots jobs use memory set map slot memory size on tt mem map slot memory size on tt mem get memory size map slot tt see link set map slot memory size on tt get map slot memory size on tt return map slot memory size on tt set memory size reduce slot tt this used jt accounting slots jobs use memory set reduce slot memory size on tt mem reduce slot memory size on tt mem get memory size reduce slot tt see link set reduce slot memory size on tt get reduce slot memory size on tt return reduce slot memory size on tt set available disk space tt set available space avail space available space avail space will return long max space measured yet get available space return available space set amount available virtual memory tasktracker if input valid number set unavailable bytes set available virtual memory available mem available virtual memory available mem available mem unavailable get amount available virtual memory tasktracker will return unavailable cannot obtained bytes get availabel virtual memory return available virtual memory set amount available physical memory tasktracker if input valid number set unavailable tasktracker bytes set available physical memory available ram available physical memory available ram available ram unavailable get amount available physical memory tasktracker will return unavailable cannot obtained get available physical memory return available physical
1308	mapreduce\src\java\org\apache\hadoop\mapred\TaskUmbilicalProtocol.java	unrelated	package org apache hadoop mapred protocol task child process uses contact parent process the parent daemon polls central master new map reduce task runs child process all communication child parent via protocol task umbilical protocol extends versioned protocol changed version since new method get map outputs changed version progress return boolean changed version since replaced task umbilical protocol progress string string org apache hadoop mapred task status phase counters status update string task status version changed counters representation hadoop version changes task status representation hadoop version changes done api via hadoop it expects whether task output needs promoted version changes job tip task id use corresponding objects rather strings version changes counter representation hadoop version changed task status format added report next record range hadoop version adds rp cs task commit part hadoop version get map completion events also indicates events stale hence return type encapsulates events whether reset events index version changed get task method signature hadoop version changed get task method signature hadoop version adds failed unclean killed unclean states hadoop version change signature get task hadoop version modified task id aware new task types version added num required slots task status mapreduce version added fatal error child communicate fatal errors tt version id l called child task process starts get task launched jvm task get task jvm context context throws io exception report child progress parent boolean status update task attempt id task id task status task status throws io exception interrupted exception report error messages back parent calls sparing since messages held job tracker report diagnostic info task attempt id taskid string trace throws io exception report record range going process next task report next record range task attempt id taskid sorted ranges range range throws io exception periodically called child check parent still alive boolean ping task attempt id taskid throws io exception report task successfully completed failure assumed task process exits without calling done task attempt id taskid throws io exception report task complete commit pending commit pending task attempt id task id task status task status throws io exception interrupted exception polling know whether task go ahead commit boolean commit task attempt id taskid throws io exception report reduce task shuffle map outputs shuffle error task attempt id task id string message throws io exception report task encounted local filesystem error fs error task attempt id task id string message throws io exception report task encounted fatal error fatal error task attempt id task id string message throws io exception called reduce task get map output locations finished maps returns update centered around map task completion events the update also piggybacks information whether events copy task tracker changed this trigger action child process fetched map task completion events update get map completion events job id job id index max locs task attempt id id throws io exception
1309	mapreduce\src\java\org\apache\hadoop\mapred\TextInputFormat.java	unrelated	package org apache hadoop mapred an link input format plain text files files broken lines either linefeed carriage return used signal end line keys position file values line text instead text input format extends file input format long writable text implements job configurable compression codec factory compression codecs null configure job conf conf compression codecs new compression codec factory conf protected boolean splitable file system fs path file compression codec codec compression codecs get codec file null codec return true return codec instanceof splittable compression codec record reader long writable text get record reader input split generic split job conf job reporter reporter throws io exception reporter set status generic split string string delimiter job get textinputformat record delimiter byte record delimiter bytes null null delimiter record delimiter bytes delimiter get bytes return new line record reader job file split generic split record delimiter bytes
1310	mapreduce\src\java\org\apache\hadoop\mapred\TextOutputFormat.java	unrelated	package org apache hadoop mapred an link output format writes plain text files link org apache hadoop mapreduce lib output text output format instead text output format k v extends file output format k v protected line record writer k v implements record writer k v string utf utf byte newline try newline n get bytes utf catch unsupported encoding exception uee throw new illegal argument exception find utf encoding protected data output stream byte key value separator line record writer data output stream string key value separator try key value separator key value separator get bytes utf catch unsupported encoding exception uee throw new illegal argument exception find utf encoding line record writer data output stream write object byte stream handling text special case write object object throws io exception instanceof text text text write get bytes get length else write string get bytes utf synchronized write k key v value throws io exception boolean null key key null key instanceof null writable boolean null value value null value instanceof null writable null key null value return null key write object key null key null value write key value separator null value write object value write newline synchronized close reporter reporter throws io exception close record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception boolean compressed get compress output job string key value separator job get mapreduce output textoutputformat separator compressed path file file output format get task output path job name file system fs file get file system job fs data output stream file out fs create file progress return new line record writer k v file out key value separator else class extends compression codec codec class get output compressor class job gzip codec create named codec compression codec codec reflection utils new instance codec class job build filename including extension path file file output format get task output path job name codec get default extension file system fs file get file system job fs data output stream file out fs create file progress return new line record writer k v new data output stream codec create output stream file out key value separator
1311	mapreduce\src\java\org\apache\hadoop\mapred\TIPStatus.java	unrelated	package org apache hadoop mapred the states link task in progress seen job tracker enum tip status pending running complete killed failed
1312	mapreduce\src\java\org\apache\hadoop\mapred\UserLogCleaner.java	unrelated	package org apache hadoop mapred this thread task tracker cleanup user logs responsibilities thread ol li removing old user logs li ol user log cleaner extends thread log log log factory get log user log cleaner default user log retain hours day default thread sleep time hour map job id long completed jobs collections synchronized map new hash map job id long thread sleep time mr async disk service log async disk clock clock user log cleaner configuration conf throws io exception thread sleep time conf get long tt config tt userlogcleanup sleeptime default thread sleep time log async disk new mr async disk service file system get local conf task log get user log dir string set clock new clock set clock clock clock clock clock clock get clock return clock run this thread wakes every thread sleep time interval deletes old logs true try sleep thread sleep thread sleep time process completed jobs catch throwable e log warn get class get simple name encountered exception monitoring e log info ingoring exception continuing monitoring process completed jobs throws io exception clock get time iterate completed jobs remove old logs synchronized completed jobs iterator entry job id long completed job iter completed jobs entry set iterator completed job iter next entry job id long entry completed job iter next see job old enough entry get value value add job logs directory delete delete log path task log get job dir entry get key get absolute path completed job iter remove clears logs userlog directory adds job directories deletion default retain hours deletes directories this usually called reinit restart task tracker clear old user logs configuration conf throws io exception file user log dir task log get user log dir user log dir exists string log dirs user log dir list log dirs length add log dirs task logs mnonitor clock get time string log dir log dirs log dir equals log async disk tobedeleted skip continue job id jobid null try jobid job id name log dir catch illegal argument exception ie directory jobid delete immediately delete log path new file user log dir log dir get absolute path continue add job log directory default retain hours already added completed jobs contains key jobid mark job logs for deletion conf jobid get userlog retain millis configuration conf return conf null user log cleaner default user log retain hours conf get int mr job config user log retain hours user log cleaner default user log retain hours adds job user log directory cleanup thread delete logs user log retain hours if configuration null user log retain hours configured deleted value user log cleaner default user log retain hours job completion time millis the configuration user log retain hours read job id user logs deleted mark job logs for deletion job completion time configuration conf job id jobid retain time stamp job completion time get userlog retain millis conf log info adding jobid user log deletion retain time stamp retain time stamp completed jobs put jobid long value of retain time stamp remove job
1313	mapreduce\src\java\org\apache\hadoop\mapred\Utils.java	unrelated	package org apache hadoop mapred a utility it provides a path filter utility filter output part files output dir utils output file utils this filters output part files given directory it accept files filenames logs success this used list paths output directory follows path file list file util stat paths fs list status dir new output files filter output files filter extends output log filter boolean accept path path return super accept path file output committer succeeded file name equals path get name this filters log files directory given it doesnt accept paths logs this used list paths output directory follows path file list file util stat paths fs list status dir new output log filter output log filter implements path filter boolean accept path path return logs equals path get name
1314	mapreduce\src\java\org\apache\hadoop\mapred\jobcontrol\Job.java	unrelated	package org apache hadoop mapred jobcontrol job extends controlled job log log log factory get log job success waiting running ready failed dependent failed construct job job job conf job conf array list depending jobs throws io exception super new org apache hadoop mapreduce job job conf list controlled job depending jobs job job conf conf throws io exception super conf mapred framework job id get assigned job id org apache hadoop mapreduce job id temp super get mapred job id temp null return null return job id downgrade temp jobid set framework set assigned job id job id mapred job id nothing synchronized job conf get job conf return new job conf super get job get configuration set mapred job conf job synchronized set job conf job conf job conf try super set job new org apache hadoop mapreduce job job conf catch io exception ioe log info exception ioe synchronized get state state state super get job state state state success return success state state waiting return waiting state state running return running state state ready return ready state state failed return failed state state dependent failed return dependent failed return job client get job client try return new job client super get job get configuration catch io exception ioe return null array list job get depending jobs return job control cast to job list super get dependent jobs
1315	mapreduce\src\java\org\apache\hadoop\mapred\jobcontrol\JobControl.java	unrelated	package org apache hadoop mapred jobcontrol link org apache hadoop mapreduce lib jobcontrol job control instead job control extends org apache hadoop mapreduce lib jobcontrol job control construct job control group jobs job control string group name super group name array list job cast to job list list controlled job cjobs array list job ret new array list job controlled job job cjobs ret add job job return ret array list job get waiting jobs return cast to job list super get waiting job list array list job get running jobs return cast to job list super get running job list array list job get ready jobs return cast to job list super get ready jobs list array list job get successful jobs return cast to job list super get successful job list array list job get failed jobs return cast to job list super get failed job list add collection jobs add jobs collection job jobs job job jobs add job job get state thread state state super get thread state state thread state running return state thread state suspended return state thread state stopped return state thread state stopping return state thread state ready return return
1316	mapreduce\src\java\org\apache\hadoop\mapred\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop mapred join this provides implementation resetable iterator the implementation uses link java util array list store elements added replaying requested prefer link stream backed iterator link org apache hadoop mapreduce lib join array list backed iterator instead array list backed iterator x extends writable extends org apache hadoop mapreduce lib join array list backed iterator x implements resetable iterator x array list backed iterator super array list backed iterator array list x data super data
1317	mapreduce\src\java\org\apache\hadoop\mapred\join\ComposableInputFormat.java	unrelated	package org apache hadoop mapred join refinement input format requiring implementors provide composable record reader instead record reader link org apache hadoop mapreduce lib join composable input format instead composable input format k extends writable comparable v extends writable extends input format k v composable record reader k v get record reader input split split job conf job reporter reporter throws io exception
1318	mapreduce\src\java\org\apache\hadoop\mapred\join\ComposableRecordReader.java	unrelated	package org apache hadoop mapred join additional operations required record reader participate join link org apache hadoop mapreduce lib join composable record reader instead composable record reader k extends writable comparable v extends writable extends record reader k v comparable composable record reader k return position collector occupies id return key record reader would supply call next k v k key clone key head record reader object provided key k key throws io exception returns true stream empty provides guarantee call next k v succeed boolean next skip key value pairs keys less equal key provided skip k key throws io exception while key value pairs record reader match given key register join collector provided accept composite record reader join collector jc k key throws io exception
1319	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeInputFormat.java	unrelated	package org apache hadoop mapred join an input format capable performing joins set data sources sorted partitioned way a user may define new join types setting property tt mapred join define lt ident gt tt classname in expression tt mapred join expr tt identifier assumed composable record reader tt mapred join keycomparator tt classname used compare keys join link org apache hadoop mapreduce lib join composite input format instead composite input format k extends writable comparable implements composable input format k tuple writable expression parse tree if requests proxied parser node root composite input format interpret given composite expression code func ident func func func tbl path see java lang class name java lang string path see org apache hadoop fs path path java lang string reads expression tt mapred join expr tt property user supplied join types tt mapred join define lt ident gt tt types paths supplied tt tbl tt given input paths input format listed set format job conf job throws io exception add defaults add user identifiers job root parser parse job get mapred join expr null job adds default set identifiers parser protected add defaults try parser c node add identifier inner inner join record reader parser c node add identifier outer outer join record reader parser c node add identifier override record reader parser w node add identifier tbl wrapped record reader catch no such method exception e throw new runtime exception fatal failed init defaults e inform parser user defined types add user identifiers job conf job throws io exception pattern x pattern compile mapred join define w map entry string string kv job matcher x matcher kv get key matches try parser c node add identifier group job get class group null composable record reader catch no such method exception e throw io exception new io exception invalid define group init cause e build composite input split child input formats assigning ith split child ith composite split input split get splits job conf job num splits throws io exception set format job job set long mapred min split size long max value return root get splits job num splits construct composite record reader children input format defined init expression the outermost join need composable necessarily composite mandating tuple writable strictly correct composable record reader k tuple writable get record reader input split split job conf job reporter reporter throws io exception set format job return root get record reader split job reporter convenience method constructing composite formats given input format inf path p return code tbl inf p string compose class extends input format inf string path return compose inf get name intern path new string buffer string convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op class extends input format inf string path string infname inf get name string buffer ret new string buffer op string p path compose infname p ret ret append ret set char at ret length return ret string
1320	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeInputSplit.java	unrelated	package org apache hadoop mapred join this input split contains set child input splits any input split inserted collection must default constructor link org apache hadoop mapreduce lib join composite input split instead composite input split implements input split fill totsize l input split splits composite input split composite input split capacity splits new input split capacity add input split collection capacity reached add input split throws io exception null splits throw new io exception uninitialized input split fill splits length throw new io exception too many splits splits fill totsize get length get ith child input split input split get return splits return aggregate length child input splits currently added get length throws io exception return totsize get length ith child input split get length throws io exception return splits get length collect set hosts child input splits string get locations throws io exception hash set string hosts new hash set string input split splits string hints get locations hints null hints length string host hints hosts add host return hosts array new string hosts size get locations ith input split string get location throws io exception return splits get locations write splits following format code count classn split split splitn write data output throws io exception writable utils write v int splits length input split splits text write string get class get name input split splits write inherit doc faliing access checks read fields data input throws io exception card writable utils read v int splits null splits length card splits new input split card class extends input split cls new class card try card cls class name text read string subclass input split card splits reflection utils new instance cls null splits read fields catch class not found exception e throw io exception new io exception failed split init init cause e
1321	mapreduce\src\java\org\apache\hadoop\mapred\join\CompositeRecordReader.java	unrelated	package org apache hadoop mapred join a record reader effect joins record readers sharing common key type partitioning link org apache hadoop mapreduce lib join composite record reader instead composite record reader k extends writable comparable key type v extends writable accepts record reader k v children x extends writable emits writables type implements configurable id configuration conf resetable iterator x empty new resetable iterator empty x writable comparator cmp class extends writable comparable keyclass priority queue composable record reader k q protected join collector jc protected composable record reader k extends v kids protected boolean combine object srcs tuple writable value create record reader tt capacity tt children position tt id tt parent reader the id root composite record reader convention relying recommended composite record reader id capacity class extends writable comparator cmpcl throws io exception assert capacity invalid capacity id id null cmpcl cmp reflection utils new instance cmpcl null q new priority queue composable record reader k new comparator composable record reader k compare composable record reader k composable record reader k return cmp compare key key jc new join collector capacity kids new composable record reader capacity return position collector occupies id return id inherit doc set conf configuration conf conf conf inherit doc configuration get conf return conf return sorted list record readers composite protected priority queue composable record reader k get record reader queue return q return comparator defining ordering record readers composite protected writable comparator get comparator return cmp add record reader collection the id record reader determines tuple entry appear adding record readers id undefined behavior add composable record reader k extends v rr throws io exception kids rr id rr null q cmp writable comparator get rr create key get class q new priority queue composable record reader k new comparator composable record reader k compare composable record reader k composable record reader k return cmp compare key key rr next q add rr collector join values this accumulates values given key child record readers if one child rr contain duplicate keys emit cross product associated values exhausted join collector k key resetable iterator x iters pos boolean first true construct collector capable handling specified number children join collector card iters new resetable iterator card iters length iters empty register given iterator position id add id resetable iterator x throws io exception iters id return key associated collection k key return key codify contents collector iterated when called record readers registered key added resetable iterators reset k key key key first true pos iters length iters length iters reset clear state information clear key null pos iters length iters clear iters empty returns false exhausted reset k called protected boolean next return pos populate tuple iterators it case given iterators n values sources n sharing key k repeated calls next yield i x i protected boolean next tuple writable val throws io exception first pos pos iters length pos iters pos next iters pos next x val get pos pos val set written pos first false pos clear return false return true pos
1322	mapreduce\src\java\org\apache\hadoop\mapred\join\InnerJoinRecordReader.java	unrelated	package org apache hadoop mapred join full inner join link org apache hadoop mapreduce lib join inner join record reader instead inner join record reader k extends writable comparable extends join record reader k inner join record reader id job conf conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl return true iff tuple full data sources contain key protected boolean combine object srcs tuple writable dst assert srcs length dst size srcs length dst return false return true
1323	mapreduce\src\java\org\apache\hadoop\mapred\join\JoinRecordReader.java	unrelated	package org apache hadoop mapred join base composite joins returning tuples arbitrary writables link org apache hadoop mapreduce lib join join record reader instead join record reader k extends writable comparable extends composite record reader k writable tuple writable implements composable record reader k tuple writable join record reader id job conf conf capacity class extends writable comparator cmpcl throws io exception super id capacity cmpcl set conf conf emit next set key value pairs defined child record readers operation associated composite rr boolean next k key tuple writable value throws io exception jc flush value writable utils clone into key jc key return true jc clear k iterkey create key priority queue composable record reader k q get record reader queue q empty fill join collector iterkey jc reset iterkey jc flush value writable utils clone into key jc key return true jc clear return false inherit doc tuple writable create value return create internal value return iterator wrapping join collector protected resetable iterator tuple writable get delegate return new join delegation iterator since join collector effecting operation need provide iterator proxy wrapping operation protected join delegation iterator implements resetable iterator tuple writable boolean next return jc next boolean next tuple writable val throws io exception return jc flush val boolean replay tuple writable val throws io exception return jc replay val reset jc reset jc key add tuple writable item throws io exception throw new unsupported operation exception close throws io exception jc close clear jc clear
1324	mapreduce\src\java\org\apache\hadoop\mapred\join\MultiFilterRecordReader.java	unrelated	package org apache hadoop mapred join base composite join returning values derived multiple sources generally tuples link org apache hadoop mapreduce lib join multi filter record reader instead multi filter record reader k extends writable comparable v extends writable extends composite record reader k v v implements composable record reader k v class extends writable valueclass tuple writable ivalue multi filter record reader id job conf conf capacity class extends writable comparator cmpcl throws io exception super id capacity cmpcl set conf conf for tuple emitted return value typically one values tuple modifying writables tuple permitted unlikely affect join behavior cases recommended it safer clone first protected v emit tuple writable dst throws io exception default implementation offers link emit every tuple collector outer join child r rs protected boolean combine object srcs tuple writable dst return true inherit doc boolean next k key v value throws io exception jc flush ivalue writable utils clone into key jc key writable utils clone into value emit ivalue return true jc clear k iterkey create key priority queue composable record reader k q get record reader queue q empty fill join collector iterkey jc reset iterkey jc flush ivalue writable utils clone into key jc key writable utils clone into value emit ivalue return true jc clear return false inherit doc v create value null valueclass class cls kids create value get class record reader k extends v rr kids cls equals rr create value get class throw new class cast exception child value fail agree valueclass cls subclass writable ivalue create internal value return v reflection utils new instance valueclass null return iterator returning single value tuple protected resetable iterator v get delegate return new multi filter delegation iterator proxy join collector callback emit protected multi filter delegation iterator implements resetable iterator v boolean next return jc next boolean next v val throws io exception boolean ret ret jc flush ivalue writable utils clone into val emit ivalue return ret boolean replay v val throws io exception writable utils clone into val emit ivalue return true reset jc reset jc key add v item throws io exception throw new unsupported operation exception close throws io exception jc close clear jc clear
1325	mapreduce\src\java\org\apache\hadoop\mapred\join\OuterJoinRecordReader.java	unrelated	package org apache hadoop mapred join full outer join link org apache hadoop mapreduce lib join outer join record reader instead outer join record reader k extends writable comparable extends join record reader k outer join record reader id job conf conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl emit everything collector protected boolean combine object srcs tuple writable dst assert srcs length dst size return true
1326	mapreduce\src\java\org\apache\hadoop\mapred\join\OverrideRecordReader.java	unrelated	package org apache hadoop mapred join prefer quot rightmost quot data source key for example tt s s s tt prefer values s s values s s keys emitted sources link org apache hadoop mapreduce lib join override record reader instead override record reader k extends writable comparable v extends writable extends multi filter record reader k v override record reader id job conf conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl emit value highest position tuple protected v emit tuple writable dst return v dst iterator next instead filling join collector iterators data sources fill rightmost key this saves space discarding sources also emits number key value pairs preferred record reader instead repeating stream n times n cardinality cross product discarded streams given key protected fill join collector k iterkey throws io exception priority queue composable record reader k q get record reader queue q empty highpos array list composable record reader k list new array list composable record reader k kids length q peek key iterkey writable comparator cmp get comparator cmp compare q peek key iterkey composable record reader k q poll highpos list get highpos id id highpos list size list add q empty break composable record reader k list remove highpos accept jc iterkey composable record reader k rr list rr skip iterkey list add composable record reader k rr list rr next q add rr
1327	mapreduce\src\java\org\apache\hadoop\mapred\join\Parser.java	unrelated	package org apache hadoop mapred join very simple shift reduce parser join expressions this sufficient user extension permitted ought replaced parser generator complex grammars supported in particular quot shift reduce quot parser states each set formals requires different internal node type responsible interpreting list tokens receives this sufficient current grammar several annoying properties might inhibit extension in particular parenthesis always function calls algebraic filter grammar would require node type must also work around internals parser for cases adding hierarchy particularly extending join record reader multi filter record reader fairly straightforward one need relevant method usually link composite record reader combine property map value identifier parser parser enum t type cif ident comma lparen rparen quot num tagged union type tokens join expression token t type type token t type type type type t type get type return type node get node throws io exception throw new io exception expected nodetype get num throws io exception throw new io exception expected numtype string get str throws io exception throw new io exception expected strtype num token extends token num num token num super t type num num num get num return num node token extends token node node node token node node super t type cif node node node get node return node str token extends token string str str token t type type string str super type str str string get str return str simple lexer wrapping stream tokenizer this encapsulates creation tagged union tokens initializes steam tokenizer lexer stream tokenizer tok lexer string tok new stream tokenizer new char array reader char array tok quote char tok parse numbers tok ordinary char tok ordinary char tok ordinary char tok word chars tok word chars token next throws io exception type tok next token switch type case stream tokenizer tt eof case stream tokenizer tt eol return null case stream tokenizer tt number return new num token tok nval case stream tokenizer tt word return new str token t type ident tok sval case return new str token t type quot tok sval default switch type case return new token t type comma case return new token t type lparen case return new token t type rparen default throw new io exception unexpected type node implements composable input format return node type registered particular identifier by default c node composite node w node quot wrapped quot nodes user nodes likely composite nodes node ident string ident throws io exception try node cstr map contains key ident throw new io exception no nodetype ident return node cstr map get ident new instance ident catch illegal access exception e throw io exception new io exception init cause e catch instantiation exception e throw io exception new io exception init cause e catch invocation target exception e throw io exception new io exception init cause e class ncstr sig string map string constructor extends node node cstr map new hash map string constructor extends node protected map string constructor extends composable record reader rr cstr map new hash map string constructor extends composable record reader
1328	mapreduce\src\java\org\apache\hadoop\mapred\join\ResetableIterator.java	unrelated	package org apache hadoop mapred join this defines stateful iterator replay elements added directly note extend link java util iterator link org apache hadoop mapreduce lib join resetable iterator instead resetable iterator t extends writable extends org apache hadoop mapreduce lib join resetable iterator t empty u extends writable extends org apache hadoop mapreduce lib join resetable iterator empty u implements resetable iterator u
1329	mapreduce\src\java\org\apache\hadoop\mapred\join\StreamBackedIterator.java	unrelated	package org apache hadoop mapred join this provides implementation resetable iterator this implementation uses byte array store elements added link org apache hadoop mapreduce lib join stream backed iterator instead stream backed iterator x extends writable extends org apache hadoop mapreduce lib join stream backed iterator x implements resetable iterator x
1330	mapreduce\src\java\org\apache\hadoop\mapred\join\TupleWritable.java	unrelated	package org apache hadoop mapred join writable type storing multiple link org apache hadoop io writable this general purpose tuple type in almost cases users encouraged implement serializable types perform better validation provide efficient encodings capable tuple writable relies join framework type safety assumes instances rarely persisted assumptions incompatible contrary general case link org apache hadoop mapreduce lib join tuple writable instead tuple writable extends org apache hadoop mapreduce lib join tuple writable create empty tuple allocated storage writables tuple writable super initialize tuple storage unknown whether contain quot written quot values tuple writable writable vals super vals record tuple contains element position provided set written written set record tuple contain element position provided clear written written clear clear record writables written without releasing storage clear written written clear
1331	mapreduce\src\java\org\apache\hadoop\mapred\join\WrappedRecordReader.java	unrelated	package org apache hadoop mapred join proxy record reader participating join framework this keeps track quot head quot key value pair provided record reader keeps store values matching key source participating join link org apache hadoop mapreduce lib join wrapped record reader instead wrapped record reader k extends writable comparable u extends writable implements composable record reader k u boolean empty false record reader k u rr id index values inserted collector k khead key top rr u vhead value assoc khead writable comparator cmp resetable iterator u vjoin for given record reader rr occupy position id collector wrapped record reader id record reader k u rr class extends writable comparator cmpcl throws io exception id id rr rr khead rr create key vhead rr create value try cmp null cmpcl writable comparator get khead get class cmpcl new instance catch instantiation exception e throw io exception new io exception init cause e catch illegal access exception e throw io exception new io exception init cause e vjoin new stream backed iterator u next inherit doc id return id return key head rr k key return khead clone key head rr object supplied key k qkey throws io exception writable utils clone into qkey khead return true rr including k v pair stored object exhausted boolean next return empty skip key value pairs keys less equal key provided skip k key throws io exception next cmp compare khead key next read next k v pair head object return true iff rr exhausted protected boolean next throws io exception empty rr next khead vhead return next add iterator collector position occupied record reader values stream paired key provided ie register stream values source matching k collector join collector comes parent accept composite record reader join collector k key throws io exception vjoin clear cmp compare key khead vjoin add vhead next cmp compare key khead add id vjoin write key value pair head stream objects provided get next key value pair proxied rr boolean next k key u value throws io exception next writable utils clone into key khead writable utils clone into value vhead next return true return false request new key proxied rr k create key return rr create key request new value proxied rr u create value return rr create value request progress proxied rr get progress throws io exception return rr get progress request position proxied rr get pos throws io exception return rr get pos forward close request proxied rr close throws io exception rr close implement comparable contract compare key head proxied rr another compare to composable record reader k return cmp compare key key return true iff compare to retn true boolean equals object return instanceof composable record reader compare to composable record reader hash code assert false hash code designed return
1332	mapreduce\src\java\org\apache\hadoop\mapred\lib\BinaryPartitioner.java	unrelated	package org apache hadoop mapred lib partition link binary comparable keys using configurable part bytes array returned link binary comparable get bytes link org apache hadoop mapreduce lib partition binary partitioner instead binary partitioner v extends org apache hadoop mapreduce lib partition binary partitioner v implements partitioner binary comparable v configure job conf job super set conf job
1333	mapreduce\src\java\org\apache\hadoop\mapred\lib\Chain.java	unrelated	package org apache hadoop mapred lib the chain provides common functionality link chain mapper link chain reducer chain extends org apache hadoop mapreduce lib chain chain string mapper by value chain mapper value string reducer by value chain reducer value job conf chain job conf list mapper mappers new array list mapper reducer reducer cache key value output serializations chain element avoid everytime lookup list serialization mappers key serialization new array list serialization list serialization mappers value serialization new array list serialization serialization reducer key serialization serialization reducer value serialization creates chain instance configured mapper reducer reducer chain boolean map super map adds mapper chain job job conf p the configuration properties chain job precedence configuration properties mapper reducer next mapper chain it recommended use job conf without default values using code job conf boolean load defaults code constructor false k v k v add mapper boolean map job conf job conf class extends mapper k v k v klass class extends k input key class class extends v input value class class extends k output key class class extends v output value class boolean value job conf mapper conf string prefix get prefix map reducer chain check reducer already set check reducer already set map job conf prefix true set mapper index get index job conf prefix job conf set class prefix chain mapper class index klass mapper validate key value types map job conf input key class input value class output key class output value class index prefix mapper job conf create empty one mapper conf null using job conf without defaults make lightweight still chain job conf may defaults conf overlapped chain job conf one mapper conf new job conf true store mapper conf works value reference mapper conf set boolean mapper by value value set mapper conf map job conf input key class input value class output key class output value class mapper conf index prefix sets reducer chain job job conf p the configuration properties chain job precedence configuration properties reducer next mapper chain it recommended use job conf without default values using code job conf boolean load defaults code constructor false k v k v set reducer job conf job conf class extends reducer k v k v klass class extends k input key class class extends v input value class class extends k output key class class extends v output value class boolean value job conf reducer conf string prefix get prefix false check reducer already set false job conf prefix false job conf set class prefix chain reducer class klass reducer reducer job conf create empty one reducer conf null using job conf without defaults make lightweight still chain job conf may defaults conf overlapped chain job conf one reducer conf new job conf false store reducer conf input output reducer works value reference reducer conf set boolean reducer by value value set reducer conf job conf input key class input value class output key class output value class reducer conf prefix configures chain elements task configure job conf job conf string prefix get
1334	mapreduce\src\java\org\apache\hadoop\mapred\lib\ChainMapper.java	unrelated	package org apache hadoop mapred lib the chain mapper allows use multiple mapper within single map task p the mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p the key functionality feature mappers chain need aware executed chain this enables reusable specialized mappers combined perform composite operations within single task p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use maching output input key value conversion done chaining code p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p important there need specify output key value chain mapper done add mapper last mapper chain p chain mapper usage pattern p pre conf set job name chain conf set input format text input format conf set output format text output format p job conf map a conf new job conf false chain mapper add mapper conf a map long writable text text text true map a conf p job conf map b conf new job conf false chain mapper add mapper conf b map text text long writable text false map b conf p job conf reduce conf new job conf false chain reducer set reducer conf x reduce long writable text text text true reduce conf p chain reducer add mapper conf c map text text long writable text false null p chain reducer add mapper conf d map long writable text long writable long writable true null p file input format set input paths conf dir file output format set output path conf dir p job client jc new job client conf running job job jc submit job conf pre use link org apache hadoop mapreduce lib chain chain mapper instead chain mapper implements mapper adds mapper chain job job conf p it specified key values passed one element chain next value reference if mapper leverages assumed semantics key values modified collector value must used if mapper expect semantics optimization avoid serialization deserialization reference used p for added mapper configuration given code mapper conf code precedence job job conf this precedence effect task running p important there need specify output key value chain mapper done add mapper last mapper chain p next mapper chain it recommended use job conf without default values using code job conf boolean load defaults code constructor false k v k v add mapper job conf job class extends mapper k v k v klass class extends k input key class class extends v input value class class extends k output key class class extends v output value class boolean value job conf mapper conf job set mapper class chain mapper job set map output key class output key class job set map output value class output value class chain add mapper true job klass input key class input value class output key class output value class value mapper conf chain chain constructor chain mapper chain new chain true configures
1335	mapreduce\src\java\org\apache\hadoop\mapred\lib\ChainReducer.java	unrelated	package org apache hadoop mapred lib the chain reducer allows chain multiple mapper reducer within reducer task p for record output reducer mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p the key functionality feature mappers chain need aware executed reducer chain this enables reusable specialized mappers combined perform composite operations within single task p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use maching output input key value conversion done chaining code p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p important there need specify output key value chain reducer done set reducer add mapper last element chain p chain reducer usage pattern p pre conf set job name chain conf set input format text input format conf set output format text output format p job conf map a conf new job conf false chain mapper add mapper conf a map long writable text text text true map a conf p job conf map b conf new job conf false chain mapper add mapper conf b map text text long writable text false map b conf p job conf reduce conf new job conf false chain reducer set reducer conf x reduce long writable text text text true reduce conf p chain reducer add mapper conf c map text text long writable text false null p chain reducer add mapper conf d map long writable text long writable long writable true null p file input format set input paths conf dir file output format set output path conf dir p job client jc new job client conf running job job jc submit job conf pre use link org apache hadoop mapreduce lib chain chain reducer instead chain reducer implements reducer sets reducer chain job job conf p it specified key values passed one element chain next value reference if reducer leverages assumed semantics key values modified collector value must used if reducer expect semantics optimization avoid serialization deserialization reference used p for added reducer configuration given code reducer conf code precedence job job conf this precedence effect task running p important there need specify output key value chain reducer done set reducer add mapper last element chain next mapper chain it recommended use job conf without default values using code job conf boolean load defaults code constructor false k v k v set reducer job conf job class extends reducer k v k v klass class extends k input key class class extends v input value class class extends k output key class class extends v output value class boolean value job conf reducer conf job set reducer class chain reducer job set output key class output key class job set output value class output value class chain set reducer job klass input key class input value class output key class output value class value reducer conf adds mapper chain job job conf
1336	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileInputFormat.java	pooling	package org apache hadoop mapred lib an link org apache hadoop mapred input format returns link combine file split link org apache hadoop mapred input format get splits job conf method splits constructed files input paths a split cannot files different pools each split returned may contain blocks different files if max split size specified blocks node combined form single split blocks left combined blocks rack if max split size specified blocks rack combined single split attempt made create node local splits if max split size equal block size similar default spliting behaviour hadoop block locally processed split subclasses implement link org apache hadoop mapred input format get record reader input split job conf reporter construct code record reader code code combine file split code link org apache hadoop mapreduce lib input combine file input format combine file input format k v extends org apache hadoop mapreduce lib input combine file input format k v implements input format k v default constructor combine file input format input split get splits job conf job num splits throws io exception list org apache hadoop mapreduce input split new style splits super get splits new job job input split ret new input split new style splits size pos pos new style splits size pos org apache hadoop mapreduce lib input combine file split new style split org apache hadoop mapreduce lib input combine file split new style splits get pos ret pos new combine file split job new style split get paths new style split get start offsets new style split get lengths new style split get locations return ret create new pool add filters a split cannot files different pools protected create pool job conf conf list path filter filters create pool filters create new pool add filters a pathname satisfy one specified filters a split cannot files different pools protected create pool job conf conf path filter filters create pool filters this implemented yet record reader k v get record reader input split split job conf job reporter reporter throws io exception method super implemented return null org apache hadoop mapreduce record reader k v create record reader org apache hadoop mapreduce input split split task attempt context context throws io exception return null
1337	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileRecordReader.java	unrelated	package org apache hadoop mapred lib a generic record reader hand different record readers chunk link combine file split a combine file split combine data chunks multiple files this allows using different record readers processing data chunks different files link org apache hadoop mapreduce lib input combine file record reader combine file record reader k v implements record reader k v class constructor signature new class combine file split configuration reporter integer protected combine file split split protected job conf jc protected reporter reporter protected class record reader k v rr class protected constructor record reader k v rr constructor protected file system fs protected idx protected progress protected record reader k v cur reader boolean next k key v value throws io exception cur reader null cur reader next key value init next record reader return false return true k create key return cur reader create key v create value return cur reader create value return amount data processed get pos throws io exception return progress close throws io exception cur reader null cur reader close cur reader null return progress based amount data processed far get progress throws io exception return math min f progress split get length a generic record reader hand different record readers chunk combine file split combine file record reader job conf job combine file split split reporter reporter class record reader k v rr class throws io exception split split jc job rr class rr class reporter reporter idx cur reader null progress try rr constructor rr class get declared constructor constructor signature rr constructor set accessible true catch exception e throw new runtime exception rr class get name valid constructor e init next record reader get record reader next chunk combine file split protected boolean init next record reader throws io exception cur reader null cur reader close cur reader null idx progress split get length idx done processing far chunks processed nothing idx split get num paths return false get record reader idx th chunk try cur reader rr constructor new instance new object split jc reporter integer value of idx setup helper config variables jc set job context map input file split get path idx string jc set long job context map input start split get offset idx jc set long job context map input path split get length idx catch exception e throw new runtime exception e idx return true
1338	mapreduce\src\java\org\apache\hadoop\mapred\lib\CombineFileSplit.java	unrelated	package org apache hadoop mapred lib link org apache hadoop mapreduce lib input combine file split combine file split extends org apache hadoop mapreduce lib input combine file split implements input split job conf job combine file split combine file split job conf job path files start lengths string locations super files start lengths locations job job combine file split job conf job path files lengths super files lengths job job copy constructor combine file split combine file split old throws io exception super old job conf get job return job
1339	mapreduce\src\java\org\apache\hadoop\mapred\lib\DelegatingInputFormat.java	unrelated	package org apache hadoop mapred lib an link input format delegates behaviour paths multiple input formats link org apache hadoop mapreduce lib input delegating input format instead delegating input format k v implements input format k v input split get splits job conf conf num splits throws io exception job conf conf copy new job conf conf list input split splits new array list input split map path input format format map multiple inputs get input format map conf map path class extends mapper mapper map multiple inputs get mapper type map conf map class extends input format list path format paths new hash map class extends input format list path first build map input formats paths entry path input format entry format map entry set format paths contains key entry get value get class format paths put entry get value get class new linked list path format paths get entry get value get class add entry get key entry class extends input format list path format entry format paths entry set class extends input format format class format entry get key input format format input format reflection utils new instance format class conf list path paths format entry get value map class extends mapper list path mapper paths new hash map class extends mapper list path now set paths common input format build map mappers paths used path path paths class extends mapper mapper class mapper map get path mapper paths contains key mapper class mapper paths put mapper class new linked list path mapper paths get mapper class add path now set paths common input format mapper added job split together entry class extends mapper list path map entry mapper paths entry set paths map entry get value class extends mapper mapper class map entry get key mapper class null mapper class conf get mapper class file input format set input paths conf copy paths array new path paths size get splits input path tag input format mapper types wrapping tagged input split input split path splits format get splits conf copy num splits input split path split path splits splits add new tagged input split path split conf format get class mapper class return splits array new input split splits size record reader k v get record reader input split split job conf conf reporter reporter throws io exception find input format record reader tagged input split tagged input split tagged input split tagged input split split input format k v input format input format k v reflection utils new instance tagged input split get input format class conf return input format get record reader tagged input split get input split conf reporter
1340	mapreduce\src\java\org\apache\hadoop\mapred\lib\DelegatingMapper.java	unrelated	package org apache hadoop mapred lib an link mapper delegates behaviour paths multiple mappers link org apache hadoop mapreduce lib input delegating mapper instead delegating mapper k v k v implements mapper k v k v job conf conf mapper k v k v mapper map k key v value output collector k v output collector reporter reporter throws io exception mapper null find mapper tagged input split tagged input split input split tagged input split reporter get input split mapper mapper k v k v reflection utils new instance input split get mapper class conf mapper map key value output collector reporter configure job conf conf conf conf close throws io exception mapper null mapper close
1341	mapreduce\src\java\org\apache\hadoop\mapred\lib\FieldSelectionMapReduce.java	unrelated	package org apache hadoop mapred lib this implements mapper reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the reducer extracts output key value pairs similar manner except key never ignored link field selection reducer instead field selection map reduce k v implements mapper k v text text reducer text text text text string map output key value spec boolean ignore input key string field separator list integer map output key field list new array list integer list integer map output value field list new array list integer map value fields from string reduce output key value spec list integer reduce output key field list new array list integer list integer reduce output value field list new array list integer reduce value fields from log log log factory get log field selection map reduce string spec to string string buffer sb new string buffer sb append field separator append field separator append n sb append map output key value spec append map output key value spec append n sb append reduce output key value spec append reduce output key value spec append n sb append map value fields from append map value fields from append n sb append reduce value fields from append reduce value fields from append n sb append map output key field list length append map output key field list size append n map output key field list size sb append append map output key field list get append n sb append map output value field list length append map output value field list size append n map output value field list size sb append append map output value field list get append n sb append reduce output key field list length append reduce output key field list size append n reduce output key field list size sb append append reduce output key field list get append n sb append reduce output value field list length append reduce output value field list size append n reduce output value field list size sb append append reduce output value field list get append n return sb string the identify function input key value pair written directly output map k key v
1342	mapreduce\src\java\org\apache\hadoop\mapred\lib\FilterOutputFormat.java	unrelated	package org apache hadoop mapred lib filter output format convenience wraps output format link org apache hadoop mapreduce lib output filter output format instead filter output format k v implements output format k v protected output format k v base out filter output format base out null create filter output format based supplied output format filter output format output format k v base out record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception return get base out get record writer ignored job name progress check output specs file system ignored job conf job throws io exception get base out check output specs ignored job output format k v get base out throws io exception base out null throw new io exception outputformat set filter output format return base out code filter record writer code convenience wrapper implements link record writer filter record writer k v implements record writer k v protected record writer k v raw writer null filter record writer throws io exception raw writer null filter record writer record writer k v raw writer throws io exception raw writer raw writer close reporter reporter throws io exception get raw writer close reporter write k key v value throws io exception get raw writer write key value record writer k v get raw writer throws io exception raw writer null throw new io exception record writer set filter record writer return raw writer
1343	mapreduce\src\java\org\apache\hadoop\mapred\lib\HashPartitioner.java	unrelated	package org apache hadoop mapred lib partition keys link object hash code link org apache hadoop mapreduce lib partition hash partitioner instead hash partitioner k v implements partitioner k v configure job conf job use link object hash code partition get partition k key v value num reduce tasks return key hash code integer max value num reduce tasks
1344	mapreduce\src\java\org\apache\hadoop\mapred\lib\IdentityMapper.java	unrelated	package org apache hadoop mapred lib implements identity function mapping inputs directly outputs identity mapper k v extends map reduce base implements mapper k v k v the identify function input key value pair written directly output map k key v val output collector k v output reporter reporter throws io exception output collect key val
1345	mapreduce\src\java\org\apache\hadoop\mapred\lib\IdentityReducer.java	unrelated	package org apache hadoop mapred lib performs reduction writing input values directly output identity reducer k v extends map reduce base implements reducer k v k v writes keys values directly output reduce k key iterator v values output collector k v output reporter reporter throws io exception values next output collect key values next
1346	mapreduce\src\java\org\apache\hadoop\mapred\lib\InputSampler.java	unrelated	package org apache hadoop mapred lib link org apache hadoop mapreduce lib partition input sampler input sampler k v extends org apache hadoop mapreduce lib partition input sampler k v input sampler job conf conf super conf k v write partition file job conf job sampler k v sampler throws io exception class not found exception interrupted exception write partition file new job job sampler
1347	mapreduce\src\java\org\apache\hadoop\mapred\lib\InverseMapper.java	unrelated	package org apache hadoop mapred lib a link mapper swaps keys values instead inverse mapper k v extends map reduce base implements mapper k v v k the inverse function input keys values swapped map k key v value output collector v k output reporter reporter throws io exception output collect value key
1348	mapreduce\src\java\org\apache\hadoop\mapred\lib\KeyFieldBasedComparator.java	unrelated	package org apache hadoop mapred lib this comparator implementation provides subset features provided unix gnu sort in particular supported features n sort numerically r reverse result comparison k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options nr described we assume fields key separated link job context map output key field seperator link org apache hadoop mapreduce lib partition key field based comparator instead key field based comparator k v extends org apache hadoop mapreduce lib partition key field based comparator k v implements job configurable configure job conf job super set conf job
1349	mapreduce\src\java\org\apache\hadoop\mapred\lib\KeyFieldBasedPartitioner.java	unrelated	package org apache hadoop mapred lib defines way partition keys based certain key fields also see link key field based comparator the key specification supported form k pos pos pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field link org apache hadoop mapreduce lib partition key field based partitioner instead key field based partitioner k v extends org apache hadoop mapreduce lib partition key field based partitioner k v implements partitioner k v configure job conf job super set conf job
1350	mapreduce\src\java\org\apache\hadoop\mapred\lib\LazyOutputFormat.java	unrelated	package org apache hadoop mapred lib a convenience creates output lazily link org apache hadoop mapreduce lib output lazy output format instead lazy output format k v extends filter output format k v set underlying output format lazy output format set output format class job conf job class extends output format class job set output format lazy output format job set class mapreduce output lazyoutputformat outputformat class output format record writer k v get record writer file system ignored job conf job string name progressable progress throws io exception base out null get base output format job return new lazy record writer k v job base out name progress check output specs file system ignored job conf job throws io exception base out null get base output format job super check output specs ignored job get base output format job conf job throws io exception base out reflection utils new instance job get class mapreduce output lazyoutputformat outputformat null output format job base out null throw new io exception ouput format set lazy output format code lazy record writer code convenience works lazy output format lazy record writer k v extends filter record writer k v output format string name progressable progress job conf job lazy record writer job conf job output format string name progressable progress throws io exception job job name name progress progress close reporter reporter throws io exception raw writer null raw writer close reporter write k key v value throws io exception raw writer null create record writer super write key value create record writer throws io exception file system fs file system get job raw writer get record writer fs job name progress
1351	mapreduce\src\java\org\apache\hadoop\mapred\lib\LongSumReducer.java	unrelated	package org apache hadoop mapred lib a link reducer sums values instead long sum reducer k extends map reduce base implements reducer k long writable k long writable reduce k key iterator long writable values output collector k long writable output reporter reporter throws io exception sum values key sum values next sum values next get output sum output collect key new long writable sum
1352	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleInputs.java	unrelated	package org apache hadoop mapred lib this supports map reduce jobs multiple input paths different link input format link mapper path link org apache hadoop mapreduce lib input multiple inputs instead multiple inputs add link path custom link input format list inputs map reduce job add input path job conf conf path path class extends input format input format class string input format mapping path string input format class get name string input formats conf get mapreduce input multipleinputs dir formats conf set mapreduce input multipleinputs dir formats input formats null input format mapping input formats input format mapping conf set input format delegating input format add link path custom link input format link mapper list inputs map reduce job add input path job conf conf path path class extends input format input format class class extends mapper mapper class add input path conf path input format class string mapper mapping path string mapper class get name string mappers conf get mapreduce input multipleinputs dir mappers conf set mapreduce input multipleinputs dir mappers mappers null mapper mapping mappers mapper mapping conf set mapper class delegating mapper retrieves map link path link input format used map path input format get input format map job conf conf map path input format new hash map path input format string path mappings conf get mapreduce input multipleinputs dir formats split string path mapping path mappings string split path mapping split input format input format try input format input format reflection utils new instance conf get class by name split conf catch class not found exception e throw new runtime exception e put new path split input format return retrieves map link path link mapper used map path class extends mapper get mapper type map job conf conf conf get mapreduce input multipleinputs dir mappers null return collections empty map map path class extends mapper new hash map path class extends mapper string path mappings conf get mapreduce input multipleinputs dir mappers split string path mapping path mappings string split path mapping split class extends mapper map class try map class class extends mapper conf get class by name split catch class not found exception e throw new runtime exception e put new path split map class return
1353	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends file output format allowing write output data different output files there three basic use cases case one this used map reduce job least one reducer the reducer wants write data different files depending actual keys it assumed key value encodes actual key value desired location actual key value case two this used map job the job wants use output file name either part input file name input data derivation case three this used map job the job wants use output file name depends keys input file name link org apache hadoop mapreduce lib output multiple outputs instead multiple output format k v extends file output format k v create composite record writer write key value data different output files file system use job conf job leaf file name output file part progressable reporting progress record writer k v get record writer file system fs job conf job string name progressable arg throws io exception file system fs fs string name generate leaf file name name job conf job job progressable progressable arg return new record writer k v cache storing record writers different output files tree map string record writer k v record writers new tree map string record writer k v write k key v value throws io exception get file name based key string key based path generate file name for key value key value name get file name based input file name string path get input file based output file name job key based path get actual key k actual key generate actual key key value v actual value generate actual value key value record writer k v rw record writers get path rw null record writer yet path create one add cache rw get base record writer fs job path progressable record writers put path rw rw write actual key actual value close reporter reporter throws io exception iterator string keys record writers key set iterator keys next record writer k v rw record writers get keys next rw close reporter record writers clear generate leaf name output file name the default behavior change leaf file name part leaf file name output file protected string generate leaf file name string name return name generate file output file name based given key leaf file name the default behavior file name depend key key output data leaf file name protected string generate file name for key value k key v value string name return name generate actual key given key value the default behavior actual key equal given key key output data value output data protected k generate actual key k key v value return key generate actual value given key value the default behavior actual value equal given value key output data value output data protected v generate actual value k key v value return value generate outfile name based given anme input file name if link job context map input file exists e map job given name returned unchanged if config value num trailing legs use set set negative given
1354	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleOutputs.java	unrelated	package org apache hadoop mapred lib the multiple outputs simplifies writting additional outputs job default output via code output collector code passed code map code code reduce code methods code mapper code code reducer code implementations p each additional output named output may configured code output format code key value p a named output single file multi file the later refered multi named output p a multi named output unbound set files sharing code output format code key value configuration p when named outputs used within code mapper code implementation key values written name output part reduce phase key values written job code output collector code part reduce phase p multiple outputs supports counters default disabled the counters group link multiple outputs name p the names counters named outputs for multi named outputs name counter concatenation named output underscore multiname p job configuration usage pattern pre job conf conf new job conf conf set input path dir file output format set output path conf dir conf set mapper class mo map conf set reducer class mo reduce defines additional single text based output text job multiple outputs add named output conf text text output format long writable text defines additional multi sequencefile based output sequence job multiple outputs add multi named output conf seq sequence file output format long writable text job client jc new job client running job job jc submit job conf pre p job configuration usage pattern pre mo reduce implements reducer lt writable comparable writable gt multiple outputs mos configure job conf conf mos new multiple outputs conf reduce writable comparable key iterator lt writable gt values output collector output reporter reporter throws io exception mos get collector text reporter collect key new text hello mos get collector seq a reporter collect key new text bye mos get collector seq b reporter collect key new text chau close throws io exception mos close pre link org apache hadoop mapreduce lib output multiple outputs instead multiple outputs string named outputs mo named outputs string mo prefix mo named output string format format string key key string value value string multi multi string counters enabled mo counters counters group used counters multiple outputs string counters group multiple outputs get name checks named output already defined named output checked depending value already defined parameter check named output job conf conf string named output boolean already defined list string defined channels get named outputs list conf already defined defined channels contains named output throw new illegal argument exception named output named output already already defined else already defined defined channels contains named output throw new illegal argument exception named output named output defined checks named output name valid token check token name string named output named output null named output length throw new illegal argument exception name cannot null emtpy char ch named output char array ch a ch z continue ch ch z continue ch ch continue throw new illegal argument exception name cannot ch char checks named output name valid check named output name string named output check token name
1355	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleSequenceFileOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends multiple output format allowing write output data different output files sequence file output format link org apache hadoop mapreduce lib output multiple outputs instead multiple sequence file output format k v extends multiple output format k v sequence file output format k v sequence file output format null protected record writer k v get base record writer file system fs job conf job string name progressable arg throws io exception sequence file output format null sequence file output format new sequence file output format k v return sequence file output format get record writer fs job name arg
1356	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultipleTextOutputFormat.java	unrelated	package org apache hadoop mapred lib this extends multiple output format allowing write output data different output files text output format link org apache hadoop mapreduce lib output multiple outputs instead multiple text output format k v extends multiple output format k v text output format k v text output format null protected record writer k v get base record writer file system fs job conf job string name progressable arg throws io exception text output format null text output format new text output format k v return text output format get record writer fs job name arg
1357	mapreduce\src\java\org\apache\hadoop\mapred\lib\MultithreadedMapRunner.java	pooling	package org apache hadoop mapred lib multithreaded implementation link org apache hadoop mapred map runnable p it used instead default implementation bound order improve throughput p map implementations using map runnable must thread safe p the map reduce job configured use map runnable using job conf set map runner class method number thread thread pool use code mapred map multithreadedrunner threads code property default value threads p multithreaded map runner k v k v implements map runnable k v k v log log log factory get log multithreaded map runner get name job conf job mapper k v k v mapper executor service executor service volatile io exception io exception volatile runtime exception runtime exception boolean incr proc count configure job conf job conf number of threads job conf get int multithreaded mapper num threads log debug enabled log debug configuring job conf job conf get job name use number of threads threads job job conf increment processed counter skipping feature enabled incr proc count skip bad records get mapper max skip records job skip bad records get auto incr mapper proc count job mapper reflection utils new instance job conf get mapper class job conf creating threadpool configured size execute mapper map method parallel executor service new thread pool executor number of threads number of threads l time unit milliseconds new blocking array queue number of threads a blocking array queue replaces offer add throws full queue put waits full queue blocking array queue extends array blocking queue runnable serial version uid l blocking array queue capacity super capacity boolean offer runnable r return add r boolean add runnable r try put r catch interrupted exception ie thread current thread interrupt return true check for exceptions from processing threads throws io exception runtime exception checking mapper map within runnable generated io exception if rethrow force abort map operation thus keeping semantics default implementation io exception null throw io exception checking mapper map within runnable generated runtime exception if rethrow force abort map operation thus keeping semantics default implementation runtime exception null throw runtime exception run record reader k v input output collector k v output reporter reporter throws io exception try allocate key value instances objects reused execution mapper map serialized k key input create key v value input create value input next key value executor service execute new mapper invoke runable key value output reporter check for exceptions from processing threads allocate new key value instances mapper running parallel key input create key value input create value log debug enabled log debug finished dispatching mappper map calls job job get job name graceful shutdown threadpool let scheduled runnables end executor service shutdown try now waiting runnables end executor service await termination time unit milliseconds log debug enabled log debug awaiting running mappper map calls finish job job get job name note mapper map dispatching concluded still map calls progress exceptions would thrown check for exceptions from processing threads note could map call exception call await termination returing true and edge case could happen check for exceptions from processing threads catch
1358	mapreduce\src\java\org\apache\hadoop\mapred\lib\NLineInputFormat.java	unrelated	package org apache hadoop mapred lib n line input format splits n lines input one split in many pleasantly parallel applications process mapper processes input file computations controlled different parameters referred parameter sweeps one way achieve specify set parameters one set per line input control file input path map reduce application input dataset specified via config variable job conf the n line input format used applications splits input file default one line fed value one map task key offset e k v long writable text the location hints span whole mapred cluster link org apache hadoop mapreduce lib input n line input format instead n line input format extends file input format long writable text implements job configurable n record reader long writable text get record reader input split generic split job conf job reporter reporter throws io exception reporter set status generic split string return new line record reader job file split generic split logically splits set input files job splits n lines input one split input split get splits job conf job num splits throws io exception array list file split splits new array list file split file status status list status job org apache hadoop mapreduce lib input file split split org apache hadoop mapreduce lib input n line input format get splits for file status job n splits add new file split split return splits array new file split splits size configure job conf conf n conf get int mapreduce input lineinputformat linespermap
1359	mapreduce\src\java\org\apache\hadoop\mapred\lib\NullOutputFormat.java	unrelated	package org apache hadoop mapred lib consume outputs put dev null link org apache hadoop mapreduce lib output null output format instead null output format k v implements output format k v record writer k v get record writer file system ignored job conf job string name progressable progress return new record writer k v write k key v value close reporter reporter check output specs file system ignored job conf job
1360	mapreduce\src\java\org\apache\hadoop\mapred\lib\RegexMapper.java	unrelated	package org apache hadoop mapred lib a link mapper extracts text matching regular expression regex mapper k extends map reduce base implements mapper k text text long writable pattern pattern group configure job conf job pattern pattern compile job get org apache hadoop mapreduce lib map regex mapper pattern group job get int org apache hadoop mapreduce lib map regex mapper group map k key text value output collector text long writable output reporter reporter throws io exception string text value string matcher matcher pattern matcher text matcher find output collect new text matcher group group new long writable
1361	mapreduce\src\java\org\apache\hadoop\mapred\lib\TaggedInputSplit.java	unrelated	package org apache hadoop mapred lib an link input split tags another input split extra data use link delegating input format link delegating mapper link org apache hadoop mapreduce lib input tagged input split instead tagged input split implements configurable input split class extends input split input split class input split input split class extends input format input format class class extends mapper mapper class configuration conf tagged input split default constructor creates new tagged input split tagged input split input split input split configuration conf class extends input format input format class class extends mapper mapper class input split class input split get class input split input split conf conf input format class input format class mapper class mapper class retrieves original input split input split get input split return input split retrieves input format use split class extends input format get input format class return input format class retrieves mapper use split class extends mapper get mapper class return mapper class get length throws io exception return input split get length string get locations throws io exception return input split get locations read fields data input throws io exception input split class class extends input split read class input split input split reflection utils new instance input split class conf input split read fields input format class class extends input format read class mapper class class extends mapper read class class read class data input throws io exception string name text read string try return conf get class by name name catch class not found exception e throw new runtime exception read object find e write data output throws io exception text write string input split class get name input split write text write string input format class get name text write string mapper class get name configuration get conf return conf set conf configuration conf conf conf
1362	mapreduce\src\java\org\apache\hadoop\mapred\lib\TokenCountMapper.java	unrelated	package org apache hadoop mapred lib a link mapper maps text values token freq pairs uses link string tokenizer break text tokens link org apache hadoop mapreduce lib map token counter mapper instead token count mapper k extends map reduce base implements mapper k text text long writable map k key text value output collector text long writable output reporter reporter throws io exception get input text string text value string value line text tokenize value string tokenizer st new string tokenizer text st more tokens output token pairs output collect new text st next token new long writable
1363	mapreduce\src\java\org\apache\hadoop\mapred\lib\TotalOrderPartitioner.java	unrelated	package org apache hadoop mapred lib partitioner effecting total order reading split points externally generated source link org apache hadoop mapreduce lib partition total order partitioner total order partitioner k extends writable comparable v extends org apache hadoop mapreduce lib partition total order partitioner k v implements partitioner k v total order partitioner configure job conf job super set conf job
1364	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\DoubleValueSum.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator sums sequence values link org apache hadoop mapreduce lib aggregate double value sum instead double value sum extends org apache hadoop mapreduce lib aggregate double value sum implements value aggregator string
1365	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueMax.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain maximum sequence values link org apache hadoop mapreduce lib aggregate long value max instead long value max extends org apache hadoop mapreduce lib aggregate long value max implements value aggregator string
1366	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueMin.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain minimum sequence values link org apache hadoop mapreduce lib aggregate long value min instead long value min extends org apache hadoop mapreduce lib aggregate long value min implements value aggregator string
1367	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\LongValueSum.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator sums sequence values link org apache hadoop mapreduce lib aggregate long value sum instead long value sum extends org apache hadoop mapreduce lib aggregate long value sum implements value aggregator string
1368	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\StringValueMax.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain biggest sequence strings link org apache hadoop mapreduce lib aggregate string value max instead string value max extends org apache hadoop mapreduce lib aggregate string value max implements value aggregator string
1369	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\StringValueMin.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator maintain smallest sequence strings link org apache hadoop mapreduce lib aggregate string value min instead string value min extends org apache hadoop mapreduce lib aggregate string value min implements value aggregator string
1370	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\UniqValueCount.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator dedupes sequence objects link org apache hadoop mapreduce lib aggregate uniq value count instead uniq value count extends org apache hadoop mapreduce lib aggregate uniq value count implements value aggregator object default constructor uniq value count super constructor uniq value count max num super max num
1371	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\UserDefinedValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this implements wrapper user defined value aggregator descriptor it servs two functions one create object value aggregator descriptor name user defined may dynamically loaded the deligate inviokations generate key val pairs function created object link org apache hadoop mapreduce lib aggregate user defined value aggregator descriptor instead user defined value aggregator descriptor extends org apache hadoop mapreduce lib aggregate user defined value aggregator descriptor implements value aggregator descriptor create instance given object create instance string name return org apache hadoop mapreduce lib aggregate user defined value aggregator descriptor create instance name user defined value aggregator descriptor string name job conf job super name job value aggregator descriptor aggregator descriptor configure job do nothing configure job conf job
1372	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregator.java	unrelated	package org apache hadoop mapred lib aggregate this defines minimal protocol value aggregators link org apache hadoop mapreduce lib aggregate value aggregator instead value aggregator e extends org apache hadoop mapreduce lib aggregate value aggregator e
1373	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorBaseDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this implements common functionalities subclasses value aggregator descriptor link org apache hadoop mapreduce lib aggregate value aggregator base descriptor instead value aggregator base descriptor extends org apache hadoop mapreduce lib aggregate value aggregator base descriptor implements value aggregator descriptor string uniq value count org apache hadoop mapreduce lib aggregate value aggregator base descriptor uniq value count string long value sum org apache hadoop mapreduce lib aggregate value aggregator base descriptor long value sum string double value sum org apache hadoop mapreduce lib aggregate value aggregator base descriptor double value sum string value histogram org apache hadoop mapreduce lib aggregate value aggregator base descriptor value histogram string long value max org apache hadoop mapreduce lib aggregate value aggregator base descriptor long value max string long value min org apache hadoop mapreduce lib aggregate value aggregator base descriptor long value min string string value max org apache hadoop mapreduce lib aggregate value aggregator base descriptor string value max string string value min org apache hadoop mapreduce lib aggregate value aggregator base descriptor string value min max num items long max value aggregation type entry text text generate entry string type string id text val return org apache hadoop mapreduce lib aggregate value aggregator base descriptor generate entry type id val value aggregator generate value aggregator string type value aggregator retv null type compare to ignore case long value sum retv new long value sum type compare to ignore case long value max retv new long value max else type compare to ignore case long value min retv new long value min else type compare to ignore case string value max retv new string value max else type compare to ignore case string value min retv new string value min else type compare to ignore case double value sum retv new double value sum else type compare to ignore case uniq value count retv new uniq value count max num items else type compare to ignore case value histogram retv new value histogram return retv get input file name configure job conf job super configure job max num items job get long aggregate max num unique values long max value
1374	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorCombiner.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic combiner aggregate link org apache hadoop mapreduce lib aggregate value aggregator combiner instead value aggregator combiner k extends writable comparable v extends writable extends value aggregator job base k v combiner need configure configure job conf job combines values given key type aggregation aggregate values reduce text key iterator text values output collector text text output reporter reporter throws io exception string key str key string pos key str index of value aggregator descriptor type separator string type key str substring pos value aggregator aggregator value aggregator base descriptor generate value aggregator type values next aggregator add next value values next iterator outputs aggregator get combiner output iterator outputs next object v outputs next v instanceof text output collect key text v else output collect key new text v string do nothing close throws io exception do nothing should called map k arg v arg output collector text text arg reporter arg throws io exception throw new io exception called n
1375	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapred lib aggregate this defines contract value aggregator descriptor must support such descriptor configured job conf object its main function generate list aggregation id value pairs an aggregation id encodes aggregation type used guide way aggregate value reduce combiner phrase aggregate based job the mapper aggregate based map reduce job may create one value aggregator descriptor objects configuration time for input key value pair mapper use objects create aggregation id value pairs link org apache hadoop mapreduce lib aggregate value aggregator descriptor instead value aggregator descriptor extends org apache hadoop mapreduce lib aggregate value aggregator descriptor string type separator org apache hadoop mapreduce lib aggregate value aggregator descriptor type separator text one org apache hadoop mapreduce lib aggregate value aggregator descriptor one configure object job conf object may contain information used configure object configure job conf job
1376	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorJob.java	unrelated	package org apache hadoop mapred lib aggregate this main creating map reduce job using aggregate framework the aggregate specialization map reduce framework specilizing performing various simple aggregations generally speaking order implement application using map reduce model developer implement map reduce functions possibly combine function however lot applications related counting statistics computing similar characteristics aggregate abstracts general patterns functions implementing patterns in particular package provides generic mapper redducer combiner set built value aggregators generic utility helps user create map reduce jobs using generic the built aggregators sum numeric values count number distinct values compute histogram values compute minimum maximum media average standard deviation numeric values the developer using aggregate need provide plugin conforming following value aggregator descriptor array list entry generate key val pairs object key object value configure job confjob the package also provides base value aggregator base descriptor implementing the user extend base implement generate key val pairs accordingly the primary work generate key val pairs emit one key value pairs based input key value pair the key output key value pair encode two pieces information aggregation type aggregation id the value aggregated onto aggregation id according aggregation type this offers function generate map reduce job using aggregate framework the function takes following parameters input directory spec input format text sequence file output directory file specifying user plugin link org apache hadoop mapreduce lib aggregate value aggregator job instead value aggregator job job control create value aggregator jobs string args class extends value aggregator descriptor descriptors throws io exception job control control new job control value aggregator jobs array list job depending jobs new array list job job conf job conf create value aggregator job args descriptors null set aggregator descriptors job conf descriptors job job new job job conf depending jobs control add job job return control job control create value aggregator jobs string args throws io exception return create value aggregator jobs args null create aggregate based map reduce job arguments accepted job conf create value aggregator job string args throws io exception configuration conf new configuration generic options parser generic parser new generic options parser conf args args generic parser get remaining args args length system println usage input dirs dir num of reducer textinputformat seq specfile job name generic options parser print generic command usage system system exit string input dir args string output dir args num of reducers args length num of reducers integer parse int args class extends input format input format text input format args length args compare to ignore case textinputformat input format text input format else input format sequence file input format path spec file null args length spec file new path args string job name args length job name args job conf job new job conf conf spec file null job add resource spec file string user jar file job get user jar file user jar file null job set jar by class value aggregator else job set jar user jar file job set job name value aggregator job job name file input format add input paths job input dir job
1377	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorJobBase.java	unrelated	package org apache hadoop mapred lib aggregate this implements common functionalities generic mapper reducer combiner aggregate link org apache hadoop mapreduce lib aggregate value aggregator job base instead value aggregator job base k extends writable comparable v extends writable implements mapper k v text text reducer text text text text protected array list value aggregator descriptor aggregator descriptor list null configure job conf job initialize my spec job log spec value aggregator descriptor get value aggregator descriptor string spec job conf job spec null return null string segments spec split string type segments type compare to ignore case user defined string name segments return new user defined value aggregator descriptor name job return null array list value aggregator descriptor get aggregator descriptors job conf job string advn aggregator descriptor num job get int advn num array list value aggregator descriptor retv new array list value aggregator descriptor num num string spec job get advn value aggregator descriptor ad get value aggregator descriptor spec job ad null retv add ad return retv initialize my spec job conf job aggregator descriptor list get aggregator descriptors job aggregator descriptor list size aggregator descriptor list add new user defined value aggregator descriptor value aggregator base descriptor get canonical name job protected log spec close throws io exception
1378	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorMapper.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic mapper aggregate link org apache hadoop mapreduce lib aggregate value aggregator mapper instead value aggregator mapper k extends writable comparable v extends writable extends value aggregator job base k v map function it iterates value aggregator descriptor list generate aggregation id value pairs emit map k key v value output collector text text output reporter reporter throws io exception iterator iter aggregator descriptor list iterator iter next value aggregator descriptor ad value aggregator descriptor iter next iterator entry text text ens ad generate key val pairs key value iterator ens next entry text text en ens next output collect en get key en get value do nothing should called reduce text arg iterator text arg output collector text text arg reporter arg throws io exception throw new io exception called n
1379	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueAggregatorReducer.java	unrelated	package org apache hadoop mapred lib aggregate this implements generic reducer aggregate link org apache hadoop mapreduce lib aggregate value aggregator reducer instead value aggregator reducer k extends writable comparable v extends writable extends value aggregator job base k v key expected text object whose prefix indicates type aggregation aggregate values in effect data driven computing achieved it assumed aggregator get report method emits appropriate output aggregator this may customiized reduce text key iterator text values output collector text text output reporter reporter throws io exception string key str key string pos key str index of value aggregator descriptor type separator string type key str substring pos key str key str substring pos value aggregator descriptor type separator length value aggregator aggregator value aggregator base descriptor generate value aggregator type values next aggregator add next value values next string val aggregator get report key new text key str output collect key new text val do nothing should called map k arg v arg output collector text text arg reporter arg throws io exception throw new io exception called n
1380	mapreduce\src\java\org\apache\hadoop\mapred\lib\aggregate\ValueHistogram.java	unrelated	package org apache hadoop mapred lib aggregate this implements value aggregator computes histogram sequence strings link org apache hadoop mapreduce lib aggregate value histogram instead value histogram extends org apache hadoop mapreduce lib aggregate value histogram implements value aggregator string
1381	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBConfiguration.java	unrelated	package org apache hadoop mapred lib db link org apache hadoop mapreduce lib db db configuration instead db configuration extends org apache hadoop mapreduce lib db db configuration the jdbc driver name string driver class property org apache hadoop mapreduce lib db db configuration driver class property jdbc database access url string url property org apache hadoop mapreduce lib db db configuration url property user name access database string username property org apache hadoop mapreduce lib db db configuration username property password access database string password property org apache hadoop mapreduce lib db db configuration password property input table name string input table name property org apache hadoop mapreduce lib db db configuration input table name property field names input table string input field names property org apache hadoop mapreduce lib db db configuration input field names property where clause input select statement string input conditions property org apache hadoop mapreduce lib db db configuration input conditions property order by clause input select statement string input order by property org apache hadoop mapreduce lib db db configuration input order by property whole input query exluding limit offset string input query org apache hadoop mapreduce lib db db configuration input query input query get count records string input count query org apache hadoop mapreduce lib db db configuration input count query class name implementing db writable hold input tuples string input class property org apache hadoop mapreduce lib db db configuration input class property output table name string output table name property org apache hadoop mapreduce lib db db configuration output table name property field names output table string output field names property org apache hadoop mapreduce lib db db configuration output field names property number fields output table string output field count property org apache hadoop mapreduce lib db db configuration output field count property sets db access related fields job conf configure db job conf job string driver class string db url string user name string passwd job set driver class property driver class job set url property db url user name null job set username property user name passwd null job set password property passwd sets db access related fields job conf configure db job conf job string driver class string db url configure db job driver class db url null null db configuration job conf job super job
1382	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBInputFormat.java	unrelated	package org apache hadoop mapred lib db use link org apache hadoop mapreduce lib db db input format instead db input format t extends db writable extends org apache hadoop mapreduce lib db db input format t implements input format long writable t job configurable a record reader reads records sql table emits long writables containing record number key db writables value protected db record reader extends org apache hadoop mapreduce lib db db record reader t implements record reader long writable t protected db record reader db input split split class t input class job conf job connection conn db configuration db config string cond string fields string table throws sql exception super split input class job conn db config cond fields table inherit doc long writable create key return new long writable inherit doc t create value return super create value get pos throws io exception return super get pos inherit doc boolean next long writable key t value throws io exception return super next key value a record reader implementation passes wrapped record reader built new api db record reader wrapper t extends db writable implements record reader long writable t org apache hadoop mapreduce lib db db record reader t rr db record reader wrapper org apache hadoop mapreduce lib db db record reader t inner rr inner close throws io exception rr close long writable create key return new long writable t create value return rr create value get progress throws io exception return rr get progress get pos throws io exception return rr get pos boolean next long writable key t value throws io exception return rr next key value a class nothing implementing db writable null db writable extends org apache hadoop mapreduce lib db db input format null db writable implements db writable writable a input split spans set rows protected db input split extends org apache hadoop mapreduce lib db db input format db input split implements input split default constructor db input split convenience constructor db input split start end super start end inherit doc configure job conf job super set conf job inherit doc record reader long writable t get record reader input split split job conf job reporter reporter throws io exception wrap dbrr shim deal api differences return new db record reader wrapper t org apache hadoop mapreduce lib db db record reader t create db record reader org apache hadoop mapreduce lib db db input format db input split split job inherit doc input split get splits job conf job chunks throws io exception list org apache hadoop mapreduce input split new splits super get splits new job job input split ret new input split new splits size org apache hadoop mapreduce input split new splits org apache hadoop mapreduce lib db db input format db input split split org apache hadoop mapreduce lib db db input format db input split ret new db input split split get start split get end return ret initializes map part job appropriate input settings java object holding tuple fields and length
1383	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBOutputFormat.java	unrelated	package org apache hadoop mapred lib db db output format k extends db writable v extends org apache hadoop mapreduce lib db db output format k v implements output format k v a record writer writes reduce output sql table protected db record writer extends org apache hadoop mapreduce lib db db output format k v db record writer implements record writer k v protected db record writer connection connection prepared statement statement throws sql exception super connection statement inherit doc close reporter reporter throws io exception super close null inherit doc check output specs file system filesystem job conf job throws io exception inherit doc record writer k v get record writer file system filesystem job conf job string name progressable progress throws io exception org apache hadoop mapreduce record writer k v w super get record writer new task attempt context impl job task attempt id name job get mr job config task attempt id org apache hadoop mapreduce lib db db output format db record writer writer org apache hadoop mapreduce lib db db output format db record writer w try return new db record writer writer get connection writer get statement catch sql exception se throw new io exception se initializes reduce part job appropriate output settings set output job conf job string table name string field names field names length field names null db configuration db conf set output job table name db conf set output field names field names else field names length set output job table name field names length else throw new illegal argument exception field names must greater initializes reduce part job appropriate output settings set output job conf job string table name field count db configuration db conf set output job table name db conf set output field count field count db configuration set output job conf job string table name job set output format db output format job set reduce speculative execution false db configuration db conf new db configuration job db conf set output table name table name return db conf
1384	mapreduce\src\java\org\apache\hadoop\mapred\lib\db\DBWritable.java	unrelated	package org apache hadoop mapred lib db use link org apache hadoop mapreduce lib db db writable instead db writable extends org apache hadoop mapreduce lib db db writable
1385	mapreduce\src\java\org\apache\hadoop\mapred\pipes\Application.java	authenticate	package org apache hadoop mapred pipes this responsible launching communicating child process application k extends writable comparable v extends writable k extends writable comparable v extends writable log log log factory get log application get name server socket server socket process process socket client socket output handler k v handler downward protocol k v downlink boolean windows system get property os name starts with windows start child process handle task us application job conf conf record reader float writable null writable record reader output collector k v output reporter reporter class extends k output key class class extends v output value class throws io exception interrupted exception server socket new server socket map string string env new hash map string string add tmpdir environment variable value java io tmpdir env put tmpdir system get property java io tmpdir env put submitter port integer string server socket get local port add token environment security enabled token job token identifier job token token cache get job token conf get credentials this password used shared secret key application child pipes process byte password job token get password string local password file new file path separator job token password write password to local file local password file password conf env put hadoop pipes shared secret location local password file list string cmd new array list string string interpretor conf get submitter interpretor interpretor null cmd add interpretor string executable distributed cache get local cache files conf string new file executable execute linux task controller sets x permissions distcache files already in case default task controller set permissions file util chmod executable u x cmd add executable wrap command stdout stderr capture starting map reduce task pipes job cleanup attempt task attempt id taskid task attempt id name conf get mr job config task attempt id file stdout task log get task log file taskid false task log log name stdout file stderr task log get task log file taskid false task log log name stderr log length task log get task log length conf cmd task log capture out and error null cmd stdout stderr log length false process run client cmd env client socket server socket accept string challenge get security challenge string digest to send create digest password challenge string digest expected create digest password digest to send handler new output handler k v output reporter record reader digest expected k output key k reflection utils new instance output key class conf v output value v reflection utils new instance output value class conf downlink new binary protocol k v k v client socket handler output key output value conf downlink authenticate digest to send challenge wait for authentication log debug authentication succeeded downlink start downlink set job conf conf string get security challenge random rand new random system current time millis use random integers random bytes string builder str builder new string builder str builder append rand next int x fffffff str builder append rand next int x fffffff str builder append rand next int x fffffff str builder append rand next int
1386	mapreduce\src\java\org\apache\hadoop\mapred\pipes\BinaryProtocol.java	unrelated	package org apache hadoop mapred pipes this protocol binary implementation pipes protocol binary protocol k extends writable comparable v extends writable k extends writable comparable v extends writable implements downward protocol k v current protocol version the buffer size command socket buffer size data output stream stream data output buffer buffer new data output buffer log log log factory get log binary protocol get name uplink reader thread uplink the integer codes represent different messages these must match c codes massive confusion result enum message type start set job conf set input types run map map item run reduce reduce key reduce value close abort authentication req output partitioned output status progress done register counter increment counter authentication resp code message type code code code uplink reader thread k extends writable comparable v extends writable extends thread data input stream stream upward protocol k v handler k key v value boolean auth pending true uplink reader thread input stream stream upward protocol k v handler k key v value throws io exception stream new data input stream new buffered input stream stream buffer size handler handler key key value value close connection throws io exception stream close run true try thread current thread interrupted throw new interrupted exception cmd writable utils read v int stream log debug handling uplink command cmd cmd message type authentication resp code string digest text read string stream auth pending handler authenticate digest else auth pending log warn message cmd received authentication complete ignoring continue else cmd message type output code read object key read object value handler output key value else cmd message type partitioned output code part writable utils read v int stream read object key read object value handler partitioned output part key value else cmd message type status code handler status text read string stream else cmd message type progress code handler progress stream read float else cmd message type register counter code id writable utils read v int stream string group text read string stream string name text read string stream handler register counter id group name else cmd message type increment counter code id writable utils read v int stream amount writable utils read v long stream handler increment counter id amount else cmd message type done code log debug pipe child done handler done return else throw new io exception bad command code cmd catch interrupted exception e return catch throwable e log error string utils stringify exception e handler failed e return read object writable obj throws io exception num bytes writable utils read v int stream byte buffer for bytes writable text use specified length set length causes obvious translations work so emit abc c shows abc obj instanceof bytes writable buffer new byte num bytes stream read fully buffer bytes writable obj set buffer num bytes else obj instanceof text buffer new byte num bytes stream read fully buffer text obj set buffer else obj read fields stream an output stream save copy data file tee output stream extends filter output stream output stream file tee output
1387	mapreduce\src\java\org\apache\hadoop\mapred\pipes\DownwardProtocol.java	unrelated	package org apache hadoop mapred pipes the description downward java c pipes protocol all calls asynchronous return message processed downward protocol k extends writable comparable v extends writable request authentication authenticate string digest string challenge throws io exception start communication start throws io exception set job conf task set job conf job conf conf throws io exception set input types maps set input types string key type string value type throws io exception run map task child run map input split split num reduces boolean piped input throws io exception for maps piped input key value pairs sent via messaage map item k key v value throws io exception run reduce task child run reduce reduce boolean piped output throws io exception the reduce given new key reduce key k key throws io exception the reduce given new value reduce value v value throws io exception the task input coming finish processing input end of input throws io exception the task stop soon possible something gone wrong abort throws io exception flush data buffers flush throws io exception close connection close throws io exception interrupted exception
1388	mapreduce\src\java\org\apache\hadoop\mapred\pipes\OutputHandler.java	authenticate	package org apache hadoop mapred pipes handles upward c java messages application output handler k extends writable comparable v extends writable implements upward protocol k v reporter reporter output collector k v collector progress value f boolean done false throwable exception null record reader float writable null writable record reader null map integer counters counter registered counters new hash map integer counters counter string expected digest null boolean digest received false create handler handle records output application output handler output collector k v collector reporter reporter record reader float writable null writable record reader string expected digest reporter reporter collector collector record reader record reader expected digest expected digest the task output normal record output k key v value throws io exception collector collect key value the task output record partition number attached partitioned output reduce k key v value throws io exception pipes partitioner set next partition reduce collector collect key value update status message task status string msg reporter set status msg float writable progress key new float writable f null writable null value null writable get update amount done call progress reporter progress progress throws io exception progress value progress reporter progress record reader null progress key set progress record reader next progress key null value the task finished successfully done throws io exception synchronized done true notify get current amount done get progress return progress value the task failed exception failed throwable e synchronized exception e notify wait task finish abort synchronized boolean wait for finish throws throwable done exception null wait exception null throw exception return done register counter id string group string name throws io exception counters counter counter reporter get counter group name registered counters put id counter increment counter id amount throws io exception id registered counters size counters counter counter registered counters get id counter increment amount else throw new io exception invalid counter id id synchronized boolean authenticate string digest throws io exception boolean success true expected digest equals digest exception new io exception authentication failed expected digest expected digest received digest received success false digest received true notify return success this called application blocks thread authentication response received synchronized wait for authentication throws io exception interrupted exception digest received false exception null wait exception null throw new io exception exception get message
1389	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesMapRunner.java	unrelated	package org apache hadoop mapred pipes an adaptor run c mapper pipes map runner k extends writable comparable v extends writable k extends writable comparable v extends writable extends map runner k v k v job conf job get new configuration configure job conf job job job disable auto increment counter for pipes processed records could different equal less records input skip bad records set auto incr mapper proc count job false run map task run record reader k v input output collector k v output reporter reporter throws io exception application k v k v application null try record reader float writable null writable fake input submitter get is java record reader job submitter get is java mapper job record reader float writable null writable input null application new application k v k v job fake input output reporter class extends k job get output key class class extends v job get output value class catch interrupted exception ie throw new runtime exception interrupted ie downward protocol k v downlink application get downlink boolean java input submitter get is java record reader job downlink run map reporter get input split job get num reduce tasks java input boolean skipping job get boolean mr job config skip records false try java input allocate key value instances used entries k key input create key v value input create value downlink set input types key get class get name value get class get name input next key value map pair output downlink map item key value skipping flush streams every record input running skip mode buffer records surrounding bad record downlink flush downlink end of input application wait for finish catch throwable application abort finally application cleanup
1390	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesNonJavaInputFormat.java	unrelated	package org apache hadoop mapred pipes dummy input format used non java link record reader used pipes application the useful thing set map reduce job get link pipes dummy record reader everything else left actual input format specified user given mapreduce pipes inputformat pipes non java input format implements input format float writable null writable record reader float writable null writable get record reader input split generic split job conf job reporter reporter throws io exception return new pipes dummy record reader job generic split input split get splits job conf job num splits throws io exception delegate generation input splits original input format return reflection utils new instance job get class submitter input format text input format input format job get splits job num splits a dummy link org apache hadoop mapred record reader help track progress hadoop pipes applications using non java code record reader code the code pipes dummy record reader code informed progress task link output handler progress calls link next float writable null writable progress code key code pipes dummy record reader implements record reader float writable null writable progress f pipes dummy record reader configuration job input split split throws io exception float writable create key return null null writable create value return null synchronized close throws io exception synchronized get pos throws io exception return get progress return progress synchronized boolean next float writable key null writable value throws io exception progress key get return true
1391	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesPartitioner.java	unrelated	package org apache hadoop mapred pipes this partitioner one either set manually per record fall back onto java partitioner set user pipes partitioner k extends writable comparable v extends writable implements partitioner k v thread local integer cache new thread local integer partitioner k v part null configure job conf conf part reflection utils new instance submitter get java partitioner conf conf set next key given partition set next partition new value cache set new value if partition result set manually return otherwise call java partitioner get partition k key v value num partitions integer result cache get result null return part get partition key value num partitions else return result
1392	mapreduce\src\java\org\apache\hadoop\mapred\pipes\PipesReducer.java	unrelated	package org apache hadoop mapred pipes this used talk c reduce task pipes reducer k extends writable comparable v extends writable k extends writable comparable v extends writable implements reducer k v k v log log log factory get log pipes reducer get name job conf job application k v k v application null downward protocol k v downlink null boolean ok true boolean skipping false configure job conf job job job disable auto increment counter for pipes processed records could different equal less records input skip bad records set auto incr reducer proc count job false skipping job get boolean mr job config skip records false process keys values start application started yet reduce k key iterator v values output collector k v output reporter reporter throws io exception ok false start application output reporter downlink reduce key key values next downlink reduce value values next skipping flush streams every record input running skip mode buffer records surrounding bad record downlink flush ok true start application output collector k v output reporter reporter throws io exception application null try log info starting application application new application k v k v job null output reporter class extends k job get output key class class extends v job get output value class downlink application get downlink catch interrupted exception ie throw new runtime exception interrupted ie reduce downlink run reduce reduce submitter get is java record writer job handle end input closing application close throws io exception started application nothing ok output collector k v null collector new output collector k v collect k key v value throws io exception null start application null collector reporter null try ok application get downlink end of input else send abort application let clean application get downlink abort log info waiting finish application wait for finish log info got done catch throwable application abort finally application cleanup
1393	mapreduce\src\java\org\apache\hadoop\mapred\pipes\Submitter.java	unrelated	package org apache hadoop mapred pipes the main entry point job submitter it may either used command line based api based method launch pipes jobs submitter extends configured implements tool protected log log log factory get log submitter string preserve commandfile mapreduce pipes commandfile preserve string executable mapreduce pipes executable string interpretor mapreduce pipes executable interpretor string is java map mapreduce pipes isjavamapper string is java rr mapreduce pipes isjavarecordreader string is java rw mapreduce pipes isjavarecordwriter string is java reduce mapreduce pipes isjavareducer string partitioner mapreduce pipes partitioner string input format mapreduce pipes inputformat string port mapreduce pipes command port submitter new configuration submitter configuration conf set conf conf get uri application executable string get executable job conf conf return conf get submitter executable set uri application executable normally hdfs location set executable job conf conf string executable conf set submitter executable executable set whether job using java record reader set is java record reader job conf conf boolean value conf set boolean submitter is java rr value check whether job using java record reader boolean get is java record reader job conf conf return conf get boolean submitter is java rr false set whether mapper written java set is java mapper job conf conf boolean value conf set boolean submitter is java map value check whether job using java mapper boolean get is java mapper job conf conf return conf get boolean submitter is java map false set whether reducer written java set is java reducer job conf conf boolean value conf set boolean submitter is java reduce value check whether job using java reducer boolean get is java reducer job conf conf return conf get boolean submitter is java reduce false set whether job use java record writer set is java record writer job conf conf boolean value conf set boolean submitter is java rw value will reduce use java record writer boolean get is java record writer job conf conf return conf get boolean submitter is java rw false set configuration already value given key set if unset job conf conf string key string value conf get key null conf set key value save away user original partitioner set java partitioner job conf conf class cls conf set submitter partitioner cls get name get user original partitioner class extends partitioner get java partitioner job conf conf return conf get class submitter partitioner hash partitioner partitioner does user want keep command file debugging if true pipes write copy command data file task directory named downlink data may used run c program debugger you probably also want set job conf set keep failed task files true keep entire directory deleted to run using data file set environment variable mapreduce pipes commandfile point file boolean get keep command file job conf conf return conf get boolean submitter preserve commandfile false set whether keep command file debugging set keep command file job conf conf boolean keep conf set boolean submitter preserve commandfile keep submit job map reduce cluster all necessary modifications job run pipes made configuration running job submit job job conf
1394	mapreduce\src\java\org\apache\hadoop\mapred\pipes\UpwardProtocol.java	authenticate	package org apache hadoop mapred pipes the messages come child all calls asynchronous return message processed upward protocol k extends writable comparable v extends writable output record child output k key v value throws io exception map functions application defined partition function output records along partition partitioned output reduce k key v value throws io exception update task status message status string msg throws io exception report making progress current progress progress progress throws io exception report application finished processing inputs successfully done throws io exception report application likely communication failed failed throwable e register counter given id group name register counter id string group string name throws io exception increment value registered counter increment counter id amount throws io exception handles authentication response client it must notify threads waiting authentication response boolean authenticate string digest throws io exception
1395	mapreduce\src\java\org\apache\hadoop\mapred\tools\GetGroups.java	unrelated	package org apache hadoop mapred tools mr implementation tool getting groups given user belongs get groups extends get groups base configuration add default resource mapred default xml configuration add default resource mapred site xml get groups configuration conf super conf get groups configuration conf print stream super conf protected inet socket address get protocol address configuration conf throws io exception return job tracker get address conf main string argv throws exception res tool runner run new get groups new configuration argv system exit res
1396	mapreduce\src\java\org\apache\hadoop\mapred\tools\MRAdmin.java	unrelated	package org apache hadoop mapred tools administrative access hadoop map reduce currently provides ability connect link job tracker refresh service level authorization policy refresh queue acl properties mr admin extends configured implements tool mr admin super mr admin configuration conf super conf print help string cmd string summary hadoop mradmin command execute map reduce administrative commands n the full syntax n n hadoop mradmin refresh service acl refresh queues refresh nodes refresh user to groups mappings refresh super user groups configuration help cmd n string refresh service acl refresh service acl reload service level authorization policy file n jobtracker reload authorization policy file n string refresh queues refresh queues reload queues acls states scheduler specific properties n job tracker reload mapred queues configuration file n string refresh user to groups mappings refresh user to groups mappings refresh user groups mappings n string refresh super user groups configuration refresh super user groups configuration refresh superuser proxy groups mappings n string refresh nodes refresh nodes refresh hosts information jobtracker n string help help cmd displays help given command commands none n tis specified n refresh service acl equals cmd system println refresh service acl else refresh queues equals cmd system println refresh queues else refresh user to groups mappings equals cmd system println refresh user to groups mappings else refresh nodes equals cmd system println refresh nodes else refresh super user groups configuration equals cmd system println refresh super user groups configuration else help equals cmd system println help else system println summary system println refresh service acl system println refresh queues system println refresh user to groups mappings system println refresh super user groups configuration system println refresh nodes system println help system println tool runner print generic command usage system displays format commands print usage string cmd refresh service acl equals cmd system err println usage java mr admin refresh service acl else refresh queues equals cmd system err println usage java mr admin refresh queues else refresh user to groups mappings equals cmd system err println usage java mr admin refresh user to groups mappings else refresh super user groups configuration equals cmd system err println usage java dfs admin refresh super user groups configuration else refresh nodes equals cmd system err println usage java mr admin refresh nodes else system err println usage java mr admin system err println refresh service acl system err println refresh queues system err println refresh user to groups mappings system err println refresh super user groups configuration system err println refresh nodes system err println help cmd system err println tool runner print generic command usage system err user group information get ugi configuration conf throws io exception return user group information get current user refresh authorization policy throws io exception get current configuration configuration conf get conf security authorization server principal call jt one job conf j conf new job conf conf conf set common configuration keys hadoop security service user name key j conf get job tracker jt user name create client refresh authorization policy protocol refresh protocol refresh authorization policy protocol
1397	mapreduce\src\java\org\apache\hadoop\mapred\tools\package-info.java	unrelated	package org apache hadoop mapred tools
1398	mapreduce\src\java\org\apache\hadoop\mapreduce\Cluster.java	unrelated	package org apache hadoop mapreduce provides way access information map reduce cluster cluster enum job tracker status initializing running client protocol provider client protocol provider client protocol client user group information ugi configuration conf file system fs null path sys dir null path staging area dir null path job history dir null config util load resources cluster configuration conf throws io exception conf conf ugi user group information get current user client protocol provider provider service loader load client protocol provider client protocol client protocol provider create conf client protocol null client protocol provider provider client client protocol break cluster inet socket address job track addr configuration conf throws io exception conf conf ugi user group information get current user client protocol provider provider service loader load client protocol provider client protocol client protocol provider create job track addr conf client protocol null client protocol provider provider client client protocol break client protocol get client return client configuration get conf return conf close code cluster code synchronized close throws io exception client protocol provider close client job get jobs job status stats throws io exception list job jobs new array list job job status stat stats jobs add new job stat new job conf stat get job file return jobs array new job get file system job specific files stored synchronized file system get file system throws io exception interrupted exception fs null try fs ugi as new privileged exception action file system file system run throws io exception interrupted exception path sys dir new path client get system dir return sys dir get file system get conf catch interrupted exception e throw new runtime exception e return fs get job corresponding jobid job get job job id job id throws io exception interrupted exception job status status client get job status job id status null return new job status new job conf status get job file return null get queues cluster queue info get queues throws io exception interrupted exception return client get queues get queue information specified name queue info get queue string name throws io exception interrupted exception return client get queue name get current cluster status cluster metrics get cluster status throws io exception interrupted exception return client get cluster metrics get active trackers cluster task tracker info get active task trackers throws io exception interrupted exception return client get active trackers get blacklisted trackers task tracker info get black listed task trackers throws io exception interrupted exception return client get blacklisted trackers get jobs cluster job get all jobs throws io exception interrupted exception return get jobs client get all jobs get job status jobs cluster job status get all job statuses throws io exception interrupted exception return client get all jobs grab jobtracker system directory path job specific files placed path get system dir throws io exception interrupted exception sys dir null sys dir new path client get system dir return sys dir grab jobtracker view staging directory path job specific files placed path get staging area dir throws io exception interrupted exception staging area dir
1399	mapreduce\src\java\org\apache\hadoop\mapreduce\ClusterMetrics.java	unrelated	package org apache hadoop mapreduce status information current state map reduce cluster p code cluster metrics code provides clients information ol li size cluster li li number blacklisted decommissioned trackers li li slot capacity cluster li li the number currently occupied reserved map reduce slots li li the number currently running map reduce tasks li li the number job submissions li ol p p clients query latest code cluster metrics code via link cluster get cluster status p cluster metrics implements writable running maps running reduces occupied map slots occupied reduce slots reserved map slots reserved reduce slots total map slots total reduce slots total job submissions num trackers num blacklisted trackers num decommissioned trackers cluster metrics cluster metrics running maps running reduces occupied map slots occupied reduce slots reserved map slots reserved reduce slots map slots reduce slots total job submissions num trackers num blacklisted trackers num decommissioned nodes running maps running maps running reduces running reduces occupied map slots occupied map slots occupied reduce slots occupied reduce slots reserved map slots reserved map slots reserved reduce slots reserved reduce slots total map slots map slots total reduce slots reduce slots total job submissions total job submissions num trackers num trackers num blacklisted trackers num blacklisted trackers num decommissioned trackers num decommissioned nodes get number running map tasks cluster get running maps return running maps get number running reduce tasks cluster get running reduces return running reduces get number occupied map slots cluster get occupied map slots return occupied map slots get number occupied reduce slots cluster get occupied reduce slots return occupied reduce slots get number reserved map slots cluster get reserved map slots return reserved map slots get number reserved reduce slots cluster get reserved reduce slots return reserved reduce slots get total number map slots cluster get map slot capacity return total map slots get total number reduce slots cluster get reduce slot capacity return total reduce slots get total number job submissions cluster get total job submissions return total job submissions get number active trackers cluster get task tracker count return num trackers get number blacklisted trackers cluster get black listed task tracker count return num blacklisted trackers get number decommissioned trackers cluster get decommissioned task tracker count return num decommissioned trackers read fields data input throws io exception running maps read int running reduces read int occupied map slots read int occupied reduce slots read int reserved map slots read int reserved reduce slots read int total map slots read int total reduce slots read int total job submissions read int num trackers read int num blacklisted trackers read int num decommissioned trackers read int write data output throws io exception write int running maps write int running reduces write int occupied map slots write int occupied reduce slots write int reserved map slots write int reserved reduce slots write int total map slots write int total reduce slots write int total job submissions write int num trackers write int num blacklisted trackers write int num decommissioned trackers
1400	mapreduce\src\java\org\apache\hadoop\mapreduce\Counter.java	unrelated	package org apache hadoop mapreduce a named counter tracks progress map reduce job p code counters code represent global counters defined either map reduce framework applications each code counter code named link enum value p p code counters code bunched groups comprising counters particular code enum code counter implements writable string name string display name value protected counter protected counter string name string display name name name display name display name create counter counter string name string display name value name name display name display name value value protected synchronized set display name string display name display name display name read binary representation counter synchronized read fields data input throws io exception name text read string read boolean display name text read string else display name name value writable utils read v long write binary representation counter synchronized write data output throws io exception text write string name boolean distinct display name name equals display name write boolean distinct display name distinct display name text write string display name writable utils write v long value synchronized string get name return name get name counter synchronized string get display name return display name what current value counter synchronized get value return value set counter given value synchronized set value value value value increment counter given value synchronized increment incr value incr synchronized boolean equals object generic right generic right instanceof counter synchronized generic right counter right counter generic right return name equals right name display name equals right display name value right value return false synchronized hash code return name hash code display name hash code
1401	mapreduce\src\java\org\apache\hadoop\mapreduce\CounterGroup.java	unrelated	package org apache hadoop mapreduce a group link counter logically belong together typically link enum subclass counters values counter group implements writable iterable counter string name string display name tree map string counter counters new tree map string counter optional resource bundle localization group counter names resource bundle bundle null returns specified resource bundle throws exception resource bundle get resource bundle string enum class name string bundle name enum class name replace return resource bundle get bundle bundle name protected counter group string name name name try bundle get resource bundle name catch missing resource exception never mind display name localize counter group name name create counter group counter group string name string display name name name display name display name get internal name group synchronized string get name return name get display name group synchronized string get display name return display name add counter group synchronized add counter counter counter counters put counter get name counter find counter group counter find counter string counter name string display name counter result counters get counter name result null result new counter counter name display name counters put counter name result return result synchronized counter find counter string counter name counter result counters get counter name result null string display name localize counter name counter name result new counter counter name display name counters put counter name result return result synchronized iterator counter iterator return counters values iterator synchronized write data output throws io exception text write string display name writable utils write v int counters size counter counter counters values counter write synchronized read fields data input throws io exception display name text read string counters clear size writable utils read v int size counter counter new counter counter read fields counters put counter get name counter looks key resource bundle returns corresponding value if bundle key exist returns default value string localize string key string default value string result default value bundle null try result bundle get string key catch missing resource exception mre return result returns number counters group synchronized size return counters size synchronized boolean equals object generic right generic right instanceof counter group iterator counter right counter group generic right counters values iterator iterator counter left counters values iterator left next right next left next equals right next return false return right next return false synchronized hash code return counters hash code synchronized incr all counters counter group right group counter right right group counters values counter left find counter right get name right get display name left increment right get value
1402	mapreduce\src\java\org\apache\hadoop\mapreduce\Counters.java	unrelated	package org apache hadoop mapreduce counters implements writable iterable counter group a cache enum values associated counter dramatically speeds typical usage map enum counter cache new identity hash map enum counter tree map string counter group groups new tree map string counter group counters utility method create counters object org apache hadoop mapred counters counters org apache hadoop mapred counters counters org apache hadoop mapred counters group group counters string name group get name counter group new group new counter group name group get display name groups put name new group counter counter group new group add counter counter add group add group counter group group groups put group get name group counter find counter string group name string counter name counter group grp get group group name return grp find counter counter name find counter given enum the enum always return counter synchronized counter find counter enum key counter counter cache get key counter null counter find counter key get declaring class get name key string cache put key counter return counter returns names counter synchronized collection string get group names return groups key set iterator counter group iterator return groups values iterator returns named counter group empty group none specified name synchronized counter group get group string group name counter group grp groups get group name grp null grp new counter group group name groups put group name grp return grp returns total number counters summing number counters group synchronized count counters result counter group group result group size return result write set groups the external format groups group name group e number groups followed groups group form group display name counters false true counter counter form name false true display name value synchronized write data output throws io exception write int groups size org apache hadoop mapreduce counter group group groups values text write string group get name group write read set groups synchronized read fields data input throws io exception num classes read int groups clear num classes string group name text read string counter group group new counter group group name group read fields groups put group name group return textual representation counter values synchronized string string string builder sb new string builder counters count counters counter group group sb append n group get display name counter counter group sb append n counter get display name counter get value return sb string increments multiple counters amounts another counters instance synchronized incr all counters counters map entry string counter group right entry groups entry set counter group left groups get right entry get key counter group right right entry get value left null left new counter group right get name right get display name groups put right entry get key left left incr all counters right boolean equals object generic right generic right instanceof counters iterator counter group right counters generic right groups values iterator iterator counter group left groups values iterator left next right next left next equals right next return false return right next return false hash code return groups hash code
1403	mapreduce\src\java\org\apache\hadoop\mapreduce\ID.java	unrelated	package org apache hadoop mapreduce a general identifier internally stores id integer this super link job id link task id link task attempt id id implements writable comparable id protected char separator protected id constructs id object given id id id id protected id returns represents identifier get id return id string string return string value of id hash code return id boolean equals object return true null return false get class get class id id return id id else return false compare i ds associated numbers compare to id return id id read fields data input throws io exception id read int write data output throws io exception write int id
1404	mapreduce\src\java\org\apache\hadoop\mapreduce\InputFormat.java	unrelated	package org apache hadoop mapreduce code input format code describes input specification map reduce job p the map reduce framework relies code input format code job p ol li validate input specification job li split input file logical link input split assigned individual link mapper li li provide link record reader implementation used glean input records logical code input split code processing link mapper li ol p the default behavior file based link input format typically sub link file input format split input logical link input split based total size bytes input files however link file system blocksize input files treated upper bound input splits a lower bound split size set via href doc root mapred default html mapreduce input fileinputformat split minsize mapreduce input fileinputformat split minsize p p clearly logical splits based input size insufficient many applications since record boundaries respected in cases application also implement link record reader lies responsibility respect record boundaries present record oriented view logical code input split code individual task input format k v logically split set input files job p each link input split assigned individual link mapper processing p p note the split logical split inputs input files physically split chunks for e g split could lt input file path start offset gt tuple the input format also creates link record reader read link input split list input split get splits job context context throws io exception interrupted exception create record reader given split the framework call link record reader initialize input split task attempt context split used record reader k v create record reader input split split task attempt context context throws io exception interrupted exception
1405	mapreduce\src\java\org\apache\hadoop\mapreduce\InputSplit.java	unrelated	package org apache hadoop mapreduce code input split code represents data processed individual link mapper p typically presents byte oriented view input responsibility link record reader job process present record oriented view input split get size split input splits sorted size get length throws io exception interrupted exception get list nodes name data split would local the locations need serialized string get locations throws io exception interrupted exception
1406	mapreduce\src\java\org\apache\hadoop\mapreduce\Job.java	scheduler	package org apache hadoop mapreduce the job submitter view job p it allows user configure job submit control execution query state the set methods work job submitted afterwards throw illegal state exception p p normally user creates application describes various facets job via link job submits job monitor progress p p here example submit job p p blockquote pre create new job job job new job new configuration job set jar by class my job specify various job specific parameters job set job name myjob job set input path new path job set output path new path job set mapper class my job my mapper job set reducer class my job my reducer submit job poll progress job complete job wait for completion true pre blockquote p job extends job context impl implements job context log log log factory get log job enum job state define running max jobstatus age string output filter mapreduce client output filter key mapred xml sets completion poll inverval millis string completion poll interval key mapreduce client completion pollinterval default completion poll interval millis ms default completion poll interval key mapred xml sets prog monitor poll interval millis string progress monitor poll interval key mapreduce client progressmonitor pollinterval default prog monitor poll interval millis ms default monitor poll interval string used generic parser mapreduce client genericoptionsparser used string submit replication mapreduce client submit file replication string tasklog pull timeout key mapreduce client tasklog timeout default tasklog timeout enum task status filter none killed failed succeeded all config util load resources job state state job state define job status status statustime cluster cluster job throws io exception new configuration job configuration conf throws io exception new cluster conf conf job configuration conf string job name throws io exception conf set job name job name job cluster cluster throws io exception cluster new configuration job cluster cluster configuration conf throws io exception super conf null cluster cluster job cluster cluster job status status configuration conf throws io exception cluster conf set job id status get job id status status state job state running creates new link job particular link cluster a cluster created generic link configuration job get instance throws io exception create null cluster return get instance new configuration creates new link job particular link cluster a cluster created conf parameter needed job get instance configuration conf throws io exception create null cluster return new job null conf creates new link job particular link cluster given job name a cluster created conf parameter needed job get instance configuration conf string job name throws io exception create null cluster job result new job null conf result set job name job name return result job get instance cluster cluster throws io exception return new job cluster job get instance cluster cluster configuration conf throws io exception return new job cluster conf job get instance cluster cluster job status status configuration conf throws io exception return new job cluster status conf ensure state job state state throws illegal state exception state state throw new illegal state exception job state state instead
1407	mapreduce\src\java\org\apache\hadoop\mapreduce\JobACL.java	scheduler	package org apache hadoop mapreduce job related ac ls enum job acl acl viewing job dictates view job related details view job mr job config job acl view job acl modifying job dictates modify job e g killing job killing failing task job setting priority job modify job mr job config job acl modify job string acl name job acl string name acl name name get name acl here name configuration property specifying acl job string get acl name return acl name
1408	mapreduce\src\java\org\apache\hadoop\mapreduce\JobContext.java	unrelated	package org apache hadoop mapreduce a read view job provided tasks running job context extends mr job config return configuration job configuration get configuration get credentials job credentials get credentials get unique id job job id get job id get configured number reduce tasks job defaults code code get num reduce tasks get current working directory default file system path get working directory throws io exception get key job output data class get output key class get value job outputs class get output value class get key map output data if set use output key this allows map output key different output key class get map output key class get value map output data if set use output value this allows map output value different output value class get map output value class get user specified job name this used identify job user string get job name get link input format job class extends input format get input format class throws class not found exception get link mapper job class extends mapper get mapper class throws class not found exception get combiner job class extends reducer get combiner class throws class not found exception get link reducer job class extends reducer get reducer class throws class not found exception get link output format job class extends output format get output format class throws class not found exception get link partitioner job class extends partitioner get partitioner class throws class not found exception get link raw comparator comparator used compare keys raw comparator get sort comparator get pathname job jar string get jar get user defined link raw comparator comparator grouping keys inputs reduce raw comparator get grouping comparator get whether job setup job cleanup needed job boolean get job setup cleanup needed get whether task cleanup needed job boolean get task cleanup needed get whether task profiling enabled boolean get profile enabled get profiler configuration arguments the default value property agentlib hprof cpu samples heap sites force n thread verbose n file string get profile params get range maps reduces profile integer ranges get profile task range boolean map get reported username job string get user this method checks see symlinks create localized cache files current working directory boolean get symlink get archive entries classpath array path path get archive class paths get cache archives set configuration uri get cache archives throws io exception get cache files set configuration uri get cache files throws io exception return path array localized caches path get local cache archives throws io exception return path array localized files path get local cache files throws io exception get file entries classpath array path path get file class paths get timestamps archives used internal distributed cache map reduce code string get archive timestamps get timestamps files used internal distributed cache map reduce code string get file timestamps get configured number maximum attempts made run map task specified code mapred map max attempts code property if property already set default attempts get max map attempts get configured number maximum attempts made run reduce task specified code mapred reduce max
1409	mapreduce\src\java\org\apache\hadoop\mapreduce\JobCounter.java	unrelated	package org apache hadoop mapreduce per job counters enum job counter num failed maps num failed reduces total launched maps total launched reduces other local maps data local maps rack local maps slots millis maps slots millis reduces fallow slots millis maps fallow slots millis reduces
1410	mapreduce\src\java\org\apache\hadoop\mapreduce\JobID.java	unrelated	package org apache hadoop mapreduce job id represents immutable unique identifier job job id consists two parts first part represents jobtracker identifier job id jobtracker map defined for cluster setup jobtracker start time local setting local second part job id job number br an example job id code job code represents third job running jobtracker started code code p applications never construct parse job id strings rather use appropriate constructors link name string method job id extends org apache hadoop mapred id implements comparable id protected string job job jobid regex various tools framework components string jobid regex job separator separator text jt identifier protected number format id format number format get instance id format set grouping used false id format set minimum integer digits constructs job id object job id string jt identifier id super id jt identifier new text jt identifier job id jt identifier new text string get jt identifier return jt identifier string boolean equals object super equals return false job id job id return jt identifier equals jt identifier compare job ids first jt identifiers job numbers compare to id job id job id jt comp jt identifier compare to jt identifier jt comp return id id else return jt comp add stuff job prefix given builder this useful sub ids use substring start string builder append to string builder builder builder append separator builder append jt identifier builder append separator builder append id format format id return builder hash code return jt identifier hash code id string string return append to new string builder job string read fields data input throws io exception super read fields jt identifier read fields write data output throws io exception super write jt identifier write construct job id object given job id name string str throws illegal argument exception str null return null try string parts str split parts length parts equals job return new org apache hadoop mapred job id parts integer parse int parts catch exception ex fall throw new illegal argument exception job id str properly formed
1411	mapreduce\src\java\org\apache\hadoop\mapreduce\JobPriority.java	unrelated	package org apache hadoop mapreduce used describe priority running job enum job priority very high high normal low very low
1412	mapreduce\src\java\org\apache\hadoop\mapreduce\JobStatus.java	unrelated	package org apache hadoop mapreduce describes current status job job status implements writable cloneable register ctor writable factories set factory job status new writable factory writable new instance return new job status current state job enum state running succeeded failed prep killed value state value value value get value return value job id jobid map progress reduce progress cleanup progress setup progress state run state start time string user string queue job priority priority string scheduling info na map job acl access control list job ac ls new hash map job acl access control list string job name string job file finish time boolean retired string history file string tracking url job status create job status object given jobid job status job id jobid setup progress map progress reduce progress cleanup progress state run state job priority jp string user string job name string job file string tracking url jobid setup progress map progress reduce progress cleanup progress run state jp user job name default job file tracking url create job status object given jobid job status job id jobid setup progress map progress reduce progress cleanup progress state run state job priority jp string user string job name string queue string job file string tracking url jobid jobid setup progress setup progress map progress map progress reduce progress reduce progress cleanup progress cleanup progress run state run state user user queue queue jp null throw new illegal argument exception job priority cannot null priority jp job name job name job file job file tracking url tracking url sets map progress job protected synchronized set map progress p map progress math min math max p sets cleanup progress job protected synchronized set cleanup progress p cleanup progress math min math max p sets setup progress job protected synchronized set setup progress p setup progress math min math max p sets reduce progress job protected synchronized set reduce progress p reduce progress math min math max p set priority job defaulting normal protected synchronized set priority job priority jp jp null throw new illegal argument exception job priority cannot null priority jp set finish time job protected synchronized set finish time finish time finish time finish time set job history file url completed job protected synchronized set history file string history file history file history file set link web ui details job protected synchronized set tracking url string tracking url tracking url tracking url set job retire flag true protected synchronized set retired retired true change current run state job protected synchronized set state state state run state state set start time job protected synchronized set start time start time start time start time protected synchronized set username string user name user user name used set scheduling information associated particular job protected synchronized set scheduling info string scheduling info scheduling info scheduling info set job acls protected synchronized set job ac ls map job acl access control list acls job ac ls acls set queue name protected synchronized set queue string queue queue queue get queue name synchronized string get queue return queue
1413	mapreduce\src\java\org\apache\hadoop\mapreduce\JobSubmissionFiles.java	unrelated	package org apache hadoop mapreduce a utility manage job submission files job submission files job submission directory fs permission job dir permission fs permission create immutable short rwx job files world wide readable owner writable fs permission job file permission fs permission create immutable short rw r r path get job split file path job submission dir return new path job submission dir job split path get job split meta file path job submission dir return new path job submission dir job splitmetainfo get job conf path path get job conf path path job submit dir return new path job submit dir job xml get job jar path path get job jar path job submit dir return new path job submit dir job jar get job distributed cache files path path get job dist cache files path job submit dir return new path job submit dir files get job distributed cache archives path path get job dist cache archives path job submit dir return new path job submit dir archives get job distributed cache libjars path path get job dist cache libjars path job submit dir return new path job submit dir libjars initializes staging directory returns path it also keeps track necessary ownership permissions path get staging dir cluster cluster configuration conf throws io exception interrupted exception path staging area cluster get staging area dir file system fs staging area get file system conf string real user string current user user group information ugi user group information get login user real user ugi get short user name current user user group information get current user get short user name fs exists staging area file status fs status fs get file status staging area string owner fs status get owner owner equals current user owner equals real user fs status get permission equals job dir permission throw new io exception the ownership permissions staging directory staging area expected it owned owner permissions fs status get permission the directory must owned submitter current user real user permissions must rwx else fs mkdirs staging area new fs permission job dir permission return staging area
1414	mapreduce\src\java\org\apache\hadoop\mapreduce\JobSubmitter.java	unrelated	package org apache hadoop mapreduce job submitter protected log log log factory get log job submitter file system jt fs client protocol submit client string submit host name string submit host address job submitter file system submit fs client protocol submit client throws io exception submit client submit client jt fs submit fs see two file systems boolean compare fs file system src fs file system dest fs uri src uri src fs get uri uri dst uri dest fs get uri src uri get scheme null return false src uri get scheme equals dst uri get scheme return false string src host src uri get host string dst host dst uri get host src host null dst host null try src host inet address get by name src host get canonical host name dst host inet address get by name dst host get canonical host name catch unknown host exception ue return false src host equals dst host return false else src host null dst host null return false else src host null dst host null return false check ports src uri get port dst uri get port return false return true copies file jobtracker filesystem returns path copied path copy remote files path parent dir path original path configuration conf short replication throws io exception check need copy files jt using file system checking uri strings dns lookups see filesystems this optimal avoids name resolution file system remote fs null remote fs original path get file system conf compare fs remote fs jt fs return original path might name collisions copy throw exception parse original path create new path path new path new path parent dir original path get name file util copy remote fs original path jt fs new path false conf jt fs set replication new path replication return new path configures files libjars archives copy and configure files job job path submit job dir short replication throws io exception configuration conf job get configuration conf get boolean job used generic parser false log warn use generic options parser parsing arguments applications implement tool get command line arguments passed user conf string files conf get tmpfiles string libjars conf get tmpjars string archives conf get tmparchives string job jar job get jar figure fs job tracker using copy job temporary name this allows dfs work local fs also provides unix like object loading semantics job file deleted right submission still run submission completion create number filenames job tracker fs namespace log debug default file system jt fs get uri jt fs exists submit job dir throw new io exception not submitting job job directory submit job dir already exists this unexpected please check directory submit job dir jt fs make qualified submit job dir submit job dir new path submit job dir uri get path fs permission mapred sys perms new fs permission job submission files job dir permission file system mkdirs jt fs submit job dir mapred sys perms path files dir job submission files get job dist cache files submit job dir path archives dir job
1415	mapreduce\src\java\org\apache\hadoop\mapreduce\MapContext.java	unrelated	package org apache hadoop mapreduce the context given link mapper map context keyin valuein keyout valueout extends task input output context keyin valuein keyout valueout get input split map input split get input split
1416	mapreduce\src\java\org\apache\hadoop\mapreduce\Mapper.java	unrelated	package org apache hadoop mapreduce maps input key value pairs set intermediate key value pairs p maps individual tasks transform input records intermediate records the transformed intermediate records need type input records a given input pair may map zero many output pairs p p the hadoop map reduce framework spawns one map task link input split generated link input format job code mapper code implementations access link configuration job via link job context get configuration p the framework first calls link setup org apache hadoop mapreduce mapper context followed link map object object context key value pair code input split code finally link cleanup context called p p all intermediate values associated given output key subsequently grouped framework passed link reducer determine output users control sorting grouping specifying two key link raw comparator p p the code mapper code outputs partitioned per code reducer code users control keys hence records go code reducer code implementing custom link partitioner p users optionally specify code combiner code via link job set combiner class class perform local aggregation intermediate outputs helps cut amount data transferred code mapper code code reducer code p applications specify intermediate outputs compressed link compression codec used via code configuration code p p if job zero reduces output code mapper code directly written link output format without sorting keys p p example p p blockquote pre token counter mapper extends mapper lt object text text int writable gt int writable one new int writable text word new text map object key text value context context throws io exception interrupted exception string tokenizer itr new string tokenizer value string itr more tokens word set itr next token context write word one pre blockquote p p applications may link run context method exert greater control map processing e g multi threaded code mapper code etc p mapper keyin valuein keyout valueout the code context code passed link mapper implementations context implements map context keyin valuein keyout valueout called beginning task protected setup context context throws io exception interrupted exception nothing called key value pair input split most applications default identity function protected map keyin key valuein value context context throws io exception interrupted exception context write keyout key valueout value called end task protected cleanup context context throws io exception interrupted exception nothing expert users method complete control execution mapper run context context throws io exception interrupted exception setup context context next key value map context get current key context get current value context cleanup context
1417	mapreduce\src\java\org\apache\hadoop\mapreduce\MarkableIterator.java	unrelated	package org apache hadoop mapreduce code markable iterator code wrapper iterator implements link markable iterator interface markable iterator value implements markable iterator interface value markable iterator interface value base iterator create new iterator layered input iterator markable iterator iterator value itr itr instanceof markable iterator interface throw new illegal argument exception input iterator markable base iterator markable iterator interface value itr mark throws io exception base iterator mark reset throws io exception base iterator reset clear mark throws io exception base iterator clear mark boolean next return base iterator next value next return base iterator next remove throw new unsupported operation exception remove not implemented
1418	mapreduce\src\java\org\apache\hadoop\mapreduce\MarkableIteratorInterface.java	unrelated	package org apache hadoop mapreduce code markable iterator interface code iterator supports mark reset functionality p mark called point iteration process reset go back last record call previous mark markable iterator interface value extends iterator value mark current record a subsequent call reset rewind iterator record mark throws io exception reset iterator last record call previous mark reset throws io exception clear previously set mark clear mark throws io exception
1419	mapreduce\src\java\org\apache\hadoop\mapreduce\MRConfig.java	unrelated	package org apache hadoop mapreduce place holder cluster level configuration keys these keys used link job tracker link task tracker the keys mapreduce cluster prefix mr config cluster level configuration parameters string temp dir mapreduce cluster temp dir string local dir mapreduce cluster local dir string mapmemory mb mapreduce cluster mapmemory mb string reducememory mb mapreduce cluster reducememory mb string mr acls enabled mapreduce cluster acls enabled string mr admins mapreduce cluster administrators string mr supergroup mapreduce cluster permissions supergroup delegation token related keys string delegation key update interval key mapreduce cluster delegation key update interval delegation key update interval default day string delegation token renew interval key mapreduce cluster delegation token renew interval delegation token renew interval default day string delegation token max lifetime key mapreduce cluster delegation token max lifetime delegation token max lifetime default days string framework name mapreduce framework name
1420	mapreduce\src\java\org\apache\hadoop\mapreduce\MRJobConfig.java	scheduler	package org apache hadoop mapreduce mr job config put attribute names job job context consistent string input format class attr mapreduce job inputformat string map class attr mapreduce job map string combine class attr mapreduce job combine string reduce class attr mapreduce job reduce string output format class attr mapreduce job outputformat string partitioner class attr mapreduce job partitioner string setup cleanup needed mapreduce job committer setup cleanup needed string task cleanup needed mapreduce job committer task cleanup needed string jar mapreduce job jar string id mapreduce job id string job name mapreduce job name string jar unpack pattern mapreduce job jar unpack pattern string user name mapreduce job user name string priority mapreduce job priority string queue name mapreduce job queuename string jvm numtasks torun mapreduce job jvm numtasks string split file mapreduce job splitfile string num maps mapreduce job maps string max task failures per tracker mapreduce job maxtaskfailures per tracker string completed maps for reduce slowstart mapreduce job reduce slowstart completedmaps string num reduces mapreduce job reduces string skip records mapreduce job skiprecords string skip outdir mapreduce job skip outdir string speculative slownode threshold mapreduce job speculative slownodethreshold string speculative slowtask threshold mapreduce job speculative slowtaskthreshold string speculativecap mapreduce job speculative speculativecap string job local dir mapreduce job local dir string output key class mapreduce job output key string output value class mapreduce job output value string key comparator mapreduce job output key comparator string group comparator class mapreduce job output group comparator string working dir mapreduce job working dir string end notification url mapreduce job end notification url string end notification retries mapreduce job end notification retry attempts string end notification retrie interval mapreduce job end notification retry interval string classpath archives mapreduce job classpath archives string classpath files mapreduce job classpath files string cache files mapreduce job cache files string cache archives mapreduce job cache archives string cache files sizes mapreduce job cache files filesizes internal use string cache archives sizes mapreduce job cache archives filesizes ditto string cache localfiles mapreduce job cache local files string cache localarchives mapreduce job cache local archives string cache file timestamps mapreduce job cache files timestamps string cache archives timestamps mapreduce job cache archives timestamps string cache file visibilities mapreduce job cache files visibilities string cache archives visibilities mapreduce job cache archives visibilities string cache symlink mapreduce job cache symlink create string user log retain hours mapreduce job userlog retain hours string io sort factor mapreduce task io sort factor string io sort mb mapreduce task io sort mb string index cache memory limit mapreduce task index cache limit bytes string preserve failed task files mapreduce task files preserve failedtasks string preserve files pattern mapreduce task files preserve filepattern string task temp dir mapreduce task tmp dir string task debugout lines mapreduce task debugout lines string records before progress mapreduce task merge progress records string skip start attempts mapreduce task skip start attempts string task attempt id mapreduce task attempt id string task ismap mapreduce task ismap string task partition mapreduce task partition string task profile mapreduce task profile string task
1421	mapreduce\src\java\org\apache\hadoop\mapreduce\OutputCommitter.java	unrelated	package org apache hadoop mapreduce code output committer code describes commit task output map reduce job p the map reduce framework relies code output committer code job p ol li setup job initialization for example create temporary output directory job initialization job li li cleanup job job completion for example remove temporary output directory job completion li li setup task temporary output li li check whether task needs commit this avoid commit procedure task need commit li li commit task output li li discard task commit li ol output committer for framework setup job output initialization setup job job context job context throws io exception for cleaning job output job completion link abort job job context job status state instead cleanup job job context job context throws io exception for committing job output successful job completion note invoked jobs runstate successful commit job job context job context throws io exception cleanup job job context for aborting unsuccessful job output note invoked jobs runstate link job status state failed link job status state killed abort job job context job context job status state state throws io exception cleanup job job context sets output task setup task task attempt context task context throws io exception check whether task needs commit boolean needs task commit task attempt context task context throws io exception to promote task temporary output output location the task output moved job output directory commit task task attempt context task context throws io exception discard task output abort task task attempt context task context throws io exception
1422	mapreduce\src\java\org\apache\hadoop\mapreduce\OutputFormat.java	unrelated	package org apache hadoop mapreduce code output format code describes output specification map reduce job p the map reduce framework relies code output format code job p ol li validate output specification job for e g check output directory already exist li provide link record writer implementation used write output files job output files stored link file system li ol output format k v get link record writer given task record writer k v get record writer task attempt context context throws io exception interrupted exception check validity output specification job p this validate output specification job job submitted typically checks already exist throwing exception already exists output overwritten p check output specs job context context throws io exception interrupted exception get output committer output format this responsible ensuring output committed correctly output committer get output committer task attempt context context throws io exception interrupted exception
1423	mapreduce\src\java\org\apache\hadoop\mapreduce\Partitioner.java	unrelated	package org apache hadoop mapreduce partitions key space p code partitioner code controls partitioning keys intermediate map outputs the key subset key used derive partition typically hash function the total number partitions number reduce tasks job hence controls code code reduce tasks intermediate key hence record sent reduction p note if require partitioner obtain job configuration object implement link configurable partitioner key value get partition number given key hence record given total number partitions e number reduce tasks job p typically hash function subset key p get partition key key value value num partitions
1424	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueAclsInfo.java	unrelated	package org apache hadoop mapreduce class encapsulate queue ac ls particular user queue acls info implements writable string queue name string operations default constructor queue acls info queue acls info construct new queue acls info object using queue name queue operations array queue acls info string queue name string operations queue name queue name operations operations get queue name string get queue name return queue name protected set queue name string queue name queue name queue name get opearations allowed queue string get operations return operations read fields data input throws io exception queue name text read string operations writable utils read string array write data output throws io exception text write string queue name writable utils write string array operations
1425	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueInfo.java	unrelated	package org apache hadoop mapreduce class contains information regarding job queues maintained hadoop map reduce framework queue info implements writable string queue name the scheduling information object read back string once scheduling information set way recover string scheduling info queue state queue state jobs submitted queue job status stats list queue info children properties props default constructor queue info queue info make running default queue state queue state running children new array list queue info props new properties construct new queue info object using queue name scheduling information passed queue queue info string queue name string scheduling info queue name queue name scheduling info scheduling info queue info string queue name string scheduling info queue state state job status stats queue name scheduling info queue state state stats stats set queue name job queue info protected set queue name string queue name queue name queue name get queue name job queue info string get queue name return queue name set scheduling information associated particular job queue protected set scheduling info string scheduling info scheduling info scheduling info gets scheduling information associated particular job queue if nothing set would return b n a b string get scheduling info scheduling info null return scheduling info else return n a set state queue protected set state queue state state queue state state return queue state queue state get state return queue state protected set job statuses job status stats stats stats get immediate children list queue info get queue children return children protected set queue children list queue info children children children get properties properties get properties return props protected set properties properties props props props get jobs submitted queue job status get job statuses return stats read fields data input throws io exception queue name text read string queue state writable utils read enum queue state scheduling info text read string length read int stats new job status length length stats new job status stats read fields count read int children clear count queue info child queue info new queue info child queue info read fields children add child queue info write data output throws io exception text write string queue name writable utils write enum queue state scheduling info null text write string scheduling info else text write string n a write int stats length job status stat stats stat write write int children size queue info child queue info children child queue info write
1426	mapreduce\src\java\org\apache\hadoop\mapreduce\QueueState.java	unrelated	package org apache hadoop mapreduce enum representing queue state enum queue state stopped stopped running running undefined undefined string state name map string queue state enum map new hash map string queue state queue state state queue state values enum map put state get state name state queue state string state name state name state name string get state name return state name queue state get state string state queue state q state enum map get state q state null return undefined return q state string string return state name
1427	mapreduce\src\java\org\apache\hadoop\mapreduce\RecordReader.java	unrelated	package org apache hadoop mapreduce the record reader breaks data key value pairs input link mapper record reader keyin valuein implements closeable called initialization initialize input split split task attempt context context throws io exception interrupted exception read next key value pair boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception interrupted exception the current progress record reader data get progress throws io exception interrupted exception close record reader close throws io exception
1428	mapreduce\src\java\org\apache\hadoop\mapreduce\RecordWriter.java	unrelated	package org apache hadoop mapreduce code record writer code writes output lt key value gt pairs output file p code record writer code implementations write job outputs link file system record writer k v writes key value pair write k key v value throws io exception interrupted exception close code record writer code future operations close task attempt context context throws io exception interrupted exception
1429	mapreduce\src\java\org\apache\hadoop\mapreduce\ReduceContext.java	unrelated	package org apache hadoop mapreduce the context passed link reducer reduce context keyin valuein keyout valueout extends task input output context keyin valuein keyout valueout start processing next unique key boolean next key throws io exception interrupted exception iterate values current key reusing value object stored context objects returned directly indirectly method reused iterable valuein get values throws io exception interrupted exception link iterator iterate values given group records value iterator valuein extends markable iterator interface valuein this method called reducer moves one key another reset backup store throws io exception
1430	mapreduce\src\java\org\apache\hadoop\mapreduce\Reducer.java	unrelated	package org apache hadoop mapreduce reduces set intermediate values share key smaller set values p code reducer code implementations access link configuration job via link job context get configuration method p p code reducer code primary phases p ol li id shuffle shuffle p the code reducer code copies sorted output link mapper using http across network p li li id sort sort p the framework merge sorts code reducer code inputs code key code since different code mapper code may output key p p the shuffle sort phases occur simultaneously e outputs fetched merged p id secondary sort secondary sort p to achieve secondary sort values returned value iterator application extend key secondary key define grouping comparator the keys sorted using entire key grouped using grouping comparator decide keys values sent call reduce the grouping comparator specified via link job set grouping comparator class class the sort order controlled link job set sort comparator class class p for example say want find duplicate web pages tag url best known example you would set job like ul li map input key url li li map input value document li li map output key document checksum url pagerank li li map output value url li li partitioner checksum li li output key comparator checksum decreasing pagerank li li output value grouping comparator checksum li ul li li id reduce reduce p in phase link reduce object iterable context method called code lt key collection values gt code sorted inputs p p the output reduce task typically written link record writer via link context write object object p li ol p the output code reducer code b sorted b p p example p p blockquote pre int sum reducer lt key gt extends reducer lt key int writable key int writable gt int writable result new int writable reduce key key iterable lt int writable gt values context context throws io exception interrupted exception sum int writable val values sum val get result set sum context write key result pre blockquote p reducer keyin valuein keyout valueout the code context code passed link reducer implementations context implements reduce context keyin valuein keyout valueout called start task protected setup context context throws io exception interrupted exception nothing this method called key most applications define reduce overriding method the default implementation identity function protected reduce keyin key iterable valuein values context context throws io exception interrupted exception valuein value values context write keyout key valueout value called end task protected cleanup context context throws io exception interrupted exception nothing advanced application writers use link run org apache hadoop mapreduce reducer context method control reduce task works run context context throws io exception interrupted exception setup context context next key reduce context get current key context get values context if back store used reset reduce context value iterator context get values iterator reset backup store cleanup context
1431	mapreduce\src\java\org\apache\hadoop\mapreduce\StatusReporter.java	unrelated	package org apache hadoop mapreduce status reporter counter get counter enum name counter get counter string group string name progress get current progress progress get progress set status string status
1432	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskAttemptContext.java	unrelated	package org apache hadoop mapreduce the context task attempts task attempt context extends job context progressable get unique name task attempt task attempt id get task attempt id set current status task given set status string msg get last set status message string get status the current progress task attempt progress get progress get link counter given code counter name code counter get counter enum counter name get link counter given code group name code code counter name code code counter name code counter get counter string group name string counter name
1433	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskAttemptID.java	unrelated	package org apache hadoop mapreduce task attempt id represents immutable unique identifier task attempt each task attempt one particular instance map reduce task identified task id task attempt id consists parts first part link task id task attempt id belongs second part task attempt number br an example task attempt id code attempt code represents zeroth task attempt fifth map task third job running jobtracker started code code p applications never construct parse task attempt id strings rather use appropriate constructors link name string method task attempt id extends org apache hadoop mapred id protected string attempt attempt task id task id constructs task attempt id object given link task id task attempt id task id task id id super id task id null throw new illegal argument exception task id cannot null task id task id constructs task id object given parts task attempt id string jt identifier job id task type type task id id new task id jt identifier job id type task id id task attempt id task id new task id returns link job id object task attempt belongs job id get job id return task id get job id returns link task id object task attempt belongs task id get task id return task id returns task type task attempt id task type get task type return task id get task type boolean equals object super equals return false task attempt id task attempt id return task id equals task id add unique string builder protected string builder append to string builder builder return task id append to builder append separator append id read fields data input throws io exception super read fields task id read fields write data output throws io exception super write task id write hash code return task id hash code id compare task ids first tip ids task numbers compare to id task attempt id task attempt id tip comp task id compare to task id tip comp return id id else return tip comp string string return append to new string builder attempt string construct task attempt id object given task attempt id name string str throws illegal argument exception str null return null try string parts str split character string separator parts length parts equals attempt string type parts task type task id get task type type char at null return new org apache hadoop mapred task attempt id parts integer parse int parts integer parse int parts integer parse int parts else throw new exception catch exception ex fall throw new illegal argument exception task attempt id str properly formed
1434	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskCompletionEvent.java	unrelated	package org apache hadoop mapreduce this used track task completion events job tracker task completion event implements writable enum status failed killed succeeded obsolete tipfailed event id string task tracker http task run time using since runtime time difference task attempt id task id status status boolean map false id within job task completion event empty array new task completion event default constructor writable task completion event task id new task attempt id constructor event id created externally incremented per event job incrementally starting task completion event event id task attempt id task id id within job boolean map status status string task tracker http task id task id id within job id within job map map event id event id status status task tracker http task tracker http returns event id get event id return event id returns task id task attempt id get task attempt id return task id returns enum status sucess status failure status get status return status http location tasktracker task ran string get task tracker http return task tracker http returns time millisec task took complete get task run time return task run time set task completion time protected set task run time task completion time task run time task completion time set event id assigned incrementally starting protected set event id event id event id event id sets task id protected set task attempt id task attempt id task id task id task id set task status protected set task status status status status status set task tracker http location protected set task tracker http string task tracker http task tracker http task tracker http string string string buffer buf new string buffer buf append task id buf append task id buf append status buf append status name return buf string boolean equals object null return false get class equals get class task completion event event task completion event return map event map task event id event get event id id within job event id within job status equals event get status task id equals event get task attempt id task run time event get task run time task tracker http equals event get task tracker http return false hash code return string hash code boolean map task return map id within job return id within job writable write data output throws io exception task id write writable utils write v int id within job write boolean map writable utils write enum status writable utils write string task tracker http writable utils write v int task run time writable utils write v int event id read fields data input throws io exception task id read fields id within job writable utils read v int map read boolean status writable utils read enum status task tracker http writable utils read string task run time writable utils read v int event id writable utils read v int
1435	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskCounter.java	unrelated	package org apache hadoop mapreduce counters used task enum task counter map input records map output records map skipped records map output bytes map output materialized bytes split raw bytes combine input records combine output records reduce input groups reduce shuffle bytes reduce input records reduce output records reduce skipped groups reduce skipped records spilled records shuffled maps failed shuffle merged map outputs gc time millis cpu milliseconds physical memory bytes virtual memory bytes committed heap bytes
1436	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskID.java	unrelated	package org apache hadoop mapreduce task id represents immutable unique identifier map reduce task each task id encompasses multiple attempts made execute map reduce task uniquely indentified task attempt id task id consists parts first part link job id task in progress belongs second part task id either r representing whether task map task reduce task and third part task number br an example task id code task code represents fifth map task third job running jobtracker started code code p applications never construct parse task id strings rather use appropriate constructors link name string method task id extends org apache hadoop mapred id protected string task task protected number format id format number format get instance id format set grouping used false id format set minimum integer digits job id job id task type type constructs task id object given link job id task id job id job id task type type id super id job id null throw new illegal argument exception job id cannot null job id job id type type constructs task in progress id object given parts task id string jt identifier job id task type type id new job id jt identifier job id type id task id job id new job id returns link job id object tip belongs job id get job id return job id get type task task type get task type return type boolean equals object super equals return false task id task id return type type job id equals job id compare task in progress ids first job ids tip numbers reduces defined greater maps compare to id task id task id job comp job id compare to job id job comp type type return id id else return type compare to type else return job comp string string return append to new string builder task string add unique given builder protected string builder append to string builder builder return job id append to builder append separator append char task type maps get representing character type append separator append id format format id hash code return job id hash code id read fields data input throws io exception super read fields job id read fields type writable utils read enum task type write data output throws io exception super write job id write writable utils write enum type construct task id object given task id name string str throws illegal argument exception str null return null try string parts str split parts length parts equals task string type parts task type char task type maps get task type type char at null return new org apache hadoop mapred task id parts integer parse int parts integer parse int parts else throw new exception catch exception ex fall throw new illegal argument exception task id str properly formed gets character representing link task type char get representing character task type type return char task type maps get representing character type gets link task type corresponding character task type get task type char c return char task type maps get task type
1437	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskInputOutputContext.java	unrelated	package org apache hadoop mapreduce a context object allows input output task it supplied link mapper link reducer task input output context keyin valuein keyout valueout extends task attempt context advance next key value pair returning null end boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception interrupted exception generate output key value pair write keyout key valueout value throws io exception interrupted exception get link output committer task attempt output committer get output committer
1438	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskReport.java	unrelated	package org apache hadoop mapreduce a report state task task report implements writable task id taskid progress string state string diagnostics start time finish time counters counters tip status current status collection task attempt id running attempts new array list task attempt id task attempt id successful attempt new task attempt id task report taskid new task id creates new task report object task report task id taskid progress string state string diagnostics tip status current status start time finish time counters counters taskid taskid progress progress state state diagnostics diagnostics current status current status start time start time finish time finish time counters counters the id task task id get task id return taskid the amount completed zero one get progress return progress the recent state reported reporter string get state return state a list error messages string get diagnostics return diagnostics a table counters counters get task counters return counters the current status tip status get current status return current status get finish time task get finish time return finish time set finish time task protected set finish time finish time finish time finish time get start time task get start time return start time set start time task protected set start time start time start time start time set successful attempt id task protected set successful attempt id task attempt id successful attempt get attempt id took task completion task attempt id get successful task attempt id return successful attempt set running attempt task protected set running task attempt ids collection task attempt id running attempts running attempts running attempts get running task attempt i ds task collection task attempt id get running task attempt ids return running attempts boolean equals object null return false get class equals get class task report report task report return counters equals report get task counters arrays string diagnostics equals arrays string report get diagnostics finish time report get finish time progress report get progress start time report get start time state equals report get state taskid equals report get task id return false hash code return counters string arrays string diagnostics finish time progress start time state taskid string hash code writable write data output throws io exception taskid write write float progress text write string state write long start time write long finish time writable utils write string array diagnostics counters write writable utils write enum current status current status tip status running writable utils write v int running attempts size task attempt id new task attempt id running attempts array length write else current status tip status complete successful attempt write read fields data input throws io exception taskid read fields progress read float state text read string start time read long finish time read long diagnostics writable utils read string array counters new counters counters read fields current status writable utils read enum tip status current status tip status running num writable utils read v int num task attempt id new task attempt id read fields running attempts add else current status tip status complete successful attempt read fields
1439	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskTrackerInfo.java	unrelated	package org apache hadoop mapreduce information task tracker task tracker info implements writable string name boolean blacklisted false string reason for blacklist string blacklist report task tracker info construct active tracker task tracker info string name name name construct blacklisted tracker task tracker info string name string reason for blacklist string report name name blacklisted true reason for blacklist reason for blacklist blacklist report report gets tasktracker name string get task tracker name return name whether tracker blacklisted false otherwise boolean blacklisted return blacklisted gets reason tasktracker blacklisted string get reason for blacklist return reason for blacklist gets descriptive report tasktracker blacklisted string get blacklist report return blacklist report read fields data input throws io exception name text read string blacklisted read boolean reason for blacklist text read string blacklist report text read string write data output throws io exception text write string name write boolean blacklisted text write string reason for blacklist text write string blacklist report
1440	mapreduce\src\java\org\apache\hadoop\mapreduce\TaskType.java	unrelated	package org apache hadoop mapreduce enum map reduce job setup job cleanup task cleanup task types enum task type map reduce job setup job cleanup task cleanup
1441	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\ClientDistributedCacheManager.java	unrelated	package org apache hadoop mapreduce filecache manages internal configuration cache client job submission client distributed cache manager determines timestamps files cached stores configuration this intended used internally job client cache files added this internal method determine timestamps configuration job throws io exception uri tarchives distributed cache get cache archives job tarchives null file status status get file status job tarchives string builder archive file sizes new string builder string value of status get len string builder archive timestamps new string builder string value of status get modification time tarchives length status get file status job tarchives archive file sizes append archive file sizes append string value of status get len archive timestamps append archive timestamps append string value of status get modification time job set mr job config cache archives sizes archive file sizes string set archive timestamps job archive timestamps string uri tfiles distributed cache get cache files job tfiles null file status status get file status job tfiles string builder file sizes new string builder string value of status get len string builder file timestamps new string builder string value of status get modification time tfiles length status get file status job tfiles file sizes append file sizes append string value of status get len file timestamps append file timestamps append string value of status get modification time job set mr job config cache files sizes file sizes string set file timestamps job file timestamps string for archive cache file get corresponding delegation token get delegation tokens configuration job credentials credentials throws io exception uri tarchives distributed cache get cache archives job uri tfiles distributed cache get cache files job size tarchives null tarchives length tfiles null tfiles length path ps new path size tarchives null tarchives length ps new path tarchives string tfiles null j j tfiles length j ps j new path tfiles j string token cache obtain tokens for namenodes credentials ps job determines visibilities distributed cache files archives the visibility cache path leaf component read permissions others parent subdirs execute permissions others determine cache visibilities configuration job throws io exception uri tarchives distributed cache get cache archives job tarchives null string builder archive visibilities new string builder string value of public job tarchives tarchives length archive visibilities append archive visibilities append string value of public job tarchives set archive visibilities job archive visibilities string uri tfiles distributed cache get cache files job tfiles null string builder file visibilities new string builder string value of public job tfiles tfiles length file visibilities append file visibilities append string value of public job tfiles set file visibilities job file visibilities string this check visibility archives localized the order order archives added set archive visibilities configuration conf string booleans conf set mr job config cache archives visibilities booleans this check visibility files localized the order order files added set file visibilities configuration conf string booleans conf set mr job config cache file visibilities booleans this check timestamp archives localized the order order archives added set archive timestamps configuration conf string timestamps conf set mr job config cache archives timestamps timestamps
1442	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\DistributedCache.java	unrelated	package org apache hadoop mapreduce filecache distribute application specific large read files efficiently p code distributed cache code facility provided map reduce framework cache files text archives jars etc needed applications p p applications specify files via urls hdfs http cached via link org apache hadoop mapred job conf the code distributed cache code assumes files specified via urls already present link file system path specified url accessible every machine cluster p p the framework copy necessary files slave node tasks job executed node its efficiency stems fact files copied per job ability cache archives un archived slaves p p code distributed cache code used distribute simple read data text files complex types archives jars etc archives zip tar tgz tar gz files un archived slave nodes jars may optionally added classpath tasks rudimentary software distribution mechanism files execution permissions optionally users also direct symlink distributed cache file working directory task p p code distributed cache code tracks modification timestamps cache files clearly cache files modified application externally job executing p p here illustrative example use code distributed cache code p p blockquote pre setting cache application copy requisite files code file system code bin hadoop fs copy from local lookup dat myapp lookup dat bin hadoop fs copy from local map zip myapp map zip bin hadoop fs copy from local mylib jar myapp mylib jar bin hadoop fs copy from local mytar tar myapp mytar tar bin hadoop fs copy from local mytgz tgz myapp mytgz tgz bin hadoop fs copy from local mytargz tar gz myapp mytargz tar gz setup application code job conf code job conf job new job conf distributed cache add cache file new uri myapp lookup dat lookup dat job distributed cache add cache archive new uri myapp map zip job distributed cache add file to class path new path myapp mylib jar job distributed cache add cache archive new uri myapp mytar tar job distributed cache add cache archive new uri myapp mytgz tgz job distributed cache add cache archive new uri myapp mytargz tar gz job use cached files link org apache hadoop mapred mapper link org apache hadoop mapred reducer map class extends map reduce base implements mapper lt k v k v gt path local archives path local files configure job conf job get cached archives files local archives distributed cache get local cache archives job local files distributed cache get local cache files job map k key v value output collector lt k v gt output reporter reporter throws io exception use data cached archives files output collect k v pre blockquote p it also common use distributed cache using link org apache hadoop util generic options parser this includes methods used users specifically mentioned example well link distributed cache add archive to class path path configuration well methods intended use map reduce framework e g link org apache hadoop mapred job client distributed cache set configuration given set archives intended used user code set cache archives uri archives configuration conf string sarchives string utils uri to string archives conf
1443	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\package-info.java	unrelated	package org apache hadoop mapreduce filecache
1444	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\TaskDistributedCacheManager.java	unrelated	package org apache hadoop mapreduce filecache helper link tracker distributed cache manager represents cached files single task this used task runner local job runner parse job configuration setup local caches task distributed cache manager tracker distributed cache manager distributed cache manager configuration task conf list cache file cache files new array list cache file list string paths new array list string boolean setup called false struct representing single cached file there four permutations archive file put classpath put classpath cache file uri configuration uri uri enum file type regular archive boolean public true whether decompress file type type timestamp whether added classpath boolean be added to class path boolean localized false the owner localized file relevant tasktrackers string owner cache file uri uri file type type boolean public timestamp boolean path throws io exception uri uri type type public public timestamp timestamp be added to class path path owner tracker distributed cache manager get localized cache owner public converts scheme used distributed cache serialize files cache configuration cache file objects represent files list cache file make cache files uri uris string timestamps string cache visibilities path paths file type type throws io exception list cache file ret new array list cache file uris null uris length timestamps length throw new illegal argument exception mismatched uris timestamps map string path paths new hash map string path paths null path p paths paths put p uri get path string p uris length uri u uris boolean class path null paths get u get path long parse long timestamps ret add new cache file u type boolean value of cache visibilities class path return ret boolean get localized return localized set localized boolean val localized val task distributed cache manager tracker distributed cache manager distributed cache manager configuration task conf throws io exception distributed cache manager distributed cache manager task conf task conf cache files add all cache file make cache files distributed cache get cache files task conf distributed cache get file timestamps task conf tracker distributed cache manager get file visibilities task conf distributed cache get file class paths task conf cache file file type regular cache files add all cache file make cache files distributed cache get cache archives task conf distributed cache get archive timestamps task conf tracker distributed cache manager get archive visibilities task conf distributed cache get archive class paths task conf cache file file type archive retrieve files local cache updates task configuration passed via constructor it caller responsibility write task configuration xml file necessary setup local dir allocator dir alloc file work dir string cache subdir string cache sub dir throws io exception setup called true cache files empty return array list path local archives new array list path array list path local files new array list path path workdir path new path work dir get absolute path cache file cache file cache files uri uri cache file uri file system file system file system get uri task conf file status file status file system get file status new path uri get path string cache subdir cache
1445	mapreduce\src\java\org\apache\hadoop\mapreduce\filecache\TrackerDistributedCacheManager.java	pooling	package org apache hadoop mapreduce filecache manages single machine instance cross job cache this would typically instantiated task tracker something emulates like local job runner tracker distributed cache manager cache id cache status mapping linked hash map string cache status cached archives new linked hash map string cache status default total cache size gb default cache size l default cache subdir limit this default pulled air set based read world use cases default cache keep around pct f allowed cache size allowed cache subdirs allowed cache size cleanup goal allowed cache subdirs cleanup goal log log log factory get log tracker distributed cache manager local file system local fs local dir allocator dir allocator task controller task controller configuration tracker conf random random new random mr async disk service async disk service protected base dir manager base dir manager new base dir manager protected cleanup thread cleanup thread tracker distributed cache manager configuration conf task controller task controller throws io exception local fs file system get local conf tracker conf conf dir allocator new local dir allocator tt config local dir task controller task controller setting cache size default gb allowed cache size conf get long tt config tt local cache size default cache size setting cache number subdirectories limit default allowed cache subdirs conf get long tt config tt local cache subdirs limit default cache subdir limit cleanup pct conf get float tt config tt local cache keep around pct default cache keep around pct allowed cache size cleanup goal allowed cache size cleanup pct allowed cache subdirs cleanup goal allowed cache subdirs cleanup pct cleanup thread new cleanup thread conf creates tracker distributed cache manager mr async disk service operations tracker distributed cache manager configuration conf task controller task controller mr async disk service async disk service throws io exception conf task controller async disk service async disk service get locally cached file archive could either previously cached valid copy link file system new uri scheme scheme specific part absolute path file linkname files archives archive zip jar tar tgz tar gz extension unzipped unjarred untarred automatically directory archive unzipped unjarred untarred returned path in case file path file returned file cached changed since job started symlinks locally cached files archives created even conf says required optimization task launches note this effectively always since r since code path use archives path file file copied locally path get local cache uri cache configuration conf string sub dir file status file status boolean archive conf file stamp path current work dir boolean honor sym link conf boolean public throws io exception string key key get key cache conf conf file stamp get localized cache owner public archive cache status lcache status synchronized cached archives lcache status cached archives get key lcache status null never localized string unique string string value of random next long string cache path new path sub dir new path unique string make relative cache conf string path local path dir allocator get local path for write cache path file status get len tracker conf lcache status new cache
1446	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\EventReader.java	scheduler	package org apache hadoop mapreduce jobhistory event reader implements closeable string version schema schema data input stream decoder decoder datum reader reader create new event reader event reader file system fs path name throws io exception fs open name create new event reader event reader data input stream throws io exception version read line event writer version equals version throw new io exception incompatible event log version version schema schema parse read line reader new specific datum reader schema decoder new json decoder schema get next event stream history event get next event throws io exception event wrapper try wrapper event reader read null decoder catch eof exception e eof return null history event result switch wrapper type case job submitted result new job submitted event break case job inited result new job inited event break case job finished result new job finished event break case job priority changed result new job priority change event break case job status changed result new job status changed event break case job failed result new job unsuccessful completion event break case job killed result new job unsuccessful completion event break case job info changed result new job info change event break case task started result new task started event break case task finished result new task finished event break case task failed result new task failed event break case task updated result new task updated event break case map attempt started result new task attempt started event break case map attempt finished result new map attempt finished event break case map attempt failed result new task attempt unsuccessful completion event break case map attempt killed result new task attempt unsuccessful completion event break case reduce attempt started result new task attempt started event break case reduce attempt finished result new reduce attempt finished event break case reduce attempt failed result new task attempt unsuccessful completion event break case reduce attempt killed result new task attempt unsuccessful completion event break case setup attempt started result new task attempt started event break case setup attempt finished result new task attempt finished event break case setup attempt failed result new task attempt unsuccessful completion event break case setup attempt killed result new task attempt unsuccessful completion event break case cleanup attempt started result new task attempt started event break case cleanup attempt finished result new task attempt finished event break case cleanup attempt failed result new task attempt unsuccessful completion event break case cleanup attempt killed result new task attempt unsuccessful completion event break default throw new runtime exception unexpected event type result set datum wrapper event return result close event reader close throws io exception null close null counters avro jh counters counters counters result new counters jh counter group g counters groups counter group group new counter group g name string g display name string jh counter c g counts group add counter new counter c name string c display name string c value result add group group return result
1447	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\EventWriter.java	unrelated	package org apache hadoop mapreduce jobhistory event writer utility used write events underlying stream typically one event writer translates one stream created per job event writer string version avro json fs data output stream datum writer event writer new specific datum writer event event encoder encoder event writer fs data output stream throws io exception write bytes version write bytes n write bytes event schema string write bytes n encoder new json encoder event schema synchronized write history event event throws io exception event wrapper new event wrapper type event get event type wrapper event event get datum writer write wrapper encoder encoder flush write bytes n flush throws io exception encoder flush flush close throws io exception encoder flush close schema groups schema create array jh counter group schema schema counters schema create array jh counter schema jh counters avro counters counters return avro counters counters jh counters avro counters counters string name jh counters result new jh counters result name new utf name result groups new generic data array jh counter group groups counters null return result counter group group counters jh counter group g new jh counter group g name new utf group get name g display name new utf group get display name g counts new generic data array jh counter group size counters counter counter group jh counter c new jh counter c name new utf counter get name c display name new utf counter get display name c value counter get value g counts add c result groups add g return result
1448	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\HistoryEvent.java	unrelated	package org apache hadoop mapreduce jobhistory interface event wrapper implementations wrap avro generated adding constructors accessor methods history event return event type event type get event type return avro datum wrapped object get datum set avro datum wrapped set datum object datum
1449	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\HistoryViewer.java	unrelated	package org apache hadoop mapreduce jobhistory history viewer used parse view job history files history viewer simple date format date format new simple date format mmm yyyy hh mm ss file system fs job info job string job id boolean print all constructs history viewer object history viewer string history file configuration conf boolean print all throws io exception print all print all string error msg unable initialize history viewer try path job file new path history file fs job file get file system conf string job details job file get name split job details length not valid name system err println ignore unrecognized file job file get name throw new io exception error msg job history parser parser new job history parser fs job file job parser parse job id job get job id string catch exception e throw new io exception error msg e print job task attempt summary information print throws io exception print job details print task summary print job analysis print tasks task type job setup task status state failed string print tasks task type job setup task status state killed string print tasks task type map task status state failed string print tasks task type map task status state killed string print tasks task type reduce task status state failed string print tasks task type reduce task status state killed string print tasks task type job cleanup task status state failed string print tasks task type job cleanup job status get job run state job status killed print all print tasks task type job setup task status state succeeded string print tasks task type map task status state succeeded string print tasks task type reduce task status state succeeded string print tasks task type job cleanup task status state succeeded string print all task attempts task type job setup print all task attempts task type map print all task attempts task type reduce print all task attempts task type job cleanup filtered job filter new filtered job job task status state failed string print failed attempts filter filter new filtered job job task status state killed string print failed attempts filter print job details string buffer job details new string buffer job details append n hadoop job append job get job id job details append n job details append n user append job get username job details append n job name append job get jobname job details append n job conf append job get job conf path job details append n submitted at append string utils get formatted time with diff date format job get submit time job details append n launched at append string utils get formatted time with diff date format job get launch time job get submit time job details append n finished at append string utils get formatted time with diff date format job get finish time job get launch time job details append n status append job get job status null incomplete job get job status print counters job details job get total counters job get map counters job get reduce
1450	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion job job finished event implements history event job finished datum new job finished create event record successful job completion job finished event job id id finish time finished maps finished reduces failed maps failed reduces counters map counters counters reduce counters counters total counters datum jobid new utf id string datum finish time finish time datum finished maps finished maps datum finished reduces finished reduces datum failed maps failed maps datum failed reduces failed reduces datum map counters event writer avro map counters map counters datum reduce counters event writer avro reduce counters reduce counters datum total counters event writer avro total counters total counters job finished event object get datum return datum set datum object datum datum job finished datum event type get event type return event type job finished get job id job id get jobid return job id name datum jobid string get job finish time get finish time return datum finish time get number finished maps job get finished maps return datum finished maps get number finished reducers job get finished reduces return datum finished reduces get number failed maps job get failed maps return datum failed maps get number failed reducers job get failed reduces return datum failed reduces get counters job counters get total counters return event reader avro datum total counters get map counters job counters get map counters return event reader avro datum map counters get reduce counters job counters get reduce counters return event reader avro datum reduce counters
1451	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobHistory.java	pooling	package org apache hadoop mapreduce jobhistory job history responsible creating maintaining job history information job history log log log factory get log job history job history block size map job id meta info file map collections job id meta info synchronized map new hash map job id meta info thread pool executor executor null fs permission history dir permission fs permission create immutable short rwxr x fs permission history file permission fs permission create immutable short rwxr job tracker job tracker default history max age l week file system log dir fs log dir fs file system done dir fs done dir fs path log dir null path done null folder completed jobs string old suffix old version prefix history files string history version history cleaner history cleaner thread null map job id moved file info job history file map collections job id moved file info synchronized map new linked hash map job id moved file info job history filename regex pattern jobhistory filename regex pattern compile job id jobid regex job history conf filename regex pattern conf filename regex pattern compile job id jobid regex conf xml old moved file info string history file timestamp moved file info string history file timestamp history file history file timestamp timestamp initialize job history module init job tracker jt job conf conf string hostname job tracker start time throws io exception get create log folder string log dir loc conf get jt config jt jobhistory location file new file system get property hadoop log dir get absolute path file separator history log info history log directory log dir loc log dir new path log dir loc log dir fs log dir get file system conf log dir fs exists log dir log dir fs mkdirs log dir new fs permission history dir permission throw new io exception mkdirs failed create log dir string conf set jt config jt jobhistory location log dir loc job history block size conf get long jt config jt jobhistory block size job tracker jt initialize done directory start history cleaner thread init done job conf conf file system fs throws io exception completed job history location set use string done location conf get jt config jt jobhistory completed location done location null path done path new path done location done dir fs done path get file system conf done done dir fs make qualified done path else done log dir fs make qualified new path log dir done done dir fs log dir fs if already present create done folder appropriate permission done dir fs exists done log info creating done folder done done dir fs mkdirs done new fs permission history dir permission throw new io exception mkdirs failed create done string log info inited done directory done string move old files start file mover threads start history cleaner thread max age of history files conf get long jt config jt jobhistory maxage default history max age history cleaner thread new history cleaner max age of history files history cleaner thread start move completed job completed folder this
1452	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobHistoryParser.java	unrelated	package org apache hadoop mapreduce jobhistory default parser job history files typical usage job history parser parser new job history parser fs history file job parser parse job history parser fs data input stream job info info null create job history parser given history file using given file system job history parser file system fs string file throws io exception fs new path file create job history parser given history file using given file system job history parser file system fs path history file throws io exception fs open history file create history parser based input stream job history parser fs data input stream parse entire history file populate job info object the first invocation populate object subsequent calls return already parsed object the input stream closed return synchronized job info parse throws io exception info null return info event reader reader new event reader history event event info new job info try event reader get next event null handle event event finally close return info handle event history event event throws io exception event type type event get event type switch type case job submitted handle job submitted event job submitted event event break case job status changed break case job info changed handle job info change event job info change event event break case job inited handle job inited event job inited event event break case job priority changed handle job priority change event job priority change event event break case job failed case job killed handle job failed event job unsuccessful completion event event break case job finished handle job finished event job finished event event break case task started handle task started event task started event event break case task failed handle task failed event task failed event event break case task updated handle task updated event task updated event event break case task finished handle task finished event task finished event event break case map attempt started case cleanup attempt started case reduce attempt started case setup attempt started handle task attempt started event task attempt started event event break case map attempt failed case cleanup attempt failed case reduce attempt failed case setup attempt failed case map attempt killed case cleanup attempt killed case reduce attempt killed case setup attempt killed handle task attempt failed event task attempt unsuccessful completion event event break case map attempt finished handle map attempt finished event map attempt finished event event break case reduce attempt finished handle reduce attempt finished event reduce attempt finished event event break case setup attempt finished case cleanup attempt finished handle task attempt finished event task attempt finished event event break default break handle task attempt finished event task attempt finished event event task info task info info tasks map get event get task id task attempt info attempt info task info attempts map get event get attempt id attempt info finish time event get finish time attempt info status event get task status attempt info state event get state attempt info counters event get counters attempt info hostname event get hostname handle reduce attempt
1453	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobInfoChangeEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record changes submit launch time job job info change event implements history event job info change datum new job info change create event record submit launch time job job info change event job id id submit time launch time datum jobid new utf id string datum submit time submit time datum launch time launch time job info change event object get datum return datum set datum object datum datum job info change datum get job id job id get job id return job id name datum jobid string get job submit time get submit time return datum submit time get job launch time get launch time return datum launch time event type get event type return event type job info changed
1454	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobInitedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record initialization job job inited event implements history event job inited datum new job inited create event record job initialization job inited event job id id launch time total maps total reduces string job status datum jobid new utf id string datum launch time launch time datum total maps total maps datum total reduces total reduces datum job status new utf job status job inited event object get datum return datum set datum object datum datum job inited datum get job id job id get job id return job id name datum jobid string get launch time get launch time return datum launch time get total number maps get total maps return datum total maps get total number reduces get total reduces return datum total reduces get status string get status return datum job status string get event type event type get event type return event type job inited
1455	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobPriorityChangeEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record change priority job job priority change event implements history event job priority change datum new job priority change generate event record changes job priority job priority change event job id id job priority priority datum jobid new utf id string datum priority new utf priority name job priority change event object get datum return datum set datum object datum datum job priority change datum get job id job id get job id return job id name datum jobid string get job priority job priority get priority return job priority value of datum priority string get event type event type get event type return event type job priority changed
1456	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobStatusChangedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record change status job job status changed event implements history event job status changed datum new job status changed create event record change job status job status changed event job id id string job status datum jobid new utf id string datum job status new utf job status job status changed event object get datum return datum set datum object datum datum job status changed datum get job id job id get job id return job id name datum jobid string get event status string get status return datum job status string get event type event type get event type return event type job status changed
1457	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobSubmittedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record submission job job submitted event implements history event job submitted datum new job submitted create event record job submission job submitted event job id id string job name string user name submit time string job conf path map job acl access control list job ac ls string job queue name datum jobid new utf id string datum job name new utf job name datum user name new utf user name datum submit time submit time datum job conf path new utf job conf path map utf utf job acls new hash map utf utf entry job acl access control list entry job ac ls entry set job acls put new utf entry get key get acl name new utf entry get value get acl string datum acls job acls job queue name null datum job queue name new utf job queue name job submitted event object get datum return datum set datum object datum datum job submitted datum get job id job id get job id return job id name datum jobid string get job name string get job name return datum job name string get job queue name string get job queue name datum job queue name null return datum job queue name string return null get user name string get user name return datum user name string get submit time get submit time return datum submit time get path job configuration file string get job conf path return datum job conf path string get acls configured job map job acl access control list get job acls map job acl access control list job acls new hash map job acl access control list job acl job acl job acl values utf job ac ls utf new utf job acl get acl name datum acls contains key job ac ls utf job acls put job acl new access control list datum acls get job ac ls utf string return job acls get event type event type get event type return event type job submitted
1458	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\JobUnsuccessfulCompletionEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record failed killed completion jobs job unsuccessful completion event implements history event job unsuccessful completion datum new job unsuccessful completion create event record unsuccessful completion killed failed jobs job unsuccessful completion event job id id finish time finished maps finished reduces string status datum jobid new utf id string datum finish time finish time datum finished maps finished maps datum finished reduces finished reduces datum job status new utf status job unsuccessful completion event object get datum return datum set datum object datum datum job unsuccessful completion datum get job id job id get job id return job id name datum jobid string get job finish time get finish time return datum finish time get number finished maps get finished maps return datum finished maps get number finished reduces get finished reduces return datum finished reduces get status string get status return datum job status string get event type event type get event type failed equals get status return event type job failed else return event type job killed
1459	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\MapAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion map attempt map attempt finished event implements history event map attempt finished datum new map attempt finished create event successful completion map attempts map attempt finished event task attempt id id task type task type string task status map finish time finish time string hostname string state counters counters datum taskid new utf id get task id string datum attempt id new utf id string datum task type new utf task type name datum task status new utf task status datum map finish time map finish time datum finish time finish time datum hostname new utf hostname datum state new utf state datum counters event writer avro counters map attempt finished event object get datum return datum set datum object datum datum map attempt finished datum get task id task id get task id return task id name datum taskid string get attempt id task attempt id get attempt id return task attempt id name datum attempt id string get task type task type get task type return task type value of datum task type string get task status string get task status return datum task status string get map phase finish time get map finish time return datum map finish time get attempt finish time get finish time return datum finish time get host name string get hostname return datum hostname string get state string get state return datum state string get counters counters get counters return event reader avro datum counters get event type event type get event type return event type map attempt finished
1460	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\package-info.java	unrelated	package org apache hadoop mapreduce jobhistory
1461	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\ReduceAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion reduce attempt reduce attempt finished event implements history event reduce attempt finished datum new reduce attempt finished create event record completion reduce attempt reduce attempt finished event task attempt id id task type task type string task status shuffle finish time sort finish time finish time string hostname string state counters counters datum taskid new utf id get task id string datum attempt id new utf id string datum task type new utf task type name datum task status new utf task status datum shuffle finish time shuffle finish time datum sort finish time sort finish time datum finish time finish time datum hostname new utf hostname datum state new utf state datum counters event writer avro counters reduce attempt finished event object get datum return datum set datum object datum datum reduce attempt finished datum get task id task id get task id return task id name datum taskid string get attempt id task attempt id get attempt id return task attempt id name datum attempt id string get task type task type get task type return task type value of datum task type string get task status string get task status return datum task status string get finish time sort phase get sort finish time return datum sort finish time get finish time shuffle phase get shuffle finish time return datum shuffle finish time get finish time attempt get finish time return datum finish time get name host attempt ran string get hostname return datum hostname string get state string get state return datum state string get counters attempt counters get counters return event reader avro datum counters get event type event type get event type return event type reduce attempt finished
1462	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful task completion task attempt finished event implements history event task attempt finished datum new task attempt finished create event record successful finishes setup cleanup attempts task attempt finished event task attempt id id task type task type string task status finish time string hostname string state counters counters datum taskid new utf id get task id string datum attempt id new utf id string datum task type new utf task type name datum task status new utf task status datum finish time finish time datum hostname new utf hostname datum state new utf state datum counters event writer avro counters task attempt finished event object get datum return datum set datum object datum datum task attempt finished datum get task id task id get task id return task id name datum taskid string get task attempt id task attempt id get attempt id return task attempt id name datum attempt id string get task type task type get task type return task type value of datum task type string get task status string get task status return datum task status string get attempt finish time get finish time return datum finish time get host attempt executed string get hostname return datum hostname string get state string get state return datum state string get counters attempt counters get counters return event reader avro datum counters get event type event type get event type note task type setup map reduce cleanup attempt type map reduce return get task id get task type task type map event type map attempt finished event type reduce attempt finished
1463	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptStartedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record start task attempt task attempt started event implements history event task attempt started datum new task attempt started create event record start attempt task attempt started event task attempt id attempt id task type task type start time string tracker name http port datum attempt id new utf attempt id string datum taskid new utf attempt id get task id string datum start time start time datum task type new utf task type name datum tracker name new utf tracker name datum http port http port task attempt started event object get datum return datum set datum object datum datum task attempt started datum get task id task id get task id return task id name datum taskid string get tracker name string get tracker name return datum tracker name string get start time get start time return datum start time get task type task type get task type return task type value of datum task type string get http port get http port return datum http port get attempt id task attempt id get task attempt id return task attempt id name datum attempt id string get event type event type get event type note task type setup map reduce cleanup attempt type map reduce return get task id get task type task type map event type map attempt started event type reduce attempt started
1464	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskAttemptUnsuccessfulCompletionEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record unsuccessful killed failed completion task attempts task attempt unsuccessful completion event implements history event task attempt unsuccessful completion datum new task attempt unsuccessful completion create event record unsuccessful completion attempts task attempt unsuccessful completion event task attempt id id task type task type string status finish time string hostname string error datum taskid new utf id get task id string datum task type new utf task type name datum attempt id new utf id string datum finish time finish time datum hostname new utf hostname datum error new utf error datum status new utf status task attempt unsuccessful completion event object get datum return datum set datum object datum datum task attempt unsuccessful completion datum get task id task id get task id return task id name datum taskid string get task type task type get task type return task type value of datum task type string get attempt id task attempt id get task attempt id return task attempt id name datum attempt id string get finish time get finish time return datum finish time get name host attempt executed string get hostname return datum hostname string get error string get error return datum error string get task status string get task status return datum status string get event type event type get event type note task type setup map reduce cleanup attempt type map reduce find task failed got killed boolean failed task status state failed string equals get task status return get task id get task type task type map failed event type map attempt failed event type map attempt killed failed event type reduce attempt failed event type reduce attempt killed
1465	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskFailedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record failure task task failed event implements history event task failed datum new task failed create event record task failure task failed event task id id finish time task type task type string error string status task attempt id failed due to attempt datum taskid new utf id string datum error new utf error datum finish time finish time datum task type new utf task type name datum failed due to attempt failed due to attempt null null new utf failed due to attempt string datum status new utf status task failed event object get datum return datum set datum object datum datum task failed datum get task id task id get task id return task id name datum taskid string get error string get error return datum error string get finish time attempt get finish time return datum finish time get task type task type get task type return task type value of datum task type string get attempt id due task failed task attempt id get failed attempt id return datum failed due to attempt null null task attempt id name datum failed due to attempt string get task status string get task status return datum status string get event type event type get event type return event type task failed
1466	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskFinishedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record successful completion task task finished event implements history event task finished datum new task finished create event record successful completion task task finished event task id id finish time task type task type string status counters counters datum taskid new utf id string datum finish time finish time datum counters event writer avro counters datum task type new utf task type name datum status new utf status task finished event object get datum return datum set datum object datum datum task finished datum get task id task id get task id return task id name datum taskid string get task finish time get finish time return datum finish time get task counters counters get counters return event reader avro datum counters get task type task type get task type return task type value of datum task type string get task status string get task status return datum status string get event type event type get event type return event type task finished
1467	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskStartedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record start task task started event implements history event task started datum new task started create event record start task task started event task id id start time task type task type string split locations datum taskid new utf id string datum split locations new utf split locations datum start time start time datum task type new utf task type name task started event object get datum return datum set datum object datum datum task started datum get task id task id get task id return task id name datum taskid string get split locations applicable map tasks string get split locations return datum split locations string get start time task get start time return datum start time get task type task type get task type return task type value of datum task type string get event type event type get event type return event type task started
1468	mapreduce\src\java\org\apache\hadoop\mapreduce\jobhistory\TaskUpdatedEvent.java	unrelated	package org apache hadoop mapreduce jobhistory event record updates task task updated event implements history event task updated datum new task updated create event record task updates task updated event task id id finish time datum taskid new utf id string datum finish time finish time task updated event object get datum return datum set datum object datum datum task updated datum get task id task id get task id return task id name datum taskid string get task finish time get finish time return datum finish time get event type event type get event type return event type task updated
1469	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\DoubleValueSum.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator sums sequence values double value sum implements value aggregator string sum the default constructor double value sum reset add value aggregator object whose representation represents value add next value object val sum double parse double val string add value aggregator value add next value val sum val string get report return sum get sum return sum reset aggregator reset sum representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add sum return retv
1470	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueMax.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain maximum sequence values long value max implements value aggregator string max val long min value default constructor long value max reset add value aggregator object whose representation represents value add next value object val new val long parse long val string max val new val max val new val add value aggregator value add next value new val max val new val max val new val get val return max val string get report return max val reset aggregator reset max val long min value representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add max val return retv
1471	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueMin.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain minimum sequence values long value min implements value aggregator string min val long max value default constructor long value min reset add value aggregator object whose representation represents value add next value object val new val long parse long val string min val new val min val new val add value aggregator value add next value new val min val new val min val new val get val return min val string get report return min val reset aggregator reset min val long max value representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add min val return retv
1472	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\LongValueSum.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator sums sequence values long value sum implements value aggregator string sum default constructor long value sum reset add value aggregator object whose representation represents value add next value object val sum long parse long val string add value aggregator value add next value val sum val get sum return sum string get report return sum reset aggregator reset sum representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add sum return retv
1473	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\StringValueMax.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain biggest sequence strings string value max implements value aggregator string string max val null default constructor string value max reset add value aggregator add next value object val string new val val string max val null max val compare to new val max val new val string get val return max val string get report return max val reset aggregator reset max val null representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add max val return retv
1474	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\StringValueMin.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator maintain smallest sequence strings string value min implements value aggregator string string min val null default constructor string value min reset add value aggregator add next value object val string new val val string min val null min val compare to new val min val new val string get val return min val string get report return min val reset aggregator reset min val null representation aggregated value the return value expected used combiner array list string get combiner output array list string retv new array list string retv add min val return retv
1475	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\UniqValueCount.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator dedupes sequence objects uniq value count implements value aggregator object string max num unique values mapreduce aggregate max num unique values tree map object object uniq items null num items max num items long max value default constructor uniq value count long max value constructor uniq value count max num uniq items new tree map object object num items max num items long max value max num max num items max num set limit number unique values set max items n n num items max num items n else max num items num items max num items num items return max num items add value aggregator object add next value object val num items max num items uniq items put val string num items uniq items size string get report return uniq items size set object get unique items return uniq items key set reset aggregator reset uniq items new tree map object object expected used combiner array list object get combiner output object key null iterator object iter uniq items key set iterator array list object retv new array list object iter next key iter next retv add key return retv
1476	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\UserDefinedValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements wrapper user defined value aggregator descriptor it serves two functions one create object value aggregator descriptor name user defined may dynamically loaded the delegate invocations generate key val pairs function created object user defined value aggregator descriptor implements value aggregator descriptor string name protected value aggregator descriptor aggregator descriptor null class arg array new class create instance given object create instance string name object retv null try class loader loader thread current thread get context class loader class filter class class name name true loader constructor meth filter class get declared constructor arg array meth set accessible true retv meth new instance catch exception e throw new runtime exception e return retv create aggregator configuration conf aggregator descriptor null aggregator descriptor value aggregator descriptor create instance name aggregator descriptor configure conf user defined value aggregator descriptor string name configuration conf name name create aggregator conf generate list aggregation id value pairs given key value pairs delegating invocation real object input key input value aggregation type used guide way aggregate value reduce combiner phrase aggregate based job array list entry text text generate key val pairs object key object val array list entry text text retv new array list entry text text aggregator descriptor null retv aggregator descriptor generate key val pairs key val return retv string string return user defined value aggregator descriptor name name do nothing configure configuration conf
1477	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregator.java	unrelated	package org apache hadoop mapreduce lib aggregate this defines minimal protocol value aggregators value aggregator e add value aggregator add next value object val reset aggregator reset string get report array list e get combiner output
1478	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorBaseDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements common functionalities subclasses value aggregator descriptor value aggregator base descriptor implements value aggregator descriptor string uniq value count uniq value count string long value sum long value sum string double value sum double value sum string value histogram value histogram string long value max long value max string long value min long value min string string value max string value max string string value min string value min string input file null my entry implements entry text text text key text val text get key return key text get value return val text set value text val val val return val my entry text key text val key key val val aggregation type entry text text generate entry string type string id text val text key new text type type separator id return new my entry key val type uniq value count value aggregator generate value aggregator string type uniq count type compare to ignore case long value sum return new long value sum type compare to ignore case long value max return new long value max else type compare to ignore case long value min return new long value min else type compare to ignore case string value max return new string value max else type compare to ignore case string value min return new string value min else type compare to ignore case double value sum return new double value sum else type compare to ignore case uniq value count return new uniq value count uniq count else type compare to ignore case value histogram return new value histogram return null generate aggregation id value pairs given key value pair the first id type long value sum record count aggregation id if input file split second id type generated file name aggregation id this achieves behavior counting total number records input data number records input file input key input value aggregation type used guide way aggregate value reduce combiner phrase aggregate based job array list entry text text generate key val pairs object key object val array list entry text text retv new array list entry text text string count type long value sum string id record count entry text text e generate entry count type id one e null retv add e input file null e generate entry count type input file one e null retv add e return retv get input file name configure configuration conf input file conf get mr job config map input file
1479	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorCombiner.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic combiner aggregate value aggregator combiner k extends writable comparable v extends writable extends reducer text text text text combines values given key type aggregation aggregate values reduce text key iterable text values context context throws io exception interrupted exception string key str key string pos key str index of value aggregator descriptor type separator string type key str substring pos uniq count context get configuration get long uniq value count max num unique values long max value value aggregator aggregator value aggregator base descriptor generate value aggregator type uniq count text val values aggregator add next value val iterator outputs aggregator get combiner output iterator outputs next object v outputs next v instanceof text context write key text v else context write key new text v string
1480	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorDescriptor.java	unrelated	package org apache hadoop mapreduce lib aggregate this defines contract value aggregator descriptor must support such descriptor configured link configuration object its main function generate list aggregation id value pairs an aggregation id encodes aggregation type used guide way aggregate value reduce combiner phrase aggregate based job the mapper aggregate based map reduce job may create one value aggregator descriptor objects configuration time for input key value pair mapper use objects create aggregation id value pairs value aggregator descriptor string type separator text one new text generate list aggregation id value pairs given key value pair this function usually called mapper aggregate based job input key input value aggregation type used guide way aggregate value reduce combiner phrase aggregate based job array list entry text text generate key val pairs object key object val configure object configuration object may contain information used configure object configure configuration conf
1481	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorJob.java	unrelated	package org apache hadoop mapreduce lib aggregate this main creating map reduce job using aggregate framework the aggregate specialization map reduce framework specializing performing various simple aggregations generally speaking order implement application using map reduce model developer implement map reduce functions possibly combine function however lot applications related counting statistics computing similar characteristics aggregate abstracts general patterns functions implementing patterns in particular package provides generic mapper redducer combiner set built value aggregators generic utility helps user create map reduce jobs using generic the built aggregators sum numeric values count number distinct values compute histogram values compute minimum maximum media average standard deviation numeric values the developer using aggregate need provide plugin conforming following value aggregator descriptor array list entry generate key val pairs object key object value configure configuration conf the package also provides base value aggregator base descriptor implementing the user extend base implement generate key val pairs accordingly the primary work generate key val pairs emit one key value pairs based input key value pair the key output key value pair encode two pieces information aggregation type aggregation id the value aggregated onto aggregation id according aggregation type this offers function generate map reduce job using aggregate framework the function takes following parameters input directory spec input format text sequence file output directory file specifying user plugin value aggregator job job control create value aggregator jobs string args class extends value aggregator descriptor descriptors throws io exception job control control new job control value aggregator jobs array list controlled job depending jobs new array list controlled job configuration conf new configuration descriptors null conf set aggregator descriptors descriptors job job create value aggregator job conf args controlled job cjob new controlled job job depending jobs control add job cjob return control job control create value aggregator jobs string args throws io exception return create value aggregator jobs args null create aggregate based map reduce job arguments accepted job create value aggregator job configuration conf string args throws io exception generic options parser generic parser new generic options parser conf args args generic parser get remaining args args length system println usage input dirs dir num of reducer textinputformat seq specfile job name generic options parser print generic command usage system system exit string input dir args string output dir args num of reducers args length num of reducers integer parse int args class extends input format input format null args length args compare to ignore case textinputformat input format text input format else input format sequence file input format path spec file null args length spec file new path args string job name args length job name args spec file null conf add resource spec file string user jar file conf get value aggregator job base user jar user jar file null conf set mr job config jar user jar file job job new job conf user jar file null job set jar by class value aggregator job set job name value aggregator job job name file input format add input paths job input dir job set input format class
1482	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorJobBase.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements common functionalities generic mapper reducer combiner aggregate value aggregator job base k extends writable comparable v extends writable string descriptor mapreduce aggregate descriptor string descriptor num mapreduce aggregate descriptor num string user jar mapreduce aggregate user jar file protected array list value aggregator descriptor aggregator descriptor list null setup configuration job initialize my spec job log spec protected value aggregator descriptor get value aggregator descriptor string spec configuration conf spec null return null string segments spec split string type segments type compare to ignore case user defined string name segments return new user defined value aggregator descriptor name conf return null protected array list value aggregator descriptor get aggregator descriptors configuration conf num conf get int descriptor num array list value aggregator descriptor retv new array list value aggregator descriptor num num string spec conf get descriptor value aggregator descriptor ad get value aggregator descriptor spec conf ad null retv add ad return retv initialize my spec configuration conf aggregator descriptor list get aggregator descriptors conf aggregator descriptor list size aggregator descriptor list add new user defined value aggregator descriptor value aggregator base descriptor get canonical name conf protected log spec
1483	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorMapper.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic mapper aggregate value aggregator mapper k extends writable comparable v extends writable extends mapper k v text text setup context context throws io exception interrupted exception value aggregator job base setup context get configuration map function it iterates value aggregator descriptor list generate aggregation id value pairs emit map k key v value context context throws io exception interrupted exception iterator iter value aggregator job base aggregator descriptor list iterator iter next value aggregator descriptor ad value aggregator descriptor iter next iterator entry text text ens ad generate key val pairs key value iterator ens next entry text text en ens next context write en get key en get value
1484	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueAggregatorReducer.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements generic reducer aggregate value aggregator reducer k extends writable comparable v extends writable extends reducer text text text text setup context context throws io exception interrupted exception value aggregator job base setup context get configuration key expected text object whose prefix indicates type aggregation aggregate values in effect data driven computing achieved it assumed aggregator get report method emits appropriate output aggregator this may customized reduce text key iterable text values context context throws io exception interrupted exception string key str key string pos key str index of value aggregator descriptor type separator string type key str substring pos key str key str substring pos value aggregator descriptor type separator length uniq count context get configuration get long uniq value count max num unique values long max value value aggregator aggregator value aggregator base descriptor generate value aggregator type uniq count text value values aggregator add next value value string val aggregator get report key new text key str context write key new text val
1485	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\aggregate\ValueHistogram.java	unrelated	package org apache hadoop mapreduce lib aggregate this implements value aggregator computes histogram sequence strings value histogram implements value aggregator string tree map object object items null value histogram items new tree map object object add given val aggregator form xxxx tnum meaning xxxx num occurrences add next value object val string val count str val string pos val count str last index of string val str val count str string count str pos val str val count str substring pos count str val count str substring pos long count long items get val str inc long parse long count str count null count inc else count count value inc items put val str count it includes following basic statistics histogram number unique values minimum value media value maximum value average value standard deviation string get report counts new items size string buffer sb new string buffer iterator object iter items values iterator iter next long count long iter next counts count value arrays sort counts sb append counts length acc counts length next val counts j j counts length counts j next val j acc next val j j average sd counts length sb append append counts sb append append counts counts length sb append append counts counts length average acc counts length sb append append average counts length next diff counts average sd next diff next diff sd math sqrt sd counts length sb append append sd return sb string histogram string get report details string buffer sb new string buffer iterator entry object object iter items entry set iterator iter next entry object object en iter next object val en get key long count long en get value sb append append val string append append count value append n return sb string the return value expected used reducer array list string get combiner output array list string retv new array list string iterator entry object object iter items entry set iterator iter next entry object object en iter next object val en get key long count long en get value retv add val string count value return retv tree map object object get report items return items reset aggregator reset items new tree map object object
1486	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\Chain.java	unrelated	package org apache hadoop mapreduce lib chain the chain provides common functionality link chain mapper link chain reducer chain protected string chain mapper mapreduce chain mapper protected string chain reducer mapreduce chain reducer protected string chain mapper size size protected string chain mapper class mapper protected string chain mapper config mapper config protected string chain reducer class reducer protected string chain reducer config reducer config protected string mapper input key class mapreduce chain mapper input key protected string mapper input value class mapreduce chain mapper input value protected string mapper output key class mapreduce chain mapper output key protected string mapper output value class mapreduce chain mapper output value protected string reducer input key class mapreduce chain reducer input key protected string reducer input value class maperduce chain reducer input value protected string reducer output key class mapreduce chain reducer output key protected string reducer output value class mapreduce chain reducer output value protected boolean map list mapper mappers new array list mapper reducer reducer list configuration conf list new array list configuration configuration r conf list thread threads new array list thread list chain blocking queue blocking queues new array list chain blocking queue throwable throwable null creates chain instance configured mapper reducer true indicates chain mapper false reducer protected chain boolean map map map key value pair k v k key v value boolean end of input key value pair k key v value key key value value end of input false key value pair boolean eof key null value null end of input eof chain record reader either reads blocking queue task context chain record reader keyin valuein extends record reader keyin valuein class key class class value class keyin key valuein value configuration conf task input output context keyin valuein input context null chain blocking queue key value pair keyin valuein input queue null constructor read blocking queue chain record reader class key class class value class chain blocking queue key value pair keyin valuein input queue configuration conf key class key class value class value class input queue input queue conf conf constructor read context chain record reader task input output context keyin valuein context input context context initialize input split split task attempt context context throws io exception interrupted exception advance next key value pair returning null end boolean next key value throws io exception interrupted exception input queue null return read from queue else input context next key value key input context get current key value input context get current value return true else return false boolean read from queue throws io exception interrupted exception key value pair keyin valuein kv null wait input queue kv input queue dequeue kv end of input return false key keyin reflection utils new instance key class conf value valuein reflection utils new instance value class conf reflection utils copy conf kv key key reflection utils copy conf kv value value return true get current key keyin get current key throws io exception interrupted exception return key get current value valuein get current value throws io exception interrupted exception
1487	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainMapContextImpl.java	unrelated	package org apache hadoop mapreduce lib chain a simple wrapper delegates functionality underlying context overrides methods record readers record writers configuration chain map context impl keyin valuein keyout valueout implements map context keyin valuein keyout valueout record reader keyin valuein reader record writer keyout valueout output task input output context keyin valuein keyout valueout base configuration conf chain map context impl task input output context keyin valuein keyout valueout base record reader keyin valuein rr record writer keyout valueout rw configuration conf reader rr output rw base base conf conf keyin get current key throws io exception interrupted exception return reader get current key valuein get current value throws io exception interrupted exception return reader get current value boolean next key value throws io exception interrupted exception return reader next key value input split get input split base instanceof map context map context keyin valuein keyout valueout mc map context keyin valuein keyout valueout base return mc get input split else return null counter get counter enum counter name return base get counter counter name counter get counter string group name string counter name return base get counter group name counter name output committer get output committer return base get output committer write keyout key valueout value throws io exception interrupted exception output write key value string get status return base get status task attempt id get task attempt id return base get task attempt id set status string msg base set status msg path get archive class paths return base get archive class paths string get archive timestamps return base get archive timestamps uri get cache archives throws io exception return base get cache archives uri get cache files throws io exception return base get cache files class extends reducer get combiner class throws class not found exception return base get combiner class configuration get configuration return conf path get file class paths return base get file class paths string get file timestamps return base get file timestamps raw comparator get grouping comparator return base get grouping comparator class extends input format get input format class throws class not found exception return base get input format class string get jar return base get jar job id get job id return base get job id string get job name return base get job name boolean get job setup cleanup needed return base get job setup cleanup needed boolean get task cleanup needed return base get task cleanup needed path get local cache archives throws io exception return base get local cache archives path get local cache files throws io exception return base get local cache archives class get map output key class return base get map output key class class get map output value class return base get map output value class class extends mapper get mapper class throws class not found exception return base get mapper class get max map attempts return base get max map attempts get max reduce attempts return base get max reduce attempts get num reduce tasks return base get num reduce tasks class extends output format
1488	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainMapper.java	unrelated	package org apache hadoop mapreduce lib chain the chain mapper allows use multiple mapper within single map task p the mapper invoked chained piped fashion output first becomes input second last mapper output last mapper written task output p p the key functionality feature mappers chain need aware executed chain this enables reusable specialized mappers combined perform composite operations within single task p p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use matching output input key value conversion done chaining code p p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p p important there need specify output key value chain mapper done add mapper last mapper chain p chain mapper usage pattern p pre job new job conf p configuration map a conf new configuration false chain mapper add mapper job a map long writable text text text true map a conf p configuration map b conf new configuration false chain mapper add mapper job b map text text long writable text false map b conf p p job wait for complettion true pre chain mapper keyin valuein keyout valueout extends mapper keyin valuein keyout valueout adds link mapper chain mapper p the key values passed one element chain next value for added mapper configuration given code mapper conf code precedence job configuration this precedence effect task running p p important there need specify output key value chain mapper done add mapper last mapper chain p the job mapper add mapper input key mapper input value mapper output key mapper output value configuration mapper it recommended use configuration without default values using code configuration boolean load defaults code constructor false add mapper job job class extends mapper klass class input key class class input value class class output key class class output value class configuration mapper conf throws io exception job set mapper class chain mapper job set map output key class output key class job set map output value class output value class chain add mapper true job klass input key class input value class output key class output value class mapper conf chain chain protected setup context context chain new chain true chain setup context get configuration run context context throws io exception interrupted exception setup context num mappers chain get all mappers size num mappers return chain blocking queue chain key value pair inputqueue chain blocking queue chain key value pair outputqueue num mappers chain run mapper context else add mappers proper context add first mapper outputqueue chain create blocking queue chain add mapper context outputqueue add mappers num mappers inputqueue outputqueue outputqueue chain create blocking queue chain add mapper inputqueue outputqueue context add last mapper chain add mapper outputqueue context num mappers start threads chain start all threads wait threads chain join all threads
1489	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainReduceContextImpl.java	unrelated	package org apache hadoop mapreduce lib chain a simple wrapper delegates functionality underlying context overrides methods record writer configuration chain reduce context impl keyin valuein keyout valueout implements reduce context keyin valuein keyout valueout reduce context keyin valuein keyout valueout base record writer keyout valueout rw configuration conf chain reduce context impl reduce context keyin valuein keyout valueout base record writer keyout valueout output configuration conf base base rw output conf conf iterable valuein get values throws io exception interrupted exception return base get values boolean next key throws io exception interrupted exception return base next key counter get counter enum counter name return base get counter counter name counter get counter string group name string counter name return base get counter group name counter name keyin get current key throws io exception interrupted exception return base get current key valuein get current value throws io exception interrupted exception return base get current value output committer get output committer return base get output committer boolean next key value throws io exception interrupted exception return base next key value write keyout key valueout value throws io exception interrupted exception rw write key value string get status return base get status task attempt id get task attempt id return base get task attempt id set status string msg base set status msg path get archive class paths return base get archive class paths string get archive timestamps return base get archive timestamps uri get cache archives throws io exception return base get cache archives uri get cache files throws io exception return base get cache files class extends reducer get combiner class throws class not found exception return base get combiner class configuration get configuration return conf path get file class paths return base get file class paths string get file timestamps return base get file timestamps raw comparator get grouping comparator return base get grouping comparator class extends input format get input format class throws class not found exception return base get input format class string get jar return base get jar job id get job id return base get job id string get job name return base get job name boolean get job setup cleanup needed return base get job setup cleanup needed boolean get task cleanup needed return base get task cleanup needed path get local cache archives throws io exception return base get local cache archives path get local cache files throws io exception return base get local cache files class get map output key class return base get map output key class class get map output value class return base get map output value class class extends mapper get mapper class throws class not found exception return base get mapper class get max map attempts return base get max map attempts get max reduce attempts return base get max map attempts get num reduce tasks return base get num reduce tasks class extends output format get output format class throws class not found exception return base get output format class class get output key class return base get output
1490	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\chain\ChainReducer.java	unrelated	package org apache hadoop mapreduce lib chain the chain reducer allows chain multiple mapper reducer within reducer task p for record output reducer mapper invoked chained piped fashion the output reducer becomes input first mapper output first becomes input second last mapper output last mapper written task output p p the key functionality feature mappers chain need aware executed reducer chain this enables reusable specialized mappers combined perform composite operations within single task p p special care taken creating chains key values output mapper valid following mapper chain it assumed mappers reduce chain use matching output input key value conversion done chaining code p p using chain mapper chain reducer possible compose map reduce jobs look like code map reduce map code and immediate benefit pattern dramatic reduction disk io p p important there need specify output key value chain reducer done set reducer add mapper last element chain p chain reducer usage pattern p pre job new job conf p configuration reduce conf new configuration false chain reducer set reducer job x reduce long writable text text text true reduce conf p chain reducer add mapper job c map text text long writable text false null p chain reducer add mapper job d map long writable text long writable long writable true null p p job wait for completion true pre chain reducer keyin valuein keyout valueout extends reducer keyin valuein keyout valueout sets link reducer chain job p the key values passed one element chain next value for added reducer configuration given code reducer conf code precedence job configuration this precedence effect task running p p important there need specify output key value chain reducer done set reducer add mapper last element chain p job reducer add reducer input key reducer input value reducer output key reducer output value configuration reducer it recommended use configuration without default values using code configuration boolean load defaults code constructor false set reducer job job class extends reducer klass class input key class class input value class class output key class class output value class configuration reducer conf job set reducer class chain reducer job set output key class output key class job set output value class output value class chain set reducer job klass input key class input value class output key class output value class reducer conf adds link mapper chain reducer p the key values passed one element chain next value for added mapper configuration given code mapper conf code precedence job configuration this precedence effect task running p p important there need specify output key value chain mapper done add mapper last mapper chain p the job mapper add mapper input key mapper input value mapper output key mapper output value configuration mapper it recommended use configuration without default values using code configuration boolean load defaults code constructor false add mapper job job class extends mapper klass class input key class class input value class class output key class class output value class configuration mapper conf throws io exception job set output key class output key class job set output
1491	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\BigDecimalSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter big decimal values big decimal splitter implements db splitter log log log factory get log big decimal splitter list input split split configuration conf result set results string col name throws sql exception big decimal min val results get big decimal big decimal max val results get big decimal string low clause prefix col name string high clause prefix col name big decimal num splits new big decimal conf get int mr job config num maps min val null max val null range null null return null split accordingly list input split splits new array list input split splits add new data driven db input format data driven db input split col name is null col name is null return splits min val null max val null don know reasonable min max value interpolation fail log error cannot find range numeric decimal fields one end null return null get split points together list big decimal split points split num splits min val max val list input split splits new array list input split turn split points set intervals big decimal start split points get split points size big decimal end split points get split points size this last one use closed interval splits add new data driven db input format data driven db input split low clause prefix start string col name end string else normal open interval case splits add new data driven db input format data driven db input split low clause prefix start string high clause prefix end string start end return splits big decimal min increment new big decimal double min value divide numerator denominator if impossible exact mode use rounding protected big decimal try divide big decimal numerator big decimal denominator try return numerator divide denominator catch arithmetic exception ae return numerator divide denominator big decimal round half up returns list big decimals one element longer list input splits this represents boundaries input splits all splits open top end except last one so list would represent splits capturing intervals note closed interval last split list big decimal split big decimal num splits big decimal min val big decimal max val throws sql exception list big decimal splits new array list big decimal use num splits hint may need extra task size divide cleanly big decimal split size try divide max val subtract min val num splits split size compare to min increment split size min increment log warn set big decimal split size min increment big decimal cur val min val cur val compare to max val splits add cur val cur val cur val add split size splits get splits size compare to max val splits size we end max val add end list splits add max val return splits
1492	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\BooleanSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter boolean values boolean splitter implements db splitter list input split split configuration conf result set results string col name throws sql exception list input split splits new array list input split results get string null results get string null range null null return null split accordingly splits add new data driven db input format data driven db input split col name is null col name is null return splits boolean min val results get boolean boolean max val results get boolean use one two splits min val splits add new data driven db input format data driven db input split col name false col name false max val splits add new data driven db input format data driven db input split col name true col name true results get string null results get string null include null value splits add new data driven db input format data driven db input split col name is null col name is null return splits
1493	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DataDrivenDBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table operates like db input format instead using limit offset demarcate splits tries generate where clauses separate data roughly equivalent shards data driven db input format t extends db writable extends db input format t implements configurable log log log factory get log data driven db input format if users providing query following expected appear where clause substituted pair conditions input allow input splits parallelise import string substitute token conditions a input split spans set rows data driven db input split extends db input format db input split string lower bound clause string upper bound clause default constructor data driven db input split convenience constructor data driven db input split string lower string upper lower bound clause lower upper bound clause upper get length throws io exception return unfortunately know inherit doc read fields data input input throws io exception lower bound clause text read string input upper bound clause text read string input inherit doc write data output output throws io exception text write string output lower bound clause text write string output upper bound clause string get lower clause return lower bound clause string get upper clause return upper bound clause protected db splitter get splitter sql data type switch sql data type case types numeric case types decimal return new big decimal splitter case types bit case types boolean return new boolean splitter case types integer case types tinyint case types smallint case types bigint return new integer splitter case types real case types float case types double return new float splitter case types char case types varchar case types longvarchar return new text splitter case types date case types time case types timestamp return new date splitter default todo support binary varbinary longvarbinary distinct clob blob array struct ref datalink java object return null inherit doc list input split get splits job context job throws io exception target num tasks job get configuration get int mr job config num maps target num tasks there need run bounding vals query return split separates nothing this considerably optimal large table index list input split singleton split new array list input split singleton split add new data driven db input split return singleton split result set results null statement statement null connection connection get connection try statement connection create statement results statement execute query get bounding vals query results next based type results use different mechanism interpolating split points e numeric splits text splits dates etc sql data type results get meta data get column type db splitter splitter get splitter sql data type null splitter throw new io exception unknown sql data type sql data type return splitter split job get configuration results get db conf get input order by catch sql exception e throw new io exception e get message finally more less ignore sql exceptions log case need try null results results close catch sql exception se log debug sql exception closing resultset se string try null statement statement close catch sql exception se
1494	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records sql table using data driven where clause splits emits long writables containing record number key db writables value data driven db record reader t extends db writable extends db record reader t log log log factory get log data driven db record reader string db product name database manufacturer data driven db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table string db product throws sql exception super split input class conf conn db config cond fields table db product name db product returns query selecting records subclasses custom behaviour protected string get select query string builder query new string builder data driven db input format data driven db input split data split data driven db input format data driven db input split get split db configuration db conf get db conf string field names get field names string table name get table name string conditions get conditions build where clauses associated data split first we need branches function string builder condition clauses new string builder condition clauses append append data split get lower clause condition clauses append and append data split get upper clause condition clauses append db conf get input query null we need generate entire query query append select field names length query append field names field names length query append query append from append table name db product name starts with oracle seems necessary hsqldb oracle explicitly use clause query append as append table name query append where conditions null conditions length put user conditions first query append append conditions append and now append conditions associated split query append condition clauses string else user provided query we replace special token where clause string input query db conf get input query input query index of data driven db input format substitute token log error could find clause substitution token data driven db input format substitute token query input query parallel splits may work correctly query append input query replace data driven db input format substitute token condition clauses string log debug using query query string return query string
1495	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DateSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter date time values make use logic integer splitter since date time longs java date splitter extends integer splitter log log log factory get log date splitter list input split split configuration conf result set results string col name throws sql exception min val max val sql data type results get meta data get column type min val result set col to long results sql data type max val result set col to long results sql data type string low clause prefix col name string high clause prefix col name num splits conf get int mr job config num maps num splits num splits min val long min value max val long min value the range acceptable dates null null just create single split list input split splits new array list input split splits add new data driven db input format data driven db input split col name is null col name is null return splits gather split point integers list long split points split num splits min val max val list input split splits new array list input split turn split points set intervals start split points get date start date to date start sql data type sql data type types timestamp the lower bound nanos value needs match actual lower bound nanos try java sql timestamp start date set nanos results get timestamp get nanos catch null pointer exception npe if lower bound null get npe ignore set nanos split points size end split points get date end date to date end sql data type split points size sql data type types timestamp the upper bound nanos value needs match actual upper bound nanos try java sql timestamp end date set nanos results get timestamp get nanos catch null pointer exception npe if upper bound null get npe ignore set nanos this last one use closed interval splits add new data driven db input format data driven db input split low clause prefix date to string start date col name date to string end date else normal open interval case splits add new data driven db input format data driven db input split low clause prefix date to string start date high clause prefix date to string end date start end start date end date min val long min value max val long min value add extra split handle null case saw splits add new data driven db input format data driven db input split col name is null col name is null return splits retrieve value column type appropriate manner return timestamp since epoch if column null return long min value this cause special split generated null case may also cause poorly balanced splits actual dates positive time since epoch etc result set col to long result set rs col num sql data type throws sql exception try switch sql data type case types date return rs get date col num get time case types time return rs get time col num get time case types timestamp return rs get
1496	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBConfiguration.java	unrelated	package org apache hadoop mapreduce lib db a container configuration property names jobs db input output the job configured using methods link db input format link db output format alternatively properties set configuration proper values db configuration the jdbc driver name string driver class property mapreduce jdbc driver jdbc database access url string url property mapreduce jdbc url user name access database string username property mapreduce jdbc username password access database string password property mapreduce jdbc password input table name string input table name property mapreduce jdbc input table name field names input table string input field names property mapreduce jdbc input field names where clause input select statement string input conditions property mapreduce jdbc input conditions order by clause input select statement string input order by property mapreduce jdbc input orderby whole input query exluding limit offset string input query mapreduce jdbc input query input query get count records string input count query mapreduce jdbc input count query input query get max min values jdbc input query string input bounding query mapred jdbc input bounding query class name implementing db writable hold input tuples string input class property mapreduce jdbc input output table name string output table name property mapreduce jdbc output table name field names output table string output field names property mapreduce jdbc output field names number fields output table string output field count property mapreduce jdbc output field count sets db access related fields link configuration configure db configuration conf string driver class string db url string user name string passwd conf set driver class property driver class conf set url property db url user name null conf set username property user name passwd null conf set password property passwd sets db access related fields job conf configure db configuration job string driver class string db url configure db job driver class db url null null configuration conf db configuration configuration job conf job returns connection object db connection get connection throws class not found exception sql exception class name conf get db configuration driver class property conf get db configuration username property null return driver manager get connection conf get db configuration url property else return driver manager get connection conf get db configuration url property conf get db configuration username property conf get db configuration password property configuration get conf return conf string get input table name return conf get db configuration input table name property set input table name string table name conf set db configuration input table name property table name string get input field names return conf get strings db configuration input field names property set input field names string field names conf set strings db configuration input field names property field names string get input conditions return conf get db configuration input conditions property set input conditions string conditions conditions null conditions length conf set db configuration input conditions property conditions string get input order by return conf get db configuration input order by property set input order by string orderby orderby null orderby length conf set db configuration input order by
1497	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table p db input format emits long writables containing record number key db writables value the sql query input using one two set input methods db input format t extends db writable extends input format long writable t implements configurable string db product name default a class nothing implementing db writable null db writable implements db writable writable read fields data input throws io exception read fields result set arg throws sql exception write data output throws io exception write prepared statement arg throws sql exception a input split spans set rows db input split extends input split implements writable end start default constructor db input split convenience constructor db input split start end start start end end inherit doc string get locations throws io exception todo add layer enable sql sharding support locality return new string get start return start get end return end get length throws io exception return end start inherit doc read fields data input input throws io exception start input read long end input read long inherit doc write data output output throws io exception output write long start output write long end string conditions connection connection string table name string field names db configuration db conf inherit doc set conf configuration conf db conf new db configuration conf try get connection database meta data db meta connection get meta data db product name db meta get database product name upper case catch exception ex throw new runtime exception ex table name db conf get input table name field names db conf get input field names conditions db conf get input conditions configuration get conf return db conf get conf db configuration get db conf return db conf connection get connection try null connection the connection closed reinstantiate connection db conf get connection connection set auto commit false connection set transaction isolation connection transaction serializable catch exception e throw new runtime exception e return connection string get db product name return db product name protected record reader long writable t create db record reader db input split split configuration conf throws io exception class t input class class t db conf get input class try use database product name determine appropriate record reader db product name starts with oracle use oracle specific db reader return new oracle db record reader t split input class conf get connection get db conf conditions field names table name else db product name starts with mysql use my sql specific db reader return new my sqldb record reader t split input class conf get connection get db conf conditions field names table name else generic reader return new db record reader t split input class conf get connection get db conf conditions field names table name catch sql exception ex throw new io exception ex get message inherit doc record reader long writable t create record reader input split split task attempt context context throws io exception interrupted exception return create db record reader db input split
1498	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBOutputFormat.java	unrelated	package org apache hadoop mapreduce lib db a output format sends reduce output sql table p link db output format accepts lt key value gt pairs key type extending db writable returned link record writer writes b key b database batch sql query db output format k extends db writable v extends output format k v log log log factory get log db output format check output specs job context context throws io exception interrupted exception output committer get output committer task attempt context context throws io exception interrupted exception return new file output committer file output format get output path context context a record writer writes reduce output sql table db record writer extends record writer k v connection connection prepared statement statement db record writer throws sql exception db record writer connection connection prepared statement statement throws sql exception connection connection statement statement connection set auto commit false connection get connection return connection prepared statement get statement return statement inherit doc close task attempt context context throws io exception try statement execute batch connection commit catch sql exception e try connection rollback catch sql exception ex log warn string utils stringify exception ex throw new io exception e get message finally try statement close connection close catch sql exception ex throw new io exception ex get message inherit doc write k key v value throws io exception try key write statement statement add batch catch sql exception e e print stack trace constructs query used prepared statement insert data table insert fields insert if field names unknown supply array nulls string construct query string table string field names field names null throw new illegal argument exception field names may null string builder query new string builder query append insert into append table field names length field names null query append field names length query append field names field names length query append query append query append values field names length query append field names length query append query append return query string inherit doc record writer k v get record writer task attempt context context throws io exception db configuration db conf new db configuration context get configuration string table name db conf get output table name string field names db conf get output field names field names null field names new string db conf get output field count try connection connection db conf get connection prepared statement statement null statement connection prepare statement construct query table name field names return new db record writer connection statement catch exception ex throw new io exception ex get message initializes reduce part job appropriate output settings set output job job string table name string field names throws io exception field names length field names null db configuration db conf set output job table name db conf set output field names field names else field names length set output job table name field names length else throw new illegal argument exception field names must greater initializes reduce part job appropriate output settings set output job job string table name field count throws io
1499	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records sql table emits long writables containing record number key db writables value db record reader t extends db writable extends record reader long writable t log log log factory get log db record reader result set results null class t input class configuration conf db input format db input split split pos long writable key null t value null connection connection protected prepared statement statement db configuration db conf string conditions string field names string table name db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception input class input class split split conf conf connection conn db conf db config conditions cond field names fields table name table protected result set execute query string query throws sql exception statement connection prepare statement query result set type forward only result set concur read only return statement execute query returns query selecting records subclasses custom behaviour protected string get select query string builder query new string builder default codepath my sql hsqldb etc relies limit offset splits db conf get input query null query append select field names length query append field names field names length query append query append from append table name query append as append table name hsqldb necessary conditions null conditions length query append where append conditions append string order by db conf get input order by order by null order by length query append order by append order by else prebuilt query query append db conf get input query try query append limit append split get length query append offset append split get start catch io exception ex ignore throw return query string inherit doc close throws io exception try null results results close null statement statement close null connection connection commit connection close catch sql exception e throw new io exception e get message initialize input split split task attempt context context throws io exception interrupted exception nothing inherit doc long writable get current key return key inherit doc t get current value return value t create value return reflection utils new instance input class conf get pos throws io exception return pos boolean next long writable key t value throws io exception key key value value return next key value inherit doc get progress throws io exception return pos split get length inherit doc boolean next key value throws io exception try key null key new long writable value null value create value null results first time method run query results execute query get select query results next return false set key field value output key value key set pos split get start value read fields results pos catch sql exception e throw new io exception sql exception next key value e return true protected db input format db input split get split return split protected string get field names return field names protected string get table name return table name protected string
1500	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBSplitter.java	unrelated	package org apache hadoop mapreduce lib db db splitter generate db input splits use data driven db input format data driven db input format needs interpolate two values represent lowest highest valued records import depending data type column requires different behavior db splitter implementations perform data type family data types db splitter given result set containing one record already advanced record two columns low value high value type determine set splits span given values list input split split configuration conf result set results string col name throws sql exception
1501	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\DBWritable.java	unrelated	package org apache hadoop mapreduce lib db objects read written database implement code db writable code db writable similar link writable except link write prepared statement method takes link prepared statement link read fields result set takes link result set p implementations responsible writing fields object prepared statement reading fields object result set p example p if following table database pre create table my table counter integer not null timestamp bigint not null pre read write tuples table p pre my writable implements writable db writable some data counter timestamp writable write implementation write data output throws io exception write int counter write long timestamp writable read fields implementation read fields data input throws io exception counter read int timestamp read long write prepared statement statement throws sql exception statement set int counter statement set long timestamp read fields result set result set throws sql exception counter result set get int timestamp result set get long pre p db writable sets fields object link prepared statement write prepared statement statement throws sql exception reads fields object link result set read fields result set result set throws sql exception
1502	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\FloatSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter floating point values float splitter implements db splitter log log log factory get log float splitter min increment double min value list input split split configuration conf result set results string col name throws sql exception log warn generating splits floating point index column due log warn imprecise representation floating point values java log warn may result incomplete import log warn you strongly encouraged choose integral split column list input split splits new array list input split results get string null results get string null range null null return null split accordingly splits add new data driven db input format data driven db input split col name is null col name is null return splits min val results get double max val results get double use hint may need extra task size divide cleanly num splits conf get int mr job config num maps split size max val min val num splits split size min increment split size min increment string low clause prefix col name string high clause prefix col name cur lower min val cur upper cur lower split size cur upper max val splits add new data driven db input format data driven db input split low clause prefix double string cur lower high clause prefix double string cur upper cur lower cur upper cur upper split size catch overage create closed interval last split cur lower max val splits size splits add new data driven db input format data driven db input split low clause prefix double string cur upper col name double string max val results get string null results get string null at least one extrema null add null split splits add new data driven db input format data driven db input split col name is null col name is null return splits
1503	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\IntegerSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter integer values integer splitter implements db splitter list input split split configuration conf result set results string col name throws sql exception min val results get long max val results get long string low clause prefix col name string high clause prefix col name num splits conf get int mr job config num maps num splits num splits results get string null results get string null range null null return null split accordingly list input split splits new array list input split splits add new data driven db input format data driven db input split col name is null col name is null return splits get split points together list long split points split num splits min val max val list input split splits new array list input split turn split points set intervals start split points get split points size end split points get split points size this last one use closed interval splits add new data driven db input format data driven db input split low clause prefix long string start col name long string end else normal open interval case splits add new data driven db input format data driven db input split low clause prefix long string start high clause prefix long string end start end results get string null results get string null at least one extrema null add null split splits add new data driven db input format data driven db input split col name is null col name is null return splits returns list longs one element longer list input splits this represents boundaries input splits all splits open top end except last one so list would represent splits capturing intervals note closed interval last split list long split num splits min val max val throws sql exception list long splits new array list long use num splits hint may need extra task size divide cleanly split size max val min val num splits split size split size cur val min val cur val max val splits add cur val cur val split size splits get splits size max val splits size we end max val add end list splits add max val return splits
1504	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\MySQLDataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records my sql table via data driven db record reader my sql data driven db record reader t extends db writable extends data driven db record reader t my sql data driven db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception super split input class conf conn db config cond fields table mysql execute statements mysql unbuffered mode protected result set execute query string query throws sql exception statement get connection prepare statement query result set type forward only result set concur read only statement set fetch size integer min value my sql read row time return statement execute query
1505	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\MySQLDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records my sql table my sqldb record reader t extends db writable extends db record reader t my sqldb record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception super split input class conf conn db config cond fields table execute statements mysql unbuffered mode protected result set execute query string query throws sql exception statement get connection prepare statement query result set type forward only result set concur read only statement set fetch size integer min value my sql read row time return statement execute query
1506	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDataDrivenDBInputFormat.java	unrelated	package org apache hadoop mapreduce lib db a input format reads input data sql table oracle db oracle data driven db input format t extends db writable extends data driven db input format t implements configurable protected db splitter get splitter sql data type switch sql data type case types date case types time case types timestamp return new oracle date splitter default return super get splitter sql data type protected record reader long writable t create db record reader db input split split configuration conf throws io exception db configuration db conf get db conf class t input class class t db conf get input class try use oracle specific db reader return new oracle data driven db record reader t split input class conf get connection db conf db conf get input conditions db conf get input field names db conf get input table name catch sql exception ex throw new io exception ex get message
1507	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDataDrivenDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records oracle table via data driven db record reader oracle data driven db record reader t extends db writable extends data driven db record reader t oracle data driven db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception super split input class conf conn db config cond fields table oracle must initialize tz used connection oracle oracle db record reader set session time zone conf conn
1508	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDateSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter date time values returned oracle db make use logic date splitter since needs use oracle specific functions formatting end generating input splits oracle date splitter extends date splitter protected string date to string date oracle data objects always actually timestamps return to timestamp string yyyy mm dd hh mi ss ff
1509	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\OracleDBRecordReader.java	unrelated	package org apache hadoop mapreduce lib db a record reader reads records oracle sql table oracle db record reader t extends db writable extends db record reader t configuration key set timezone string session timezone key oracle session time zone log log log factory get log oracle db record reader oracle db record reader db input format db input split split class t input class configuration conf connection conn db configuration db config string cond string fields string table throws sql exception super split input class conf conn db config cond fields table set session time zone conf conn returns query selecting records oracle db protected string get select query string builder query new string builder db configuration db conf get db conf string conditions get conditions string table name get table name string field names get field names oracle specific codepath use rownum instead limit offset db conf get input query null query append select field names length query append field names field names length query append query append from append table name conditions null conditions length query append where append conditions string order by db conf get input order by order by null order by length query append order by append order by else prebuilt query query append db conf get input query try db input format db input split split get split split get length split get start string querystring query string query new string builder query append select from select rownum dbif rno from query append querystring query append where rownum append split get start query append append split get length query append where dbif rno append split get start catch io exception ex ignore throw return query string set session time zone we read oracle session time zone property set session time zone configuration conf connection conn throws sql exception need use reflection call method set session time zone oracle connection oracle specific java libraries accessible context method method try method conn get class get method set session time zone new class string catch exception ex log error could find method set session time zone conn get class get name ex rethrow sql exception throw new sql exception ex need set time zone order java correctly access column timestamp with local time zone we easily get correct oracle specific timezone java let user set timezone property string client time zone conf get session timezone key gmt try method set accessible true method invoke conn client time zone log info time zone set client time zone catch exception ex log warn time zone client time zone could set oracle database log warn setting default time zone gmt try gmt timezone guaranteed exist method invoke conn gmt catch exception ex log error could set time zone oracle connection ex rethrow sql exception throw new sql exception ex
1510	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\db\TextSplitter.java	unrelated	package org apache hadoop mapreduce lib db implement db splitter text strings text splitter extends big decimal splitter log log log factory get log text splitter this method needs determine splits two user provided strings in case user strings a z hard could create two splits a m m z splits strings beginning letter etc if user provided us strings ham haze however need create splits differ third letter the algorithm used follows since unicode characters interpret characters digits base given containing characters n interpret number n base having mapped low high strings floating point values use big decimal splitter establish even split points map resulting floating point values back strings list input split split configuration conf result set results string col name throws sql exception log warn generating splits textual index column log warn if database sorts case insensitive order may result partial import duplicate records log warn you strongly encouraged choose integral split column string min string results get string string max string results get string boolean min is null false if min value null switch empty instead purposes interpolation then add null null special case split null min string min string min is null true null max string if max null min null just return special split case list input split splits new array list input split splits add new data driven db input format data driven db input split col name is null col name is null return splits use hint may need extra task size divide cleanly num splits conf get int mr job config num maps string low clause prefix col name string high clause prefix col name if common prefix min string max string establish pull min string max string max prefix len math min min string length max string length shared len shared len shared len max prefix len shared len char c min string char at shared len char c max string char at shared len c c break the common prefix length shared len extract string common prefix min string substring shared len min string min string substring shared len max string max string substring shared len list string split strings split num splits min string max string common prefix list input split splits new array list input split convert list split point strings actual set input splits string start split strings get split strings size string end split strings get split strings size this last one use closed interval splits add new data driven db input format data driven db input split low clause prefix start col name end else normal open interval case splits add new data driven db input format data driven db input split low clause prefix start high clause prefix end min is null add special null split end splits add new data driven db input format data driven db input split col name is null col name is null return splits list string split num splits string min string string max string string common prefix throws sql exception big decimal min val to big decimal min
1511	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionHelper.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements mapper reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the reducer extracts output key value pairs similar manner except key never ignored field selection helper text empty text new text string data field seperator mapreduce fieldsel data field separator string map output key value spec mapreduce fieldsel map output key value fields spec string reduce output key value spec mapreduce fieldsel reduce output key value fields spec extract actual field numbers given field specs if field spec form n like n return value otherwise returned extract fields string field list spec list integer field list fields from j pos string field spec null field list spec length field spec field list spec field spec length continue pos field spec index of pos integer fn new integer field spec field list add fn else string start field spec substring pos string end field spec substring pos start length start end length fields from integer parse int start continue start pos integer parse int start end pos integer parse int end j start pos j end pos j field list add j return fields from string select fields string fields list integer field list fields from string separator string retv null string buffer sb null field list null field list size sb null sb new string buffer integer index field list index fields length sb append fields index sb append separator fields from sb null sb new string buffer fields from fields length sb append fields append separator sb null retv sb string retv length retv retv substring retv length return retv parse output key value spec string key value spec list integer key field list list integer value field list string key val specs key value spec split string key spec key val specs split string val spec new string key val specs length val spec key val specs split field selection helper extract fields key spec key field list return field selection helper extract fields val spec value field list string spec to string string field separator string key value spec value fields from list integer key field list list integer value
1512	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionMapper.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements mapper used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form map output keys list fields form map output values if inputformat text input format mapper ignore key map function fields value otherwise fields union key value the field separator attribute mapreduce fieldsel data field separator the map output field list spec attribute mapreduce fieldsel map output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values field selection mapper k v extends mapper k v text text string map output key value spec boolean ignore input key string field separator list integer map output key field list new array list integer list integer map output value field list new array list integer map value fields from log log log factory get log field selection map reduce setup context context throws io exception interrupted exception configuration conf context get configuration field separator conf get field selection helper data field seperator map output key value spec conf get field selection helper map output key value spec try ignore input key text input format get canonical name equals context get input format class get canonical name catch class not found exception e throw new io exception input format found e map value fields from field selection helper parse output key value spec map output key value spec map output key field list map output value field list log info field selection helper spec to string field separator map output key value spec map value fields from map output key field list map output value field list nignore input key ignore input key the identify function input key value pair written directly output map k key v val context context throws io exception interrupted exception field selection helper helper new field selection helper field selection helper empty text field selection helper empty text helper extract output key value key string val string field separator map output key field list map output value field list map value fields from ignore input key true context write helper get key helper get value
1513	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\fieldsel\FieldSelectionReducer.java	unrelated	package org apache hadoop mapreduce lib fieldsel this implements reducer used perform field selections manner similar unix cut the input data treated fields separated user specified separator default value the user specify list fields form reduce output keys list fields form reduce output values the fields union key value the field separator attribute mapreduce fieldsel data field separator the reduce output field list spec attribute mapreduce fieldsel reduce output key value fields spec the value expected like key fields spec value fields spec key value fields spec comma separated field spec field spec field spec field spec each field spec simple number e g specifying specific field range like specify range fields open range like specifying fields starting field the open range field spec applies value fields they effect key fields here example it specifies use fields keys use fields values field selection reducer k v extends reducer text text text text string field separator string reduce output key value spec list integer reduce output key field list new array list integer list integer reduce output value field list new array list integer reduce value fields from log log log factory get log field selection map reduce setup context context throws io exception interrupted exception configuration conf context get configuration field separator conf get field selection helper data field seperator reduce output key value spec conf get field selection helper reduce output key value spec reduce value fields from field selection helper parse output key value spec reduce output key value spec reduce output key field list reduce output value field list log info field selection helper spec to string field separator reduce output key value spec reduce value fields from reduce output key field list reduce output value field list reduce text key iterable text values context context throws io exception interrupted exception string key str key string field separator text val values field selection helper helper new field selection helper helper extract output key value key str val string field separator reduce output key field list reduce output value field list reduce value fields from false false context write helper get key helper get value
1514	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileInputFormat.java	pooling	package org apache hadoop mapreduce lib input an link input format returns link combine file split link input format get splits job context method splits constructed files input paths a split cannot files different pools each split returned may contain blocks different files if max split size specified blocks node combined form single split blocks left combined blocks rack if max split size specified blocks rack combined single split attempt made create node local splits if max split size equal block size similar default splitting behavior hadoop block locally processed split subclasses implement link input format create record reader input split task attempt context construct code record reader code code combine file split code combine file input format k v extends file input format k v string split minsize pernode mapreduce input fileinputformat split minsize per node string split minsize perrack mapreduce input fileinputformat split minsize per rack ability limit size single split max split size min split size node min split size rack a pool input paths filters a split cannot blocks files across multiple pools array list multi path filter pools new array list multi path filter mapping rack name set nodes rack hash map string set string rack to nodes new hash map string set string specify maximum size bytes split each split approximately equal specified size protected set max split size max split size max split size max split size specify minimum size bytes split per node this applies data left combining data single node splits maximum size specified max split size this leftover data combined split size exceeds min split size node protected set min split size node min split size node min split size node min split size node specify minimum size bytes split per rack this applies data left combining data single rack splits maximum size specified max split size this leftover data combined split size exceeds min split size rack protected set min split size rack min split size rack min split size rack min split size rack create new pool add filters a split cannot files different pools protected create pool list path filter filters pools add new multi path filter filters create new pool add filters a pathname satisfy one specified filters a split cannot files different pools protected create pool path filter filters multi path filter multi new multi path filter path filter f filters multi add f pools add multi protected boolean splitable job context context path file compression codec codec new compression codec factory context get configuration get codec file null codec return true return codec instanceof splittable compression codec default constructor combine file input format list input split get splits job context job throws io exception min size node min size rack max size configuration conf job get configuration values specified setxxx split size takes precedence values might specified config min split size node min size node min split size node else min size node conf get long split minsize pernode min split size rack min size rack min split size rack else min size rack conf get
1515	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileRecordReader.java	unrelated	package org apache hadoop mapreduce lib input a generic record reader hand different record readers chunk link combine file split a combine file split combine data chunks multiple files this allows using different record readers processing data chunks different files combine file record reader k v extends record reader k v class constructor signature new class combine file split task attempt context integer protected combine file split split protected class extends record reader k v rr class protected constructor extends record reader k v rr constructor protected file system fs protected task attempt context context protected idx protected progress protected record reader k v cur reader initialize input split split task attempt context context throws io exception interrupted exception split combine file split split context context null cur reader cur reader initialize split context boolean next key value throws io exception interrupted exception cur reader null cur reader next key value init next record reader return false return true k get current key throws io exception interrupted exception return cur reader get current key v get current value throws io exception interrupted exception return cur reader get current value close throws io exception cur reader null cur reader close cur reader null return progress based amount data processed far get progress throws io exception interrupted exception subprogress bytes processed current split null cur reader idx always one past current subsplit true index subprogress cur reader get progress split get length idx return math min f progress subprogress split get length a generic record reader hand different record readers chunk combine file split combine file record reader combine file split split task attempt context context class extends record reader k v rr class throws io exception split split context context rr class rr class idx cur reader null progress try rr constructor rr class get declared constructor constructor signature rr constructor set accessible true catch exception e throw new runtime exception rr class get name valid constructor e init next record reader get record reader next chunk combine file split protected boolean init next record reader throws io exception cur reader null cur reader close cur reader null idx progress split get length idx done processing far chunks processed nothing idx split get num paths return false get record reader idx th chunk try configuration conf context get configuration setup helper config variables conf set mr job config map input file split get path idx string conf set long mr job config map input start split get offset idx conf set long mr job config map input path split get length idx cur reader rr constructor new instance new object split context integer value of idx idx initialize first record reader called map task responsible initializing subsequent record readers cur reader initialize split context catch exception e throw new runtime exception e idx return true
1516	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\CombineFileSplit.java	unrelated	package org apache hadoop mapreduce lib input a sub collection input files unlike link file split combine file split represent split file split input files smaller sets a split may contain blocks different file blocks split probably local rack br combine file split used implement link record reader reading one record per file combine file split extends input split implements writable path paths startoffset lengths string locations tot length default constructor combine file split combine file split path files start lengths string locations init split files start lengths locations combine file split path files lengths startoffset new files length startoffset length startoffset string locations new string files length locations length locations init split files startoffset lengths locations init split path files start lengths string locations startoffset start lengths lengths paths files tot length locations locations length lengths tot length length copy constructor combine file split combine file split old throws io exception old get paths old get start offsets old get lengths old get locations get length return tot length returns array containing start offsets files split get start offsets return startoffset returns array containing lengths files split get lengths return lengths returns start offset sup th sup path get offset return startoffset returns length sup th sup path get length return lengths returns number paths split get num paths return paths length returns sup th sup path path get path return paths returns paths split path get paths return paths returns paths input split resides string get locations throws io exception return locations read fields data input throws io exception tot length read long arr length read int lengths new arr length arr length lengths read long files length read int paths new path files length files length paths new path text read string arr length read int startoffset new arr length arr length startoffset read long write data output throws io exception write long tot length write int lengths length length lengths write long length write int paths length path p paths text write string p string write int startoffset length length startoffset write long length string string string buffer sb new string buffer paths length sb append paths sb append paths uri get path startoffset lengths paths length sb append locations null string locs string buffer locsb new string buffer locations length locsb append locations locs locsb string sb append locations locs return sb string
1517	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format delegates behavior paths multiple input formats delegating input format k v extends input format k v list input split get splits job context job throws io exception interrupted exception configuration conf job get configuration job job copy new job conf list input split splits new array list input split map path input format format map multiple inputs get input format map job map path class extends mapper mapper map multiple inputs get mapper type map job map class extends input format list path format paths new hash map class extends input format list path first build map input formats paths entry path input format entry format map entry set format paths contains key entry get value get class format paths put entry get value get class new linked list path format paths get entry get value get class add entry get key entry class extends input format list path format entry format paths entry set class extends input format format class format entry get key input format format input format reflection utils new instance format class conf list path paths format entry get value map class extends mapper list path mapper paths new hash map class extends mapper list path now set paths common input format build map mappers paths used path path paths class extends mapper mapper class mapper map get path mapper paths contains key mapper class mapper paths put mapper class new linked list path mapper paths get mapper class add path now set paths common input format mapper added job split together entry class extends mapper list path map entry mapper paths entry set paths map entry get value class extends mapper mapper class map entry get key mapper class null try mapper class job get mapper class catch class not found exception e throw new io exception mapper found e file input format set input paths job copy paths array new path paths size get splits input path tag input format mapper types wrapping tagged input split list input split path splits format get splits job copy input split path split path splits splits add new tagged input split path split conf format get class mapper class return splits record reader k v create record reader input split split task attempt context context throws io exception interrupted exception return new delegating record reader k v split context
1518	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingMapper.java	unrelated	package org apache hadoop mapreduce lib input an link mapper delegates behavior paths multiple mappers delegating mapper k v k v extends mapper k v k v mapper k v k v mapper protected setup context context throws io exception interrupted exception find mapper tagged input split tagged input split input split tagged input split context get input split mapper mapper k v k v reflection utils new instance input split get mapper class context get configuration run context context throws io exception interrupted exception setup context mapper run context cleanup context
1519	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\DelegatingRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this delegating record reader delegates functionality underlying record reader link tagged input split delegating record reader k v extends record reader k v record reader k v original rr constructs delegating record reader delegating record reader input split split task attempt context context throws io exception interrupted exception find input format record reader tagged input split tagged input split tagged input split tagged input split split input format k v input format input format k v reflection utils new instance tagged input split get input format class context get configuration original rr input format create record reader tagged input split get input split context close throws io exception original rr close k get current key throws io exception interrupted exception return original rr get current key v get current value throws io exception interrupted exception return original rr get current value get progress throws io exception interrupted exception return original rr get progress initialize input split split task attempt context context throws io exception interrupted exception original rr initialize tagged input split split get input split context boolean next key value throws io exception interrupted exception return original rr next key value
1520	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileInputFormat.java	unrelated	package org apache hadoop mapreduce lib input a base file based link input format p code file input format code base file based code input format code this provides generic implementation link get splits job context subclasses code file input format code also link splitable job context path method ensure input files split processed whole link mapper file input format k v extends input format k v string input dir mapreduce input fileinputformat inputdir string split maxsize mapreduce input fileinputformat split maxsize string split minsize mapreduce input fileinputformat split minsize string pathfilter class mapreduce input path filter string num input files mapreduce input fileinputformat numinputfiles log log log factory get log file input format split slop slop path filter hidden file filter new path filter boolean accept path p string name p get name return name starts with name starts with proxy path filter accepts path filters given constructor used list paths apply built hidden file filter together user provided one multi path filter implements path filter list path filter filters multi path filter list path filter filters filters filters boolean accept path path path filter filter filters filter accept path return false return true get lower bound split size imposed format protected get format min split size return is given filename splitable usually true file stream compressed code file input format code implementations return code false code ensure individual input files never split link mapper process entire files protected boolean splitable job context context path filename return true set path filter applied input paths map reduce job set input path filter job job class extends path filter filter job get configuration set class pathfilter class filter path filter set minimum input split size set min input split size job job size job get configuration set long split minsize size get minimum split size get min split size job context job return job get configuration get long split minsize l set maximum split size set max input split size job job size job get configuration set long split maxsize size get maximum split size get max split size job context context return context get configuration get long split maxsize long max value get path filter instance filter set input paths path filter get input path filter job context context configuration conf context get configuration class filter class conf get class pathfilter class null path filter return filter class null path filter reflection utils new instance filter class conf null list input directories subclasses may e g select files matching regular expression protected list file status list status job context job throws io exception list file status result new array list file status path dirs get input paths job dirs length throw new io exception no input paths specified job get tokens required file systems token cache obtain tokens for namenodes job get credentials dirs job get configuration list io exception errors new array list io exception creates multi path filter hidden file filter user provided one list path filter filters new array list path filter filters add hidden file filter path filter
1521	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileInputFormatCounter.java	unrelated	package org apache hadoop mapreduce lib input counters used task enum file input format counter bytes read
1522	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\FileSplit.java	unrelated	package org apache hadoop mapreduce lib input a section input file returned link input format get splits job context passed link input format create record reader input split task attempt context file split extends input split implements writable path file start length string hosts file split constructs split host information file split path file start length string hosts file file start start length length hosts hosts the file containing split data path get path return file the position first byte file process get start return start the number bytes file process get length return length string string return file start length writable methods write data output throws io exception text write string file string write long start write long length read fields data input throws io exception file new path text read string start read long length read long hosts null string get locations throws io exception hosts null return new string else return hosts
1523	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\InvalidInputException.java	unrelated	package org apache hadoop mapreduce lib input this wraps list problems input user get list problems together instead finding fixing one one invalid input exception extends io exception serial version uid l list io exception problems create exception given list invalid input exception list io exception probs problems probs get complete list problems reported list io exception get problems return problems get summary message problems found string get message string buffer result new string buffer iterator io exception itr problems iterator itr next result append itr next get message itr next result append n return result string
1524	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\KeyValueLineRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this treats line input key value pair separated separator character the separator specified config file attribute name mapreduce input keyvaluelinerecordreader key value separator the default separator tab character key value line record reader extends record reader text text string key value seperator mapreduce input keyvaluelinerecordreader key value separator line record reader line record reader byte separator byte text inner value text key text value key value line record reader configuration conf throws io exception line record reader new line record reader string sep str conf get key value seperator separator byte sep str char at initialize input split generic split task attempt context context throws io exception line record reader initialize generic split context find separator byte utf start length byte sep start start length utf sep return return set key value text key text value byte line line len pos pos key set line line len value set else key set line pos value set line pos line len pos read key value pair line synchronized boolean next key value throws io exception byte line null line len line record reader next key value inner value line record reader get current value line inner value get bytes line len inner value get length else return false line null return false key null key new text value null value new text pos find separator line line len separator set key value key value line line len pos return true text get current key return key text get current value return value get progress throws io exception return line record reader get progress synchronized close throws io exception line record reader close
1525	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\KeyValueTextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format plain text files files broken lines either line feed carriage return used signal end line each line divided key value parts separator byte if byte exists key entire line value empty key value text input format extends file input format text text protected boolean splitable job context context path file compression codec codec new compression codec factory context get configuration get codec file null codec return true return codec instanceof splittable compression codec record reader text text create record reader input split generic split task attempt context context throws io exception context set status generic split string return new key value line record reader context get configuration
1526	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\LineRecordReader.java	pooling	package org apache hadoop mapreduce lib input treats keys offset file value line line record reader extends record reader long writable text log log log factory get log line record reader string max line length mapreduce input linerecordreader line maxlength compression codec factory compression codecs null start pos end line reader fs data input stream file in seekable file position max line length long writable key null text value null compression codec codec decompressor decompressor byte record delimiter bytes line record reader line record reader byte record delimiter record delimiter bytes record delimiter initialize input split generic split task attempt context context throws io exception file split split file split generic split configuration job context get configuration max line length job get int max line length integer max value start split get start end start split get length path file split get path compression codecs new compression codec factory job codec compression codecs get codec file open file seek start split file system fs file get file system job file in fs open file compressed input decompressor codec pool get decompressor codec codec instanceof splittable compression codec split compression input stream c in splittable compression codec codec create input stream file in decompressor start end splittable compression codec read mode byblock null record delimiter bytes new line reader c in job else new line reader c in job record delimiter bytes start c in get adjusted start end c in get adjusted end file position c in else null record delimiter bytes new line reader codec create input stream file in decompressor job else new line reader codec create input stream file in decompressor job record delimiter bytes file position file in else file in seek start null record delimiter bytes new line reader file in job else new line reader file in job record delimiter bytes file position file in if first split always throw away first record always except last split read one extra line next method start start read line new text max bytes to consume start pos start boolean compressed input return codec null max bytes to consume pos return compressed input integer max value math min integer max value end pos get file position throws io exception ret val compressed input null file position ret val file position get pos else ret val pos return ret val boolean next key value throws io exception key null key new long writable key set pos value null value new text new size we always read one extra line lies outside upper split limit e end get file position end new size read line value max line length math max max bytes to consume pos max line length new size break pos new size new size max line length break line try log info skipped line size new size pos pos new size new size key null value null return false else return true long writable get current key return key text get current value return value get progress within split get progress throws io exception start end return f
1527	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\MultipleInputs.java	unrelated	package org apache hadoop mapreduce lib input this supports map reduce jobs multiple input paths different link input format link mapper path multiple inputs string dir formats mapreduce input multipleinputs dir formats string dir mappers mapreduce input multipleinputs dir mappers add link path custom link input format list inputs map reduce job add input path job job path path class extends input format input format class string input format mapping path string input format class get name configuration conf job get configuration string input formats conf get dir formats conf set dir formats input formats null input format mapping input formats input format mapping job set input format class delegating input format add link path custom link input format link mapper list inputs map reduce job add input path job job path path class extends input format input format class class extends mapper mapper class add input path job path input format class configuration conf job get configuration string mapper mapping path string mapper class get name string mappers conf get dir mappers conf set dir mappers mappers null mapper mapping mappers mapper mapping job set mapper class delegating mapper retrieves map link path link input format used map path input format get input format map job context job map path input format new hash map path input format configuration conf job get configuration string path mappings conf get dir formats split string path mapping path mappings string split path mapping split input format input format try input format input format reflection utils new instance conf get class by name split conf catch class not found exception e throw new runtime exception e put new path split input format return retrieves map link path link mapper used map path class extends mapper get mapper type map job context job configuration conf job get configuration conf get dir mappers null return collections empty map map path class extends mapper new hash map path class extends mapper string path mappings conf get dir mappers split string path mapping path mappings string split path mapping split class extends mapper map class try map class class extends mapper conf get class by name split catch class not found exception e throw new runtime exception e put new path split map class return
1528	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\NLineInputFormat.java	unrelated	package org apache hadoop mapreduce lib input n line input format splits n lines input one split in many pleasantly parallel applications process mapper processes input file computations controlled different parameters referred parameter sweeps one way achieve specify set parameters one set per line input control file input path map reduce application input dataset specified via config variable job conf the n line input format used applications splits input file default one line fed value one map task key offset e k v long writable text the location hints span whole mapred cluster n line input format extends file input format long writable text string lines per map mapreduce input lineinputformat linespermap record reader long writable text create record reader input split generic split task attempt context context throws io exception context set status generic split string return new line record reader logically splits set input files job splits n lines input one split list input split get splits job context job throws io exception list input split splits new array list input split num lines per split get num lines per split job file status status list status job splits add all get splits for file status job get configuration num lines per split return splits list file split get splits for file file status status configuration conf num lines per split throws io exception list file split splits new array list file split path file name status get path status directory throw new io exception not file file name file system fs file name get file system conf line reader lr null try fs data input stream fs open file name lr new line reader conf text line new text num lines begin length num num lr read line line num lines length num num lines num lines per split n line input format uses line record reader always reads consumes least one character upper split boundary so make sure mapper gets n lines move back upper split limits split one character begin splits add new file split file name begin length new string else splits add new file split file name begin length new string begin length length num lines num lines splits add new file split file name begin length new string finally lr null lr close return splits set number lines per split set num lines per split job job num lines job get configuration set int lines per map num lines get number lines per split get num lines per split job context job return job get configuration get int lines per map
1529	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsBinaryInputFormat.java	unrelated	package org apache hadoop mapreduce lib input input format reading keys values sequence files binary raw format sequence file as binary input format extends sequence file input format bytes writable bytes writable sequence file as binary input format super record reader bytes writable bytes writable create record reader input split split task attempt context context throws io exception return new sequence file as binary record reader read records sequence file binary raw bytes sequence file as binary record reader extends record reader bytes writable bytes writable sequence file reader start end boolean done false data output buffer buffer new data output buffer sequence file value bytes vbytes bytes writable key null bytes writable value null initialize input split split task attempt context context throws io exception interrupted exception path path file split split get path configuration conf context get configuration file system fs path get file system conf new sequence file reader fs path conf end file split split get start split get length file split split get start get position sync file split split get start sync start start get position vbytes create value bytes done start end bytes writable get current key throws io exception interrupted exception return key bytes writable get current value throws io exception interrupted exception return value retrieve name key sequence file string get key class name return get key class name retrieve name value sequence file string get value class name return get value class name read raw bytes sequence file synchronized boolean next key value throws io exception interrupted exception done return false pos get position boolean eof next raw key buffer eof key null key new bytes writable value null value new bytes writable key set buffer get data buffer get length buffer reset next raw value vbytes vbytes write uncompressed bytes buffer value set buffer get data buffer get length buffer reset return done eof pos end sync seen close throws io exception close return progress within input split get progress throws io exception interrupted exception end start return f else return math min f get position start end start
1530	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsTextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input this similar sequence file input format except generates sequence file as text record reader converts input keys values string forms calling string method sequence file as text input format extends sequence file input format text text sequence file as text input format super record reader text text create record reader input split split task attempt context context throws io exception context set status split string return new sequence file as text record reader
1531	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileAsTextRecordReader.java	unrelated	package org apache hadoop mapreduce lib input this converts input keys values string forms calling string method this sequence file as text input format line record reader text input format sequence file as text record reader extends record reader text text sequence file record reader writable comparable writable sequence file record reader text key text value sequence file as text record reader throws io exception sequence file record reader new sequence file record reader writable comparable writable initialize input split split task attempt context context throws io exception interrupted exception sequence file record reader initialize split context text get current key throws io exception interrupted exception return key text get current value throws io exception interrupted exception return value read key value pair line synchronized boolean next key value throws io exception interrupted exception sequence file record reader next key value return false key null key new text value null value new text key set sequence file record reader get current key string value set sequence file record reader get current value string return true get progress throws io exception interrupted exception return sequence file record reader get progress synchronized close throws io exception sequence file record reader close
1532	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileInputFilter.java	unrelated	package org apache hadoop mapreduce lib input a allows map red job work sample sequence files the sample decided filter set job sequence file input filter k v extends sequence file input format k v log log log factory get log file input format string filter class mapreduce input sequencefileinputfilter string filter frequency mapreduce input sequencefileinputfilter frequency string filter regex mapreduce input sequencefileinputfilter regex sequence file input filter create record reader given split record reader k v create record reader input split split task attempt context context throws io exception context set status split string return new filter record reader k v context get configuration set filter set filter class job job class filter class job get configuration set filter class filter class get name filter filter extends configurable filter function decide record filtered boolean accept object key base filters filter base implements filter configuration conf configuration get conf return conf records filter matching key regex regex filter extends filter base pattern p define filtering regex stores conf set pattern configuration conf string regex throws pattern syntax exception try pattern compile regex catch pattern syntax exception e throw new illegal argument exception invalid pattern regex conf set filter regex regex regex filter configure filter checking configuration set conf configuration conf string regex conf get filter regex regex null throw new runtime exception filter regex set p pattern compile regex conf conf filtering method if key matches regex return true otherwise return false boolean accept object key return p matcher key string matches this returns percentage records the percentage determined filtering frequency f using criteria record f for example frequency one records returned percent filter extends filter base frequency count set frequency stores conf set frequency configuration conf frequency frequency throw new illegal argument exception negative filter frequency frequency conf set int filter frequency frequency percent filter configure filter checking configuration set conf configuration conf frequency conf get int filter frequency frequency throw new runtime exception negative filter frequency frequency conf conf filtering method if record frequency return true otherwise return false boolean accept object key boolean accepted false count accepted true count frequency count return accepted this returns set records examing md digest key filtering frequency f the filtering criteria md key f md filter extends filter base frequency message digest digester md len byte digest new byte md len try digester message digest get instance md catch no such algorithm exception e throw new runtime exception e set filtering frequency configuration set frequency configuration conf frequency frequency throw new illegal argument exception negative filter frequency frequency conf set int filter frequency frequency md filter configure filter according configuration set conf configuration conf frequency conf get int filter frequency frequency throw new runtime exception negative filter frequency frequency conf conf filtering method if md key frequency return true otherwise return false boolean accept object key try hashcode key instanceof text hashcode md hashcode text key else key instanceof bytes writable hashcode md hashcode bytes writable key else byte buffer bb bb text encode key string hashcode md hashcode bb array bb limit
1533	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format link sequence file sequence file input format k v extends file input format k v record reader k v create record reader input split split task attempt context context throws io exception return new sequence file record reader k v protected get format min split size return sequence file sync interval protected list file status list status job context job throws io exception list file status files super list status job len files size len file status file files get file directory map file path p file get path file system fs p get file system job get configuration use data file files set fs get file status new path p map file data file name return files
1534	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\SequenceFileRecordReader.java	unrelated	package org apache hadoop mapreduce lib input an link record reader link sequence file sequence file record reader k v extends record reader k v sequence file reader start end boolean true k key null v value null protected configuration conf initialize input split split task attempt context context throws io exception interrupted exception file split file split file split split conf context get configuration path path file split get path file system fs path get file system conf new sequence file reader fs path conf end file split get start file split get length file split get start get position sync file split get start sync start start get position start end boolean next key value throws io exception interrupted exception return false pos get position key k next key key null pos end sync seen false key null value null else value v get current value value return k get current key return key v get current value return value return progress within input split get progress throws io exception end start return f else return math min f get position start end start synchronized close throws io exception close
1535	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\TaggedInputSplit.java	unrelated	package org apache hadoop mapreduce lib input an link input split tags another input split extra data use link delegating input format link delegating mapper tagged input split extends input split implements configurable writable class extends input split input split class input split input split class extends input format input format class class extends mapper mapper class configuration conf tagged input split default constructor creates new tagged input split tagged input split input split input split configuration conf class extends input format input format class class extends mapper mapper class input split class input split get class input split input split conf conf input format class input format class mapper class mapper class retrieves original input split input split get input split return input split retrieves input format use split class extends input format get input format class return input format class retrieves mapper use split class extends mapper get mapper class return mapper class get length throws io exception interrupted exception return input split get length string get locations throws io exception interrupted exception return input split get locations read fields data input throws io exception input split class class extends input split read class input format class class extends input format read class mapper class class extends mapper read class input split input split reflection utils new instance input split class conf serialization factory factory new serialization factory conf deserializer deserializer factory get deserializer input split class deserializer open data input stream input split input split deserializer deserialize input split class read class data input throws io exception string name text read string try return conf get class by name name catch class not found exception e throw new runtime exception read object find e write data output throws io exception text write string input split class get name text write string input format class get name text write string mapper class get name serialization factory factory new serialization factory conf serializer serializer factory get serializer input split class serializer open data output stream serializer serialize input split configuration get conf return conf set conf configuration conf conf conf
1536	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\input\TextInputFormat.java	unrelated	package org apache hadoop mapreduce lib input an link input format plain text files files broken lines either linefeed carriage return used signal end line keys position file values line text text input format extends file input format long writable text record reader long writable text create record reader input split split task attempt context context string delimiter context get configuration get textinputformat record delimiter byte record delimiter bytes null null delimiter record delimiter bytes delimiter get bytes return new line record reader record delimiter bytes protected boolean splitable job context context path file compression codec codec new compression codec factory context get configuration get codec file null codec return true return codec instanceof splittable compression codec
1537	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\jobcontrol\ControlledJob.java	unrelated	package org apache hadoop mapreduce lib jobcontrol this encapsulates map reduce job dependency it monitors states depending jobs updates state job a job starts waiting state if depending jobs depending jobs success state job state become ready if depending jobs fail job fail when ready state job submitted hadoop execution state changing running state from running state job get success failed state depending status job execution controlled job a job one following states enum state success waiting running ready failed dependent failed string create dir mapreduce jobcontrol createdir ifnotexist state state string control id assigned used job control job job mapreduce job executed info human consumption e g reason job failed string message jobs current job depends list controlled job depending jobs construct job controlled job job job list controlled job depending jobs throws io exception job job depending jobs depending jobs state state waiting control id unassigned message initialized construct job controlled job configuration conf throws io exception new job conf null string string string buffer sb new string buffer sb append job name append job get job name append n sb append job id append control id append n sb append job state append state append n sb append job mapred id append job get job id append n sb append job message append message append n depending jobs null depending jobs size sb append job depending job append n else sb append job append depending jobs size append dependeng jobs n depending jobs size sb append depending job append append sb append depending jobs get get job name append n return sb string string get job name return job get job name set job name job set job name string job name job set job name job name string get job id return control id set job id job set job id string id control id id mapred framework job id get mapred job id return job get job id synchronized job get job return job set mapreduce job synchronized set job job job job job synchronized state get job state return state set state job protected synchronized set job state state state state state synchronized string get message return message set message job synchronized set message string message message message list controlled job get dependent jobs return depending jobs add job jobs dependency list dependent jobs added job waiting run afterwards synchronized boolean add depending job controlled job depending job state state waiting allowed add jobs waiting depending jobs null depending jobs new array list controlled job return depending jobs add depending job else return false synchronized boolean completed return state state failed state state dependent failed state state success synchronized boolean ready return state state ready kill job throws io exception interrupted exception job kill job check state running job the state may remain become success failed check running state throws io exception interrupted exception try job complete job successful state state success else state state failed message job failed catch io exception ioe state state failed message string utils stringify exception ioe try job null job
1538	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\jobcontrol\JobControl.java	unrelated	package org apache hadoop mapreduce lib jobcontrol this encapsulates set map reduce jobs dependency it tracks states jobs placing different tables according states this provides ap is client app add job group get jobs group different states when job added id unique group assigned job this thread submits jobs become ready monitors states running jobs updates states jobs based state changes depending jobs states the provides ap is suspending resuming thread stopping thread job control implements runnable the thread one following state enum thread state running suspended stopped stopping ready thread state runner state thread state map string controlled job waiting jobs map string controlled job ready jobs map string controlled job running jobs map string controlled job successful jobs map string controlled job failed jobs next job id string group name construct job control group jobs job control string group name waiting jobs new hashtable string controlled job ready jobs new hashtable string controlled job running jobs new hashtable string controlled job successful jobs new hashtable string controlled job failed jobs new hashtable string controlled job next job id group name group name runner state thread state ready list controlled job list map string controlled job jobs array list controlled job retv new array list controlled job synchronized jobs controlled job job jobs values retv add job return retv list controlled job get waiting job list return list waiting jobs list controlled job get running job list return list running jobs list controlled job get ready jobs list return list ready jobs list controlled job get successful job list return list successful jobs list controlled job get failed job list return list failed jobs string get next job id next job id return group name next job id add to queue controlled job job map string controlled job queue synchronized queue queue put job get job id job add to queue controlled job job map string controlled job queue get queue job get job state add to queue job queue map string controlled job get queue state state map string controlled job retv null state state waiting retv waiting jobs else state state ready retv ready jobs else state state running retv running jobs else state state success retv successful jobs else state state failed state state dependent failed retv failed jobs return retv add new job synchronized string add job controlled job job string id get next job id job set job id id job set job state state waiting add to queue job return id add collection jobs add job collection collection controlled job jobs controlled job job jobs add job job thread state get thread state return runner state set thread state stopping thread stop wakes stop runner state thread state stopping suspend running thread suspend runner state thread state running runner state thread state suspended resume suspended thread resume runner state thread state suspended runner state thread state running synchronized check running jobs throws io exception interrupted exception map string controlled job old jobs null old jobs running jobs running jobs new hashtable string controlled job controlled
1539	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ArrayListBackedIterator.java	unrelated	package org apache hadoop mapreduce lib join this provides implementation resetable iterator the implementation uses link java util array list store elements added replaying requested prefer link stream backed iterator array list backed iterator x extends writable implements resetable iterator x iterator x iter array list x data x hold null configuration conf new configuration array list backed iterator new array list x array list backed iterator array list x data data data iter data iterator boolean next return iter next boolean next x val throws io exception iter next reflection utils copy conf iter next val null hold hold writable utils clone val null else reflection utils copy conf val hold return true return false boolean replay x val throws io exception reflection utils copy conf hold val return true reset iter data iterator add x item throws io exception data add writable utils clone item null close throws io exception iter null data null clear data clear reset
1540	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ComposableInputFormat.java	unrelated	package org apache hadoop mapreduce lib join refinement input format requiring implementors provide composable record reader instead record reader composable input format k extends writable comparable v extends writable extends input format k v composable record reader k v create record reader input split split task attempt context context throws io exception interrupted exception
1541	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ComposableRecordReader.java	unrelated	package org apache hadoop mapreduce lib join additional operations required record reader participate join composable record reader k extends writable comparable v extends writable extends record reader k v implements comparable composable record reader k return position collector occupies id return key record reader would supply call next k v k key clone key head record reader object provided key k key throws io exception create instance key k create key create instance value v create value returns true stream empty provides guarantee call next k v succeed boolean next skip key value pairs keys less equal key provided skip k key throws io exception interrupted exception while key value pairs record reader match given key register join collector provided accept composite record reader join collector jc k key throws io exception interrupted exception
1542	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeInputFormat.java	unrelated	package org apache hadoop mapreduce lib join an input format capable performing joins set data sources sorted partitioned way a user may define new join types setting property tt mapreduce join define lt ident gt tt classname in expression tt mapreduce join expr tt identifier assumed composable record reader tt mapreduce join keycomparator tt classname used compare keys join composite input format k extends writable comparable extends input format k tuple writable string join expr mapreduce join expr string join comparator mapreduce join keycomparator expression parse tree if requests proxied parser node root composite input format interpret given composite expression code func ident func func func tbl path see java lang class name java lang string path see org apache hadoop fs path path java lang string reads expression tt mapreduce join expr tt property user supplied join types tt mapreduce join define lt ident gt tt types paths supplied tt tbl tt given input paths input format listed set format configuration conf throws io exception add defaults add user identifiers conf root parser parse conf get join expr null conf adds default set identifiers parser protected add defaults try parser c node add identifier inner inner join record reader parser c node add identifier outer outer join record reader parser c node add identifier override record reader parser w node add identifier tbl wrapped record reader catch no such method exception e throw new runtime exception fatal failed init defaults e inform parser user defined types add user identifiers configuration conf throws io exception pattern x pattern compile mapreduce join define w map entry string string kv conf matcher x matcher kv get key matches try parser c node add identifier group conf get class group null composable record reader catch no such method exception e throw new io exception invalid define group e build composite input split child input formats assigning ith split child ith composite split list input split get splits job context job throws io exception interrupted exception set format job get configuration job get configuration set long mapreduce input fileinputformat split minsize long max value return root get splits job construct composite record reader children input format defined init expression the outermost join need composable necessarily composite mandating tuple writable strictly correct record reader k tuple writable create record reader input split split task attempt context task context throws io exception interrupted exception set format task context get configuration return root create record reader split task context convenience method constructing composite formats given input format inf path p return code tbl inf p string compose class extends input format inf string path return compose inf get name intern path new string buffer string convenience method constructing composite formats given operation op object inf set paths p return code op tbl inf p tbl inf p tbl inf pn string compose string op class extends input format inf string path string infname inf get name string buffer ret new string buffer op string p path compose infname p ret ret append ret set char at ret length return
1543	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeInputSplit.java	unrelated	package org apache hadoop mapreduce lib join this input split contains set child input splits any input split inserted collection must default constructor composite input split extends input split implements writable fill totsize l input split splits configuration conf new configuration composite input split composite input split capacity splits new input split capacity add input split collection capacity reached add input split throws io exception interrupted exception null splits throw new io exception uninitialized input split fill splits length throw new io exception too many splits splits fill totsize get length get ith child input split input split get return splits return aggregate length child input splits currently added get length throws io exception return totsize get length ith child input split get length throws io exception interrupted exception return splits get length collect set hosts child input splits string get locations throws io exception interrupted exception hash set string hosts new hash set string input split splits string hints get locations hints null hints length string host hints hosts add host return hosts array new string hosts size get locations ith input split string get location throws io exception interrupted exception return splits get locations write splits following format code count classn split split splitn write data output throws io exception writable utils write v int splits length input split splits text write string get class get name input split splits serialization factory factory new serialization factory conf serializer serializer factory get serializer get class serializer open data output stream serializer serialize inherit doc failing access checks read fields data input throws io exception card writable utils read v int splits null splits length card splits new input split card class extends input split cls new class card try card cls class name text read string subclass input split card splits reflection utils new instance cls null serialization factory factory new serialization factory conf deserializer deserializer factory get deserializer cls deserializer open data input stream splits input split deserializer deserialize splits catch class not found exception e throw new io exception failed split init e
1544	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\CompositeRecordReader.java	scheduler	package org apache hadoop mapreduce lib join a record reader effect joins record readers sharing common key type partitioning composite record reader k extends writable comparable key type v extends writable accepts record reader k v children x extends writable emits writables type extends composable record reader k x implements configurable id protected configuration conf resetable iterator x empty new resetable iterator empty x writable comparator cmp protected class extends writable comparable keyclass null priority queue composable record reader k q protected join collector jc protected composable record reader k extends v kids protected boolean combine object srcs tuple writable value protected k key protected x value create record reader tt capacity tt children position tt id tt parent reader the id root composite record reader convention relying recommended composite record reader id capacity class extends writable comparator cmpcl throws io exception assert capacity invalid capacity id id null cmpcl cmp reflection utils new instance cmpcl null q new priority queue composable record reader k new comparator composable record reader k compare composable record reader k composable record reader k return cmp compare key key jc new join collector capacity kids new composable record reader capacity initialize input split split task attempt context context throws io exception interrupted exception kids null kids length kids initialize composite input split split get context kids key null continue get keyclass keyclass null keyclass kids create key get class subclass writable comparable create priority queue null q cmp writable comparator get keyclass q new priority queue composable record reader k new comparator composable record reader k compare composable record reader k composable record reader k return cmp compare key key explicit check key agreement keyclass equals kids key get class throw new class cast exception child key fail agree add kid priority queue elements kids next q add kids return position collector occupies id return id inherit doc set conf configuration conf conf conf inherit doc configuration get conf return conf return sorted list record readers composite protected priority queue composable record reader k get record reader queue return q return comparator defining ordering record readers composite protected writable comparator get comparator return cmp add record reader collection the id record reader determines tuple entry appear adding record readers id undefined behavior add composable record reader k extends v rr throws io exception interrupted exception kids rr id rr collector join values this accumulates values given key child record readers if one child rr contain duplicate keys emit cross product associated values exhausted join collector k key resetable iterator x iters pos boolean first true construct collector capable handling specified number children join collector card iters new resetable iterator card iters length iters empty register given iterator position id add id resetable iterator x throws io exception iters id return key associated collection k key return key codify contents collector iterated when called record readers registered key added resetable iterators reset k key key key first true pos iters length iters length iters reset clear state information clear key null pos iters length iters clear iters
1545	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\InnerJoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join full inner join inner join record reader k extends writable comparable extends join record reader k inner join record reader id configuration conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl return true iff tuple full data sources contain key protected boolean combine object srcs tuple writable dst assert srcs length dst size srcs length dst return false return true
1546	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\JoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join base composite joins returning tuples arbitrary writables join record reader k extends writable comparable extends composite record reader k writable tuple writable join record reader id configuration conf capacity class extends writable comparator cmpcl throws io exception super id capacity cmpcl set conf conf emit next set key value pairs defined child record readers operation associated composite rr boolean next key value throws io exception interrupted exception key null key create key jc flush value reflection utils copy conf jc key key return true jc clear value null value create value priority queue composable record reader k q get record reader queue k iterkey create key q null q empty fill join collector iterkey jc reset iterkey jc flush value reflection utils copy conf jc key key return true jc clear return false tuple writable create value return create tuple writable return iterator wrapping join collector protected resetable iterator tuple writable get delegate return new join delegation iterator since join collector effecting operation need provide iterator proxy wrapping operation protected join delegation iterator implements resetable iterator tuple writable boolean next return jc next boolean next tuple writable val throws io exception return jc flush val boolean replay tuple writable val throws io exception return jc replay val reset jc reset jc key add tuple writable item throws io exception throw new unsupported operation exception close throws io exception jc close clear jc clear
1547	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\MultiFilterRecordReader.java	scheduler	package org apache hadoop mapreduce lib join base composite join returning values derived multiple sources generally tuples multi filter record reader k extends writable comparable v extends writable extends composite record reader k v v tuple writable ivalue null multi filter record reader id configuration conf capacity class extends writable comparator cmpcl throws io exception super id capacity cmpcl set conf conf for tuple emitted return value typically one values tuple modifying writables tuple permitted unlikely affect join behavior cases recommended it safer clone first protected v emit tuple writable dst throws io exception default implementation offers link emit every tuple collector outer join child r rs protected boolean combine object srcs tuple writable dst return true inherit doc boolean next key value throws io exception interrupted exception key null key create key value null value create value jc flush ivalue reflection utils copy conf jc key key reflection utils copy conf emit ivalue value return true ivalue null ivalue create tuple writable jc clear priority queue composable record reader k q get record reader queue k iterkey create key q null q empty fill join collector iterkey jc reset iterkey jc flush ivalue reflection utils copy conf jc key key reflection utils copy conf emit ivalue value return true jc clear return false initialize input split split task attempt context context throws io exception interrupted exception super initialize split context return iterator returning single value tuple protected resetable iterator v get delegate return new multi filter delegation iterator proxy join collector callback emit protected multi filter delegation iterator implements resetable iterator v boolean next return jc next boolean next v val throws io exception boolean ret ret jc flush ivalue reflection utils copy get conf emit ivalue val return ret boolean replay v val throws io exception reflection utils copy get conf emit ivalue val return true reset jc reset jc key add v item throws io exception throw new unsupported operation exception close throws io exception jc close clear jc clear
1548	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\OuterJoinRecordReader.java	unrelated	package org apache hadoop mapreduce lib join full outer join outer join record reader k extends writable comparable extends join record reader k outer join record reader id configuration conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl emit everything collector protected boolean combine object srcs tuple writable dst assert srcs length dst size return true
1549	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\OverrideRecordReader.java	unrelated	package org apache hadoop mapreduce lib join prefer quot rightmost quot data source key for example tt s s s tt prefer values s s values s s keys emitted sources override record reader k extends writable comparable v extends writable extends multi filter record reader k v override record reader id configuration conf capacity class extends writable comparator cmpcl throws io exception super id conf capacity cmpcl class extends writable valueclass null emit value highest position tuple protected v emit tuple writable dst return v dst iterator next v create value null valueclass class cls kids kids length create value get class kids length cls equals null writable cls kids create value get class valueclass cls subclass writable valueclass equals null writable return v null writable get return v reflection utils new instance valueclass null instead filling join collector iterators data sources fill rightmost key this saves space discarding sources also emits number key value pairs preferred record reader instead repeating stream n times n cardinality cross product discarded streams given key protected fill join collector k iterkey throws io exception interrupted exception priority queue composable record reader k q get record reader queue q null q empty highpos array list composable record reader k list new array list composable record reader k kids length q peek key iterkey writable comparator cmp get comparator cmp compare q peek key iterkey composable record reader k q poll highpos list get highpos id id highpos list size list add q empty break composable record reader k list remove highpos accept jc iterkey composable record reader k rr list rr skip iterkey list add composable record reader k rr list rr next q add rr
1550	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\Parser.java	unrelated	package org apache hadoop mapreduce lib join very simple shift reduce parser join expressions this sufficient user extension permitted ought replaced parser generator complex grammars supported in particular quot shift reduce quot parser states each set formals requires different internal node type responsible interpreting list tokens receives this sufficient current grammar several annoying properties might inhibit extension in particular parenthesis always function calls algebraic filter grammar would require node type must also work around internals parser for cases adding hierarchy particularly extending join record reader multi filter record reader fairly straightforward one need relevant method usually link composite record reader combine property map value identifier parser parser enum t type cif ident comma lparen rparen quot num tagged union type tokens join expression token t type type token t type type type type t type get type return type node get node throws io exception throw new io exception expected nodetype get num throws io exception throw new io exception expected numtype string get str throws io exception throw new io exception expected strtype num token extends token num num token num super t type num num num get num return num node token extends token node node node token node node super t type cif node node node get node return node str token extends token string str str token t type type string str super type str str string get str return str simple lexer wrapping stream tokenizer this encapsulates creation tagged union tokens initializes steam tokenizer lexer stream tokenizer tok lexer string tok new stream tokenizer new char array reader char array tok quote char tok parse numbers tok ordinary char tok ordinary char tok ordinary char tok word chars tok word chars token next throws io exception type tok next token switch type case stream tokenizer tt eof case stream tokenizer tt eol return null case stream tokenizer tt number return new num token tok nval case stream tokenizer tt word return new str token t type ident tok sval case return new str token t type quot tok sval default switch type case return new token t type comma case return new token t type lparen case return new token t type rparen default throw new io exception unexpected type node extends composable input format return node type registered particular identifier by default c node composite node w node quot wrapped quot nodes user nodes likely composite nodes node ident string ident throws io exception try node cstr map contains key ident throw new io exception no nodetype ident return node cstr map get ident new instance ident catch illegal access exception e throw new io exception e catch instantiation exception e throw new io exception e catch invocation target exception e throw new io exception e class ncstr sig string map string constructor extends node node cstr map new hash map string constructor extends node protected map string constructor extends composable record reader rr cstr map new hash map string constructor extends composable record reader for given identifier add mapping nodetype parse tree composable record reader
1551	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\ResetableIterator.java	unrelated	package org apache hadoop mapreduce lib join this defines stateful iterator replay elements added directly note extend link java util iterator resetable iterator t extends writable empty u extends writable implements resetable iterator u boolean next return false reset close throws io exception clear boolean next u val throws io exception return false boolean replay u val throws io exception return false add u item throws io exception throw new unsupported operation exception true call next may return value this permitted false positives false negatives boolean next assign next value actual it required elements added resetable iterator returned order call link reset fifo note call may fail nested joins e elements available none satisfying constraints join boolean next t val throws io exception assign last value returned actual boolean replay t val throws io exception set iterator return start range must called calling link add avoid concurrent modification exception reset add element collection elements iterate add t item throws io exception close datasources release resources calling methods iterator calling close undefined behavior xxx necessary close throws io exception close datasources release internal resources calling method permit object reused different datasource clear
1552	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\StreamBackedIterator.java	unrelated	package org apache hadoop mapreduce lib join this provides implementation resetable iterator this implementation uses byte array store elements added stream backed iterator x extends writable implements resetable iterator x replayable byte input stream extends byte array input stream replayable byte input stream byte arr super arr reset stream mark reset byte array output stream outbuf new byte array output stream data output stream outfbuf new data output stream outbuf replayable byte input stream inbuf data input stream infbuf stream backed iterator boolean next return infbuf null inbuf available boolean next x val throws io exception next inbuf mark val read fields infbuf return true return false boolean replay x val throws io exception inbuf reset inbuf available return false val read fields infbuf return true reset null outfbuf inbuf new replayable byte input stream outbuf byte array infbuf new data input stream inbuf outfbuf null inbuf reset stream add x item throws io exception item write outfbuf close throws io exception null infbuf infbuf close null outfbuf outfbuf close clear null inbuf inbuf reset stream outbuf reset outfbuf new data output stream outbuf
1553	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\TupleWritable.java	unrelated	package org apache hadoop mapreduce lib join writable type storing multiple link org apache hadoop io writable this general purpose tuple type in almost cases users encouraged implement serializable types perform better validation provide efficient encodings capable tuple writable relies join framework type safety assumes instances rarely persisted assumptions incompatible contrary general case tuple writable implements writable iterable writable protected bit set written writable values create empty tuple allocated storage writables tuple writable written new bit set initialize tuple storage unknown whether contain quot written quot values tuple writable writable vals written new bit set vals length values vals return true tuple element position provided boolean return written get get ith writable tuple writable get return values the number children tuple size return values length inherit doc boolean equals object instanceof tuple writable tuple writable tuple writable written equals written return false values length continue values equals get return false return true return false hash code assert false hash code designed return written hash code return iterator elements tuple note flatten tuple one may receive tuples iterator iterator writable iterator tuple writable return new iterator writable bit index written next set bit boolean next return bit index writable next return index bit index return index throw new no such element exception bit index written next set bit bit index return get return index remove written get bit index throw new illegal state exception attempt remove non existent val written clear bit index convert tuple string following tt child child childn tt string string string buffer buf new string buffer values length buf append values string buf append values length buf set char at buf length else buf append return buf string writable writes writable code code tuple writable format code count type type typen obj obj objn write data output throws io exception writable utils write v int values length write bit set values length written values length text write string values get class get name values length values write inherit doc read fields data input throws io exception card writable utils read v int values new writable card read bit set card written class extends writable cls new class card try card cls class name text read string subclass writable card cls equals null writable values null writable get else values cls new instance values read fields catch class not found exception e throw new io exception failed tuple init e catch illegal access exception e throw new io exception failed tuple init e catch instantiation exception e throw new io exception failed tuple init e record tuple contains element position provided set written written set record tuple contain element position provided clear written written clear clear record writables written without releasing storage clear written written clear writes bit set stream the first bit positions bit set written v long backwards compatibility older versions tuple writable all bit positions encoded byte every bit positions write bit set data output stream nbits bit set bit set throws io exception bits l bit set index bit set next set bit bit set index
1554	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\join\WrappedRecordReader.java	unrelated	package org apache hadoop mapreduce lib join proxy record reader participating join framework this keeps track quot head quot key value pair provided record reader keeps store values matching key source participating join wrapped record reader k extends writable comparable u extends writable extends composable record reader k u protected boolean empty false record reader k u rr id index values inserted collector protected writable comparator cmp null k key key top rr u value value assoc key resetable iterator u vjoin configuration conf new configuration class extends writable comparable keyclass null class extends writable valueclass null protected wrapped record reader id id id vjoin new stream backed iterator u for given record reader rr occupy position id collector wrapped record reader id record reader k u rr class extends writable comparator cmpcl throws io exception interrupted exception id id rr rr cmpcl null try cmp cmpcl new instance catch instantiation exception e throw new io exception e catch illegal access exception e throw new io exception e vjoin new stream backed iterator u initialize input split split task attempt context context throws io exception interrupted exception rr initialize split context conf context get configuration next key value empty keyclass key get class subclass writable comparable valueclass value get class cmp null cmp writable comparator get keyclass request new key proxied rr k create key keyclass null return k reflection utils new instance keyclass conf return k null writable get u create value valueclass null return u reflection utils new instance valueclass conf return u null writable get inherit doc id return id return key head rr k key return key clone key head rr object supplied key k qkey throws io exception reflection utils copy conf key qkey return true rr including k v pair stored object exhausted boolean next return empty skip key value pairs keys less equal key provided skip k key throws io exception interrupted exception next cmp compare key key next add iterator collector position occupied record reader values stream paired key provided ie register stream values source matching k collector accept composite record reader join collector k key throws io exception interrupted exception vjoin clear key null cmp compare key key vjoin add value next cmp compare key key add id vjoin read next k v pair head object return true iff rr exhausted boolean next key value throws io exception interrupted exception next next return true return false read next k v pair head object return true iff rr exhausted boolean next throws io exception interrupted exception empty rr next key value key rr get current key value rr get current value return empty get current key k get current key throws io exception interrupted exception return rr get current key get current value u get current value throws io exception interrupted exception return rr get current value request progress proxied rr get progress throws io exception interrupted exception return rr get progress forward close request proxied rr close throws io exception rr close implement comparable contract compare key head proxied rr another compare to
1555	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\InverseMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper swaps keys values inverse mapper k v extends mapper k v v k the inverse function input keys values swapped map k key v value context context throws io exception interrupted exception context write value key
1556	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\MultithreadedMapper.java	pooling	package org apache hadoop mapreduce lib map multithreaded implementation link org apache hadoop mapreduce mapper p it used instead default implementation bound order improve throughput p mapper implementations using map runnable must thread safe p the map reduce job configured mapper use via link set mapper class configuration class number thread thread pool use link get number of threads configuration method the default value threads p multithreaded mapper k v k v extends mapper k v k v log log log factory get log multithreaded mapper string num threads mapreduce mapper multithreadedmapper threads string map class mapreduce mapper multithreadedmapper mapclass class extends mapper k v k v map class context outer list map runner runners the number threads thread pool run map function get number of threads job context job return job get configuration get int num threads set number threads pool running maps set number of threads job job threads job get configuration set int num threads threads get application mapper k v k v class mapper k v k v get mapper class job context job return class mapper k v k v job get configuration get class map class mapper set application mapper k v k v set mapper class job job class extends mapper k v k v cls multithreaded mapper assignable from cls throw new illegal argument exception can recursive multithreaded mapper instances job get configuration set class map class cls mapper run application maps using thread pool run context context throws io exception interrupted exception outer context number of threads get number of threads context map class get mapper class context log debug enabled log debug configuring multithread runner use number of threads threads runners new array list map runner number of threads number of threads map runner thread new map runner context thread start runners add thread number of threads map runner thread runners get thread join throwable th thread throwable th null th instanceof io exception throw io exception th else th instanceof interrupted exception throw interrupted exception th else throw new runtime exception th sub map record reader extends record reader k v k key v value configuration conf close throws io exception get progress throws io exception interrupted exception return initialize input split split task attempt context context throws io exception interrupted exception conf context get configuration boolean next key value throws io exception interrupted exception synchronized outer outer next key value return false key reflection utils copy outer get configuration outer get current key key value reflection utils copy conf outer get current value value return true k get current key return key v get current value return value sub map record writer extends record writer k v close task attempt context context throws io exception interrupted exception write k key v value throws io exception interrupted exception synchronized outer outer write key value sub map status reporter extends status reporter counter get counter enum name return outer get counter name counter get counter string group string name return outer get counter group name progress outer progress set status string status
1557	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\RegexMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper extracts text matching regular expression regex mapper k extends mapper k text text long writable string pattern mapreduce mapper regex string group mapreduce mapper regexmapper group pattern pattern group setup context context configuration conf context get configuration pattern pattern compile conf get pattern group conf get int group map k key text value context context throws io exception interrupted exception string text value string matcher matcher pattern matcher text matcher find context write new text matcher group group new long writable
1558	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\TokenCounterMapper.java	unrelated	package org apache hadoop mapreduce lib map tokenize input values emit word count token counter mapper extends mapper object text text int writable int writable one new int writable text word new text map object key text value context context throws io exception interrupted exception string tokenizer itr new string tokenizer value string itr more tokens word set itr next token context write word one
1559	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\map\WrappedMapper.java	unrelated	package org apache hadoop mapreduce lib map a link mapper wraps given one allow custom link mapper context implementations wrapped mapper keyin valuein keyout valueout extends mapper keyin valuein keyout valueout get wrapped link mapper context custom implementations mapper keyin valuein keyout valueout context get map context map context keyin valuein keyout valueout map context return new context map context context extends mapper keyin valuein keyout valueout context protected map context keyin valuein keyout valueout map context context map context keyin valuein keyout valueout map context map context map context get input split map input split get input split return map context get input split keyin get current key throws io exception interrupted exception return map context get current key valuein get current value throws io exception interrupted exception return map context get current value boolean next key value throws io exception interrupted exception return map context next key value counter get counter enum counter name return map context get counter counter name counter get counter string group name string counter name return map context get counter group name counter name output committer get output committer return map context get output committer write keyout key valueout value throws io exception interrupted exception map context write key value string get status return map context get status task attempt id get task attempt id return map context get task attempt id set status string msg map context set status msg path get archive class paths return map context get archive class paths string get archive timestamps return map context get archive timestamps uri get cache archives throws io exception return map context get cache archives uri get cache files throws io exception return map context get cache archives class extends reducer get combiner class throws class not found exception return map context get combiner class configuration get configuration return map context get configuration path get file class paths return map context get file class paths string get file timestamps return map context get file timestamps raw comparator get grouping comparator return map context get grouping comparator class extends input format get input format class throws class not found exception return map context get input format class string get jar return map context get jar job id get job id return map context get job id string get job name return map context get job name boolean get job setup cleanup needed return map context get job setup cleanup needed boolean get task cleanup needed return map context get task cleanup needed path get local cache archives throws io exception return map context get local cache archives path get local cache files throws io exception return map context get local cache files class get map output key class return map context get map output key class class get map output value class return map context get map output value class class extends mapper get mapper class throws class not found exception return map context get mapper class get max map attempts return map context get max map attempts get max reduce attempts return map context
1560	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputCommitter.java	unrelated	package org apache hadoop mapreduce lib output an link output committer commits files specified job output directory e mapreduce output fileoutputformat outputdir file output committer extends output committer log log log factory get log file output committer temporary directory name protected string temp dir name temporary string succeeded file name success string successful job output dir marker mapreduce fileoutputcommitter marksuccessfuljobs file system output file system null path output path null path work path null create file output committer file output committer path output path task attempt context context throws io exception output path null output path output path output file system output path get file system context get configuration work path new path output path file output committer temp dir name path separator context get task attempt id string make qualified output file system create temporary directory root task work directories setup job job context context throws io exception output path null path tmp dir new path output path file output committer temp dir name file system file sys tmp dir get file system context get configuration file sys mkdirs tmp dir log error mkdirs failed create tmp dir string true job requires output dir marked successful job note default set true boolean mark output dir configuration conf return conf get boolean successful job output dir marker true create success file job output dir mark output dir successful mr job config context throws io exception output path null create file output folder mark job completion path file path new path output path succeeded file name output file system create file path close delete temporary directory including work directories create success file make successful commit job job context context throws io exception delete temporary folder create done file p folder cleanup job context mark output dir context get configuration mark output dir successful context cleanup job job context context throws io exception output path null path tmp dir new path output path file output committer temp dir name file system file sys tmp dir get file system context get configuration file sys exists tmp dir file sys delete tmp dir true else log warn output path null cleanup delete temporary directory including work directories abort job job context context job status state state throws io exception delete temporary folder cleanup job context no task setup required setup task task attempt context context throws io exception file output committer setup task anything because temporary task directory created demand task writing move files work directory job output directory commit task task attempt context context throws io exception task attempt id attempt id context get task attempt id work path null context progress output file system exists work path move task outputs place move task outputs context output file system output path work path delete temporary task specific output directory output file system delete work path true log warn failed delete temporary output directory task attempt id work path log info saved output task attempt id output path move files work directory output move task outputs task attempt context context file system fs path job output
1561	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output a base link output format read link file system file output format k v extends output format k v construct output file names output directory listing sorted lexicographically positions correspond output partitions number format number format number format get instance protected string base output name mapreduce output basename protected string part part number format set minimum integer digits number format set grouping used false file output committer committer null string compress mapreduce output fileoutputformat compress string compress codec mapreduce output fileoutputformat compress codec string compress type mapreduce output fileoutputformat compress type string outdir mapreduce output fileoutputformat outputdir set whether output job compressed set compress output job job boolean compress job get configuration set boolean file output format compress compress is job output compressed code false code otherwise boolean get compress output job context job return job get configuration get boolean file output format compress false set link compression codec used compress job outputs compress job outputs set output compressor class job job class extends compression codec codec class set compress output job true job get configuration set class file output format compress codec codec class compression codec get link compression codec compressing job outputs job outputs class extends compression codec get output compressor class job context job class extends compression codec default value class extends compression codec codec class default value configuration conf job get configuration string name conf get file output format compress codec name null try codec class conf get class by name name subclass compression codec catch class not found exception e throw new illegal argument exception compression codec name found e return codec class record writer k v get record writer task attempt context job throws io exception interrupted exception check output specs job context job throws file already exists exception io exception ensure output directory set already path dir get output path job dir null throw new invalid job conf exception output directory set get delegation token dir file system token cache obtain tokens for namenodes job get credentials new path dir job get configuration dir get file system job get configuration exists dir throw new file already exists exception output directory dir already exists set link path output directory map reduce job map reduce job set output path job job path output dir throws io exception output dir output dir get file system job get configuration make qualified output dir job get configuration set file output format outdir output dir string get link path output directory map reduce job path get output path job context job string name job get configuration get file output format outdir return name null null new path name get link path task temporary output directory map reduce job id side effect files tasks side effect files p some applications need create write side files differ actual job outputs p in cases could issues instances tip running simultaneously e g speculative tasks trying open write file path hdfs hence application writer pick unique names per task attempt e g using attemptid say tt attempt tt
1562	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FileOutputFormatCounter.java	unrelated	package org apache hadoop mapreduce lib output counters used task enum file output format counter bytes written
1563	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\FilterOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output filter output format convenience wraps output format filter output format k v extends output format k v protected output format k v base out filter output format base out null create filter output format based underlying output format filter output format output format k v base out base out base out record writer k v get record writer task attempt context context throws io exception interrupted exception return get base out get record writer context check output specs job context context throws io exception interrupted exception get base out check output specs context output committer get output committer task attempt context context throws io exception interrupted exception return get base out get output committer context output format k v get base out throws io exception base out null throw new io exception output format set filter output format return base out code filter record writer code convenience wrapper extends link record writer filter record writer k v extends record writer k v protected record writer k v raw writer null filter record writer raw writer null filter record writer record writer k v rwriter raw writer rwriter write k key v value throws io exception interrupted exception get raw writer write key value close task attempt context context throws io exception interrupted exception get raw writer close context record writer k v get raw writer throws io exception raw writer null throw new io exception record writer set filter record writer return raw writer
1564	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\LazyOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output a convenience creates output lazily lazy output format k v extends filter output format k v string output format mapreduce output lazyoutputformat outputformat set underlying output format lazy output format set output format class job job class extends output format class job set output format class lazy output format job get configuration set class output format class output format get base output format configuration conf throws io exception base out output format k v reflection utils new instance conf get class output format null conf base out null throw new io exception output format set lazy output format record writer k v get record writer task attempt context context throws io exception interrupted exception base out null get base output format context get configuration return new lazy record writer k v base out context check output specs job context context throws io exception interrupted exception base out null get base output format context get configuration super check output specs context output committer get output committer task attempt context context throws io exception interrupted exception base out null get base output format context get configuration return super get output committer context a convenience used lazy output format lazy record writer k v extends filter record writer k v output format k v output format task attempt context task context lazy record writer output format k v task attempt context task context throws io exception interrupted exception output format task context task context write k key v value throws io exception interrupted exception raw writer null raw writer output format get record writer task context raw writer write key value close task attempt context context throws io exception interrupted exception raw writer null raw writer close context
1565	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\MapFileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link org apache hadoop mapreduce output format writes link map file map file output format extends file output format writable comparable writable record writer writable comparable writable get record writer task attempt context context throws io exception configuration conf context get configuration compression codec codec null compression type compression type compression type none get compress output context find kind compression compression type sequence file output format get output compression type context find right codec class codec class get output compressor class context default codec codec compression codec reflection utils new instance codec class conf path file get default work file context file system fs file get file system conf ignore progress parameter since map file local map file writer new map file writer conf fs file string context get output key class subclass writable comparable context get output value class subclass writable compression type codec context return new record writer writable comparable writable write writable comparable key writable value throws io exception append key value close task attempt context context throws io exception close open output generated format map file reader get readers path dir configuration conf throws io exception file system fs dir get file system conf path names file util stat paths fs list status dir sort names hash partitioning works arrays sort names map file reader parts new map file reader names length names length parts new map file reader fs names string conf return parts get entry output generated k extends writable comparable v extends writable writable get entry map file reader readers partitioner k v partitioner k key v value throws io exception part partitioner get partition key value readers length return readers part get key value
1566	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\MultipleOutputs.java	unrelated	package org apache hadoop mapreduce lib output the multiple outputs simplifies writing output data multiple outputs p case one writing additional outputs job default output each additional output named output may configured code output format code key value p case two write data different files provided user p p multiple outputs supports counters default disabled the counters group link multiple outputs name the names counters output name these count number records written output name p usage pattern job submission pre job job new job file input format set input path job dir file output format set output path job dir job set mapper class mo map job set reducer class mo reduce defines additional single text based output text job multiple outputs add named output job text text output format long writable text defines additional sequence file based output sequence job multiple outputs add named output job seq sequence file output format long writable text job wait for completion true pre p usage reducer pre k v string generate file name k k v v return k string v string mo reduce extends reducer lt writable comparable writable writable comparable writable gt multiple outputs mos setup context context mos new multiple outputs context reduce writable comparable key iterator lt writable gt values context context throws io exception mos write text key new text hello mos write seq long writable new text bye seq mos write seq long writable key new text chau seq b mos write key new text value generate file name key new text value cleanup context throws io exception mos close pre multiple outputs keyout valueout string multiple outputs mapreduce multipleoutputs string mo prefix mapreduce multipleoutputs named output string format format string key key string value value string counters enabled mapreduce multipleoutputs counters counters group used counters multiple outputs string counters group multiple outputs get name cache task contexts map string task attempt context task contexts new hash map string task attempt context checks named output name valid token check token name string named output named output null named output length throw new illegal argument exception name cannot null emtpy char ch named output char array ch a ch z continue ch ch z continue ch ch continue throw new illegal argument exception name cannot ch char checks output name valid name cannot name used default output check base output path string output path output path equals file output format part throw new illegal argument exception output name cannot part checks named output name valid check named output name job context job string named output boolean already defined check token name named output check base output path named output list string defined channels get named outputs list job already defined defined channels contains named output throw new illegal argument exception named output named output already already defined else already defined defined channels contains named output throw new illegal argument exception named output named output defined returns list channel names list string get named outputs list job context job list string names new array list string string tokenizer st new
1567	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\NullOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output consume outputs put dev null null output format k v extends output format k v record writer k v get record writer task attempt context context return new record writer k v write k key v value close task attempt context context check output specs job context context output committer get output committer task attempt context context return new output committer abort task task attempt context task context cleanup job job context job context commit task task attempt context task context boolean needs task commit task attempt context task context return false setup job job context job context setup task task attempt context task context
1568	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\SequenceFileAsBinaryOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link org apache hadoop mapreduce output format writes keys values link sequence file binary raw format sequence file as binary output format extends sequence file output format bytes writable bytes writable string key class mapreduce output seqbinaryoutputformat key string value class mapreduce output seqbinaryoutputformat value inner used append raw writable value bytes implements value bytes bytes writable value writable value bytes value null writable value bytes bytes writable value value value reset bytes writable value value value write uncompressed bytes data output stream stream throws io exception stream write value get bytes value get length write compressed bytes data output stream stream throws illegal argument exception io exception throw new unsupported operation exception writable value bytes support record compression get size return value get length set key link sequence file p this allows user specify key different actual link bytes writable used writing p set sequence file output key class job job class class job get configuration set class key class class object set value link sequence file p this allows user specify value different actual link bytes writable used writing p set sequence file output value class job job class class job get configuration set class value class class object get key link sequence file class extends writable comparable get sequence file output key class job context job return job get configuration get class key class job get output key class subclass writable comparable writable comparable get value link sequence file class extends writable get sequence file output value class job context job return job get configuration get class value class job get output value class subclass writable writable record writer bytes writable bytes writable get record writer task attempt context context throws io exception sequence file writer get sequence writer context get sequence file output key class context get sequence file output value class context return new record writer bytes writable bytes writable writable value bytes wvaluebytes new writable value bytes write bytes writable bkey bytes writable bvalue throws io exception wvaluebytes reset bvalue append raw bkey get bytes bkey get length wvaluebytes wvaluebytes reset null close task attempt context context throws io exception close check output specs job context job throws io exception super check output specs job get compress output job get output compression type job compression type record throw new invalid job conf exception sequence file as binary output format support record compression
1569	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\SequenceFileOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link output format writes link sequence file sequence file output format k v extends file output format k v protected sequence file writer get sequence writer task attempt context context class key class class value class throws io exception configuration conf context get configuration compression codec codec null compression type compression type compression type none get compress output context find kind compression compression type get output compression type context find right codec class codec class get output compressor class context default codec codec compression codec reflection utils new instance codec class conf get path temporary output file path file get default work file context file system fs file get file system conf return sequence file create writer fs conf file key class value class compression type codec context record writer k v get record writer task attempt context context throws io exception interrupted exception sequence file writer get sequence writer context context get output key class context get output value class return new record writer k v write k key v value throws io exception append key value close task attempt context context throws io exception close get link compression type output link sequence file defaulting link compression type record compression type get output compression type job context job string val job get configuration get file output format compress type compression type record string return compression type value of val set link compression type output link sequence file link sequence file set output compression type job job compression type style set compress output job true job get configuration set file output format compress type style string
1570	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\output\TextOutputFormat.java	unrelated	package org apache hadoop mapreduce lib output an link output format writes plain text files text output format k v extends file output format k v string seperator mapreduce output textoutputformat separator protected line record writer k v extends record writer k v string utf utf byte newline try newline n get bytes utf catch unsupported encoding exception uee throw new illegal argument exception find utf encoding protected data output stream byte key value separator line record writer data output stream string key value separator try key value separator key value separator get bytes utf catch unsupported encoding exception uee throw new illegal argument exception find utf encoding line record writer data output stream write object byte stream handling text special case write object object throws io exception instanceof text text text write get bytes get length else write string get bytes utf synchronized write k key v value throws io exception boolean null key key null key instanceof null writable boolean null value value null value instanceof null writable null key null value return null key write object key null key null value write key value separator null value write object value write newline synchronized close task attempt context context throws io exception close record writer k v get record writer task attempt context job throws io exception interrupted exception configuration conf job get configuration boolean compressed get compress output job string key value separator conf get seperator compression codec codec null string extension compressed class extends compression codec codec class get output compressor class job gzip codec codec compression codec reflection utils new instance codec class conf extension codec get default extension path file get default work file job extension file system fs file get file system conf compressed fs data output stream file out fs create file false return new line record writer k v file out key value separator else fs data output stream file out fs create file false return new line record writer k v new data output stream codec create output stream file out key value separator
1571	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\BinaryPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition p partition link binary comparable keys using configurable part bytes array returned link binary comparable get bytes p p the subarray used partitioning defined means following properties ul li mapreduce partition binarypartitioner left offset left offset array default li li mapreduce partition binarypartitioner right offset right offset array default li ul like python negative positive offsets allowed meaning slightly different in case array length instance possible offsets pre code b b b b b code pre the first row numbers gives position offsets array second row gives corresponding negative offsets contrary python specified subarray byte code code code j code first last element repectively code code code j code left right offset p for hadoop programs written java advisable use one following convenience methods setting offsets ul li link set offsets li li link set left offset li li link set right offset li ul p binary partitioner v extends partitioner binary comparable v implements configurable string left offset property name mapreduce partition binarypartitioner left offset string right offset property name mapreduce partition binarypartitioner right offset set subarray used partitioning code bytes left right code python syntax set offsets configuration conf left right conf set int left offset property name left conf set int right offset property name right set subarray used partitioning code bytes offset code python syntax set left offset configuration conf offset conf set int left offset property name offset set subarray used partitioning code bytes offset code python syntax set right offset configuration conf offset conf set int right offset property name offset configuration conf left offset right offset set conf configuration conf conf conf left offset conf get int left offset property name right offset conf get int right offset property name configuration get conf return conf use specified slice array returned link binary comparable get bytes partition get partition binary comparable key v value num partitions length key get length left index left offset length length right index right offset length length hash writable comparator hash bytes key get bytes left index right index left index return hash integer max value num partitions
1572	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\HashPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition partition keys link object hash code hash partitioner k v extends partitioner k v use link object hash code partition get partition k key v value num reduce tasks return key hash code integer max value num reduce tasks
1573	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\InputSampler.java	unrelated	package org apache hadoop mapreduce lib partition utility collecting samples writing partition file link total order partitioner input sampler k v extends configured implements tool log log log factory get log input sampler print usage system println sampler r reduces n format input format n key class map input output key n split random pcnt num samples maxsplits sample random splits random general n split sample num samples maxsplits sample first records splits random data n split interval pcnt maxsplits sample splits intervals sorted data system println default sampler split random tool runner print generic command usage system return input sampler configuration conf set conf conf interface sample using link org apache hadoop mapreduce input format sampler k v for given job collect return subset keys input data k get sample input format k v inf job job throws io exception interrupted exception samples first n records splits inexpensive way sample random data split sampler k v implements sampler k v num samples max splits sampled create split sampler sampling em em splits takes first num samples num splits records split splits split sampler num samples num samples integer max value create new split sampler splits split sampler num samples max splits sampled num samples num samples max splits sampled max splits sampled from split sampled take first num samples num splits records k get sample input format k v inf job job throws io exception interrupted exception list input split splits inf get splits job array list k samples new array list k num samples splits to sample math min max splits sampled splits size samples per split num samples splits to sample records splits to sample task attempt context sampling context new task attempt context impl job get configuration new task attempt id record reader k v reader inf create record reader splits get sampling context reader initialize splits get sampling context reader next key value samples add reflection utils copy job get configuration reader get current key null records samples per split records break reader close return k samples array sample random points input general purpose sampler takes num samples max splits sampled inputs split random sampler k v implements sampler k v freq num samples max splits sampled create new random sampler sampling em em splits this read every split client expensive splits random sampler freq num samples freq num samples integer max value create new random sampler splits random sampler freq num samples max splits sampled freq freq num samples num samples max splits sampled max splits sampled randomize split order take specified number keys split sampled key selected specified probability possibly replaced subsequently selected key quota keys split satisfied k get sample input format k v inf job job throws io exception interrupted exception list input split splits inf get splits job array list k samples new array list k num samples splits to sample math min max splits sampled splits size random r new random seed r next long r set seed seed log debug seed seed shuffle splits splits size input split tmp splits
1574	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldBasedComparator.java	unrelated	package org apache hadoop mapreduce lib partition this comparator implementation provides subset features provided unix gnu sort in particular supported features n sort numerically r reverse result comparison k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options nr described we assume fields key separated link job context map output key field seperator key field based comparator k v extends writable comparator implements configurable key field helper key field helper new key field helper string comparator options mapreduce partition keycomparator options byte negative byte byte zero byte byte decimal byte configuration conf set conf configuration conf conf conf string option conf get comparator options string key field separator conf get mr job config map output key field seperator key field helper set key field separator key field separator key field helper parse option option configuration get conf return conf key field based comparator super text compare byte b byte b n writable utils decode v int size b n writable utils decode v int size b list key description key specs key field helper key specs key specs size return compare bytes b n n b n n length indices first key field helper get word lengths b n length indices second key field helper get word lengths b n key description key spec key specs start char first key field helper get start offset b n length indices first key spec end char first key field helper get end offset b n length indices first key spec start char second key field helper get start offset b n length indices second key spec end char second key field helper get end offset b n length indices second key spec result result compare byte sequence b start char first end char first b start char second end char second key spec return result return compare byte sequence byte first start end byte second start end key description key start key reverse return return start key reverse return return compare result key numeric compare result compare bytes first start end start second start end start key numeric compare result numerical compare first start end second start end key reverse return compare result return compare result numerical compare byte start end byte b start end start j start mul byte first byte first b b j first negative first b negative check cases like declared equal return one negative compare start end b start end first b negative first negative check cases like declared equal return one negative compare b start end start end j first b negative first negative mul skip zer os end zero break j end b j zero break j skip equal characters stopping first nondigit char the nondigit character could end j end isdigit b j break j end first j end first b b j store result difference this could result number
1575	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldBasedPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition defines way partition keys based certain key fields also see link key field based comparator the key specification supported form k pos pos pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field key field based partitioner k v extends partitioner k v implements configurable log log log factory get log key field based partitioner get name string partitioner options mapreduce partition keypartitioner options num of partition fields key field helper key field helper new key field helper configuration conf set conf configuration conf conf conf string key field separator conf get mr job config map output key field seperator key field helper set key field separator key field separator conf get num key fields partition null log warn using deprecated num key fields partition use mapreduce partition keypartitioner options instead num of partition fields conf get int num key fields partition key field helper set key field spec num of partition fields else string option conf get partitioner options key field helper parse option option configuration get conf return conf get partition k key v value num reduce tasks byte key bytes list key description key specs key field helper key specs key specs size return get partition key string hash code num reduce tasks try key bytes key string get bytes utf catch unsupported encoding exception e throw new runtime exception the current system support utf encoding e return key empty key bytes length return length indices first key field helper get word lengths key bytes key bytes length current hash key description key spec key specs start char key field helper get start offset key bytes key bytes length length indices first key spec key found continue start char continue end char key field helper get end offset key bytes key bytes length length indices first key spec current hash hash code key bytes start char end char current hash return get partition current hash num reduce tasks protected hash code byte b start end current hash start end current hash current hash b return current hash protected get partition hash num reduce tasks return hash integer max value num reduce tasks set link key field based partitioner options used link partitioner pos form f c opts f number key field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field set key field partitioner options job job string key spec job get configuration set partitioner options key spec get link key field based partitioner options string get key field partitioner option job context job return job get configuration get partitioner options
1576	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\KeyFieldHelper.java	unrelated	package org apache hadoop mapreduce lib partition this used link key field based comparator link key field based partitioner defines methods parsing key specifications the key specification form k pos pos pos form f c opts f number field use c number first character beginning field fields character posns numbered starting character position zero pos indicates field last character if c omitted pos defaults beginning field omitted pos defaults end field opts ordering options supported options nr key field helper protected key description begin field idx begin char end field idx end char boolean numeric boolean reverse string string return k begin field idx begin char end field idx end char numeric n reverse r list key description key specs new array list key description byte key field separator boolean key spec seen false set key field separator string key field separator try key field separator key field separator get bytes utf catch unsupported encoding exception e throw new runtime exception the current system support utf encoding e required backcompatibility num key fields partition link key field based partitioner set key field spec start end end start key description k new key description k begin field idx start k end field idx end key spec seen true key specs add k list key description key specs return key specs get word lengths byte b start end given like hello returns array like first element number fields key spec seen key specs whole key one word return new lengths new curr len lengths lengths length idx pos pos utf byte array utils find bytes b start end key field separator idx curr len lengths temp lengths lengths new curr len lengths curr len lengths system arraycopy temp lengths temp length lengths idx pos start start pos start end lengths idx end start lengths idx number words first element return lengths get start offset byte b start end length indices key description k k keyspec start char length indices note th element number fields key length indices k begin field idx position k begin field idx position length indices key field separator length position k begin char end start return start position k begin char return get end offset byte b start end length indices key description k k keyspec end char length indices note th element number fields key k end field idx end field specified keyspec so remaining part key considered entirety return end length indices k end field idx position k end field idx position length indices key field separator length k end char position length indices position k end char end start return start position k end char return end return end parse option string option option null option equals default comparison return string tokenizer args new string tokenizer option key description global new key description args more tokens string arg args next token arg equals n global numeric true arg equals r global reverse true arg equals nr global numeric true global reverse true arg starts with k key description k parse key arg args k null key specs
1577	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\partition\TotalOrderPartitioner.java	unrelated	package org apache hadoop mapreduce lib partition partitioner effecting total order reading split points externally generated source total order partitioner k extends writable comparable v extends partitioner k v implements configurable node partitions string default path partition lst string partitioner path mapreduce totalorderpartitioner path string max trie depth mapreduce totalorderpartitioner trie maxdepth string natural order mapreduce totalorderpartitioner naturalorder configuration conf total order partitioner read partition file build indexing data structures if keytype link org apache hadoop io binary comparable tt total order partitioner natural order tt false trie first tt total order partitioner max trie depth tt bytes built otherwise keys located using binary search partition keyset using link org apache hadoop io raw comparator defined job the input file must sorted comparator contain link job get num reduce tasks keys set conf configuration conf try conf conf string parts get partition file conf path part file new path parts file system fs default path equals parts file system get local conf assume distributed cache part file get file system conf job job new job conf class k key class class k job get map output key class k split points read partitions fs part file key class conf split points length job get num reduce tasks throw new io exception wrong number partitions keyset raw comparator k comparator raw comparator k job get sort comparator split points length comparator compare split points split points throw new io exception split points order boolean nat order conf get boolean natural order true nat order binary comparable assignable from key class partitions build trie binary comparable split points split points length new byte now blocks identical splitless trie nodes represented reentrantly develop leaf trie node one split point reason depth limit refute stack overflow bloat pathological case split points mostly look like bytes iii iixii iii therefore make default depth limit large huge conf get int max trie depth else partitions new binary search node split points comparator catch io exception e throw new illegal argument exception can read partitions file e configuration get conf return conf construction know keytype get partition k key v value num partitions return partitions find partition key set path sequence file storing sorted partition keyset it must case tt r tt reduces tt r tt keys sequence file set partition file configuration conf path p conf set partitioner path p string get path sequence file storing sorted partition keyset string get partition file configuration conf return conf get partitioner path default path interface partitioner locate key partition keyset node t locate partition keyset k st ki ki defines partition implicit k inf kn inf k partitions find partition t key base trie nodes if keytype memcomp able builds tries first tt total order partitioner max trie depth tt bytes trie node implements node binary comparable level trie node level level level get level return level for types link org apache hadoop io binary comparable disabled tt total order partitioner natural order tt search partition keyset binary search binary search node implements node k k split points raw comparator
1578	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\IntSumReducer.java	unrelated	package org apache hadoop mapreduce lib reduce int sum reducer key extends reducer key int writable key int writable int writable result new int writable reduce key key iterable int writable values context context throws io exception interrupted exception sum int writable val values sum val get result set sum context write key result
1579	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\LongSumReducer.java	unrelated	package org apache hadoop mapreduce lib reduce long sum reducer key extends reducer key long writable key long writable long writable result new long writable reduce key key iterable long writable values context context throws io exception interrupted exception sum long writable val values sum val get result set sum context write key result
1580	mapreduce\src\java\org\apache\hadoop\mapreduce\lib\reduce\WrappedReducer.java	unrelated	package org apache hadoop mapreduce lib reduce a link reducer wraps given one allow custom link reducer context implementations wrapped reducer keyin valuein keyout valueout extends reducer keyin valuein keyout valueout a wrapped link reducer context custom implementations reducer keyin valuein keyout valueout context get reducer context reduce context keyin valuein keyout valueout reduce context return new context reduce context context extends reducer keyin valuein keyout valueout context protected reduce context keyin valuein keyout valueout reduce context context reduce context keyin valuein keyout valueout reduce context reduce context reduce context keyin get current key throws io exception interrupted exception return reduce context get current key valuein get current value throws io exception interrupted exception return reduce context get current value boolean next key value throws io exception interrupted exception return reduce context next key value counter get counter enum counter name return reduce context get counter counter name counter get counter string group name string counter name return reduce context get counter group name counter name output committer get output committer return reduce context get output committer write keyout key valueout value throws io exception interrupted exception reduce context write key value string get status return reduce context get status task attempt id get task attempt id return reduce context get task attempt id set status string msg reduce context set status msg path get archive class paths return reduce context get archive class paths string get archive timestamps return reduce context get archive timestamps uri get cache archives throws io exception return reduce context get cache archives uri get cache files throws io exception return reduce context get cache archives class extends reducer get combiner class throws class not found exception return reduce context get combiner class configuration get configuration return reduce context get configuration path get file class paths return reduce context get file class paths string get file timestamps return reduce context get file timestamps raw comparator get grouping comparator return reduce context get grouping comparator class extends input format get input format class throws class not found exception return reduce context get input format class string get jar return reduce context get jar job id get job id return reduce context get job id string get job name return reduce context get job name boolean get job setup cleanup needed return reduce context get job setup cleanup needed boolean get task cleanup needed return reduce context get task cleanup needed path get local cache archives throws io exception return reduce context get local cache archives path get local cache files throws io exception return reduce context get local cache files class get map output key class return reduce context get map output key class class get map output value class return reduce context get map output value class class extends mapper get mapper class throws class not found exception return reduce context get mapper class get max map attempts return reduce context get max map attempts get max reduce attempts return reduce context get max reduce attempts get num reduce tasks return reduce context get num reduce tasks
1581	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\ClientProtocol.java	scheduler	package org apache hadoop mapreduce protocol protocol job client central job tracker use communicate the job client use methods submit job execution learn current system status server principal jt config jt user name client protocol extends versioned protocol changing version id l since get task completion events method changed changed since kill task string boolean added version added jobtracker state cluster status version max tasks cluster status replaced max map tasks max reduce tasks hadoop version change counters representation hadoop version added get all jobs hadoop version change job task id use corresponding objects rather strings version change counter representation hadoop version added get system dir hadoop version changed job profile queue name hadoop version added get cleanup task reports cleanup progress job status part hadoop version added get job queue infos get job queue info queue name get all jobs queue part hadoop version added set priority hadoop version added killed status job status part hadoop version added get setup task reports setup progress job status part hadoop version get cluster status returns amount memory used server hadoop version added blacklisted trackers cluster status hadoop version modified task report tip status modified method get cluster status take boolean argument hadoop version modified cluster status tasktracker expiry interval hadoop version modified task id aware new task types version added method get queue acls for current user get queue acls info user version modified job queue info inlucde queue state part hadoop version modified cluster status black list info encapsulates reasons report blacklisted node version added fields job status hadoop version added properties job queue info part mapreduce added new api get root queues get child queues string queue name version changed protocol use new api objects and protocol renamed job submission protocol client protocol version added get job history dir part mapreduce version added reserved slots running tasks total job submissions cluster metrics part mapreduce version job submission files uploaded staging area user home dir job tracker reads required files staging area using user credentials passed via rpc version added token storage submit job version added delegation tokens add renew cancel version added job ac ls job status part mapreduce version modified submit job use credentials instead token storage version added method get queue admins queue name part mapreduce version added method get job tracker status part mapreduce version id l allocate name job job id get new job id throws io exception interrupted exception submit job execution returns latest profile job job status submit job job id job id string job submit dir credentials ts throws io exception interrupted exception get current status cluster cluster metrics get cluster metrics throws io exception interrupted exception get job tracker state state get job tracker state throws io exception interrupted exception get job tracker status job tracker status get job tracker status throws io exception interrupted exception get task tracker expiry interval throws io exception interrupted exception get administrators given job queue this method hadoop internal use submitted access control list get queue admins string queue name throws io exception kill indicated job kill
1582	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\ClientProtocolProvider.java	unrelated	package org apache hadoop mapreduce protocol client protocol provider client protocol create configuration conf throws io exception client protocol create inet socket address addr configuration conf throws io exception close client protocol client protocol throws io exception
1583	mapreduce\src\java\org\apache\hadoop\mapreduce\protocol\package-info.java	unrelated	package org apache hadoop mapreduce protocol
1584	mapreduce\src\java\org\apache\hadoop\mapreduce\security\SecureShuffleUtils.java	unrelated	package org apache hadoop mapreduce security utilities generating kyes hashes verifying shuffle secure shuffle utils string http header url hash url hash string http header reply url hash reply hash base encoded hash msg string generate hash byte msg secret key key return new string base encode base generate byte hash msg key calculate hash msg byte generate byte hash byte msg secret key key return job token secret manager compute hash msg key verify hash equals h mac hash msg boolean verify hash byte hash byte msg secret key key byte msg hash generate byte hash msg key return utils compare bytes msg hash msg hash length hash hash length aux util calculate hash string string hash from string string enc str secret key key throws io exception return generate hash enc str get bytes key verify base hash h mac hash msg verify reply string base hash string msg secret key key throws io exception byte hash base decode base base hash get bytes boolean res verify hash hash msg get bytes key res true throw new io exception verification hash reply failed shuffle specific utils build encoding url string build msg from url url return build msg from url get path url get query url get port shuffle specific utils build encoding url string build msg from http servlet request request return build msg from request get request uri request get query string request get local port shuffle specific utils build encoding url string build msg from string uri path string uri query port return string value of port uri path uri query byte array hex string string hex byte ba byte array output stream baos new byte array output stream print stream ps new print stream baos byte b ba ps printf x b return baos string
1585	mapreduce\src\java\org\apache\hadoop\mapreduce\security\TokenCache.java	unrelated	package org apache hadoop mapreduce security this provides user facing ap is transferring secrets job client tasks the secrets stored submission jobs read task execution token cache log log log factory get log token cache auxiliary method get user secret keys byte get secret key credentials credentials text alias credentials null return null return credentials get secret key alias convenience method obtain delegation tokens namenodes corresponding paths passed obtain tokens for namenodes credentials credentials path ps configuration conf throws io exception user group information security enabled return obtain tokens for namenodes internal credentials ps conf obtain tokens for namenodes internal credentials credentials path ps configuration conf throws io exception path p ps file system fs file system get p uri conf obtain tokens for namenodes internal fs credentials conf get delegation token specific fs obtain tokens for namenodes internal file system fs credentials credentials configuration conf throws io exception get jobtracker principal id renewer kerberos name jt krb name new kerberos name conf get jt config jt user name string deleg token renewer jt krb name get short name boolean read file true string fs name fs get canonical service name token cache get delegation token credentials fs name null todo need come better place put block code reading file read file read file false string binary token filename conf get mapreduce job credentials binary binary token filename null credentials binary try binary credentials read token storage file new path file binary token filename conf catch io exception e throw new runtime exception e credentials add all binary token cache get delegation token credentials fs name null log debug dt fs name already present return token token fs get delegation token deleg token renewer token null text fs name text new text fs name token set service fs name text credentials add token fs name text token log info got dt fs get uri uri fs name service token get service file name used hdfs generated job token string job token hdfs file job token conf setting job tokens cache file name string job tokens filename mapreduce job job token file text job token new text shuffle and job token token delegation token identifier get delegation token credentials credentials string namenode return token delegation token identifier credentials get token new text namenode load job token file credentials load tokens string job token file job conf conf throws io exception path local job token file new path file job token file credentials ts credentials read token storage file local job token file conf log debug enabled log debug task loaded job token file local job token file uri get path num sec keys ts number of secret keys number tokens ts number of tokens return ts store job token set job token token extends token identifier credentials credentials credentials add token job token token job token identifier get job token credentials credentials return token job token identifier credentials get token job token
1586	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\DelegationTokenRenewal.java	unrelated	package org apache hadoop mapreduce security token delegation token renewal log log log factory get log delegation token renewal string scheme hdfs used keeping tracks dt renew delegation token to renew token delegation token identifier token job id job id configuration conf expiration date timer task timer task delegation token to renew job id j id token delegation token identifier configuration new conf new expiration date token job id j id conf new conf expiration date new expiration date timer task null token null job id null conf null throw new illegal argument exception invalid params renew token token j job id c conf set timer task timer task task timer task task string string return token exp expiration date boolean equals object obj obj return true else obj null get class obj get class return false else return token equals delegation token to renew obj token hash code return token hash code global single timer daemon timer renewal timer new timer true delegation token canceler thread delegation token cancel thread dt cancel thread new delegation token cancel thread dt cancel thread start managing list tokens using map job id list tokens set delegation token to renew delegation tokens collections synchronized set new hash set delegation token to renew delegation token cancel thread extends thread token with conf token delegation token identifier token configuration conf token with conf token delegation token identifier token configuration conf token token conf conf linked blocking queue token with conf queue new linked blocking queue token with conf delegation token cancel thread super delegation token canceler set daemon true cancel token token delegation token identifier token configuration conf token with conf token with conf new token with conf token conf queue offer token with conf log warn unable add token token cancellation will retry try thread sleep catch interrupted exception e throw new runtime exception e run true token with conf token with conf null try token with conf queue take distributed file system dfs null try rpc for need dfs object dfs get dfs for token token with conf token token with conf conf catch exception e log info get dfs cancel will retry https dfs null dfs null dfs cancel delegation token token with conf token else cancel delegation token over https token with conf token token with conf conf log debug enabled log debug canceling token token with conf token get service dfs dfs catch io exception e log warn failed cancel token token with conf token string utils stringify exception e catch interrupted exception ie return catch throwable log warn got exception string utils stringify exception exiting system exit adding token add token to list delegation token to renew delegation tokens add kind tokens currently renew text kind hdfs delegation token identifier hdfs delegation kind synchronized register delegation tokens for renewal job id job id credentials ts configuration conf ts null return nothing add collection token extends token identifier tokens ts get all tokens system current time millis token extends token identifier tokens currently check hdfs delegation tokens later add different types get
1587	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenIdentifier.java	unrelated	package org apache hadoop mapreduce security token the token identifier job token job token identifier extends token identifier text jobid text kind name new text mapreduce job default constructor job token identifier jobid new text create job token identifier jobid job token identifier text jobid jobid jobid inherit doc text get kind return kind name inherit doc user group information get user jobid null equals jobid string return null return user group information create remote user jobid string get jobid text get job id return jobid inherit doc read fields data input throws io exception jobid read fields inherit doc write data output throws io exception jobid write
1588	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenSecretManager.java	unrelated	package org apache hadoop mapreduce security token secret manager job token it used cache generated job tokens job token secret manager extends secret manager job token identifier secret key master key map string secret key current job tokens convert byte secret key secret key create secret key byte key return secret manager create secret key key compute hmac hash message using key byte compute hash byte msg secret key key return create password msg key default constructor job token secret manager master key generate secret current job tokens new tree map string secret key create new password secret given job token identifier byte create password job token identifier identifier byte result create password identifier get bytes master key return result add job token job cache add token for job string job id token job token identifier token secret key token secret create secret key token get password synchronized current job tokens current job tokens put job id token secret remove cached job token job cache remove token for job string job id synchronized current job tokens current job tokens remove job id look token password secret given job id secret key retrieve token secret string job id throws invalid token secret key token secret null synchronized current job tokens token secret current job tokens get job id token secret null throw new invalid token can find job token job job id return token secret look token password secret given job token identifier byte retrieve password job token identifier identifier throws invalid token return retrieve token secret identifier get job id string get encoded create empty job token identifier job token identifier create identifier return new job token identifier
1589	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\JobTokenSelector.java	unrelated	package org apache hadoop mapreduce security token look tokens find first job token matches service return job token selector implements token selector job token identifier token job token identifier select token text service collection token extends token identifier tokens service null return null token extends token identifier token tokens job token identifier kind name equals token get kind service equals token get service return token job token identifier token return null
1590	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\package-info.java	unrelated	package org apache hadoop mapreduce security token
1591	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenIdentifier.java	unrelated	package org apache hadoop mapreduce security token delegation a delegation token identifier specific map reduce delegation token identifier extends abstract delegation token identifier text mapreduce delegation kind new text mapreduce delegation token create empty delegation token identifier reading delegation token identifier create new delegation token identifier delegation token identifier text owner text renewer text real user super owner renewer real user text get kind return mapreduce delegation kind
1592	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenSecretManager.java	unrelated	package org apache hadoop mapreduce security token delegation a map reduce specific delegation token secret manager the secret manager responsible generating accepting password token delegation token secret manager extends abstract delegation token secret manager delegation token identifier create secret manager secret keys tokens expired tokens delegation token secret manager delegation key update interval delegation token max lifetime delegation token renew interval delegation token remover scan interval super delegation key update interval delegation token max lifetime delegation token renew interval delegation token remover scan interval delegation token identifier create identifier return new delegation token identifier
1593	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\DelegationTokenSelector.java	unrelated	package org apache hadoop mapreduce security token delegation a delegation token specialized map reduce delegation token selector extends abstract delegation token selector delegation token identifier delegation token selector super delegation token identifier mapreduce delegation kind
1594	mapreduce\src\java\org\apache\hadoop\mapreduce\security\token\delegation\package-info.java	unrelated	package org apache hadoop mapreduce security token delegation
1595	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\JobTrackerJspHelper.java	unrelated	package org apache hadoop mapreduce server jobtracker methods help format output job tracker xml jspx job tracker jsp helper job tracker jsp helper percent format new decimal format decimal format percent format returns xml formatted table jobs list this called repeatedly different lists jobs e g running completed failed generate job table jsp writer string label list job in progress jobs throws io exception jobs size job in progress job jobs job profile profile job get profile job status status job get status job id jobid profile get job id desired maps job desired maps desired reduces job desired reduces completed maps job finished maps completed reduces job finished reduces string name profile get job name print label job jobid jobid n print jobid jobid jobid n print user profile get user user n print name equals name nbsp name name n print map complete string utils format percent status map progress map complete n print map total desired maps map total n print maps completed completed maps maps completed n print reduce complete string utils format percent status reduce progress reduce complete n print reduce total desired reduces reduce total n print reduces completed completed reduces reduces completed n print label job n generates xml formatted block summarizes state job tracker generate summary table jsp writer job tracker tracker throws io exception cluster status status tracker get cluster status max map tasks status get max map tasks max reduce tasks status get max reduce tasks num task trackers status get task trackers string tasks per node str num task trackers tasks per node pct max map tasks max reduce tasks num task trackers tasks per node str percent format format tasks per node pct else tasks per node str print maps status get map tasks maps n reduces status get reduce tasks reduces n total submissions tracker get total submissions total submissions n nodes status get task trackers nodes n map task capacity status get max map tasks map task capacity n reduce task capacity status get max reduce tasks reduce task capacity n avg tasks per node tasks per node str avg tasks per node n
1596	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\JTConfig.java	heartbeat	package org apache hadoop mapreduce server jobtracker place holder job tracker server level configuration the keys mapreduce jobtracker prefix jt config extends mr config job tracker configuration parameters string jt ipc address mapreduce jobtracker address string jt http address mapreduce jobtracker http address string jt ipc handler count mapreduce jobtracker handler count string jt restart enabled mapreduce jobtracker restart recover string jt task scheduler mapreduce jobtracker taskscheduler string jt instrumentation mapreduce jobtracker instrumentation string jt tasks per job mapreduce jobtracker maxtasks perjob string jt heartbeats in second mapreduce jobtracker heartbeats second string jt heartbeats scaling factor mapreduce jobtracker heartbeats scaling factor string jt heartbeat interval min mapreduce jobtracker heartbeat interval min jt heartbeat interval min default string jt persist jobstatus mapreduce jobtracker persist jobstatus active string jt persist jobstatus hours mapreduce jobtracker persist jobstatus hours string jt persist jobstatus dir mapreduce jobtracker persist jobstatus dir string jt supergroup mapreduce jobtracker permissions supergroup string jt retirejobs mapreduce jobtracker retirejobs string jt retirejob cache size mapreduce jobtracker retiredjobs cache size string jt taskcache levels mapreduce jobtracker taskcache levels string jt task alloc pad fraction mapreduce jobtracker taskscheduler taskalloc capacitypad string jt jobinit threads mapreduce jobtracker jobinit threads string jt tracker expiry interval mapreduce jobtracker expire trackers interval string jt runningtasks per job mapreduce jobtracker taskscheduler maxrunningtasks perjob string jt hosts filename mapreduce jobtracker hosts filename string jt hosts exclude filename mapreduce jobtracker hosts exclude filename string jt jobhistory cache size mapreduce jobtracker jobhistory lru cache size string jt jobhistory block size mapreduce jobtracker jobhistory block size string jt jobhistory completed location mapreduce jobtracker jobhistory completed location string jt jobhistory location mapreduce jobtracker jobhistory location string jt avg blacklist threshold mapreduce jobtracker blacklist average threshold string jt system dir mapreduce jobtracker system dir string jt staging area root mapreduce jobtracker staging root dir string jt max tracker blacklists mapreduce jobtracker tasktracker maxblacklists string jt jobhistory maxage mapreduce jobtracker jobhistory maxage string jt max mapmemory mb mapreduce jobtracker maxmapmemory mb string jt max reducememory mb mapreduce jobtracker maxreducememory mb string jt max job split metainfo size mapreduce jobtracker split metainfo maxsize string jt user name mapreduce jobtracker kerberos principal string jt keytab file mapreduce jobtracker keytab file string private actions key mapreduce jobtracker webinterface trusted string jt plugins mapreduce jobtracker plugins string shuffle exception stack regex mapreduce reduce shuffle catch exception stack regex string shuffle exception msg regex mapreduce reduce shuffle catch exception message regex
1597	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\State.java	unrelated	package org apache hadoop mapreduce server jobtracker describes state job tracker enum state initializing running
1598	mapreduce\src\java\org\apache\hadoop\mapreduce\server\jobtracker\TaskTracker.java	unrelated	package org apache hadoop mapreduce server jobtracker the representation single code task tracker code seen link job tracker task tracker log log log factory get log task tracker string tracker name task tracker status status job in progress job for fallow map slot job in progress job for fallow reduce slot create new link task tracker task tracker string tracker name tracker name tracker name get unique identifier link task tracker string get tracker name return tracker name get current link task tracker status code task tracker code code task tracker code task tracker status get status return status set current link task tracker status code task tracker code code task tracker code set status task tracker status status status status get number currently available slots tasktracker given type task code task type code get available slots task type task type available slots task type task type map log debug enabled log debug tracker name get avail slots max status get max map slots occupied status count occupied map slots available slots status get available map slots else log debug enabled log debug tracker name get avail slots max r status get max reduce slots occupied r status count occupied reduce slots available slots status get available reduce slots return available slots get link job in progress fallow slot held code null code fallow slots job in progress get job for fallow slot task type task type return task type task type map job for fallow map slot job for fallow reduce slot reserve specified number slots given code job code reserved reserve slots task type task type job in progress job num slots job id job id job get job id task type task type map job for fallow map slot null job for fallow map slot get job id equals job id throw new runtime exception tracker name already slots reserved job for fallow map slot asked reserve num slots job id job for fallow map slot job else task type task type reduce job for fallow reduce slot null job for fallow reduce slot get job id equals job id throw new runtime exception tracker name already slots reserved job for fallow reduce slot asked reserve num slots job id job for fallow reduce slot job job reserve task tracker task type num slots log info tracker name reserved num slots task type slots job id free map slots code task tracker code reserved code task type code unreserve slots task type task type job in progress job job id job id job get job id task type task type map job for fallow map slot null job for fallow map slot get job id equals job id throw new runtime exception tracker name already slots reserved job for fallow map slot asked un reserve job id job for fallow map slot null else job for fallow reduce slot null job for fallow reduce slot get job id equals job id throw new runtime exception tracker name already slots reserved job for fallow reduce slot asked un reserve job
1599	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\Localizer.java	unrelated	package org apache hadoop mapreduce server tasktracker localizer log log log factory get log localizer file system fs string local dirs task controller task controller create localizer instance localizer file system file sys string dirs task controller tc fs file sys local dirs dirs task controller tc data structure synchronizing localization user directories map string atomic boolean localized users new hash map string atomic boolean initialize local directories particular user tt this involves creation setting permissions following directories ul li mapreduce cluster local dir task tracker user li li mapreduce cluster local dir task tracker user jobcache li li mapreduce cluster local dir task tracker user distcache li ul initialize user dirs string user throws io exception user null this happen general throw new io exception user null cannot initialized user directories atomic boolean localized user synchronized localized users localized users contains key user localized users put user new atomic boolean false localized user localized users get user synchronized localized user localized user get user directories already localized user log info user directories user user already initialized tt not anything return log info initializing user user tt boolean user dir status false boolean job cache dir status false boolean distributed cache dir status false string local dir local dirs path user dir new path local dir task tracker get user dir user set user directory fs exists user dir fs mkdirs user dir set permissions user directory fs set permission user dir new fs permission short user dir status true set jobcache directory path job cache dir new path local dir task tracker get job cache subdir user fs exists job cache dir fs mkdirs job cache dir set permissions jobcache directory fs set permission job cache dir new fs permission short job cache dir status true else log warn unable create job cache directory job cache dir set cache directory used distributed cache files path distributed cache dir new path local dir task tracker get private distributed cache dir user fs exists distributed cache dir fs mkdirs distributed cache dir set permissions distcache directory fs set permission distributed cache dir new fs permission short distributed cache dir status true else log warn unable create distributed cache directory distributed cache dir else log warn unable create user directory user dir user dir status throw new io exception not able initialize user directories configured local directories user user job cache dir status throw new io exception not able initialize job cache directories configured local directories user user distributed cache dir status throw new io exception not able initialize distributed cache directories configured local directories user user now run task controller specific code initialize user directories initialization context context new initialization context context user user context work dir null task controller initialize user context localization user done localized user set true prepare job directories given job to called job localization code job already localized br here set permissions job directories created disks this avoid misuse users till time link task controller initialize job job initialization context run later time set proper permissions job directories
1600	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\package-info.java	unrelated	package org apache hadoop mapreduce server tasktracker
1601	mapreduce\src\java\org\apache\hadoop\mapreduce\server\tasktracker\TTConfig.java	heartbeat	package org apache hadoop mapreduce server tasktracker place holder task tracker server level configuration the keys mapreduce tasktracker prefix tt config extends mr config task tracker configuration properties string tt health checker interval mapreduce tasktracker healthchecker interval string tt health checker script args mapreduce tasktracker healthchecker script args string tt health checker script path mapreduce tasktracker healthchecker script path string tt health checker script timeout mapreduce tasktracker healthchecker script timeout string tt local dir minspace kill mapreduce tasktracker local dir minspacekill string tt local dir minspace start mapreduce tasktracker local dir minspacestart string tt http address mapreduce tasktracker http address string tt report address mapreduce tasktracker report address string tt task controller mapreduce tasktracker taskcontroller string tt contention tracking mapreduce tasktracker contention tracking string tt static resolutions mapreduce tasktracker net resolutions string tt http threads mapreduce tasktracker http threads string tt host name mapreduce tasktracker host name string tt sleep time before sig kill mapreduce tasktracker tasks sleeptimebeforesigkill string tt dns interface mapreduce tasktracker dns string tt dns nameserver mapreduce tasktracker dns nameserver string tt max task completion events to poll mapreduce tasktracker events batchsize string tt index cache mapreduce tasktracker indexcache mb string tt instrumentation mapreduce tasktracker instrumentation string tt map slots mapreduce tasktracker map tasks maximum string tt memory calculator plugin mapreduce tasktracker memorycalculatorplugin string tt resource calculator plugin mapreduce tasktracker resourcecalculatorplugin string tt reduce slots mapreduce tasktracker reduce tasks maximum string tt memory manager monitoring interval mapreduce tasktracker taskmemorymanager monitoringinterval string tt local cache size mapreduce tasktracker cache local size string tt local cache subdirs limit mapreduce tasktracker cache local numberdirectories string tt outofband hearbeat mapreduce tasktracker outofband heartbeat string tt reserved physcialmemory mb mapreduce tasktracker reserved physicalmemory mb string tt user name mapreduce tasktracker kerberos principal string tt keytab file mapreduce tasktracker keytab file string tt group mapreduce tasktracker group string tt userlogcleanup sleeptime mapreduce tasktracker userlogcleanup sleeptime string tt distributed cache check period mapreduce tasktracker distributedcache checkperiod percentage local distributed cache kept garbage collection string tt local cache keep around pct mapreduce tasktracker cache local keep pct
1602	mapreduce\src\java\org\apache\hadoop\mapreduce\split\JobSplit.java	unrelated	package org apache hadoop mapreduce split this groups fundamental associated reading writing splits the split information divided two parts based consumer information the two parts split meta information raw split information the first part consumed job tracker create tasks locality data structures the second part used maps runtime know these pieces information written two separate files the metainformation file slurped job tracker job initialization a map task gets meta information launch reads raw split bytes directly file job split meta split version byte meta split file header try meta split file header meta spl get bytes utf catch unsupported encoding exception u throw new runtime exception u task split meta info empty task split new task split meta info this represents meta information task split the main fields start offset actual split data length processed split hosts split local split meta info implements writable start offset input data length string locations split meta info split meta info string locations start offset input data length locations locations start offset start offset input data length input data length split meta info input split split start offset throws io exception try locations split get locations input data length split get length start offset start offset catch interrupted exception ie throw new io exception ie string get locations return locations get start offset return start offset get input data length return input data length set input data locations string locations locations locations set input data length length input data length length read fields data input throws io exception len writable utils read v int locations new string len locations length locations text read string start offset writable utils read v long input data length writable utils read v long write data output throws io exception writable utils write v int locations length locations length text write string locations writable utils write v long start offset writable utils write v long input data length string string string buffer buf new string buffer buf append data size input data length n buf append start offset start offset n buf append locations n string loc locations buf append loc n return buf string this represents meta information task split job tracker creates task split meta info task split index split index input data length string locations task split meta info split index new task split index locations new string task split meta info task split index split index string locations input data length split index split index locations locations input data length input data length task split meta info input split split start offset throws interrupted exception io exception new task split index start offset split get locations split get length task split meta info string locations start offset input data length new task split index start offset locations input data length task split index get split index return split index string get split location return split index get split location get input data length return input data length string get locations return locations get start offset return split index get start offset this represents meta information task split
1603	mapreduce\src\java\org\apache\hadoop\mapreduce\split\JobSplitWriter.java	unrelated	package org apache hadoop mapreduce split the used job clients write splits meta raw bytes parts job split writer split version job split meta split version byte split file header try split file header spl get bytes utf catch unsupported encoding exception u throw new runtime exception u t extends input split create split files path job submit dir configuration conf file system fs list input split splits throws io exception interrupted exception t array t splits array new input split splits size create split files job submit dir conf fs array t extends input split create split files path job submit dir configuration conf file system fs t splits throws io exception interrupted exception fs data output stream create file fs job submission files get job split file job submit dir conf split meta info info write new splits conf splits close write job split meta info fs job submission files get job split meta file job submit dir new fs permission job submission files job file permission split version info create split files path job submit dir configuration conf file system fs org apache hadoop mapred input split splits throws io exception fs data output stream create file fs job submission files get job split file job submit dir conf split meta info info write old splits splits close write job split meta info fs job submission files get job split meta file job submit dir new fs permission job submission files job file permission split version info fs data output stream create file file system fs path split file configuration job throws io exception fs data output stream file system create fs split file new fs permission job submission files job file permission replication job get int job submit replication fs set replication split file short replication write split header return write split header fs data output stream throws io exception write split file header write int split version t extends input split split meta info write new splits configuration conf t array fs data output stream throws io exception interrupted exception split meta info info new split meta info array length array length serialization factory factory new serialization factory conf offset size t split array prev count size text write string split get class get name serializer t serializer factory get serializer class t split get class serializer open serializer serialize split curr count size info new job split split meta info split get locations offset split get length offset curr count prev count return info split meta info write old splits org apache hadoop mapred input split splits fs data output stream throws io exception split meta info info new split meta info splits length splits length offset size org apache hadoop mapred input split split splits prev len size text write string split get class get name split write curr len size info new job split split meta info split get locations offset split get length offset curr len prev len return info write job split meta info file system fs path filename fs permission p
1604	mapreduce\src\java\org\apache\hadoop\mapreduce\split\package-info.java	unrelated	package org apache hadoop mapreduce split
1605	mapreduce\src\java\org\apache\hadoop\mapreduce\split\SplitMetaInfoReader.java	unrelated	package org apache hadoop mapreduce split a utility reads split meta info creates split meta info objects split meta info reader job split task split meta info read split meta info job id job id file system fs configuration conf path job submit dir throws io exception max meta info size conf get long jt config jt max job split metainfo size l path meta split file job submission files get job split meta file job submit dir file status f status fs get file status meta split file max meta info size f status get len max meta info size throw new io exception split metadata size exceeded max meta info size aborting job job id fs data input stream fs open meta split file byte header new byte job split meta split file header length read fully header arrays equals job split meta split file header header throw new io exception invalid header split file vers writable utils read v int vers job split meta split version close throw new io exception unsupported split version vers num splits writable utils read v int todo check insane values job split task split meta info split meta info new job split task split meta info num splits num splits job split split meta info split meta info new job split split meta info split meta info read fields job split task split index split index new job split task split index job submission files get job split file job submit dir string split meta info get start offset split meta info new job split task split meta info split index split meta info get locations split meta info get input data length close return split meta info
1606	mapreduce\src\java\org\apache\hadoop\mapreduce\task\JobContextImpl.java	unrelated	package org apache hadoop mapreduce task a read view job provided tasks running job context impl implements job context protected org apache hadoop mapred job conf conf job id job id the user group information object reference current user protected user group information ugi protected credentials credentials job context impl configuration conf job id job id conf new org apache hadoop mapred job conf conf job id job id credentials conf get credentials try ugi user group information get current user catch io exception e throw new runtime exception e return configuration job configuration get configuration return conf get unique id job job id get job id return job id set job id set job id job id job id job id job id get configured number reduce tasks job defaults code code get num reduce tasks return conf get num reduce tasks get current working directory default file system path get working directory throws io exception return conf get working directory get key job output data class get output key class return conf get output key class get value job outputs class get output value class return conf get output value class get key map output data if set use output key this allows map output key different output key class get map output key class return conf get map output key class get value map output data if set use output value this allows map output value different output value class get map output value class return conf get map output value class get user specified job name this used identify job user string get job name return conf get job name get link input format job class extends input format get input format class throws class not found exception return class extends input format conf get class input format class attr text input format get link mapper job class extends mapper get mapper class throws class not found exception return class extends mapper conf get class map class attr mapper get combiner job class extends reducer get combiner class throws class not found exception return class extends reducer conf get class combine class attr null get link reducer job class extends reducer get reducer class throws class not found exception return class extends reducer conf get class reduce class attr reducer get link output format job class extends output format get output format class throws class not found exception return class extends output format conf get class output format class attr text output format get link partitioner job class extends partitioner get partitioner class throws class not found exception return class extends partitioner conf get class partitioner class attr hash partitioner get link raw comparator comparator used compare keys raw comparator get sort comparator return conf get output key comparator get pathname job jar string get jar return conf get jar get user defined link raw comparator comparator grouping keys inputs reduce raw comparator get grouping comparator return conf get output value grouping comparator get whether job setup job cleanup needed job boolean get job setup cleanup needed return
1607	mapreduce\src\java\org\apache\hadoop\mapreduce\task\MapContextImpl.java	unrelated	package org apache hadoop mapreduce task the context given link mapper map context impl keyin valuein keyout valueout extends task input output context impl keyin valuein keyout valueout implements map context keyin valuein keyout valueout record reader keyin valuein reader input split split map context impl configuration conf task attempt id taskid record reader keyin valuein reader record writer keyout valueout writer output committer committer status reporter reporter input split split super conf taskid writer committer reporter reader reader split split get input split map input split get input split return split keyin get current key throws io exception interrupted exception return reader get current key valuein get current value throws io exception interrupted exception return reader get current value boolean next key value throws io exception interrupted exception return reader next key value
1608	mapreduce\src\java\org\apache\hadoop\mapreduce\task\package-info.java	unrelated	package org apache hadoop mapreduce task
1609	mapreduce\src\java\org\apache\hadoop\mapreduce\task\ReduceContextImpl.java	unrelated	package org apache hadoop mapreduce task the context passed link reducer reduce context impl keyin valuein keyout valueout extends task input output context impl keyin valuein keyout valueout implements reduce context keyin valuein keyout valueout raw key value iterator input counter input value counter counter input key counter raw comparator keyin comparator keyin key current key valuein value current value boolean first value false first value key boolean next key is same false w key boolean more file protected progressable reporter deserializer keyin key deserializer deserializer valuein value deserializer data input buffer buffer new data input buffer bytes writable current raw key new bytes writable value iterable iterable new value iterable boolean marked false backup store keyin valuein backup store serialization factory serialization factory class keyin key class class valuein value class configuration conf task attempt id taskid current key length current value length reduce context impl configuration conf task attempt id taskid raw key value iterator input counter input key counter counter input value counter record writer keyout valueout output output committer committer status reporter reporter raw comparator keyin comparator class keyin key class class valuein value class throws interrupted exception io exception super conf taskid output committer reporter input input input key counter input key counter input value counter input value counter comparator comparator serialization factory new serialization factory conf key deserializer serialization factory get deserializer key class key deserializer open buffer value deserializer serialization factory get deserializer value class value deserializer open buffer more input next key class key class value class value class conf conf taskid taskid start processing next unique key boolean next key throws io exception interrupted exception more next key is same next key value more input key counter null input key counter increment return next key value else return false advance next key value pair boolean next key value throws io exception interrupted exception more key null value null return false first value next key is same data input buffer next key input get key current raw key set next key get data next key get position next key get length next key get position buffer reset current raw key get bytes current raw key get length key key deserializer deserialize key data input buffer next val input get value buffer reset next val get data next val get position next val get length value value deserializer deserialize value current key length next key get length next key get position current value length next val get length next val get position marked backup store write next key next val more input next more next key input get key next key is same comparator compare current raw key get bytes current raw key get length next key get data next key get position next key get length next key get position else next key is same false input value counter increment return true keyin get current key return key valuein get current value return value protected value iterator implements reduce context value iterator valuein boolean reset false boolean clear mark flag false boolean next try
1610	mapreduce\src\java\org\apache\hadoop\mapreduce\task\TaskAttemptContextImpl.java	unrelated	package org apache hadoop mapreduce task the context task attempts task attempt context impl extends job context impl implements task attempt context task attempt id task id string status status reporter reporter task attempt context impl configuration conf task attempt id task id conf task id new dummy reporter task attempt context impl configuration conf task attempt id task id status reporter reporter super conf task id get job id task id task id reporter reporter get unique name task attempt task attempt id get task attempt id return task id get last set status message string get status return status counter get counter enum counter name return reporter get counter counter name counter get counter string group name string counter name return reporter get counter group name counter name report progress progress reporter progress protected set status string string status status status set current status task given set status string status set status string status reporter set status status dummy reporter extends status reporter set status string progress counter get counter enum name return new counters find counter name counter get counter string group string name return new counters find counter group name get progress return f get progress return reporter get progress
1611	mapreduce\src\java\org\apache\hadoop\mapreduce\task\TaskInputOutputContextImpl.java	unrelated	package org apache hadoop mapreduce task a context object allows input output task it supplied link mapper link reducer task input output context impl keyin valuein keyout valueout extends task attempt context impl implements task input output context keyin valuein keyout valueout record writer keyout valueout output output committer committer task input output context impl configuration conf task attempt id taskid record writer keyout valueout output output committer committer status reporter reporter super conf taskid reporter output output committer committer advance next key value pair returning null end boolean next key value throws io exception interrupted exception get current key keyin get current key throws io exception interrupted exception get current value valuein get current value throws io exception interrupted exception generate output key value pair write keyout key valueout value throws io exception interrupted exception output write key value output committer get output committer return committer
1612	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\EventFetcher.java	scheduler	package org apache hadoop mapreduce task reduce event fetcher k v extends thread sleep time max events to fetch max retries retry period log log log factory get log event fetcher task attempt id reduce task umbilical protocol umbilical shuffle scheduler k v scheduler event id exception reporter exception reporter null max map runtime event fetcher task attempt id reduce task umbilical protocol umbilical shuffle scheduler k v scheduler exception reporter reporter set name event fetcher fetching map completion events set daemon true reduce reduce umbilical umbilical scheduler scheduler exception reporter reporter run failures log info reduce thread started get name try true try num new maps get map completion events failures num new maps log info reduce got num new maps new map outputs log debug get map events thread sleep sleep time thread sleep sleep time catch io exception ie log info exception getting events ie check see whether abort failures max retries throw new io exception many failures downloading events ie sleep bit thread sleep retry period catch interrupted exception e return catch throwable exception reporter report exception return queries link task tracker set map completion events given event id get map completion events throws io exception num new maps map task completion events update update umbilical get map completion events org apache hadoop mapred job id reduce get job id event id max events to fetch org apache hadoop mapred task attempt id reduce task completion event events update get map task completion events log debug got events length map completion events event id check reset required since ordering task completion events reducer option sync new jobtracker reset events index update reset event id scheduler reset known maps update last seen event id event id events length process task completion events save succeeded maps known outputs fetch outputs save obsolete failed killed maps obsolete outputs stop fetching maps remove tipfailed maps needed outputs since need outputs task completion event event events switch event get task status case succeeded uri u get base uri event get task tracker http scheduler add known map output u get host u get port u string event get task attempt id num new maps duration event get task run time duration max map runtime max map runtime duration scheduler inform max map run time max map runtime break case failed case killed case obsolete scheduler obsolete map output event get task attempt id log info ignoring obsolete output event get task status map task event get task attempt id break case tipfailed scheduler tip failed event get task attempt id get task id log info ignoring output failed map tip event get task attempt id break return num new maps uri get base uri string url string buffer base url new string buffer url url ends with base url append base url append map output job base url append reduce get job id base url append reduce base url append reduce get task id get id base url append map uri u uri create base url string return u
1613	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ExceptionReporter.java	unrelated	package org apache hadoop mapreduce task reduce an reporting exceptions threads exception reporter report exception throwable
1614	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\Fetcher.java	pooling	package org apache hadoop mapreduce task reduce fetcher k v extends thread log log log factory get log fetcher number ms timing copy default stalled copy timeout basic unit connection timeout milliseconds unit connect timeout default read timeout milliseconds default read timeout progressable reporter enum shuffle errors io error wrong length bad id wrong map connection wrong reduce string shuffle err grp name shuffle errors counters counter connection errs counters counter io errs counters counter wrong length errs counters counter bad id errs counters counter wrong map errs counters counter wrong reduce errs merge manager k v merger shuffle scheduler k v scheduler shuffle client metrics metrics exception reporter exception reporter id next id reduce connection timeout read timeout decompression map outputs compression codec codec decompressor decompressor secret key job token secret fetcher job conf job task attempt id reduce id shuffle scheduler k v scheduler merge manager k v merger reporter reporter shuffle client metrics metrics exception reporter exception reporter secret key job token secret reporter reporter scheduler scheduler merger merger metrics metrics exception reporter exception reporter id next id reduce reduce id get task id get id job token secret job token secret io errs reporter get counter shuffle err grp name shuffle errors io error string wrong length errs reporter get counter shuffle err grp name shuffle errors wrong length string bad id errs reporter get counter shuffle err grp name shuffle errors bad id string wrong map errs reporter get counter shuffle err grp name shuffle errors wrong map string connection errs reporter get counter shuffle err grp name shuffle errors connection string wrong reduce errs reporter get counter shuffle err grp name shuffle errors wrong reduce string job get compress map output class extends compression codec codec class job get map output compressor class default codec codec reflection utils new instance codec class job decompressor codec pool get decompressor codec else codec null decompressor null connection timeout job get int mr job config shuffle connect timeout default stalled copy timeout read timeout job get int mr job config shuffle read timeout default read timeout set name fetcher id set daemon true run try true map host host null try if merge block merger wait for in memory merge get host shuffle host scheduler get host metrics thread busy shuffle copy from host host finally host null scheduler free host host metrics thread free catch interrupted exception ie return catch throwable exception reporter report exception the crux matter shuffle available map outputs copy from host map host host throws io exception get completed maps host list task attempt id maps scheduler get maps for host host sanity check catch hosts obsolete maps especially tail large jobs maps size return log debug fetcher id going fetch host task attempt id tmp maps log debug tmp list maps fetched yet set task attempt id remaining new hash set task attempt id maps construct url connect data input stream input boolean connect succeeded false try url url get map output url host maps url connection connection url open connection generate hash
1615	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\InMemoryReader.java	unrelated	package org apache hadoop mapreduce task reduce code i file in memory reader code read map outputs present memory in memory reader k v extends reader k v task attempt id task attempt id merge manager k v merger data input buffer mem data in new data input buffer start length in memory reader merge manager k v merger task attempt id task attempt id byte data start length throws io exception super null null length start null null merger merger task attempt id task attempt id buffer data buffer size file length mem data in reset buffer start length start start length length reset offset mem data in reset buffer start offset length bytes read offset eof false get position throws io exception in memory reader initialize streams like reader get pos would work instead return number uncompressed bytes read correct since memory data compressed return bytes read get length return file length dump on error file dump file new file output task attempt id dump system err println dumping corrupt map output task attempt id dump file get absolute path try file output stream fos new file output stream dump file fos write buffer buffer size fos close catch io exception ioe system err println failed dump map output task attempt id boolean next raw key data input buffer key throws io exception try position to next record mem data in return false setup key pos mem data in get position byte data mem data in get data key reset data pos current key length position next value skipped mem data in skip current key length skipped current key length throw new io exception rec rec no failed skip past key length current key length record byte bytes read current key length return true catch io exception ioe dump on error throw ioe next raw value data input buffer value throws io exception try pos mem data in get position byte data mem data in get data value reset data pos current value length position next record skipped mem data in skip current value length skipped current value length throw new io exception rec rec no failed skip past value length current value length record byte bytes read current value length rec no catch io exception ioe dump on error throw ioe close release data in null buffer null inform merge manager merger null merger unreserve buffer size
1616	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\InMemoryWriter.java	unrelated	package org apache hadoop mapreduce task reduce in memory writer k v extends writer k v data output stream in memory writer bounded byte array output stream array stream super null new data output stream new i file output stream array stream append k key v value throws io exception throw new unsupported operation exception in memory writer append k key v value append data input buffer key data input buffer value throws io exception key length key get length key get position key length throw new io exception negative key length allowed key length key value length value get length value get position value length throw new io exception negative value length allowed value length value writable utils write v int key length writable utils write v int value length write key get data key get position key length write value get data value get position value length close throws io exception write eof marker key value length writable utils write v int i file eof marker writable utils write v int i file eof marker close stream close null
1617	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MapHost.java	unrelated	package org apache hadoop mapreduce task reduce map host enum state idle no map outputs available busy map outputs fetched pending known map outputs need fetched penalized host penalized due shuffle failures state state state idle string host name string base url list task attempt id maps new array list task attempt id map host string host name string base url host name host name base url base url state get state return state string get host name return host name string get base url return base url synchronized add known map task attempt id map id maps add map id state state idle state state pending synchronized list task attempt id get and clear known maps list task attempt id current known maps maps maps new array list task attempt id return current known maps synchronized mark busy state state busy synchronized mark penalized state state penalized synchronized get num known map outputs return maps size called node done penalty done copying synchronized state mark available maps empty state state idle else state state pending return state string string return host name mark host penalized synchronized penalize state state penalized
1618	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MapOutput.java	unrelated	package org apache hadoop mapreduce task reduce map output k v log log log factory get log map output atomic integer id new atomic integer enum type wait memory disk id merge manager k v merger task attempt id map id size byte memory bounded byte array output stream byte stream file system local fs path tmp output path path output path output stream disk type type boolean primary map output map output task attempt id map id merge manager k v merger size job conf conf local dir allocator local dir allocator fetcher boolean primary map output throws io exception id id increment and get map id map id merger merger type type disk memory null byte stream null size size local fs file system get local conf string filename map map id get task id get id string tmp output path separator task tracker get job cache subdir conf get user path separator map id get job id path separator merger get reduce id path separator output path separator filename fetcher tmp output path local dir allocator get local path for write tmp output size conf output path new path tmp output path get parent filename disk local fs create tmp output path primary map output primary map output map output task attempt id map id merge manager k v merger size boolean primary map output id id increment and get map id map id merger merger type type memory byte stream new bounded byte array output stream size memory byte stream get buffer size size local fs null disk null output path null tmp output path null primary map output primary map output map output task attempt id map id id id increment and get map id map id type type wait merger null memory null byte stream null size local fs null disk null output path null tmp output path null primary map output false boolean primary map output return primary map output boolean equals object obj obj instanceof map output return id map output obj id return false hash code return id path get output path return output path byte get memory return memory bounded byte array output stream get array stream return byte stream output stream get disk return disk task attempt id get map id return map id type get type return type get size return size commit throws io exception type type memory merger close in memory file else type type disk local fs rename tmp output path output path merger close on disk file output path else throw new io exception cannot commit map output type wait abort type type memory merger unreserve memory length else type type disk try local fs delete tmp output path false catch io exception ie log info failure clean tmp output path ie else throw new illegal argument exception cannot commit map output type wait string string return map output map id type map output comparator k v implements comparator map output k v compare map output k v map output k v id id return size
1619	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MergeManager.java	unrelated	package org apache hadoop mapreduce task reduce merge manager k v log log log factory get log merge manager maximum percentage memory limit single shuffle consume max single shuffle segment fraction f task attempt id reduce id job conf job conf file system local fs file system rfs local dir allocator local dir allocator protected map output file map output file set map output k v memory merged map outputs new tree set map output k v new map output comparator k v intermediate memory to memory merger mem to mem merger set map output k v memory map outputs new tree set map output k v new map output comparator k v in memory merger memory merger set path disk map outputs new tree set path on disk merger disk merger memory limit used memory max single shuffle limit mem to mem merge outputs threshold merge threshold io sort factor reporter reporter exception reporter exception reporter combiner run memory merge defined class extends reducer combiner class resettable collector used combine combine output collector k v combine collector counters counter spilled records counter counters counter reduce combine input counter counters counter merged map outputs counter compression codec codec progress merge phase merge manager task attempt id reduce id job conf job conf file system local fs local dir allocator local dir allocator reporter reporter compression codec codec class extends reducer combiner class combine output collector k v combine collector counters counter spilled records counter counters counter reduce combine input counter counters counter merged map outputs counter exception reporter exception reporter progress merge phase reduce id reduce id job conf job conf local dir allocator local dir allocator exception reporter exception reporter reporter reporter codec codec combiner class combiner class combine collector combine collector reduce combine input counter reduce combine input counter spilled records counter spilled records counter merged map outputs counter merged map outputs counter map output file new map output file map output file set conf job conf local fs local fs rfs local file system local fs get raw max in mem copy use job conf get float mr job config shuffle input buffer percent f max in mem copy use max in mem copy use throw new illegal argument exception invalid value mr job config shuffle input buffer percent max in mem copy use allow unit tests fix runtime memory memory limit job conf get long mr job config reduce memory total bytes math min runtime get runtime max memory integer max value max in mem copy use io sort factor job conf get int mr job config io sort factor max single shuffle limit memory limit max single shuffle segment fraction mem to mem merge outputs threshold job conf get int mr job config reduce memtomem threshold io sort factor merge threshold memory limit job conf get float mr job config shuffle merge eprcent f log info merger manager memory limit memory limit max single shuffle limit max single shuffle limit merge threshold merge threshold io sort factor io sort factor mem to mem merge outputs threshold mem
1620	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\MergeThread.java	unrelated	package org apache hadoop mapreduce task reduce merge thread t k v extends thread log log log factory get log merge thread volatile boolean progress false list t inputs new array list t protected merge manager k v manager exception reporter reporter boolean closed false merge factor merge thread merge manager k v manager merge factor exception reporter reporter manager manager merge factor merge factor reporter reporter synchronized close throws interrupted exception closed true wait for merge interrupt synchronized boolean in progress return progress synchronized start merge set t inputs closed progress true inputs new array list t iterator t iter inputs iterator ctr iter next ctr merge factor ctr inputs add iter next iter remove log info get name starting merge inputs size segments ignoring inputs size segments notify all synchronized wait for merge throws interrupted exception progress wait run true try wait notification start merge synchronized progress wait merge merge inputs catch interrupted exception ie return catch throwable reporter report exception return finally synchronized clear inputs inputs null progress false notify all merge list t inputs throws io exception
1621	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\package-info.java	unrelated	package org apache hadoop mapreduce task reduce
1622	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\Shuffle.java	scheduler	package org apache hadoop mapreduce task reduce shuffle k v implements exception reporter log log log factory get log shuffle progress frequency task attempt id reduce id job conf job conf reporter reporter shuffle client metrics metrics task umbilical protocol umbilical shuffle scheduler k v scheduler merge manager k v merger throwable throwable null string throwing thread name null progress copy phase task status task status task reduce task used status updates shuffle task attempt id reduce id job conf job conf file system local fs task umbilical protocol umbilical local dir allocator local dir allocator reporter reporter compression codec codec class extends reducer combiner class combine output collector k v combine collector counters counter spilled records counter counters counter reduce combine input counter counters counter shuffled maps counter counters counter reduce shuffle bytes counters counter failed shuffle counter counters counter merged map outputs counter task status status progress copy phase progress merge phase task reduce task reduce id reduce id job conf job conf umbilical umbilical reporter reporter metrics new shuffle client metrics reduce id job conf copy phase copy phase task status status reduce task reduce task scheduler new shuffle scheduler k v job conf status copy phase shuffled maps counter reduce shuffle bytes failed shuffle counter merger new merge manager k v reduce id job conf local fs local dir allocator reporter codec combiner class combine collector spilled records counter reduce combine input counter merged map outputs counter merge phase raw key value iterator run throws io exception interrupted exception start map completion events fetcher thread event fetcher k v event fetcher new event fetcher k v reduce id umbilical scheduler event fetcher start start map output fetcher threads num fetchers job conf get int mr job config shuffle parallel copies fetcher k v fetchers new fetcher num fetchers num fetchers fetchers new fetcher k v job conf reduce id scheduler merger reporter metrics reduce task get job token secret fetchers start wait shuffle complete successfully scheduler wait until done progress frequency reporter progress synchronized throwable null throw new shuffle error error shuffle throwing thread name throwable stop event fetcher thread event fetcher interrupt try event fetcher join catch throwable log info failed stop event fetcher get name stop map output fetcher threads fetcher k v fetcher fetchers fetcher interrupt fetcher k v fetcher fetchers fetcher join fetchers null stop scheduler scheduler close copy phase complete copy already complete task status set phase task status phase sort reduce task status update umbilical finish going merges raw key value iterator kv iter null try kv iter merger close catch throwable e throw new shuffle error error merge e sanity check synchronized throwable null throw new shuffle error error shuffle throwing thread name throwable return kv iter synchronized report exception throwable throwable null throwable throwing thread name thread current thread get name notify scheduler reporting thread finds exception immediately synchronized scheduler scheduler notify all shuffle error extends io exception serial version uid l shuffle error string msg throwable super msg
1623	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleClientMetrics.java	unrelated	package org apache hadoop mapreduce task reduce shuffle client metrics implements updater metrics record shuffle metrics null num failed fetches num success fetches num bytes num threads busy num copiers shuffle client metrics task attempt id reduce id job conf job conf num copiers job conf get int mr job config shuffle parallel copies metrics context metrics context metrics util get context mapred shuffle metrics metrics util create record metrics context shuffle input shuffle metrics set tag user job conf get user shuffle metrics set tag job name job conf get job name shuffle metrics set tag job id reduce id get job id string shuffle metrics set tag task id reduce id string shuffle metrics set tag session id job conf get session id metrics context register updater synchronized input bytes num bytes num bytes num bytes synchronized failed fetch num failed fetches synchronized success fetch num success fetches synchronized thread busy num threads busy synchronized thread free num threads busy updates metrics context unused synchronized shuffle metrics incr metric shuffle input bytes num bytes shuffle metrics incr metric shuffle failed fetches num failed fetches shuffle metrics incr metric shuffle success fetches num success fetches num copiers shuffle metrics set metric shuffle fetchers busy percent num threads busy num copiers else shuffle metrics set metric shuffle fetchers busy percent num bytes num success fetches num failed fetches shuffle metrics update
1624	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleHeader.java	unrelated	package org apache hadoop mapreduce task reduce shuffle header information sent task tracker deciphered fetcher thread reduce task shuffle header implements writable the longest possible length task attempt id accept max id length string map id uncompressed length compressed length reduce shuffle header shuffle header string map id compressed length uncompressed length reduce map id map id compressed length compressed length uncompressed length uncompressed length reduce reduce read fields data input throws io exception map id writable utils read string safely max id length compressed length writable utils read v long uncompressed length writable utils read v long reduce writable utils read v int write data output throws io exception text write string map id writable utils write v long compressed length writable utils write v long uncompressed length writable utils write v int reduce
1625	mapreduce\src\java\org\apache\hadoop\mapreduce\task\reduce\ShuffleScheduler.java	scheduler	package org apache hadoop mapreduce task reduce shuffle scheduler k v thread local long shuffle start new thread local long protected long initial value return l log log log factory get log shuffle scheduler max maps at once initial penalty penalty growth rate f report failure limit boolean finished maps total maps remaining maps map string map host map locations new hash map string map host set map host pending hosts new hash set map host set task attempt id obsolete maps new hash set task attempt id random random new random system current time millis delay queue penalty penalties new delay queue penalty referee referee new referee map task attempt id int writable failure counts new hash map task attempt id int writable map string int writable host failures new hash map string int writable task status status exception reporter reporter abort failure limit progress progress counters counter shuffled maps counter counters counter reduce shuffle bytes counters counter failed shuffle counter start time last progress time max map runtime max failed unique fetches max fetch failures before reporting total bytes shuffled till now decimal format mbps format new decimal format boolean report read error immediately true shuffle scheduler job conf job task status status exception reporter reporter progress progress counters counter shuffled maps counter counters counter reduce shuffle bytes counters counter failed shuffle counter total maps job get num map tasks abort failure limit math max total maps remaining maps total maps finished maps new boolean remaining maps reporter reporter status status progress progress shuffled maps counter shuffled maps counter reduce shuffle bytes reduce shuffle bytes failed shuffle counter failed shuffle counter start time system current time millis last progress time start time referee start max failed unique fetches math min total maps max failed unique fetches max fetch failures before reporting job get int mr job config shuffle fetch failures report failure limit report read error immediately job get boolean mr job config shuffle notify readerror true synchronized copy succeeded task attempt id map id map host host bytes millis map output k v output throws io exception failure counts remove map id host failures remove host get host name map index map id get task id get id finished maps map index output commit finished maps map index true shuffled maps counter increment remaining maps notify all update status total bytes shuffled till now bytes mbs total bytes shuffled till now maps done total maps remaining maps secs since start system current time millis start time transfer rate mbs secs since start progress set maps done total maps string status string maps done total maps copied status set state string status string progress set status copy maps done total maps mbps format format transfer rate mb reduce shuffle bytes increment bytes last progress time system current time millis log debug map map id done status string synchronized copy failed task attempt id map id map host host boolean read error host penalize failures failure counts contains key map id int writable x failure counts get map id x set
1626	mapreduce\src\java\org\apache\hadoop\mapreduce\tools\CLI.java	scheduler	package org apache hadoop mapreduce tools interprets map reduce cli options cli extends configured implements tool log log log factory get log cli cli cli configuration conf set conf conf run string argv throws exception exit code argv length display usage return exit code process arguments string cmd argv string submit job file null string jobid null string taskid null string history file null string counter group name null string counter name null job priority jp null string task type null string task state null event n events boolean get status false boolean get counter false boolean kill job false boolean list events false boolean view history false boolean view all history false boolean list jobs false boolean list all jobs false boolean list active trackers false boolean list blacklisted trackers false boolean display tasks false boolean kill task false boolean fail task false boolean set job priority false submit equals cmd argv length display usage cmd return exit code submit job file argv else status equals cmd argv length display usage cmd return exit code jobid argv get status true else counter equals cmd argv length display usage cmd return exit code get counter true jobid argv counter group name argv counter name argv else kill equals cmd argv length display usage cmd return exit code jobid argv kill job true else set priority equals cmd argv length display usage cmd return exit code jobid argv try jp job priority value of argv catch illegal argument exception iae log info iae display usage cmd return exit code set job priority true else events equals cmd argv length display usage cmd return exit code jobid argv event integer parse int argv n events integer parse int argv list events true else history equals cmd argv length argv length equals argv display usage cmd return exit code view history true argv length equals argv view all history true history file argv else history file argv else list equals cmd argv length argv length equals argv display usage cmd return exit code argv length equals argv list all jobs true else list jobs true else kill task equals cmd argv length display usage cmd return exit code kill task true taskid argv else fail task equals cmd argv length display usage cmd return exit code fail task true taskid argv else list active trackers equals cmd argv length display usage cmd return exit code list active trackers true else list blacklisted trackers equals cmd argv length display usage cmd return exit code list blacklisted trackers true else list attempt ids equals cmd argv length display usage cmd return exit code jobid argv task type argv task state argv display tasks true else display usage cmd return exit code initialize cluster cluster cluster new cluster get conf submit request try submit job file null job job job get instance cluster new job conf submit job file job submit system println created job job get job id exit code else get status job job cluster get job job id name jobid job null system println could
1627	mapreduce\src\java\org\apache\hadoop\mapreduce\tools\package-info.java	unrelated	package org apache hadoop mapreduce tools
1628	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ConfigUtil.java	heartbeat	package org apache hadoop mapreduce util place holder deprecated keys framework config util adds deprecated keys loads mapred default xml mapred site xml load resources add deprecated keys configuration add default resource mapred default xml configuration add default resource mapred site xml adds deprecated keys corresponding new keys configuration add deprecated keys configuration add deprecation mapred temp dir new string mr config temp dir configuration add deprecation mapred local dir new string mr config local dir configuration add deprecation mapred cluster map memory mb new string mr config mapmemory mb configuration add deprecation mapred cluster reduce memory mb new string mr config reducememory mb configuration add deprecation mapred acls enabled new string mr config mr acls enabled configuration add deprecation mapred cluster max map memory mb new string jt config jt max mapmemory mb configuration add deprecation mapred cluster max reduce memory mb new string jt config jt max reducememory mb configuration add deprecation mapred cluster average blacklist threshold new string jt config jt avg blacklist threshold configuration add deprecation hadoop job history location new string jt config jt jobhistory location configuration add deprecation mapred job tracker history completed location new string jt config jt jobhistory completed location configuration add deprecation mapred jobtracker job history block size new string jt config jt jobhistory block size configuration add deprecation mapred job tracker jobhistory lru cache size new string jt config jt jobhistory cache size configuration add deprecation mapred hosts new string jt config jt hosts filename configuration add deprecation mapred hosts exclude new string jt config jt hosts exclude filename configuration add deprecation mapred system dir new string jt config jt system dir configuration add deprecation mapred max tracker blacklists new string jt config jt max tracker blacklists configuration add deprecation mapred job tracker new string jt config jt ipc address configuration add deprecation mapred job tracker http address new string jt config jt http address configuration add deprecation mapred job tracker handler count new string jt config jt ipc handler count configuration add deprecation mapred jobtracker restart recover new string jt config jt restart enabled configuration add deprecation mapred jobtracker task scheduler new string jt config jt task scheduler configuration add deprecation mapred jobtracker task scheduler max running tasks per job new string jt config jt runningtasks per job configuration add deprecation mapred jobtracker instrumentation new string jt config jt instrumentation configuration add deprecation mapred jobtracker maxtasks per job new string jt config jt tasks per job configuration add deprecation mapred heartbeats second new string jt config jt heartbeats in second configuration add deprecation mapred job tracker persist jobstatus active new string jt config jt persist jobstatus configuration add deprecation mapred job tracker persist jobstatus hours new string jt config jt persist jobstatus hours configuration add deprecation mapred job tracker persist jobstatus dir new string jt config jt persist jobstatus dir configuration add deprecation mapred permissions supergroup new string mr config mr supergroup configuration add deprecation mapreduce jobtracker permissions supergroup new string mr config mr supergroup configuration add deprecation mapred task cache levels new string jt config jt taskcache levels configuration add
1629	mapreduce\src\java\org\apache\hadoop\mapreduce\util\LinuxMemoryCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate virtual physical memories linux systems use link org apache hadoop mapreduce util linux resource calculator plugin instead linux memory calculator plugin extends memory calculator plugin linux resource calculator plugin resource calculator plugin use everything linux resource calculator plugin linux memory calculator plugin resource calculator plugin new linux resource calculator plugin inherit doc get physical memory size return resource calculator plugin get physical memory size inherit doc get virtual memory size return resource calculator plugin get virtual memory size
1630	mapreduce\src\java\org\apache\hadoop\mapreduce\util\LinuxResourceCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate resource information linux systems linux resource calculator plugin extends resource calculator plugin log log log factory get log linux resource calculator plugin proc meminfo virtual file keys values format key value k b string procfs memfile proc meminfo pattern procfs memfile format pattern compile z a z k b we need values following keys meminfo string memtotal string mem total string swaptotal string swap total string memfree string mem free string swapfree string swap free string inactive string inactive patterns parsing proc cpuinfo string procfs cpuinfo proc cpuinfo pattern processor format pattern compile processor pattern frequency format pattern compile cpu m hz pattern parsing proc stat string procfs stat proc stat pattern cpu time format pattern compile cpu string procfs mem file string procfs cpu file string procfs stat file jiffy length in millis ram size swap size ram size free free ram space machine k b swap size free free swap space machine k b inactive size inactive cache memory k b num processors number processors system cpu frequency l cpu frequency system k hz cumulative cpu time l cpu used time since system ms last cumulative cpu time l cpu used time read last time ms unix timestamp reading cpu time ms cpu usage task tracker status unavailable sample time task tracker status unavailable last sample time task tracker status unavailable procfs based process tree p tree null boolean read mem info file false boolean read cpu info file false get current time get current time return system current time millis linux resource calculator plugin procfs mem file procfs memfile procfs cpu file procfs cpuinfo procfs stat file procfs stat jiffy length in millis procfs based process tree jiffy length in millis string pid system getenv get jvm pid p tree new procfs based process tree pid constructor allows assigning proc directories this used unit tests linux resource calculator plugin string procfs mem file string procfs cpu file string procfs stat file jiffy length in millis procfs mem file procfs mem file procfs cpu file procfs cpu file procfs stat file procfs stat file jiffy length in millis jiffy length in millis string pid system getenv get jvm pid p tree new procfs based process tree pid read proc meminfo parse compute memory information read proc mem info file read proc mem info file false read proc meminfo parse compute memory information read proc mem info file boolean read again read mem info file read again return read proc mem info file buffered reader null file reader f reader null try f reader new file reader procfs mem file new buffered reader f reader catch file not found exception f happen return matcher mat null try string str read line str null mat procfs memfile format matcher str mat find mat group equals memtotal string ram size long parse long mat group else mat group equals swaptotal string swap size long parse long mat group else mat group equals memfree string ram size free long parse long mat group else mat group
1631	mapreduce\src\java\org\apache\hadoop\mapreduce\util\MemoryCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate virtual physical memories system link org apache hadoop mapreduce util resource calculator plugin instead memory calculator plugin extends configured obtain total size virtual memory present system get virtual memory size obtain total size physical memory present system get physical memory size get memory calculator plugin name configure if name null method try return memory calculator plugin available system memory calculator plugin get memory calculator plugin class extends memory calculator plugin clazz configuration conf clazz null return reflection utils new instance clazz conf no given try os specific try string os name system get property os name os name starts with linux return new linux memory calculator plugin catch security exception se failed get operating system name return null not supported system return null
1632	mapreduce\src\java\org\apache\hadoop\mapreduce\util\MRAsyncDiskService.java	pooling	package org apache hadoop mapreduce util this container multiple thread pools volume schedule async disk operations easily examples async disk operations deletion files we move files be deleted folder asychronously deleting make sure caller run faster users write files be deleted folder otherwise files gone time restart mr async disk service this also contains operations performed thread pools mr async disk service log log log factory get log mr async disk service async disk service async disk service string tobedeleted be deleted create async disk services set volumes specified root directories the async disk services uses one thread pool per volume async disk operations absolte paths paths relative user dir system property cwd mr async disk service file system local file system string non canonical vols throws io exception local file system local file system volumes new string non canonical vols length v v non canonical vols length v volumes v normalize path non canonical vols v log debug normalized volume non canonical vols v volumes v async disk service new async disk service volumes create one thread pool per volume v v volumes length v create root file deletion path absolute subdir new path volumes v tobedeleted local file system mkdirs absolute subdir we tolerate missing volumes log warn cannot create tobedeleted volumes v ignored create tasks delete paths inside volumes v v volumes length v path absolute subdir new path volumes v tobedeleted file status files null try list files inside volumes tobedeleted sub directory files local file system list status absolute subdir catch exception e ignore exceptions list status we tolerate missing sub directories files null f f files length f get relative file name root volume string absolute filename files f get path uri get path string relative tobedeleted path separator char files f get path get name delete task task new delete task volumes v absolute filename relative execute volumes v task initialize mr async disk service based conf mr async disk service job conf conf throws io exception file system get local conf conf get local dirs execute task sometime future using thread pools synchronized execute string root runnable task async disk service execute root task gracefully start shut thread pools synchronized shutdown async disk service shutdown shut thread pools immediately synchronized list runnable shutdown now return async disk service shutdown now wait termination thread pools synchronized boolean await termination milliseconds throws interrupted exception return async disk service await termination milliseconds simple date format format new simple date format yyyy mm dd hh mm ss sss file system local file system string volumes atomic long unique id new atomic long a task deleting path name volume delete task implements runnable the volume file string volume the file name move string original path the file name move string path to be deleted delete file directory recursively needed containing tobedeleted delete task string volume string original path string path to be deleted volume volume original path original path path to be deleted path to be deleted string string called async disk service execute displaying error messages return deletion
1633	mapreduce\src\java\org\apache\hadoop\mapreduce\util\package-info.java	unrelated	package org apache hadoop mapreduce util
1634	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ProcessTree.java	unrelated	package org apache hadoop mapreduce util process tree related operations process tree log log log factory get log process tree default sleeptime before sigkill l sigquit sigterm sigkill string sigquit str sigquit string sigterm str sigterm string sigkill str sigkill boolean setsid available setsid supported boolean setsid supported shell command executor shexec null boolean setsid supported true try string args setsid bash c echo shexec new shell command executor args shexec execute catch io exception ioe log warn setsid available machine so using setsid supported false finally handle exit code log info setsid exited exit code shexec get exit code return setsid supported destroy process tree killed sending sigterm separate thread destroy string pid sleeptime before sigkill boolean process group boolean background process group destroy process group pid sleeptime before sigkill background else todo destroy processes subtree case also for time killing root process destroy process pid sleeptime before sigkill background destroy process sending sigterm separate thread protected destroy process string pid sleeptime before sigkill boolean background terminate process pid sig kill pid false sleeptime before sigkill background destroy process group sending sigterm separate thread protected destroy process group string pgrp id sleeptime before sigkill boolean background terminate process group pgrp id sig kill pgrp id true sleeptime before sigkill background send specified signal specified pid logging send signal string pid signal num string signal name shell command executor shexec null try string args kill signal num pid shexec new shell command executor args shexec execute catch io exception ioe log warn error executing shell command ioe finally pid starts with log info sending signal members process group pid signal name exit code shexec get exit code else log info signaling process pid signal name exit code shexec get exit code send specified signal process alive logging maybe signal process string pid signal num string signal name boolean always signal if process tree alive signal unless always signal forces always signal process tree alive pid send signal pid signal num signal name maybe signal process group string pgrp id signal num string signal name boolean always signal always signal process tree process group alive pgrp id signaling process group means using negative pid send signal pgrp id signal num signal name sends terminate signal process allowing gracefully exit terminate process string pid maybe signal process pid sigterm sigterm str true sends terminate signal process belonging passed process group allowing group gracefully exit terminate process group string pgrp id maybe signal process group pgrp id sigterm sigterm str true kills process or process group sending signal sigkill current thread sending sigterm sig kill in current thread string pid boolean process group sleep time before sig kill kill subprocesses root process even root process alive process group killed process group process tree alive pid try sleep time sending sigkill thread sleep sleep time before sig kill catch interrupted exception log warn thread sleep interrupted process group kill process group pid else kill process pid kills process or process group sending signal sigkill sending sigterm separate thread sig kill string pid boolean process group sleeptime
1635	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ProcfsBasedProcessTree.java	unrelated	package org apache hadoop mapreduce util a proc file system based process tree works linux procfs based process tree extends process tree log log log factory get log procfs based process tree string procfs proc pattern procfs stat file format pattern compile string procfs stat file stat string procfs cmdline file cmdline page size shell command executor shell executor new shell command executor new string getconf pagesize page size try shell executor execute page size long parse long shell executor get output replace n catch io exception e log error string utils stringify exception e finally page size page size jiffy length in millis millisecond shell command executor shell executor new shell command executor new string getconf clk tck jiffies per second try shell executor execute jiffies per second long parse long shell executor get output replace n catch io exception e log error string utils stringify exception e finally jiffy length in millis jiffies per second math round d jiffies per second enable testing using variable configured test directory string procfs dir integer pid long cpu time l boolean setsid used false sleeptime before sigkill default sleeptime before sigkill map integer process info process tree new hash map integer process info procfs based process tree string pid pid false default sleeptime before sigkill procfs based process tree string pid boolean setsid used sigkill interval pid setsid used sigkill interval procfs build new process tree rooted pid this method provided mainly testing purposes root proc file system adjusted killing process tree procfs based process tree string pid boolean setsid used sigkill interval string procfs dir pid get valid pid pid setsid used setsid used sleeptime before sigkill sigkill interval procfs dir procfs dir sets sigkill interval string boolean instead sending sigterm set sig kill interval interval sleeptime before sigkill interval checks procfs based process tree available system boolean available try string os name system get property os name os name starts with linux log info procfs based process tree currently supported linux return false catch security exception se log warn failed get operating system name se return false return true get process tree latest state if root process alive empty tree returned procfs based process tree get process tree pid get list processes list integer process list get process list map integer process info process info new hash map integer process info cache process tree get age processes map integer process info old procs new hash map integer process info process tree process tree clear process info null integer proc process list get information process process info p info new process info proc construct process info p info procfs dir null process info put proc p info proc equals pid p info cache process tree put proc p info null return add process parent map entry integer process info entry process info entry set integer p id entry get key p id process info p info entry get value process info parent p info process info get p info get ppid parent p info null parent p info add child p info
1636	mapreduce\src\java\org\apache\hadoop\mapreduce\util\ResourceCalculatorPlugin.java	unrelated	package org apache hadoop mapreduce util plugin calculate resource information system resource calculator plugin extends configured obtain total size virtual memory present system get virtual memory size obtain total size physical memory present system get physical memory size obtain total size available virtual memory present system get available virtual memory size obtain total size available physical memory present system get available physical memory size obtain total number processors present system get num processors obtain cpu frequency system get cpu frequency obtain cumulative cpu time since system get cumulative cpu time obtain cpu usage machine return unavailable get cpu usage obtain resource status used current process tree proc resource values get proc resource values proc resource values cumulative cpu time physical memory size virtual memory size proc resource values cumulative cpu time physical memory size virtual memory size cumulative cpu time cumulative cpu time physical memory size physical memory size virtual memory size virtual memory size obtain physical memory size used current process tree get physical memory size return physical memory size obtain virtual memory size used current process tree get virtual memory size return virtual memory size obtain cumulative cpu time used current process tree get cumulative cpu time return cumulative cpu time get resource calculator plugin name configure if name null method try return memory calculator plugin available system resource calculator plugin get resource calculator plugin class extends resource calculator plugin clazz configuration conf clazz null return reflection utils new instance clazz conf no given try os specific try string os name system get property os name os name starts with linux return new linux resource calculator plugin catch security exception se failed get operating system name return null not supported system return null
1637	mapreduce\src\java\org\apache\hadoop\util\LinuxMemoryCalculatorPlugin.java	unrelated	package org apache hadoop util plugin calculate virtual physical memories linux systems link org apache hadoop mapreduce util linux memory calculator plugin instead linux memory calculator plugin extends org apache hadoop mapreduce util linux memory calculator plugin inherits everything super
1638	mapreduce\src\java\org\apache\hadoop\util\MemoryCalculatorPlugin.java	unrelated	package org apache hadoop util plugin calculate virtual physical memories system link org apache hadoop mapreduce util memory calculator plugin instead memory calculator plugin extends org apache hadoop mapreduce util memory calculator plugin
1639	mapreduce\src\java\org\apache\hadoop\util\package-info.java	unrelated	package org apache hadoop util
1640	mapreduce\src\java\org\apache\hadoop\util\ProcessTree.java	unrelated	package org apache hadoop util process tree related operations process tree extends org apache hadoop mapreduce util process tree inherits everything super
1641	mapreduce\src\java\org\apache\hadoop\util\ProcfsBasedProcessTree.java	unrelated	package org apache hadoop util a proc file system based process tree works linux link org apache hadoop mapreduce util procfs based process tree instead procfs based process tree extends org apache hadoop mapreduce util procfs based process tree procfs based process tree string pid super pid procfs based process tree string pid boolean setsid used sigkill interval super pid setsid used sigkill interval procfs based process tree string pid boolean setsid used sigkill interval string procfs dir super pid setsid used sigkill interval procfs dir procfs based process tree get process tree return procfs based process tree super get process tree
1642	mapreduce\src\tools\org\apache\hadoop\fs\HarFileSystem.java	unrelated	package org apache hadoop fs this implementation hadoop archive filesystem this archive filesystem index files form index contents form part the index files store indexes real files the index files form masterindex index the master index level indirection index file make look ups faster index file sorted hash code paths contains master index contains pointers positions index ranges hashcodes har file system extends filter file system version map uri har meta data har meta cache new hash map uri har meta data uri representation har filesystem uri uri top level path archive underlying file system path archive path har auth string har auth pointer metadata cache har meta data metadata construction harfilesystem har file system constructor create har file system underlying filesystem har file system file system fs super fs initialize har filesystem per har archive the archive home directory top level directory filesystem contains har archive be careful method want go creating new filesystem instances per call path get file system uri har har underlyingfsscheme host port archivepath har archivepath this assumes underlying filesystem used case specified initialize uri name configuration conf throws io exception decode name uri lying uri decode har uri name conf got right har path check truly har filesystem path har path archive path new path name get scheme name get authority name get path har path null throw new io exception invalid path har filesystem name string fs null fs file system get lying uri conf uri har path uri archive path new path uri get path har auth get har auth lying uri check underlying fs containing index file path master index path new path archive path masterindex path archive index path new path archive path index fs exists master index path fs exists archive index path throw new io exception invalid path har filesystem no index file har path metadata har meta cache get uri metadata null file status stat fs get file status master index path file status stat fs get file status archive index path stat get modification time metadata get master index timestamp stat get modification time metadata get archive index timestamp archive overwritten since last read remove entry meta data cache metadata null har meta cache remove uri metadata null metadata new har meta data fs master index path archive index path metadata parse meta data har meta cache put uri metadata get version filesystem masterindex file version currently useful since first version archives get har version throws io exception metadata null return metadata get version else throw new io exception invalid meta data har filesystem find parent path archive path path the last path segment ends har path returned path archive path path p path ret path null path tmp p p depth tmp string ends with har ret path tmp break tmp tmp get parent return ret path decode raw uri get underlying uri uri decode har uri uri raw uri configuration conf throws io exception string tmp auth raw uri get authority using default file system config create underlying uri return tmp auth null create path return
1643	mapreduce\src\tools\org\apache\hadoop\fs\package-info.java	unrelated	package org apache hadoop fs
1644	mapreduce\src\tools\org\apache\hadoop\tools\DistCh.java	unrelated	package org apache hadoop tools a map reduce program recursively change files properties owner group permission dist ch extends dist tool string name distch string job dir label name job dir string op list label name op list string op count label name op count string usage java dist ch get name options path owner group permission n n the values owner group permission empty n permission octal number n n options n f urilist uri use list urilist uri src list n ignore failures n log logdir write logs logdir op per map max maps per node sync file max enum counter succeed fail enum option ignore failures name ignore failures string cmd propertyname option string cmd string propertyname cmd cmd propertyname propertyname dist ch configuration conf super create job conf conf job conf create job conf configuration conf job conf jobconf new job conf conf dist ch jobconf set job name name jobconf set map speculative execution false jobconf set input format change input format jobconf set output key class text jobconf set output value class text jobconf set mapper class change files mapper jobconf set num reduce tasks return jobconf file operations file operation implements writable path src string owner string group fs permission permission file operation file operation path src file operation src src owner owner group group permission permission check state path owner group permission e g user foo foo bar file operation string line try string line split length equals null src new path owner group permission null null new fs permission short parse short check state catch exception e throw illegal argument exception new illegal argument exception line line init cause e check state throws illegal state exception owner null group null permission null throw new illegal state exception owner null group null permission null fs permission file umask fs permission create immutable short boolean different file status original owner null owner equals original get owner return true group null group equals original get group return true permission null fs permission orig original get permission return original directory permission equals orig permission apply u mask file umask equals orig return false run configuration conf throws io exception file system fs src get file system conf permission null fs set permission src permission owner null group null fs set owner src owner group inherit doc read fields data input throws io exception src new path text read string owner dist tool read string group dist tool read string permission read boolean fs permission read null inherit doc write data output throws io exception text write string src string dist tool write string owner dist tool write string group boolean b permission null write boolean b b permission write inherit doc string string return src owner group permission responsible generating splits src file list change input format implements input format text file operation do nothing validate input job conf job produce splits greater quotient total size number splits requested input split get splits job conf job num splits throws io exception src count job get int op
1645	mapreduce\src\tools\org\apache\hadoop\tools\DistCp.java	unrelated	package org apache hadoop tools a map reduce program recursively copy directories different file systems dist cp implements tool log log log factory get log dist cp string name distcp string usage name options srcurl desturl n n options n p rbugpt preserve status n r replication number n b block size n u user n g group n p permission n modification access times n p alone equivalent prbugpt n ignore failures n basedir basedir use basedir base directory copying files srcurl n log logdir write logs logdir n num maps maximum number simultaneous copies n overwrite overwrite destination n update overwrite src size different dst size n skipcrccheck do use crc check determine src n different dest relevant update n specified n f urilist uri use list urilist uri src list n filelimit n limit total number files n n sizelimit n limit total size n bytes n delete delete files existing dst src n dryrun display count files total size files n src exit copy done n desturl speicified update n mapred ssl conf f filename ssl configuration mapper task n n note overwrite update set source uri n interpreted isomorphic update existing directory n for example nhadoop name p update hdfs a user foo bar hdfs b user foo baz n n would update descendants baz also bar would n update user foo baz bar n n note the parameter n filelimit sizelimit n specified symbolic representation for examples n k n g n bytes per map max maps per node sync file max default file retries enum counter copy skip fail bytescopied bytesexpected enum options delete delete name delete file limit filelimit name limit file size limit sizelimit name limit size ignore read failures name ignore read failures preserve status p name preserve status overwrite overwrite name overwrite always update update name overwrite ifnewer skipcrc skipcrccheck name skip crc check string cmd propertyname options string cmd string propertyname cmd cmd propertyname propertyname parse long string args offset offset args length throw new illegal argument exception n specified cmd n string utils traditional binary prefix args offset n throw new illegal argument exception n n cmd return n enum file attribute block size replication user group permission times char symbol file attribute symbol string lower case char at enum set file attribute parse string null length return enum set of file attribute enum set file attribute set enum set none of file attribute file attribute attributes values char c char array attributes length c attributes symbol attributes length set contains attributes set add attributes else throw new illegal argument exception there one attributes symbol else throw new illegal argument exception c undefined return set string tmp dir label name tmp dir string dst dir label name dest path string job dir label name job dir string max maps label name max map tasks string src list label name src list string src count label name src count string total size label name total size string dst dir list label name dst dir list string bytes per map label
1646	mapreduce\src\tools\org\apache\hadoop\tools\DistTool.java	unrelated	package org apache hadoop tools an distributed tool file related operations dist tool implements org apache hadoop util tool protected log log log factory get log dist tool protected job conf jobconf inherit doc set conf configuration conf jobconf conf jobconf conf instanceof job conf job conf conf new job conf conf inherit doc job conf get conf return jobconf protected dist tool configuration conf set conf conf random random new random protected string get random id return integer string random next int integer max value sanity check source protected check source configuration conf list path srcs throws invalid input exception list io exception ioes new array list io exception path p srcs try p get file system conf exists p ioes add new file not found exception source p exist catch io exception e ioes add e ioes empty throw new invalid input exception ioes protected string read string data input throws io exception read boolean return text read string return null protected write string data output string throws io exception boolean b null write boolean b b text write string protected list string read file configuration conf path inputfile throws io exception list string result new array list string file system fs inputfile get file system conf buffered reader input null try input new buffered reader new input stream reader fs open inputfile string line line input read line null result add line finally input close return result an exception duplicated source files duplication exception extends io exception serial version uid l error code exception error code duplication exception string message super message
1647	mapreduce\src\tools\org\apache\hadoop\tools\HadoopArchives.java	unrelated	package org apache hadoop tools archive creation utility this provides methods used create hadoop archives for understanding hadoop archives look link har file system hadoop archives implements tool version log log log factory get log hadoop archives string name har string src list label name src list string dst dir label name dest path string tmp dir label name tmp dir string job dir label name job dir string src count label name src count string total size label name total size string dst har label name archive name string src parent label name parent path size blocks created archiving string har blocksize label name block size size part files created archiving string har partsize label name partfile size size part file size part size size blocks hadoop archives block size string usage archive archive name name p parent path src dest n job conf conf set conf configuration conf conf instanceof job conf conf job conf conf else conf new job conf conf hadoop archives configuration get conf return conf hadoop archives configuration conf set conf conf check src paths check paths configuration conf list path paths throws io exception path p paths file system fs p get file system conf fs exists p throw new file not found exception source p exist assumes two types files file dir recursivels file system fs file status dir fdir list file status dir throws io exception fdir get file status file add fdir return else add fdir file status list status fs list status fdir get file status get path fdir set children list status file status stat list status file status dir fstat dir new file status dir stat null recursivels fs fstat dir har entry used link h archives mapper input value har entry implements writable string path string children har entry har entry string path string children path path children children boolean dir return children null read fields data input throws io exception path text read string read boolean children new string read int children length children text read string else children null write data output throws io exception text write string path boolean dir dir write boolean dir dir write int children length string c children text write string c input format hadoop archive job responsible generating splits file list h archive input format implements input format long writable har entry generate input splits src file lists input split get splits job conf jconf num splits throws io exception string srcfilelist jconf get src list label equals srcfilelist throw new io exception unable get src file archive generation total size jconf get long total size label total size throw new io exception invalid size files archive safe since set code path src new path srcfilelist file system fs src get file system jconf file status fstatus fs get file status src array list file split splits new array list file split num splits long writable key new long writable har entry value new har entry sequence file reader reader null remaining bytes file split remaining fstatus get len count
1648	mapreduce\src\tools\org\apache\hadoop\tools\Logalyzer.java	unrelated	package org apache hadoop tools logalyzer a utility tool archiving analyzing hadoop logs p this tool supports archiving anaylzing sort grep log files it takes input input uri serve uris logs archived b output directory mandatory b directory dfs archive logs c the sort grep patterns analyzing files separator boundaries usage logalyzer archive archive dir directory archive logs analysis directory logs log list uri grep pattern sort col col separator separator p logalyzer constants configuration fs config new configuration string sort columns logalizer logcomparator sort columns string column separator logalizer logcomparator column separator configuration add deprecation mapred reducer sort new string sort columns configuration add deprecation mapred reducer separator new string column separator a link mapper extracts text matching regular expression log regex mapper k extends writable comparable extends map reduce base implements mapper k text text long writable pattern pattern configure job conf job pattern pattern compile job get regex mapper pattern map k key text value output collector text long writable output reporter reporter throws io exception string text value string matcher matcher pattern matcher text matcher find output collect value new long writable a writable comparator optimized utf keys logs log comparator extends text comparator implements configurable log log log factory get log logalyzer job conf conf null string sort spec null string column separator null set conf configuration conf conf instanceof job conf conf job conf conf else conf new job conf conf initialize specification comparision string sort columns conf get sort columns null sort columns null sort spec sort columns split column separator column separator conf get column separator configuration get conf return conf compare byte b byte b sort spec null return super compare b b try text logline new text logline read fields new data input stream new byte array input stream b string line logline string string log columns line split column separator text logline new text logline read fields new data input stream new byte array input stream b string line logline string string log columns line split column separator log columns null log columns null return super compare b b compare column wise according sort spec sort spec length column integer value of sort spec value string c log columns column string c log columns column compare columns comparision super compare bytes c get bytes c length c get bytes c length they differ comparision return comparision catch io exception ioe log fatal caught ioe return return register comparator writable comparator define text new log comparator archive workhorse function archive log files archive string log list uri string archive directory throws io exception string dest url file system get default uri fs config archive directory dist cp copy new job conf fs config log list uri dest url null true false analyze analyze string input files directory string output directory string grep pattern string sort columns string column separator throws io exception path grep input new path input files directory path analysis output null output directory equals analysis output new path input files directory logalyzer integer string new random next int integer
1649	mapreduce\src\tools\org\apache\hadoop\tools\package-info.java	unrelated	package org apache hadoop tools
1650	mapreduce\src\tools\org\apache\hadoop\tools\rumen\AbstractClusterStory.java	unrelated	package org apache hadoop tools rumen link abstract cluster story provides partial implementation link cluster story parsing topology tree abstract cluster story implements cluster story protected set machine node machine nodes protected set rack node rack nodes protected machine node nodes flattened protected map string machine node node map protected map string rack node r node map protected maximum distance set machine node get machines parse topology tree return machine nodes synchronized set rack node get racks parse topology tree return rack nodes synchronized machine node get random machines expected random random expected return new machine node parse topology tree total machine nodes size select math min expected total nodes flattened null nodes flattened machine nodes array new machine node total machine node retval new machine node select select total select index random next int total machine node tmp nodes flattened index nodes flattened index nodes flattened total nodes flattened total tmp select system arraycopy nodes flattened total retval select else system arraycopy nodes flattened retval select return retval protected synchronized build machine node map node map null node map new hash map string machine node machine nodes size machine node mn machine nodes node map put mn get name mn machine node get machine by name string name build machine node map return node map get name distance node node b lvl get level lvl b b get level retval lvl lvl b retval lvl lvl b retval get parent else lvl lvl b retval lvl b lvl retval b b get parent b get parent b b get parent retval return retval protected synchronized build rack node map r node map null r node map new hash map string rack node rack nodes size rack node rn rack nodes r node map put rn get name rn rack node get rack by name string name build rack node map return r node map get name get maximum distance parse topology tree return maximum distance protected synchronized parse topology tree machine nodes null node root get cluster topology sorted set machine node nodes new tree set machine node sorted set rack node r nodes new tree set rack node dfs search tree deque node unvisited new array deque node deque integer dist unvisited new array deque integer unvisited add root dist unvisited add node n unvisited poll n null n unvisited poll distance dist unvisited poll n instanceof rack node r nodes add rack node n nodes add all rack node n get machines in rack distance maximum distance maximum distance distance else n instanceof machine node nodes add machine node n distance maximum distance maximum distance distance else node child n get children unvisited add first child dist unvisited add first distance machine nodes collections unmodifiable sorted set nodes rack nodes collections unmodifiable sorted set r nodes
1651	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CDFPiecewiseLinearRandomGenerator.java	unrelated	package org apache hadoop tools rumen cdf piecewise linear random generator extends cdf random generator builds cdf random value engine around link logged discrete cdf defaultly seeded rng cdf piecewise linear random generator logged discrete cdf cdf super cdf builds cdf random value engine around link logged discrete cdf explicitly seeded rng random number generator seed cdf piecewise linear random generator logged discrete cdf cdf seed super cdf seed todo this code assumes empirical minimum resp maximum epistomological minimum resp maximum this probably okay minimum likely represents task everything went well maximum may want develop way extrapolating past maximum value at probability range floor floor index probability segment prob min get ranking at range floor segment prob max get ranking at range floor segment min value get datum at range floor segment max value get datum at range floor if zero object based ill formed cdf segment prob range segment prob max segment prob min segment datum range segment max value segment min value result probability segment prob min segment prob range segment datum range segment min value return result
1652	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CDFRandomGenerator.java	unrelated	package org apache hadoop tools rumen an instance generates random values confirm embedded link logged discrete cdf the discrete cdf pointwise approximation real cdf we therefore choice interpolation rules a concrete subclass implement value at using dependent interpolation rule cdf random generator rankings values random random cdf random generator logged discrete cdf cdf cdf new random cdf random generator logged discrete cdf cdf seed cdf new random seed cdf random generator logged discrete cdf cdf random random random random rankings new cdf get rankings size values new cdf get rankings size initialize tables cdf protected initialize tables logged discrete cdf cdf rankings values cdf get minimum rankings rankings length values rankings length cdf get maximum list logged single relative ranking subjects cdf get rankings subjects size rankings subjects get get relative ranking values subjects get get datum protected floor index probe result arrays binary search rankings probe return math abs result protected get ranking at index return rankings index protected get datum at index return values index random value return value at random next double value at probability
1653	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ClusterStory.java	unrelated	package org apache hadoop tools rumen link cluster story represents configurations map reduce cluster including nodes network topology slot configurations cluster story get machines cluster set machine node get machines get racks cluster set rack node get racks get cluster topology tree node get cluster topology select random set machines machine node get random machines expected random random get link machine node host name machine node get machine by name string name get link rack node name rack node get rack by name string name determine distance two link node currently distance loosely defined length longer path either b reach common ancestor distance node node b get maximum distance possible two nodes get maximum distance
1654	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ClusterTopologyReader.java	unrelated	package org apache hadoop tools rumen reading json encoded cluster topology produce parsed link logged network topology object cluster topology reader logged network topology topology read topology json object mapper parser logged network topology parser throws io exception try topology parser get next topology null throw new io exception input file contain valid topology data finally parser close constructor path json encoded topology file possibly compressed cluster topology reader path path configuration conf throws io exception json object mapper parser logged network topology parser new json object mapper parser logged network topology path logged network topology conf read topology parser constructor the input stream json encoded topology data cluster topology reader input stream input throws io exception json object mapper parser logged network topology parser new json object mapper parser logged network topology input logged network topology read topology parser get link logged network topology object logged network topology get return topology
1655	mapreduce\src\tools\org\apache\hadoop\tools\rumen\CurrentJHParser.java	unrelated	package org apache hadoop tools rumen link job history parser parses link job history files produced link org apache hadoop mapreduce jobhistory job history source code tree rumen current jh parser implements job history parser event reader reader forked data input stream extends data input stream forked data input stream input stream input super input close code can parser parse input boolean parse input stream input throws io exception data input stream new forked data input stream input try event reader reader new event reader try reader get next event catch io exception e return false finally reader close catch io exception e return false return true current jh parser input stream input throws io exception reader new event reader new data input stream input history event next event throws io exception return reader get next event close throws io exception reader close
1656	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeepCompare.java	unrelated	package org apache hadoop tools rumen classes implement deep compare equality order another instance they deep compare if semantically significant difference implementer throws exception thrown chain causes describing chain field references indices get miscompared point deep compare comparand compared path got in root location null to process scalar code foo field root make recursive call link tree path whose code field name code bar whose code index whose code parent code null to process plural code bar field root make recursive call link tree path whose field name code foo whose code index whose code parent also code null deep compare deep compare tree path location throws deep inequality exception
1657	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeepInequalityException.java	unrelated	package org apache hadoop tools rumen we use exception unit test deep comparison run deep inequality exception extends exception serial version uid tree path path exception message path gets root inequality this constructor i intend used exception deep inequality exception string message tree path path throwable chainee super message chainee path path exception message path gets root inequality this constructor i intend used exception deep inequality exception string message tree path path super message path path
1658	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DefaultInputDemuxer.java	unrelated	package org apache hadoop tools rumen link default input demuxer acts pass demuxer it opens file returns back input stream if input compressed would return decompression stream default input demuxer implements input demuxer string name input stream input bind to path path configuration conf throws io exception name null binding previous one consumed close name path get name input new possibly decompressed input stream path conf return pair string input stream get next throws io exception name null pair string input stream ret new pair string input stream name input name null input null return ret return null close throws io exception try input null input close finally name null input null
1659	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DefaultOutputter.java	pooling	package org apache hadoop tools rumen the default link outputter outputs plain file compression applied path right suffix default outputter t implements outputter t json object mapper writer t writer compressor compressor init path path configuration conf throws io exception file system fs path get file system conf compression codec codec new compression codec factory conf get codec path output stream output codec null compressor codec pool get compressor codec output codec create output stream fs create path compressor else output fs create path writer new json object mapper writer t output conf get boolean rumen output pretty print true output t object throws io exception writer write object close throws io exception try writer close finally compressor null codec pool return compressor compressor
1660	mapreduce\src\tools\org\apache\hadoop\tools\rumen\DeskewedJobTraceReader.java	unrelated	package org apache hadoop tools rumen deskewed job trace reader implements closeable underlying engine job trace reader reader configuration variables skew buffer length boolean abort on unfixable skew state variables skew measurement latest submit time long min value returned latest submit time long min value max skew buffer needed submit time not counted repeated submit times so far occurs this situation represented time submit times so far a submit time occurs twice appears counted repeated submit times so far appropriate range value and submit times so far tree map long integer counted repeated submit times so far new tree map long integer tree set long submit times so far new tree set long priority queue logged job skew buffer log log log factory get log deskewed job trace reader job comparator implements comparator logged job compare logged job j logged job j return j get submit time j get submit time j get submit time j get submit time constructor link job trace reader protected number late jobs preced later order earlier job deskewed job trace reader job trace reader reader skew buffer length boolean abort on unfixable skew throws io exception reader reader skew buffer length skew buffer length abort on unfixable skew abort on unfixable skew skew buffer new priority queue logged job skew buffer length new job comparator fill skew buffer deskewed job trace reader job trace reader reader throws io exception reader true logged job raw next job throws io exception logged job result reader get next abort on unfixable skew skew buffer length result null time result get submit time submit times so far contains time integer count counted repeated submit times so far get time counted repeated submit times so far put time count null count else submit times so far add time time skew measurement latest submit time iterator long end cursor submit times so far descending iterator job needs skew long key needing skew end cursor next key needing skew end cursor next time integer key needs skew amount counted repeated submit times so far get key needing skew job needs skew key needs skew amount null key needs skew amount max skew buffer needed math max max skew buffer needed job needs skew skew measurement latest submit time math max time skew measurement latest submit time return result out of order exception extends runtime exception serial version uid l out of order exception string text super text logged job next job throws io exception out of order exception logged job new job raw next job new job null skew buffer add new job logged job result skew buffer poll result null result get submit time returned latest submit time log error the current job submitted earlier previous one log error its job id result get job id log error its submit time result get submit time previous one returned latest submit time abort on unfixable skew throw new out of order exception job submit time result get submit time previous one returned latest submit time result raw next job result null returned latest
1661	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Folder.java	pooling	package org apache hadoop tools rumen folder extends configured implements tool output duration input cycle concentration random seed irrelevant seeded false boolean seeded false boolean debug false boolean allow missorting false skew buffer length starts after log log log factory get log folder deskewed job trace reader reader null json generator gen null list path temp paths new linked list path path temp dir null first job submit time time dilation transcription rate fraction transcription rate integer random random ticks per second l error return codes non existent files no input cycle length empty job trace out of order jobs all jobs simultaneous io error other error set closeable closees new hash set closeable set path deletees new hash set path parse duration string duration string string numeral duration string substring duration string length char duration code duration string char at duration string length result integer parse int numeral result throw new illegal argument exception negative durations allowed switch duration code case d case return l l l ticks per second result case h case return l l ticks per second result case m case return l ticks per second result case s case return ticks per second result default throw new illegal argument exception missing invalid duration code initialize string args throws illegal argument exception string temp dir name null string input path name null string output path name null args length string arg args arg equals ignore case starts starts after parse duration args else arg equals ignore case output duration output duration parse duration args else arg equals ignore case input cycle input cycle parse duration args else arg equals ignore case concentration concentration double parse double args else arg equals ignore case debug debug true else arg equals ignore case allow missorting allow missorting true else arg equals ignore case seed seeded true random seed long parse long args else arg equals ignore case skew buffer length skew buffer length integer parse int args else arg equals ignore case temp directory temp dir name args else arg equals arg starts with throw new illegal argument exception illegal switch argument arg position else input path name arg output path name args args length throw new illegal argument exception too many non switch arguments try configuration conf get conf path path new path input path name reader new deskewed job trace reader new job trace reader path conf skew buffer length allow missorting path path new path output path name object mapper mapper new object mapper mapper configure serialization config feature can override access modifiers true json factory factory mapper get json factory file system fs path get file system conf compression codec codec new compression codec factory conf get codec path output stream output compressor compressor null codec null compressor codec pool get compressor codec output codec create output stream fs create path compressor else output fs create path gen factory create json generator output json encoding utf gen use default pretty printer temp dir temp dir name null path get parent new path temp dir name file system
1662	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Hadoop20JHParser.java	unrelated	package org apache hadoop tools rumen link job history parser parse job histories hadoop meta hadoop jh parser implements job history parser line reader reader string end line string internal version can parser parse input we deem stream good job history stream first line exactly meta version boolean parse input stream input throws io exception try line reader reader new line reader input text buffer new text return reader read line buffer buffer string equals meta version catch eof exception e return false hadoop jh parser input stream input throws io exception super reader new line reader input map string history event emitter live emitters new hash map string history event emitter queue history event remaining events new linked list history event enum line type job job jobid history event emitter create emitter return new job line history event emitter task task taskid history event emitter create emitter return new task line history event emitter map attempt map attempt task attempt id history event emitter create emitter return new map attempt line history event emitter reduce attempt reduce attempt task attempt id history event emitter create emitter return new reduce attempt line history event emitter log record type type string name line type string string name type log record type intern name name log record type record type return type string get name parsed line line return line get name history event emitter create emitter line type find line type log record type lrt line type lt line type values lt type lrt return lt return null history event next event try remaining events empty parsed line line new parsed line get full line internal version line type type line type find line type line get type type null continue string name type get name line history event emitter emitter find or make emitter name type pair queue history event history event emitter post emit action pair emitter emitter core line name pair second history event emitter post emit action remove hee live emitters remove name remaining events pair first return remaining events poll catch eof exception e return null catch io exception e return null history event emitter find or make emitter string name line type type history event emitter result live emitters get name result null result type create emitter live emitters put name result return result string get one line throws io exception text result text new text reader read line result text throw new eof exception apparent bad line return result text string string get full line throws io exception string line get one line line length end line string length line get one line line ends with end line string return line string builder sb new string builder line string added line added line get one line added line null return sb string sb append n sb append added line added line length end line string length end line string equals added line substring added line length end line string length return sb string close throws io exception reader null reader close
1663	mapreduce\src\tools\org\apache\hadoop\tools\rumen\HadoopLogsAnalyzer.java	pooling	package org apache hadoop tools rumen this main rumen log mining functionality it reads directory job tracker logs computes various information see code usage hadoop logs analyzer extends configured implements tool output streams print stream status output system print stream statistical output system print stream debug output system err the number splits task ignore maximum preferred locations this element compensate fact percentiles engine rounds expected sample count total number readings small enough need compensate slightly aggregating spread data jobs reducers together jobs many reducers small spread compensation threshold l code maximum clock skew maximum plausible difference clocks machines cluster this important event logically must follow second event considered non anomalous precedes second event provided happen different machines maximum clock skew l the regular expression used parse task attempt i ds job tracker logs pattern task attempt id pattern pattern compile pattern xml file prefix pattern compile pattern conf file header pattern compile conf xml map string pattern counter patterns new hash map string pattern the unpaired job config file currently used glean code xmx field jre options parsed config file jobconf null set code omit task details if true emit job digest statistical info detailed job trace boolean omit task details false json generator job trace gen null boolean prettyprint trace true logged job job being traced null map string logged task tasks in current job map string logged task attempt attempts in current job histogram successful map attempt times histogram successful reduce attempt times histogram failed map attempt times histogram failed reduce attempt times histogram successful nth mapper attempts histogram successful nth reducer attempts histogram mapper locality log log log factory get log hadoop logs analyzer attempt times percentiles json generator topology gen null hash set parsed host hosts new hash set parsed host number ticks per second boolean collecting false line number string rereadable line null string input filename boolean input is directory false path input directory path null string input directory files null input directory cursor line reader input null compression codec input codec null decompressor input decompressor null text input line text new text boolean debug false version number buckets spread min spread max boolean spreading false boolean delays false boolean runtimes false boolean collect task times false log record type canonical job log record type intern job log record type canonical map attempt log record type intern map attempt log record type canonical reduce attempt log record type intern reduce attempt log record type canonical task log record type intern task pattern streaming jobname pattern pattern compile streamjob jar hash set string host names new hash set string boolean file first line true string current file name null here cumulative statistics enum job outcome success failure overall these rectangular arrays link histogram indexed job type java streaming pig pipes outcome success failure histogram run time dists histogram delay time dists histogram map time spread dists histogram shuffle time spread dists histogram sort time spread dists histogram reduce time spread dists histogram map time dists histogram shuffle time dists histogram sort time dists histogram reduce time dists map
1664	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Histogram.java	unrelated	package org apache hadoop tools rumen link histogram represents ordered summary sequence code queried produce discrete approximation cumulative distribution function histogram implements iterable map entry long long tree map long long content new tree map long long string name total count histogram anonymous histogram string name super name name total count l dump print stream stream stream print dumping histogram name n iterator map entry long long iter iterator iter next map entry long long ent iter next stream print val count pair ent get key ent get value n stream print end n iterator map entry long long iterator return content entry set iterator get key long result content get key return result null result get total count return total count enter value long existing value content get value existing value null content put value l else content put value existing value l total count produces discrete approximation cdf the user provides points code y axis wants give corresponding points code x axis plus minimum maximum data denominator applied every element buckets for example code scale code code buckets element specify median output slot array less scale strictly greater predecessor we check requirements the first resp last element minimum resp maximum value ever code enter ed the rest elements correspond elements code buckets carry first element whose rank less code content elements scale bucket get cdf scale buckets total count return null result new buckets length fill min max result content first entry get key result buckets length content last entry get key iterator map entry long long iter content entry set iterator cumulative count bucket cursor loop invariant item buckets bucket cursor still reached iter number logged elements longer available iter cumulative count cumulative count total count therefore strictly less buckets bucket cursor scale iter next target cumulative count buckets bucket cursor total count scale map entry long long elt iter next cumulative count elt get value cumulative count target cumulative count result bucket cursor elt get key bucket cursor bucket cursor buckets length target cumulative count buckets bucket cursor total count scale else break bucket cursor buckets length break return result
1665	mapreduce\src\tools\org\apache\hadoop\tools\rumen\HistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen history event emitter log log log factory get log history event emitter list single event emitter non final se es list single event emitter se es protected history event emitter code enum post emit action none remove hee pair queue history event post emit action emitter core parsed line line string name queue history event results new linked list history event post emit action remove emitter post emit action none single event emitter see non final se es history event event see maybe emit event line name event null results add event single event emitter see se es history event event see maybe emit event line name event null results add event remove emitter post emit action remove hee break return new pair queue history event post emit action results remove emitter protected counters maybe parse counters string counters try return parse counters counters catch parse exception e log warn the counter counters badly formatted return null protected counters parse counters string counters throws parse exception counters null log warn history event emitters null counter detected return null counters counters replace counters counters replace counters counters replace counters counters replace counters counters replace org apache hadoop mapred counters dep form org apache hadoop mapred counters escaped compact string counters return new counters dep form
1666	mapreduce\src\tools\org\apache\hadoop\tools\rumen\InputDemuxer.java	unrelated	package org apache hadoop tools rumen link input demuxer dem ultiplexes input files individual input streams input demuxer extends closeable bind link input demuxer particular file the path file bind configuration returns true binding succeeds if file read wrong format returns false io exception reserved read errors bind to path path configuration conf throws io exception get next name input pair the name preserve original job history file job conf file name the input object closed calling get next the old input object would invalid calling get next pair string input stream get next throws io exception
1667	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Job20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen job line history event emitter extends history event emitter list single event emitter non finals new linked list single event emitter list single event emitter finals new linked list single event emitter long original submit time null non finals add new job submitted event emitter non finals add new job priority change event emitter non finals add new job status changed event emitter non finals add new job inited event emitter non finals add new job info change event emitter finals add new job unsuccessful completion event emitter finals add new job finished event emitter job line history event emitter super job submitted event emitter extends single event emitter history event maybe emit event parsed line line string job id name history event emitter thatg job id job id job id name job id name job id name null return null string submit time line get submit time string job conf line get jobconf string user line get user user null user nulluser string job name line get jobname string job queue name line get job queue could null submit time null job line history event emitter job line history event emitter thatg original submit time long parse long submit time map job acl access control list job ac ls new hash map job acl access control list return new job submitted event job id job name user original submit time job conf job ac ls job queue name return null job priority change event emitter extends single event emitter history event maybe emit event parsed line line string job id name history event emitter thatg job id job id job id name job id name job id name null return null string priority line get job priority priority null return new job priority change event job id job priority value of priority return null job inited event emitter extends single event emitter history event maybe emit event parsed line line string job id name history event emitter thatg job id name null return null job id job id job id name job id name string launch time line get launch time string status line get job status string total maps line get total maps string total reduces line get total reduces launch time null total maps null total reduces null return new job inited event job id long parse long launch time integer parse int total maps integer parse int total reduces status return null job status changed event emitter extends single event emitter history event maybe emit event parsed line line string job id name history event emitter thatg job id name null return null job id job id job id name job id name string status line get job status status null return new job status changed event job id status return null job info change event emitter extends single event emitter history event maybe emit event parsed line line string job id name history event emitter thatg job id name null return null job id job id job id name job id
1668	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobBuilder.java	scheduler	package org apache hadoop tools rumen link job builder builds one job it processes sequence link history event job builder bytes in meg string utils traditional binary prefix string job id boolean finalized false logged job result new logged job map string logged task map tasks new hash map string logged task map string logged task reduce tasks new hash map string logged task map string logged task tasks new hash map string logged task map string logged task attempt attempts new hash map string logged task attempt map parsed host parsed host hosts new hash map parsed host parsed host the number splits task ignore maximum preferred locations the regular expression used parse task attempt i ds job tracker logs pattern task attempt id pattern pattern compile attempt times percentiles null use search within java options get heap sizes the heap size number capturing group the heap size order magnitude suffix capturing group pattern heap pattern pattern compile xmx k km mg gt t properties job configuration parameters null job builder string job id job id job id string get job id return job id attempt times percentiles null attempt times percentiles new attempt times percentiles process one link history event the link history event processed process history event event finalized throw new illegal state exception job builder process history event event called logged job built lexicographical order name event instanceof job finished event process job finished event job finished event event else event instanceof job info change event process job info change event job info change event event else event instanceof job inited event process job inited event job inited event event else event instanceof job priority change event process job priority change event job priority change event event else event instanceof job status changed event process job status changed event job status changed event event else event instanceof job submitted event process job submitted event job submitted event event else event instanceof job unsuccessful completion event process job unsuccessful completion event job unsuccessful completion event event else event instanceof map attempt finished event process map attempt finished event map attempt finished event event else event instanceof reduce attempt finished event process reduce attempt finished event reduce attempt finished event event else event instanceof task attempt finished event process task attempt finished event task attempt finished event event else event instanceof task attempt started event process task attempt started event task attempt started event event else event instanceof task attempt unsuccessful completion event process task attempt unsuccessful completion event task attempt unsuccessful completion event event else event instanceof task failed event process task failed event task failed event event else event instanceof task finished event process task finished event task finished event event else event instanceof task started event process task started event task started event event else event instanceof task updated event process task updated event task updated event event else throw new illegal argument exception job builder process history event unknown event type string extract properties conf string names string default value string name names string result conf
1669	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobConfigurationParser.java	unrelated	package org apache hadoop tools rumen link job configuration parser parses job configuration xml file extracts configuration properties it parses file using stream parser thus memory efficient this optimization may postponed future release job configuration parser parse job configuration file input stream return link properties collection the input stream closed return call the input data configuration xml properties parse input stream input throws io exception properties result new properties try document builder factory dbf document builder factory new instance document builder db dbf new document builder document doc db parse input element root doc get document element configuration equals root get tag name system print root configuration node return null node list props root get child nodes props get length node prop node props item prop node instanceof element continue element prop element prop node property equals prop get tag name system print bad conf file element property node list fields prop get child nodes string attr null string value null boolean parameter false j j fields get length j node field node fields item j field node instanceof element continue element field element field node name equals field get tag name field child nodes attr text field get first child get data trim value equals field get tag name field child nodes value text field get first child get data equals field get tag name field child nodes parameter true equals text field get first child get data attr null value null result put attr value catch parser configuration exception e return null catch sax exception e return null return result
1670	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobConfPropertyNames.java	unrelated	package org apache hadoop tools rumen enum job conf property names queue names mapred job queue name mr job config queue name job names mapred job name mr job config job name task java opts s mapred child java opts map java opts s mapred child java opts mr job config map java opts reduce java opts s mapred child java opts mr job config reduce java opts string candidates job conf property names string candidates candidates candidates string get candidates return candidates
1671	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobHistoryParser.java	unrelated	package org apache hadoop tools rumen link job history parser defines job history file parser job history parser extends closeable get next link history event history event next event throws io exception
1672	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobHistoryParserFactory.java	unrelated	package org apache hadoop tools rumen link job history parser factory singleton attempts determine version job history return proper parser job history parser factory job history parser get parser rewindable input stream ris throws io exception version detector vd version detector values boolean parse vd parse ris ris rewind parse return vd new instance ris throw new io exception no suitable parser enum version detector hadoop boolean parse input stream input throws io exception return hadoop jh parser parse input job history parser new instance input stream input throws io exception return new hadoop jh parser input current boolean parse input stream input throws io exception return current jh parser parse input job history parser new instance input stream input throws io exception return new current jh parser input job history parser new instance input stream input throws io exception boolean parse input stream input throws io exception
1673	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobStory.java	unrelated	package org apache hadoop tools rumen link job story represents runtime information available completed map reduce job job story get link job conf job job conf get job conf get job name string get name get job id job id get job id get user ran job string get user get job submission time get submission time get number maps link job story get number maps get number reduce link job story get number reduces get input splits job input split get input splits get link task info given task task info get task info task type task type task number get link task attempt info given task attempt without regard impact locality e g needed make scheduling decisions task attempt info get task attempt info task type task type task number task attempt number get link task attempt info given task attempt considering impact locality task attempt info get map task attempt info adjusted task number task attempt number locality get outcome job execution values get outcome get queue job submitted string get queue name
1674	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobStoryProducer.java	unrelated	package org apache hadoop tools rumen link job story producer produces sequence link job story job story producer extends closeable get next job job story get next job throws io exception
1675	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JobTraceReader.java	unrelated	package org apache hadoop tools rumen reading json encoded job traces produce link logged job instances job trace reader extends json object mapper parser logged job constructor path json trace file possibly compressed job trace reader path path configuration conf throws io exception super path logged job conf constructor the input stream json trace job trace reader input stream input throws io exception super input logged job
1676	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JsonObjectMapperParser.java	unrelated	package org apache hadoop tools rumen a simple wrapper parsing json encoded data using object mapper the base type object parsed parser json object mapper parser t implements closeable object mapper mapper class extends t clazz json parser json parser constructor path json data file possibly compressed json object mapper parser path path class extends t clazz configuration conf throws io exception mapper new object mapper mapper configure deserialization config feature can override access modifiers true clazz clazz input stream input new possibly decompressed input stream path conf json parser mapper get json factory create json parser input constructor the input stream json data json object mapper parser input stream input class extends t clazz throws io exception mapper new object mapper mapper configure deserialization config feature can override access modifiers true clazz clazz json parser mapper get json factory create json parser input get next object trace stream t get next throws io exception try return mapper read value json parser clazz catch eof exception e return null close throws io exception json parser close
1677	mapreduce\src\tools\org\apache\hadoop\tools\rumen\JsonObjectMapperWriter.java	unrelated	package org apache hadoop tools rumen simple wrapper around link json generator write objects json format json object mapper writer t implements closeable json generator writer json object mapper writer output stream output boolean pretty print throws io exception object mapper mapper new object mapper mapper configure serialization config feature can override access modifiers true mapper get json factory writer mapper get json factory create json generator output json encoding utf pretty print writer use default pretty printer write t object throws io exception writer write object object close throws io exception writer close
1678	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedDiscreteCDF.java	unrelated	package org apache hadoop tools rumen a link logged discrete cdf discrete approximation cumulative distribution function set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged discrete cdf implements deep compare the number values cdf built number values l the least code x value minimum long min value the coordinates bulk cdf list logged single relative ranking rankings new array list logged single relative ranking the greatest code x value maximum long max value set cdf histogram data steps modulus number values data get total count cdf data get cdf modulus steps cdf null minimum cdf maximum cdf cdf length rankings new array list logged single relative ranking cdf length logged single relative ranking srr new logged single relative ranking srr set relative ranking steps modulus srr set datum cdf rankings add srr get minimum return minimum set minimum minimum minimum minimum list logged single relative ranking get rankings return rankings set rankings list logged single relative ranking rankings rankings rankings get maximum return maximum set maximum maximum maximum maximum get number values return number values set number values number values number values number values compare c c tree path loc string eltname throws deep inequality exception c c throw new deep inequality exception eltname miscompared new tree path loc eltname compare list logged single relative ranking c list logged single relative ranking c tree path loc string eltname throws deep inequality exception c null c null return c null c null c size c size throw new deep inequality exception eltname miscompared new tree path loc eltname c size c get deep compare c get new tree path loc eltname deep compare deep compare comparand tree path loc throws deep inequality exception comparand instanceof logged discrete cdf throw new deep inequality exception comparand wrong type loc logged discrete cdf logged discrete cdf comparand compare number values number values loc number values compare minimum minimum loc minimum compare maximum maximum loc maximum compare rankings rankings loc rankings
1679	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedJob.java	unrelated	package org apache hadoop tools rumen a link logged discrete cdf representation hadoop job details set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged job implements deep compare enum job type java pig streaming pipes overall enum job priority very low low normal high very high set string already seen any setter attributes new tree set string string job id string user computons per map input byte l computons per map output byte l computons per reduce input byte l computons per reduce output byte l submit time l launch time l finish time l heap megabytes total maps total reduces pre job history constants values outcome null job type jobtype job type java job priority priority job priority normal list string direct dependant jobs new array list string list logged task map tasks new array list logged task list logged task reduce tasks new array list logged task list logged task tasks new array list logged task there cd fs level locality local first array list logged discrete cdf successful map attempt cd fs there cd fs level locality local first array list logged discrete cdf failed map attempt cd fs logged discrete cdf successful reduce attempt cdf logged discrete cdf failed reduce attempt cdf string queue null string job name null cluster map mb cluster reduce mb job map mb job reduce mb relative time mapper tries to succeed failed mapper fraction properties job properties new properties logged job logged job string job id super set job id job id set configuration properties job set job properties properties conf job properties conf get configuration properties job properties get job properties return job properties adjust times adjustment submit time adjustment launch time adjustment finish time adjustment logged task task map tasks task adjust times adjustment logged task task reduce tasks task adjust times adjustment logged task task tasks task adjust times adjustment input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name string get user return user set user string user user user string get job id return job id set job id string job id job id job id job priority get priority return priority set priority job priority priority priority priority get computons per map input byte return computons per map input byte set computons per map input byte computons per map input byte computons per map input byte computons per map input byte get computons per map output byte return computons per map output byte set computons per map output byte computons per map output byte computons per map output byte computons per map output byte get computons per reduce input byte return computons per reduce input byte set computons per reduce input byte computons per reduce input byte computons per reduce input byte computons per reduce input byte get computons per reduce output byte return computons per reduce output byte
1680	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedLocation.java	unrelated	package org apache hadoop tools rumen a link logged location representation point hierarchical network represented series membership names broadest first for example network hosts grouped racks onecluster might node code node rack code rack this would represented array list two layers two link string code rack code node the details set meet requirements jackson json parser generator all methods simply accessors instance variables want write json files logged location implements deep compare map list string list string layers cache new hash map list string list string the full path root network host note assumes network topology tree list string layers collections empty list set string already seen any setter attributes new tree set string list string get layers return layers set layers list string layers layers null layers empty layers collections empty list else synchronized layers cache list string found layers cache get layers found null make copy interned list string clone new array list string layers size string layers clone add intern making read sharing list string readonly layers collections unmodifiable list clone layers cache put readonly layers readonly layers layers readonly layers else layers found input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name i treat atomic object type compare strings list string c list string c tree path loc string eltname throws deep inequality exception c null c null return tree path recurse path new tree path loc eltname c null c null c equals c throw new deep inequality exception eltname miscompared recurse path deep compare deep compare comparand tree path loc throws deep inequality exception comparand instanceof logged location throw new deep inequality exception comparand wrong type loc logged location logged location comparand compare strings layers layers loc layers
1681	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedNetworkTopology.java	unrelated	package org apache hadoop tools rumen a link logged network topology represents tree turn represents hierarchy hosts the current version requires tree leaves level all methods simply accessors instance variables want write json files logged network topology implements deep compare string name list logged network topology children new array list logged network topology set string already seen any setter attributes new tree set string logged network topology super input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name we need sort code children field that field set valued sort fields ensure comparisons bogusly fail hash table happened enumerate different order topo sort implements comparator logged network topology compare logged network topology logged network topology return name compare to name hash set link parsed host name level host recursive descent level number logged network topology set parsed host hosts string name level name name children null level parsed host number of distances hash map string hash set parsed host topologies new hash map string hash set parsed host iterator parsed host iter hosts iterator iter next parsed host host iter next string component host name component level hash set parsed host set topologies get component set null set new hash set parsed host topologies put component set set add host children new array list logged network topology map entry string hash set parsed host ent topologies entry set children add new logged network topology ent get value ent get key level else nothing logged network topology set parsed host hosts hosts root string get name return name set name string name name name list logged network topology get children return children set children list logged network topology children children children compare list logged network topology c list logged network topology c tree path loc string eltname throws deep inequality exception c null c null return c null c null c size c size throw new deep inequality exception eltname miscompared new tree path loc eltname collections sort c new topo sort collections sort c new topo sort c size c get deep compare c get new tree path loc eltname deep compare deep compare comparand tree path loc throws deep inequality exception comparand instanceof logged network topology throw new deep inequality exception comparand wrong type loc logged network topology logged network topology comparand compare children children loc children
1682	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedSingleRelativeRanking.java	unrelated	package org apache hadoop tools rumen a link logged single relative ranking represents x y coordinate single point discrete cdf all methods simply accessors instance variables want write json files logged single relative ranking implements deep compare the y coordinate fraction code d d the default value mark unfilled value relative ranking d the x coordinate datum l set string already seen any setter attributes new tree set string input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name get relative ranking return relative ranking set relative ranking relative ranking relative ranking relative ranking get datum return datum set datum datum datum datum compare c c tree path loc string eltname throws deep inequality exception c c throw new deep inequality exception eltname miscompared new tree path loc eltname compare c c tree path loc string eltname throws deep inequality exception c c throw new deep inequality exception eltname miscompared new tree path loc eltname deep compare deep compare comparand tree path loc throws deep inequality exception comparand instanceof logged single relative ranking throw new deep inequality exception comparand wrong type loc logged single relative ranking logged single relative ranking comparand compare relative ranking relative ranking loc relative ranking compare datum datum loc datum
1683	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedTask.java	unrelated	package org apache hadoop tools rumen a link logged task represents hadoop task part hadoop job it knows pssibly empty sequence attempts i o footprint runtime all methods simply accessors instance variables want write json files logged task implements deep compare input bytes l input records l output bytes l output records l string task id start time l finish time l pre job history constants values task type pre job history constants values task status list logged task attempt attempts new array list logged task attempt list logged location preferred locations collections empty list set string already seen any setter attributes new tree set string input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name logged task super adjust times adjustment start time adjustment finish time adjustment logged task attempt attempt attempts attempt adjust times adjustment get input bytes return input bytes set input bytes input bytes input bytes input bytes get input records return input records set input records input records input records input records get output bytes return output bytes set output bytes output bytes output bytes output bytes get output records return output records set output records output records output records output records string get task id return task id set task id string task id task id task id get start time return start time set start time start time start time start time get finish time return finish time set finish time finish time finish time finish time list logged task attempt get attempts return attempts set attempts list logged task attempt attempts attempts null attempts new array list logged task attempt else attempts attempts list logged location get preferred locations return preferred locations set preferred locations list logged location preferred locations preferred locations null preferred locations empty preferred locations collections empty list else preferred locations preferred locations pre job history constants values get task status return task status set task status pre job history constants values task status task status task status pre job history constants values get task type return task type set task type pre job history constants values task type task type task type incorporate map counters jh counters counters incorporate counter new set field set val task input bytes val counters hdfs bytes read incorporate counter new set field set val task output bytes val counters file bytes written incorporate counter new set field set val task input records val counters map input records incorporate counter new set field set val task output records val counters map output records incorporate reduce counters jh counters counters incorporate counter new set field set val task input bytes val counters reduce shuffle bytes incorporate counter new set field set val task output bytes val counters hdfs bytes written incorporate counter new set field set val task input records val counters reduce input records incorporate counter new set field set val task output records val counters reduce
1684	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LoggedTaskAttempt.java	unrelated	package org apache hadoop tools rumen hack alert this two subclasses might called logged map task attempt logged reduce task attempt jackson implementation json handle superclass valued field a link logged task attempt represents attempt run hadoop task hadoop job note task several attempts all methods simply accessors instance variables want write json files logged task attempt implements deep compare string attempt id pre job history constants values result start time l finish time l string host name hdfs bytes read l hdfs bytes written l file bytes read l file bytes written l map input records l map input bytes l map output bytes l map output records l combine input records l reduce input groups l reduce input records l reduce shuffle bytes l reduce output records l spilled records l shuffle finished l sort finished l logged location location initialize default object backward compatibility resource usage metrics metrics new resource usage metrics logged task attempt super set string already seen any setter attributes new tree set string input parameter ignored set unknown attribute string attribute name object ignored already seen any setter attributes contains attribute name already seen any setter attributes add attribute name system err println in logged job saw unknown attribute attribute name adjust times adjustment start time adjustment finish time adjustment get shuffle finished return shuffle finished set shuffle finished shuffle finished shuffle finished shuffle finished get sort finished return sort finished set sort finished sort finished sort finished sort finished string get attempt id return attempt id set attempt id string attempt id attempt id attempt id pre job history constants values get result return result set result pre job history constants values result result result get start time return start time set start time start time start time start time get finish time return finish time set finish time finish time finish time finish time string get host name return host name set host name string host name host name host name null null host name intern get hdfs bytes read return hdfs bytes read set hdfs bytes read hdfs bytes read hdfs bytes read hdfs bytes read get hdfs bytes written return hdfs bytes written set hdfs bytes written hdfs bytes written hdfs bytes written hdfs bytes written get file bytes read return file bytes read set file bytes read file bytes read file bytes read file bytes read get file bytes written return file bytes written set file bytes written file bytes written file bytes written file bytes written get map input records return map input records set map input records map input records map input records map input records get map output bytes return map output bytes set map output bytes map output bytes map output bytes map output bytes get map output records return map output records set map output records map output records map output records map output records get combine input records return combine input records set combine input records combine input records combine input records combine input records get reduce input groups return reduce input groups set
1685	mapreduce\src\tools\org\apache\hadoop\tools\rumen\LogRecordType.java	unrelated	package org apache hadoop tools rumen log record type map string log record type internees new hash map string log record type string name index log record type string name super name name index internees size log record type intern string type name log record type result internees get type name result null result new log record type type name internees put type name result return result log record type intern soft string type name return internees get type name string string return name string line types iterator map entry string log record type iter internees entry set iterator string result new string internees size internees size result iter next get key return result
1686	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MachineNode.java	unrelated	package org apache hadoop tools rumen link machine node represents configuration cluster node link machine node constructed link machine node builder machine node extends node memory kb map slots reduce slots memory per map slot kb memory per reduce slot kb num cores machine node string name level super name level boolean equals object obj name level sufficient return super equals obj hash code match equals return super hash code get available physical ram node get memory return memory get number map slots node get map slots return map slots get number reduce slots node get reduce slots return reduce slots get amount ram reserved map slot get memory per map slot return memory per map slot get amount ram reserved reduce slot get memory per reduce slot return memory per reduce slot get number cores node get num cores return num cores get rack node machine belongs machine belong rack rack node get rack node return rack node get parent synchronized boolean add child node child throw new illegal state exception cannot add child machine node builder node info object builder machine node node start building new node info object unique name node typically fully qualified domain name builder string name level node new machine node name level set physical memory node builder set memory memory node memory memory return set number map slot node builder set map slots map slots node map slots map slots return set number reduce slot node builder set reduce slots reduce slots node reduce slots reduce slots return set amount ram reserved map slot builder set memory per map slot memory per map slot node memory per map slot memory per map slot return set amount ram reserved reduce slot builder set memory per reduce slot memory per reduce slot node memory per reduce slot memory per reduce slot return set number cores node builder set num cores num cores node num cores num cores return clone settings reference link machine node object builder clone from machine node ref node memory ref memory node map slots ref map slots node reduce slots ref reduce slots node memory per map slot ref memory per map slot node memory per reduce slot ref memory per reduce slot node num cores ref num cores return build link machine node object machine node build machine node ret val node node null return ret val
1687	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MapAttempt20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen map attempt line history event emitter extends task attempt line event emitter list single event emitter non finals new linked list single event emitter list single event emitter finals new linked list single event emitter non finals add all task event non final se es finals add new map attempt finished event emitter protected map attempt line history event emitter super map attempt finished event emitter extends single event emitter history event maybe emit event parsed line line string task attempt id name history event emitter thatg task attempt id name null return null task attempt id task attempt id task attempt id name task attempt id name string finish time line get finish time string status line get task status finish time null status null status equals ignore case success string host name line get hostname string counters line get counters string state line get state string map attempt line history event emitter map attempt line history event emitter thatg finish time null success equals ignore case status return new map attempt finished event task attempt id original task type status long parse long finish time long parse long finish time host name state maybe parse counters counters return null list single event emitter se es return finals list single event emitter non final se es return non finals
1688	mapreduce\src\tools\org\apache\hadoop\tools\rumen\MapTaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link map task attempt info represents information regard map task attempt map task attempt info extends task attempt info runtime map task attempt info state state task info task info runtime super state task info runtime runtime get runtime return get map runtime get runtime b map b phase map task attempt get map runtime return runtime
1689	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Node.java	unrelated	package org apache hadoop tools rumen link node represents node cluster topology a node link machine node link rack node etc node implements comparable node sorted set node empty set collections unmodifiable sorted set new tree set node node parent string name level sorted set node children a unique name identify node cluster the level node cluster node string name level name null throw new illegal argument exception node name cannot null level throw new illegal argument exception level cannot negative name name level level get name node string get name return name get level node get level return level check children children null children new tree set node add child node node synchronized boolean add child node child child parent null throw new illegal argument exception the child already another node child parent check children boolean retval children add child retval child parent return retval does node children synchronized boolean children return children null children empty get children node returned the returned set read synchronized set node get children return children null empty set collections unmodifiable sorted set children get parent node node get parent return parent hash code return name hash code boolean equals object obj obj return true obj null return false obj get class get class return false node node obj return name equals name string string return name level compare to node return name compare to name
1690	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Outputter.java	unrelated	package org apache hadoop tools rumen interface output sequence objects type t outputter t extends closeable initialize link outputter specific path init path path configuration conf throws io exception output object output t object throws io exception
1691	mapreduce\src\tools\org\apache\hadoop\tools\rumen\package-info.java	unrelated	package org apache hadoop tools rumen
1692	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Pair.java	unrelated	package org apache hadoop tools rumen pair car type cdr type car type car cdr type cdr pair car type car cdr type cdr super car car cdr cdr car type first return car cdr type second return cdr
1693	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedConfigFile.java	unrelated	package org apache hadoop tools rumen parsed config file pattern job id pattern pattern compile job pattern heap pattern pattern compile xmx mg g heap megabytes string queue string job name cluster map mb cluster reduce mb job map mb job reduce mb string job id boolean valid properties properties new properties maybe get int value string prop name string attr string value old value prop name equals attr value null try return integer parse int value catch number format exception e return old value return old value parsed config file string filename line string xml string super heap megabytes string queue null string job name null cluster map mb cluster reduce mb job map mb job reduce mb string job id null boolean valid true matcher job id matcher job id pattern matcher filename line job id matcher find job id job id matcher group try input stream new byte array input stream xml string get bytes document builder factory dbf document builder factory new instance document builder db dbf new document builder document doc db parse element root doc get document element configuration equals root get tag name system print root configuration node valid false node list props root get child nodes props get length node prop node props item prop node instanceof element continue element prop element prop node property equals prop get tag name system print bad conf file element property node list fields prop get child nodes string attr null string value null boolean parameter false j j fields get length j node field node fields item j field node instanceof element continue element field element field node name equals field get tag name field child nodes attr text field get first child get data trim value equals field get tag name field child nodes value text field get first child get data equals field get tag name field child nodes parameter true equals text field get first child get data properties set property attr value mapred child java opts equals attr value null matcher matcher heap pattern matcher value matcher find string heap size matcher group heap megabytes integer parse int heap size matcher group equals ignore case g heap megabytes mr job config queue name equals attr value null queue value mr job config job name equals attr value null job name value cluster map mb maybe get int value mr config mapmemory mb attr value cluster map mb cluster reduce mb maybe get int value mr config reducememory mb attr value cluster reduce mb job map mb maybe get int value mr job config map memory mb attr value job map mb job reduce mb maybe get int value mr job config reduce memory mb attr value job reduce mb valid true catch parser configuration exception e valid false catch sax exception e valid false catch io exception e valid false heap megabytes heap megabytes queue queue job name job name cluster map mb cluster map mb cluster reduce mb cluster reduce mb job map mb job map mb job reduce mb job reduce
1694	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedHost.java	unrelated	package org apache hadoop tools rumen parsed host string rack name string node name todo following works rack host format change support arbitrary level network names pattern split pattern pattern compile todo handle arbitrary level network names number of distances return string name component throws illegal argument exception switch case return rack name case return node name default throw new illegal argument exception host location component index range hash code return rack name hash code node name hash code parsed host parse string name separate node name matcher matcher split pattern matcher name matcher matches return null return new parsed host matcher group matcher group parsed host logged location loc list string coordinates loc get layers rack name coordinates get node name coordinates get logged location make logged location logged location result new logged location list string coordinates new array list string coordinates add rack name coordinates add node name result set layers coordinates return result string get node name return node name string get rack name return rack name expects broadest name first parsed host string rack name string node name rack name rack name node name node name boolean equals object instanceof parsed host return false parsed host host parsed host return node name equals host node name rack name equals host rack name distance parsed host node name equals node name return rack name equals rack name return return
1695	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ParsedLine.java	unrelated	package org apache hadoop tools rumen parsed line properties content log record type type string key w the value enclosed quotation marks occurrences escaped so escaped value essentially escaped sequence followed character character the straightforward regex capture unfortunately java regex implementation broken perform nfa dfa conversion expressions would lead backtracking stack overflow matching strings the following manual unfolding regex get rid backtracking string value regex match key value pairs input line capture group matches key capture group matches value without quotation marks pattern key val pair pattern compile key value parsed line string full line version super content new properties first space full line index of first space first space full line length first space return this junk line sort type log record type intern full line substring first space string prop val pairs full line substring first space matcher matcher key val pair matcher prop val pairs matcher find string key matcher group string value matcher group content set property key value protected log record type get type return type protected string get string key return content get property key protected get long string key string val get key return long parse long val
1696	mapreduce\src\tools\org\apache\hadoop\tools\rumen\PossiblyDecompressedInputStream.java	pooling	package org apache hadoop tools rumen possibly decompressed input stream extends input stream decompressor decompressor input stream core input stream possibly decompressed input stream path input path configuration conf throws io exception compression codec factory codecs new compression codec factory conf compression codec input codec codecs get codec input path file system ifs input path get file system conf fs data input stream file in ifs open input path input codec null decompressor null core input stream file in else decompressor codec pool get decompressor input codec core input stream input codec create input stream file in decompressor read throws io exception return core input stream read read byte buffer offset length throws io exception return core input stream read buffer offset length close throws io exception core input stream close called returning decompressor pool core input stream close could though currently access decompressor core input stream close decompressor null codec pool return decompressor decompressor
1697	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Pre21JobHistoryConstants.java	scheduler	package org apache hadoop tools rumen pre job history constants job history files contain key value pairs keys belong enum it acts global namespace keys enum keys jobtrackerid start time finish time jobid jobname user jobconf submit time launch time total maps total reduces failed maps failed reduces finished maps finished reduces job status taskid hostname task type error task attempt id task status copy phase sort phase reduce phase shuffle finished sort finished map finished counters splits job priority http port tracker name state string version this enum contains values commonly used history log events since values history strings values name used places history file enum values success failed killed map reduce cleanup running prep setup pre regex jobhistory filename e jt identifier job id user name job name pattern jobhistory filename regex pattern compile job id jobid regex pre regex jobhistory conf filename e jt identifier job id conf xml pattern conf filename regex pattern compile job id jobid regex conf xml z a z
1698	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RackNode.java	unrelated	package org apache hadoop tools rumen link rack node represents rack node cluster topology rack node extends node rack node string name level hack ensuring rack name starts super name starts with name name level synchronized boolean add child node child child instanceof machine node throw new illegal argument exception only machine node added rack node return super add child child get machine nodes belong rack set machine node get machines in rack return set machine node set get children
1699	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RandomSeedGenerator.java	unrelated	package org apache hadoop tools rumen the purpose generate new random seeds master seed this needed make random next calls rumen mumak deterministic mumak simulations become deterministically replayable in tools need many independent streams random numbers created dynamically we seed streams sub seeds returned random seed generator for slightly complicated approach generating multiple streams random numbers better theoretical guarantees see p l ecuyer r simard e j chen w d kelton an objected oriented random number package many long streams substreams operations research http www iro umontreal ca lecuyer papers html http www iro umontreal ca lecuyer myftp streams random seed generator log log log factory get log random seed generator md algorithm instance one thread thread local message digest md holder new thread local message digest message digest md null try md message digest get instance md catch no such algorithm exception nsae throw new runtime exception can create md digests nsae return md generates new random seed vastly different random seeds get seed string stream id master seed message digest md md holder get md reset make sure get str we could fed bytes master seed one one md update instead string str stream id master seed byte digest md digest str get bytes create first bytes digest this fine md avalanche property paranoids could xor folded bytes seed seed seed digest return seed
1700	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ReduceAttempt20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen reduce attempt line history event emitter extends task attempt line event emitter list single event emitter non finals new linked list single event emitter list single event emitter finals new linked list single event emitter non finals add all task event non final se es finals add new reduce attempt finished event emitter reduce attempt line history event emitter super reduce attempt finished event emitter extends single event emitter history event maybe emit event parsed line line string task attempt id name history event emitter thatg task attempt id name null return null task attempt id task attempt id task attempt id name task attempt id name string finish time line get finish time string status line get task status finish time null status null status equals ignore case success string host name line get hostname string counters line get counters string state line get state string string shuffle finish line get shuffle finished string sort finish line get sort finished finish time null shuffle finish null sort finish null success equals ignore case status reduce attempt line history event emitter reduce attempt line history event emitter thatg return new reduce attempt finished event task attempt id original task type status long parse long shuffle finish long parse long sort finish long parse long finish time host name state maybe parse counters counters return null list single event emitter se es return finals list single event emitter non final se es return non finals
1701	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ReduceTaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link reduce task attempt info represents information regard reduce task attempt reduce task attempt info extends task attempt info shuffle time merge time reduce time reduce task attempt info state state task info task info shuffle time merge time reduce time super state task info shuffle time shuffle time merge time merge time reduce time reduce time get runtime b reduce b phase reduce task attempt get reduce runtime return reduce time get runtime b shuffle b phase reduce task attempt get shuffle runtime return shuffle time get runtime b merge b phase reduce task attempt get merge runtime return merge time get runtime return get shuffle runtime get merge runtime get reduce runtime
1702	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ResourceUsageMetrics.java	unrelated	package org apache hadoop tools rumen captures resource usage metrics resource usage metrics implements writable deep compare cumulative cpu usage virtual memory usage physical memory usage heap usage resource usage metrics get cumulative cpu usage get cumulative cpu usage return cumulative cpu usage set cumulative cpu usage set cumulative cpu usage usage cumulative cpu usage usage get virtual memory usage get virtual memory usage return virtual memory usage set virtual memory usage set virtual memory usage usage virtual memory usage usage get physical memory usage get physical memory usage return physical memory usage set physical memory usage set physical memory usage usage physical memory usage usage get total heap usage get heap usage return heap usage set total heap usage set heap usage usage heap usage usage returns size serialized data size size size writable utils get v int size cumulative cpu usage size writable utils get v int size virtual memory usage size writable utils get v int size physical memory usage size writable utils get v int size heap usage return size read fields data input throws io exception cumulative cpu usage writable utils read v long virtual memory usage writable utils read v long physical memory usage writable utils read v long heap usage writable utils read v long write data output throws io exception todo write resources version writable utils write v long cumulative cpu usage writable utils write v long virtual memory usage writable utils write v long physical memory usage writable utils write v long heap usage compare metric tree path loc throws deep inequality exception throw new deep inequality exception value miscompared loc string loc compare size resource usage metrics resource usage metrics tree path loc throws deep inequality exception size size throw new deep inequality exception size miscompared loc string loc deep compare deep compare tree path loc throws deep inequality exception instanceof resource usage metrics throw new deep inequality exception comparand wrong type loc resource usage metrics metrics resource usage metrics compare metric get cumulative cpu usage metrics get cumulative cpu usage new tree path loc cumulative cpu compare metric get virtual memory usage metrics get virtual memory usage new tree path loc virtual memory compare metric get physical memory usage metrics get physical memory usage new tree path loc physical memory compare metric get heap usage metrics get heap usage new tree path loc heap usage compare size metrics new tree path loc size
1703	mapreduce\src\tools\org\apache\hadoop\tools\rumen\RewindableInputStream.java	unrelated	package org apache hadoop tools rumen a simple wrapper make input stream rewindable it could made memory efficient grow internal buffer adaptively rewindable input stream extends input stream input stream input constructor rewindable input stream input stream input input constructor input stream maximum number bytes need remember beginning stream if link rewind called many bytes read stream link rewind would fail rewindable input stream input stream input max bytes to remember input new buffered input stream input max bytes to remember input mark max bytes to remember read throws io exception return input read read byte buffer offset length throws io exception return input read buffer offset length close throws io exception input close input stream rewind throws io exception try input reset return catch io exception e throw new io exception unable rewind stream e
1704	mapreduce\src\tools\org\apache\hadoop\tools\rumen\SingleEventEmitter.java	unrelated	package org apache hadoop tools rumen single event emitter history event maybe emit event parsed line line string name history event emitter
1705	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Task20LineHistoryEventEmitter.java	unrelated	package org apache hadoop tools rumen task line history event emitter extends history event emitter list single event emitter non finals new linked list single event emitter list single event emitter finals new linked list single event emitter long original start time null task type original task type null non finals add new task started event emitter non finals add new task updated event emitter finals add new task finished event emitter finals add new task failed event emitter protected task line history event emitter super task started event emitter extends single event emitter history event maybe emit event parsed line line string task id name history event emitter thatg task id name null return null task id task id task id name task id name string task type line get task type string start time line get start time string splits line get splits start time null task type null task line history event emitter task line history event emitter thatg original start time long parse long start time original task type version log interface utils get task type task type return new task started event task id original start time original task type splits return null task updated event emitter extends single event emitter history event maybe emit event parsed line line string task id name history event emitter thatg task id name null return null task id task id task id name task id name string finish time line get finish time finish time null return new task updated event task id long parse long finish time return null task finished event emitter extends single event emitter history event maybe emit event parsed line line string task id name history event emitter thatg task id name null return null task id task id task id name task id name string status line get task status string finish time line get finish time string error line get error string counters line get counters finish time null error null status null status equals ignore case success counters event counters maybe parse counters counters task line history event emitter task line history event emitter thatg original task type null return null return new task finished event task id long parse long finish time original task type status event counters return null task failed event emitter extends single event emitter history event maybe emit event parsed line line string task id name history event emitter thatg task id name null return null task id task id task id name task id name string status line get task status string finish time line get finish time string task type line get task type string error line get error finish time null error null status null status equals ignore case success task line history event emitter task line history event emitter thatg task type original task type original task type null version log interface utils get task type task type original task type return new task failed event task id long parse long finish time original task type error status null return null list single
1706	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskAttempt20LineEventEmitter.java	unrelated	package org apache hadoop tools rumen task attempt line event emitter extends history event emitter list single event emitter task event non final se es new linked list single event emitter list single event emitter task event final se es new linked list single event emitter default http port long original start time null org apache hadoop mapreduce task type original task type null task event non final se es add new task attempt started event emitter task event non final se es add new task attempt finished event emitter task event non final se es add new task attempt unsuccessful completion event emitter protected task attempt line event emitter super task attempt started event emitter extends single event emitter history event maybe emit event parsed line line string task attempt id name history event emitter thatg task attempt id name null return null task attempt id task attempt id task attempt id name task attempt id name string start time line get start time string task type line get task type string tracker name line get tracker name string http port line get http port start time null task type null task attempt line event emitter task attempt line event emitter thatg original start time long parse long start time original task type version log interface utils get task type task type port http port equals default http port integer parse int http port return new task attempt started event task attempt id original task type original start time tracker name port return null task attempt finished event emitter extends single event emitter history event maybe emit event parsed line line string task attempt id name history event emitter thatg task attempt id name null return null task attempt id task attempt id task attempt id name task attempt id name string finish time line get finish time string status line get task status finish time null status null status equals ignore case success string host name line get hostname string counters line get counters string state line get state string task attempt line event emitter task attempt line event emitter thatg return new task attempt finished event task attempt id original task type status long parse long finish time host name state maybe parse counters counters return null task attempt unsuccessful completion event emitter extends single event emitter history event maybe emit event parsed line line string task attempt id name history event emitter thatg task attempt id name null return null task attempt id task attempt id task attempt id name task attempt id name string finish time line get finish time string status line get task status finish time null status null status equals ignore case success string host name line get hostname string error line get error task attempt line event emitter task attempt line event emitter thatg return new task attempt unsuccessful completion event task attempt id original task type status long parse long finish time host name error return null
1707	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskAttemptInfo.java	unrelated	package org apache hadoop tools rumen link task attempt info collection statistics particular task attempt gleaned job history job task attempt info protected state state protected task info task info protected task attempt info state state task info task info state state succeeded state state failed state state else throw new illegal argument exception status cannot state task info task info get link state task attempt state get run state return state get total runtime task attempt get runtime get link task info given task attempt task info get task info return task info
1708	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TaskInfo.java	unrelated	package org apache hadoop tools rumen task info bytes in recs in bytes out recs out max memory resource usage metrics metrics task info bytes in recs in bytes out recs out max memory bytes in recs in bytes out recs out max memory new resource usage metrics task info bytes in recs in bytes out recs out max memory resource usage metrics metrics bytes in bytes in recs in recs in bytes out bytes out recs out recs out max memory max memory metrics metrics may always match input bytes task get input bytes return bytes in get input records return recs in match output bytes get output bytes return bytes out get output records return recs out get task memory return max memory resource usage metrics get resource usage metrics return metrics
1709	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TopologyBuilder.java	unrelated	package org apache hadoop tools rumen building cluster topology topology builder set parsed host hosts new hash set parsed host process one link history event the link history event processed process history event event event instanceof task attempt finished event process task attempt finished event task attempt finished event event else event instanceof task attempt unsuccessful completion event process task attempt unsuccessful completion event task attempt unsuccessful completion event event else event instanceof task started event process task started event task started event event i not expect statements exhaustive process collection job conf link properties we restrict called the job conf properties added process properties conf code request builder build object once called link topology builder would accept events job conf properties logged network topology build return new logged network topology hosts process task started event task started event event preferred location for splits event get split locations process task attempt unsuccessful completion event task attempt unsuccessful completion event event record parsed host event get hostname process task attempt finished event task attempt finished event event record parsed host event get hostname record parsed host string host name parsed host result parsed host parse host name result null hosts contains result hosts add result preferred location for splits string splits splits null string tokenizer tok new string tokenizer splits false tok more tokens string next split tok next token record parsed host next split
1710	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TraceBuilder.java	unrelated	package org apache hadoop tools rumen the main driver rumen parser trace builder extends configured implements tool log log log factory get log trace builder run method failed exit code topology builder topology builder new topology builder outputter logged job trace writer outputter logged network topology topology writer my options class extends input demuxer input demuxer class default input demuxer class extends outputter clazz trace outputter default outputter path trace output path topology output list path inputs new linked list path my options string args configuration conf throws file not found exception io exception class not found exception switch top determine input paths recursively scanned boolean recursive traversal false args switch top starts with args switch top equals ignore case demuxer input demuxer class class name args switch top subclass input demuxer else args switch top equals ignore case recursive recursive traversal true switch top trace output new path args switch top topology output new path args switch top switch top args length inputs add all process input argument args conf recursive traversal compare history file names full paths job history file name format lexicographic sort history file names result order jobs submission times history logs comparator implements comparator file status compare file status file file status file return file get path get name compare to file get path get name processes input file folder argument if input file directly considered processing trace builder if input folder history logs input folder considered processing if recursive true input path recursively scanned job history logs processing trace builder note if input represents globbed path first flattened individual paths represented globbed input path considered processing find history logs list path process input argument string input configuration conf boolean recursive throws file not found exception io exception path path new path input file system fs path get file system conf file status statuses fs glob status path list path input paths new linked list path statuses null statuses length return input paths file status status statuses path path status get path status directory find list files path recursively recursive option specified list file status history logs new array list file status remote iterator located file status iter fs list files path recursive iter next located file status child iter next string file name child get path get name file name ends with crc file name starts with history logs add child history logs size add sorted history log file names path input paths list file status sortable names history logs array new file status history logs size arrays sort sortable names new history logs comparator file status history log sortable names input paths add history log get path else input paths add path return input paths main string args trace builder builder new trace builder result run method failed exit code try result tool runner run builder args catch throwable print stack trace system err finally try builder finish finally result return system exit result string apply parser string file name pattern pattern matcher matcher pattern matcher file name matcher matches return null return matcher group
1711	mapreduce\src\tools\org\apache\hadoop\tools\rumen\TreePath.java	unrelated	package org apache hadoop tools rumen this describes path node root we use compare two trees rumen unit tests if trees identical chain converted describes path root fields compare tree path tree path parent string field name index tree path tree path parent string field name super parent parent field name field name index tree path tree path parent string field name index super parent parent field name field name index index string string string segment field name index index return parent null parent string segment
1712	mapreduce\src\tools\org\apache\hadoop\tools\rumen\Version20LogInterfaceUtils.java	unrelated	package org apache hadoop tools rumen this exists hold bunch utils it never instantiated version log interface utils task type get task type string task type try return task type value of task type catch illegal argument exception e cleanup equals task type return task type job cleanup setup equals task type return task type job setup return null
1713	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieCluster.java	unrelated	package org apache hadoop tools rumen link zombie cluster rebuilds cluster topology using information obtained job history logs zombie cluster extends abstract cluster story node root construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the network topology the default node setting zombie cluster logged network topology topology machine node default node build cluster topology default node construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the default node setting zombie cluster path path machine node default node configuration conf throws io exception new cluster topology reader path conf get default node construct homogeneous cluster we assume leaves topology link machine node parents link machine node link rack node we also expect leaf nodes level the default node setting zombie cluster input stream input machine node default node throws io exception new cluster topology reader input get default node node get cluster topology return root build cluster logged network topology topology machine node default node map logged network topology integer level mapping new identity hash map logged network topology integer deque logged network topology unvisited new array deque logged network topology unvisited add topology level mapping put topology building level mapping determine leaf level leaf level means leaf level unknown logged network topology n unvisited poll n null n unvisited poll level level mapping get n list logged network topology children n get children children null children empty leaf level leaf level level else leaf level level throw new illegal argument exception leaf nodes level else logged network topology child children level mapping put child level unvisited add first child a second pass dfs traverse topology tree path contains parent node level node path new node leaf level unvisited add topology logged network topology n unvisited poll n null n unvisited poll level level mapping get n node current level leaf level machine node machine node builder builder new machine node builder n get name level default node null builder clone from default node current builder build else current level leaf level new rack node n get name level new node n get name level path level current add children front queue logged network topology child n get children unvisited add first child level path level add child current root path
1714	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieJob.java	unrelated	package org apache hadoop tools rumen link zombie job layer link logged job raw json objects each link zombie job object represents job job history for everything exists job history contents returned unchanged faithfully to get input splits non exist task non exist task attempt ill formed task attempt proper objects made statistical sketches zombie job implements job story log log log factory get log zombie job logged job job map task id logged task logged task map map task attempt id logged task attempt logged task attempt map random random input split splits cluster story cluster job conf job conf seed num random seeds boolean random seed false map logged discrete cdf cdf random generator interpolator map new hash map logged discrete cdf cdf random generator todo fix zombie job initialize correctly observed data rack local over node local rack remote over node local this constructor creates link zombie job semantics link logged job passed parameter the dead job zombie job instance based the cluster topology dead job ran this argument null knowledge cluster topology seed random number generator filling information available zombie job zombie job logged job job cluster story cluster seed job null throw new illegal argument exception job null job job cluster cluster random new random seed seed seed random seed true this constructor creates link zombie job semantics link logged job passed parameter the dead job zombie job instance based the cluster topology dead job ran this argument null knowledge cluster topology zombie job logged job job cluster story cluster job cluster system nano time state convert state values status status values success return state succeeded else status values failed return state failed else status values killed return state killed else throw new illegal argument exception unknown status status synchronized job conf get job conf job conf null job conf new job conf add parameters configuration job trace the reason job configuration parameters seen jobconf file added first specialized values obtained rumen job conf values map entry object object entry job get job properties entry set job conf set entry get key string entry get value string todo eliminate parameters already copied job configuration file job conf set job name get name job conf set user get user job conf set num map tasks get number maps job conf set num reduce tasks get number reduces job conf set queue name get queue name return job conf input split get input splits splits null list input split splits list new array list input split path empty path new path total hosts use determine avg hosts per split logged task map task job get map tasks pre job history constants values task type map task get task type task type pre job history constants values map log warn task type map task map task map task get task id type task type null null task type string continue list logged location locations map task get preferred locations list string host list new array list string locations null logged location location locations list string layers location get layers layers
1715	mapreduce\src\tools\org\apache\hadoop\tools\rumen\ZombieJobProducer.java	unrelated	package org apache hadoop tools rumen producing link job story job trace zombie job producer implements job story producer job trace reader reader zombie cluster cluster boolean random seed false random seed zombie job producer job trace reader reader zombie cluster cluster boolean random seed random seed reader reader cluster cluster random seed random seed random seed random seed random seed system nano time constructor path json trace file possibly compressed the topology cluster corresponds jobs trace the argument null knowledge cluster topology zombie job producer path path zombie cluster cluster configuration conf throws io exception new job trace reader path conf cluster false constructor path json trace file possibly compressed the topology cluster corresponds jobs trace the argument null knowledge cluster topology use deterministic seed zombie job producer path path zombie cluster cluster configuration conf random seed throws io exception new job trace reader path conf cluster true random seed constructor the input stream json trace the topology cluster corresponds jobs trace the argument null knowledge cluster topology zombie job producer input stream input zombie cluster cluster throws io exception new job trace reader input cluster false constructor the input stream json trace the topology cluster corresponds jobs trace the argument null knowledge cluster topology use deterministic seed zombie job producer input stream input zombie cluster cluster random seed throws io exception new job trace reader input cluster true random seed zombie job get next job throws io exception logged job job reader get next job null return null else random seed sub random seed random seed generator get seed zombie job job get job id random seed return new zombie job job cluster sub random seed else return new zombie job job cluster close throws io exception reader close
